[2023-08-07 18:32:39,768 INFO train.py line 116 22900] => Loading config ...
[2023-08-07 18:32:39,768 INFO train.py line 118 22900] Save path: exp/scannet/semseg-spunet-v1m1-0-base
[2023-08-07 18:32:40,206 INFO train.py line 119 22900] Config:
weight = None
resume = False
evaluate = True
test_only = False
seed = 39335877
save_path = 'exp/scannet/semseg-spunet-v1m1-0-base'
num_worker = 8
batch_size = 2
batch_size_val = None
batch_size_test = None
epoch = 800
eval_epoch = 100
sync_bn = False
enable_amp = True
empty_cache = False
find_unused_parameters = False
mix_prob = 0.8
param_dicts = None
hooks = [
    dict(type='CheckpointLoader'),
    dict(type='IterationTimer', warmup_iter=2),
    dict(type='InformationWriter'),
    dict(type='SemSegEvaluator'),
    dict(type='CheckpointSaver', save_freq=None),
    dict(type='PreciseEvaluator', test_last=False)
]
test = dict(type='SemSegTester')
model = dict(
    type='DefaultSegmentor',
    backbone=dict(
        type='SpUNet-v1m1',
        in_channels=6,
        num_classes=20,
        channels=(32, 64, 128, 256, 256, 128, 96, 96),
        layers=(2, 3, 4, 6, 2, 2, 2, 2)),
    criteria=[dict(type='CrossEntropyLoss', loss_weight=1.0, ignore_index=-1)])
optimizer = dict(
    type='SGD', lr=0.05, momentum=0.9, weight_decay=0.0001, nesterov=True)
scheduler = dict(
    type='OneCycleLR',
    max_lr=0.05,
    pct_start=0.05,
    anneal_strategy='cos',
    div_factor=10.0,
    final_div_factor=10000.0)
dataset_type = 'ScanNetDataset'
data_root = 'data/scannet'
data = dict(
    num_classes=20,
    ignore_index=-1,
    names=[
        'wall', 'floor', 'cabinet', 'bed', 'chair', 'sofa', 'table', 'door',
        'window', 'bookshelf', 'picture', 'counter', 'desk', 'curtain',
        'refridgerator', 'shower curtain', 'toilet', 'sink', 'bathtub',
        'otherfurniture'
    ],
    train=dict(
        type='ScanNetDataset',
        split='train',
        data_root='data/scannet',
        transform=[
            dict(type='CenterShift', apply_z=True),
            dict(
                type='RandomDropout',
                dropout_ratio=0.2,
                dropout_application_ratio=0.2),
            dict(
                type='RandomRotate',
                angle=[-1, 1],
                axis='z',
                center=[0, 0, 0],
                p=0.5),
            dict(
                type='RandomRotate',
                angle=[-0.015625, 0.015625],
                axis='x',
                p=0.5),
            dict(
                type='RandomRotate',
                angle=[-0.015625, 0.015625],
                axis='y',
                p=0.5),
            dict(type='RandomScale', scale=[0.9, 1.1]),
            dict(type='RandomFlip', p=0.5),
            dict(type='RandomJitter', sigma=0.005, clip=0.02),
            dict(
                type='ElasticDistortion',
                distortion_params=[[0.2, 0.4], [0.8, 1.6]]),
            dict(type='ChromaticAutoContrast', p=0.2, blend_factor=None),
            dict(type='ChromaticTranslation', p=0.95, ratio=0.05),
            dict(type='ChromaticJitter', p=0.95, std=0.05),
            dict(
                type='GridSample',
                grid_size=0.02,
                hash_type='fnv',
                mode='train',
                return_discrete_coord=True),
            dict(type='SphereCrop', point_max=100000, mode='random'),
            dict(type='CenterShift', apply_z=False),
            dict(type='NormalizeColor'),
            dict(type='ShufflePoint'),
            dict(type='ToTensor'),
            dict(
                type='Collect',
                keys=('coord', 'discrete_coord', 'segment'),
                feat_keys=('color', 'normal'))
        ],
        test_mode=False,
        loop=8),
    val=dict(
        type='ScanNetDataset',
        split='val',
        data_root='data/scannet',
        transform=[
            dict(type='CenterShift', apply_z=True),
            dict(
                type='GridSample',
                grid_size=0.02,
                hash_type='fnv',
                mode='train',
                return_discrete_coord=True),
            dict(type='CenterShift', apply_z=False),
            dict(type='NormalizeColor'),
            dict(type='ToTensor'),
            dict(
                type='Collect',
                keys=('coord', 'discrete_coord', 'segment'),
                feat_keys=('color', 'normal'))
        ],
        test_mode=False),
    test=dict(
        type='ScanNetDataset',
        split='val',
        data_root='data/scannet',
        transform=[
            dict(type='CenterShift', apply_z=True),
            dict(type='NormalizeColor')
        ],
        test_mode=True,
        test_cfg=dict(
            voxelize=dict(
                type='GridSample',
                grid_size=0.02,
                hash_type='fnv',
                mode='test',
                return_discrete_coord=True,
                keys=('coord', 'color', 'normal')),
            crop=None,
            post_transform=[
                dict(type='CenterShift', apply_z=False),
                dict(type='ToTensor'),
                dict(
                    type='Collect',
                    keys=('coord', 'discrete_coord', 'index'),
                    feat_keys=('color', 'normal'))
            ],
            aug_transform=[[{
                'type': 'RandomRotateTargetAngle',
                'angle': [0],
                'axis': 'z',
                'center': [0, 0, 0],
                'p': 1
            }],
                           [{
                               'type': 'RandomRotateTargetAngle',
                               'angle': [0.5],
                               'axis': 'z',
                               'center': [0, 0, 0],
                               'p': 1
                           }],
                           [{
                               'type': 'RandomRotateTargetAngle',
                               'angle': [1],
                               'axis': 'z',
                               'center': [0, 0, 0],
                               'p': 1
                           }],
                           [{
                               'type': 'RandomRotateTargetAngle',
                               'angle': [1.5],
                               'axis': 'z',
                               'center': [0, 0, 0],
                               'p': 1
                           }],
                           [{
                               'type': 'RandomRotateTargetAngle',
                               'angle': [0],
                               'axis': 'z',
                               'center': [0, 0, 0],
                               'p': 1
                           }, {
                               'type': 'RandomScale',
                               'scale': [0.95, 0.95]
                           }],
                           [{
                               'type': 'RandomRotateTargetAngle',
                               'angle': [0.5],
                               'axis': 'z',
                               'center': [0, 0, 0],
                               'p': 1
                           }, {
                               'type': 'RandomScale',
                               'scale': [0.95, 0.95]
                           }],
                           [{
                               'type': 'RandomRotateTargetAngle',
                               'angle': [1],
                               'axis': 'z',
                               'center': [0, 0, 0],
                               'p': 1
                           }, {
                               'type': 'RandomScale',
                               'scale': [0.95, 0.95]
                           }],
                           [{
                               'type': 'RandomRotateTargetAngle',
                               'angle': [1.5],
                               'axis': 'z',
                               'center': [0, 0, 0],
                               'p': 1
                           }, {
                               'type': 'RandomScale',
                               'scale': [0.95, 0.95]
                           }],
                           [{
                               'type': 'RandomRotateTargetAngle',
                               'angle': [0],
                               'axis': 'z',
                               'center': [0, 0, 0],
                               'p': 1
                           }, {
                               'type': 'RandomScale',
                               'scale': [1.05, 1.05]
                           }],
                           [{
                               'type': 'RandomRotateTargetAngle',
                               'angle': [0.5],
                               'axis': 'z',
                               'center': [0, 0, 0],
                               'p': 1
                           }, {
                               'type': 'RandomScale',
                               'scale': [1.05, 1.05]
                           }],
                           [{
                               'type': 'RandomRotateTargetAngle',
                               'angle': [1],
                               'axis': 'z',
                               'center': [0, 0, 0],
                               'p': 1
                           }, {
                               'type': 'RandomScale',
                               'scale': [1.05, 1.05]
                           }],
                           [{
                               'type': 'RandomRotateTargetAngle',
                               'angle': [1.5],
                               'axis': 'z',
                               'center': [0, 0, 0],
                               'p': 1
                           }, {
                               'type': 'RandomScale',
                               'scale': [1.05, 1.05]
                           }], [{
                               'type': 'RandomFlip',
                               'p': 1
                           }]])))
num_worker_per_gpu = 8
batch_size_per_gpu = 2
batch_size_val_per_gpu = 1
batch_size_test_per_gpu = 1

[2023-08-07 18:32:40,206 INFO train.py line 120 22900] => Building model ...
[2023-08-07 18:32:40,513 INFO train.py line 194 22900] Num params: 39157844
[2023-08-07 18:32:42,805 INFO train.py line 122 22900] => Building writer ...
[2023-08-07 18:32:42,806 INFO train.py line 202 22900] Tensorboard writer logging dir: exp/scannet/semseg-spunet-v1m1-0-base
[2023-08-07 18:32:42,807 INFO train.py line 124 22900] => Building train dataset & dataloader ...
[2023-08-07 18:32:42,807 INFO scannet.py line 61 22900] Totally 39 x 8 samples in train set.
[2023-08-07 18:32:42,807 INFO train.py line 126 22900] => Building val dataset & dataloader ...
[2023-08-07 18:32:42,807 INFO scannet.py line 61 22900] Totally 24 x 1 samples in val set.
[2023-08-07 18:32:42,808 INFO train.py line 128 22900] => Building optimize, scheduler, scaler(amp) ...
[2023-08-07 18:32:42,808 INFO train.py line 132 22900] => Building hooks ...
[2023-08-07 18:32:42,808 INFO misc.py line 180 22900] => Loading checkpoint & weight ...
[2023-08-07 18:32:42,808 INFO misc.py line 199 22900] No weight found at: None
[2023-08-07 18:32:42,808 INFO train.py line 139 22900] >>>>>>>>>>>>>>>> Start Training >>>>>>>>>>>>>>>>
[2023-08-07 18:35:34,630 INFO misc.py line 115 22900] Train: [1/100][1/156] Data 1.111 (1.111) Batch 171.551 (171.551) Remain 743:20:19 loss: 2.9902 Lr: 0.00500
[2023-08-07 18:35:37,243 INFO misc.py line 115 22900] Train: [1/100][2/156] Data 0.001 (0.001) Batch 2.613 (2.613) Remain 11:19:16 loss: 2.9452 Lr: 0.00500
[2023-08-07 18:35:41,585 INFO misc.py line 115 22900] Train: [1/100][3/156] Data 0.001 (0.001) Batch 3.758 (3.758) Remain 16:16:49 loss: 2.9269 Lr: 0.00500
[2023-08-07 18:35:44,393 INFO misc.py line 115 22900] Train: [1/100][4/156] Data 0.001 (0.001) Batch 2.808 (2.808) Remain 12:09:54 loss: 2.9373 Lr: 0.00500
[2023-08-07 18:35:47,118 INFO misc.py line 115 22900] Train: [1/100][5/156] Data 0.001 (0.001) Batch 2.725 (2.767) Remain 11:59:04 loss: 2.7480 Lr: 0.00500
[2023-08-07 18:35:51,161 INFO misc.py line 115 22900] Train: [1/100][6/156] Data 0.001 (0.001) Batch 4.044 (3.192) Remain 13:49:40 loss: 2.8749 Lr: 0.00501
[2023-08-07 18:35:54,340 INFO misc.py line 115 22900] Train: [1/100][7/156] Data 0.001 (0.001) Batch 3.179 (3.189) Remain 13:48:44 loss: 2.8139 Lr: 0.00501
[2023-08-07 18:35:57,930 INFO misc.py line 115 22900] Train: [1/100][8/156] Data 0.001 (0.001) Batch 3.590 (3.269) Remain 14:09:31 loss: 2.3840 Lr: 0.00501
[2023-08-07 18:36:01,008 INFO misc.py line 115 22900] Train: [1/100][9/156] Data 0.001 (0.001) Batch 3.078 (3.237) Remain 14:01:11 loss: 2.4220 Lr: 0.00501
[2023-08-07 18:36:04,819 INFO misc.py line 115 22900] Train: [1/100][10/156] Data 0.001 (0.001) Batch 3.811 (3.319) Remain 14:22:25 loss: 2.2655 Lr: 0.00502
[2023-08-07 18:36:07,990 INFO misc.py line 115 22900] Train: [1/100][11/156] Data 0.001 (0.001) Batch 3.171 (3.301) Remain 14:17:34 loss: 2.4111 Lr: 0.00502
[2023-08-07 18:36:11,890 INFO misc.py line 115 22900] Train: [1/100][12/156] Data 0.001 (0.001) Batch 3.900 (3.367) Remain 14:34:49 loss: 2.4845 Lr: 0.00503
[2023-08-07 18:36:15,684 INFO misc.py line 115 22900] Train: [1/100][13/156] Data 0.001 (0.001) Batch 3.794 (3.410) Remain 14:45:50 loss: 2.3546 Lr: 0.00503
[2023-08-07 18:36:18,863 INFO misc.py line 115 22900] Train: [1/100][14/156] Data 0.001 (0.001) Batch 3.179 (3.389) Remain 14:40:20 loss: 2.2589 Lr: 0.00504
[2023-08-07 18:36:22,777 INFO misc.py line 115 22900] Train: [1/100][15/156] Data 0.001 (0.001) Batch 3.913 (3.433) Remain 14:51:37 loss: 2.2141 Lr: 0.00504
[2023-08-07 18:36:25,024 INFO misc.py line 115 22900] Train: [1/100][16/156] Data 0.001 (0.001) Batch 2.247 (3.341) Remain 14:27:53 loss: 1.6064 Lr: 0.00505
[2023-08-07 18:36:27,979 INFO misc.py line 115 22900] Train: [1/100][17/156] Data 0.001 (0.001) Batch 2.956 (3.314) Remain 14:20:40 loss: 2.3270 Lr: 0.00505
[2023-08-07 18:36:31,656 INFO misc.py line 115 22900] Train: [1/100][18/156] Data 0.001 (0.001) Batch 3.676 (3.338) Remain 14:26:53 loss: 2.5029 Lr: 0.00506
[2023-08-07 18:36:35,709 INFO misc.py line 115 22900] Train: [1/100][19/156] Data 0.001 (0.001) Batch 4.054 (3.383) Remain 14:38:27 loss: 2.6879 Lr: 0.00507
[2023-08-07 18:36:38,636 INFO misc.py line 115 22900] Train: [1/100][20/156] Data 0.001 (0.001) Batch 2.927 (3.356) Remain 14:31:25 loss: 1.9434 Lr: 0.00507
[2023-08-07 18:36:41,656 INFO misc.py line 115 22900] Train: [1/100][21/156] Data 0.001 (0.001) Batch 3.020 (3.337) Remain 14:26:31 loss: 2.0084 Lr: 0.00508
[2023-08-07 18:36:45,951 INFO misc.py line 115 22900] Train: [1/100][22/156] Data 0.001 (0.001) Batch 4.295 (3.388) Remain 14:39:33 loss: 2.1370 Lr: 0.00509
[2023-08-07 18:36:50,060 INFO misc.py line 115 22900] Train: [1/100][23/156] Data 0.001 (0.001) Batch 4.109 (3.424) Remain 14:48:52 loss: 2.4021 Lr: 0.00510
[2023-08-07 18:36:53,295 INFO misc.py line 115 22900] Train: [1/100][24/156] Data 0.001 (0.001) Batch 3.235 (3.415) Remain 14:46:28 loss: 2.5076 Lr: 0.00511
[2023-08-07 18:36:56,250 INFO misc.py line 115 22900] Train: [1/100][25/156] Data 0.001 (0.001) Batch 2.954 (3.394) Remain 14:40:59 loss: 2.0742 Lr: 0.00511
[2023-08-07 18:37:00,369 INFO misc.py line 115 22900] Train: [1/100][26/156] Data 0.001 (0.001) Batch 4.119 (3.425) Remain 14:49:07 loss: 2.4407 Lr: 0.00512
[2023-08-07 18:37:03,398 INFO misc.py line 115 22900] Train: [1/100][27/156] Data 0.001 (0.001) Batch 3.028 (3.409) Remain 14:44:46 loss: 1.7662 Lr: 0.00513
[2023-08-07 18:37:07,442 INFO misc.py line 115 22900] Train: [1/100][28/156] Data 0.001 (0.001) Batch 4.045 (3.434) Remain 14:51:18 loss: 1.7327 Lr: 0.00514
[2023-08-07 18:37:11,131 INFO misc.py line 115 22900] Train: [1/100][29/156] Data 0.001 (0.001) Batch 3.689 (3.444) Remain 14:53:47 loss: 1.7646 Lr: 0.00515
[2023-08-07 18:37:13,140 INFO misc.py line 115 22900] Train: [1/100][30/156] Data 0.001 (0.001) Batch 2.009 (3.391) Remain 14:39:57 loss: 1.8133 Lr: 0.00516
[2023-08-07 18:37:15,681 INFO misc.py line 115 22900] Train: [1/100][31/156] Data 0.001 (0.001) Batch 2.540 (3.361) Remain 14:32:00 loss: 1.6795 Lr: 0.00518
[2023-08-07 18:37:19,637 INFO misc.py line 115 22900] Train: [1/100][32/156] Data 0.001 (0.001) Batch 3.956 (3.381) Remain 14:37:16 loss: 1.6803 Lr: 0.00519
[2023-08-07 18:37:23,214 INFO misc.py line 115 22900] Train: [1/100][33/156] Data 0.001 (0.001) Batch 3.577 (3.388) Remain 14:38:55 loss: 1.6726 Lr: 0.00520
[2023-08-07 18:37:27,210 INFO misc.py line 115 22900] Train: [1/100][34/156] Data 0.001 (0.001) Batch 3.996 (3.407) Remain 14:43:57 loss: 1.6859 Lr: 0.00521
[2023-08-07 18:37:29,596 INFO misc.py line 115 22900] Train: [1/100][35/156] Data 0.001 (0.001) Batch 2.385 (3.375) Remain 14:35:37 loss: 1.5234 Lr: 0.00522
[2023-08-07 18:37:33,482 INFO misc.py line 115 22900] Train: [1/100][36/156] Data 0.001 (0.001) Batch 3.886 (3.391) Remain 14:39:34 loss: 1.7530 Lr: 0.00524
[2023-08-07 18:37:36,320 INFO misc.py line 115 22900] Train: [1/100][37/156] Data 0.001 (0.001) Batch 2.839 (3.375) Remain 14:35:18 loss: 1.6912 Lr: 0.00525
[2023-08-07 18:37:39,581 INFO misc.py line 115 22900] Train: [1/100][38/156] Data 0.001 (0.001) Batch 3.261 (3.371) Remain 14:34:24 loss: 1.7362 Lr: 0.00526
[2023-08-07 18:37:43,555 INFO misc.py line 115 22900] Train: [1/100][39/156] Data 0.001 (0.001) Batch 3.974 (3.388) Remain 14:38:41 loss: 1.3255 Lr: 0.00528
[2023-08-07 18:37:47,181 INFO misc.py line 115 22900] Train: [1/100][40/156] Data 0.001 (0.001) Batch 3.625 (3.394) Remain 14:40:18 loss: 1.3855 Lr: 0.00529
[2023-08-07 18:37:50,719 INFO misc.py line 115 22900] Train: [1/100][41/156] Data 0.001 (0.001) Batch 3.539 (3.398) Remain 14:41:13 loss: 1.7299 Lr: 0.00531
[2023-08-07 18:37:53,035 INFO misc.py line 115 22900] Train: [1/100][42/156] Data 0.001 (0.001) Batch 2.316 (3.371) Remain 14:33:58 loss: 1.7227 Lr: 0.00532
[2023-08-07 18:37:56,514 INFO misc.py line 115 22900] Train: [1/100][43/156] Data 0.001 (0.001) Batch 3.479 (3.373) Remain 14:34:37 loss: 1.4661 Lr: 0.00534
[2023-08-07 18:38:00,396 INFO misc.py line 115 22900] Train: [1/100][44/156] Data 0.001 (0.001) Batch 3.882 (3.386) Remain 14:37:46 loss: 2.0543 Lr: 0.00535
[2023-08-07 18:38:03,263 INFO misc.py line 115 22900] Train: [1/100][45/156] Data 0.001 (0.001) Batch 2.867 (3.373) Remain 14:34:31 loss: 1.6571 Lr: 0.00537
[2023-08-07 18:38:07,232 INFO misc.py line 115 22900] Train: [1/100][46/156] Data 0.001 (0.001) Batch 3.969 (3.387) Remain 14:38:03 loss: 1.9076 Lr: 0.00539
[2023-08-07 18:38:10,493 INFO misc.py line 115 22900] Train: [1/100][47/156] Data 0.001 (0.001) Batch 3.261 (3.384) Remain 14:37:15 loss: 1.7481 Lr: 0.00540
[2023-08-07 18:38:13,360 INFO misc.py line 115 22900] Train: [1/100][48/156] Data 0.001 (0.001) Batch 2.867 (3.373) Remain 14:34:13 loss: 2.3373 Lr: 0.00542
[2023-08-07 18:38:15,758 INFO misc.py line 115 22900] Train: [1/100][49/156] Data 0.001 (0.001) Batch 2.397 (3.352) Remain 14:28:40 loss: 1.5992 Lr: 0.00544
[2023-08-07 18:38:19,450 INFO misc.py line 115 22900] Train: [1/100][50/156] Data 0.001 (0.001) Batch 3.692 (3.359) Remain 14:30:29 loss: 1.9842 Lr: 0.00546
[2023-08-07 18:38:22,827 INFO misc.py line 115 22900] Train: [1/100][51/156] Data 0.001 (0.001) Batch 3.378 (3.359) Remain 14:30:32 loss: 1.2364 Lr: 0.00547
[2023-08-07 18:38:26,101 INFO misc.py line 115 22900] Train: [1/100][52/156] Data 0.001 (0.001) Batch 3.274 (3.357) Remain 14:30:02 loss: 1.7463 Lr: 0.00549
[2023-08-07 18:38:30,007 INFO misc.py line 115 22900] Train: [1/100][53/156] Data 0.001 (0.001) Batch 3.906 (3.368) Remain 14:32:49 loss: 1.6395 Lr: 0.00551
[2023-08-07 18:38:33,612 INFO misc.py line 115 22900] Train: [1/100][54/156] Data 0.001 (0.001) Batch 3.604 (3.373) Remain 14:33:57 loss: 1.5960 Lr: 0.00553
[2023-08-07 18:38:36,257 INFO misc.py line 115 22900] Train: [1/100][55/156] Data 0.001 (0.001) Batch 2.646 (3.359) Remain 14:30:17 loss: 1.3312 Lr: 0.00555
[2023-08-07 18:38:40,108 INFO misc.py line 115 22900] Train: [1/100][56/156] Data 0.001 (0.001) Batch 3.850 (3.368) Remain 14:32:37 loss: 1.5714 Lr: 0.00557
[2023-08-07 18:38:43,433 INFO misc.py line 115 22900] Train: [1/100][57/156] Data 0.001 (0.001) Batch 3.325 (3.368) Remain 14:32:21 loss: 1.2958 Lr: 0.00559
[2023-08-07 18:38:47,151 INFO misc.py line 115 22900] Train: [1/100][58/156] Data 0.001 (0.001) Batch 3.719 (3.374) Remain 14:33:57 loss: 1.7235 Lr: 0.00561
[2023-08-07 18:38:50,283 INFO misc.py line 115 22900] Train: [1/100][59/156] Data 0.001 (0.001) Batch 3.131 (3.370) Remain 14:32:46 loss: 1.8215 Lr: 0.00563
[2023-08-07 18:38:53,955 INFO misc.py line 115 22900] Train: [1/100][60/156] Data 0.001 (0.001) Batch 3.672 (3.375) Remain 14:34:06 loss: 1.6704 Lr: 0.00566
[2023-08-07 18:38:57,645 INFO misc.py line 115 22900] Train: [1/100][61/156] Data 0.001 (0.001) Batch 3.691 (3.380) Remain 14:35:27 loss: 1.5185 Lr: 0.00568
[2023-08-07 18:39:01,727 INFO misc.py line 115 22900] Train: [1/100][62/156] Data 0.001 (0.001) Batch 4.082 (3.392) Remain 14:38:28 loss: 1.9200 Lr: 0.00570
[2023-08-07 18:39:05,657 INFO misc.py line 115 22900] Train: [1/100][63/156] Data 0.001 (0.001) Batch 3.930 (3.401) Remain 14:40:44 loss: 2.0778 Lr: 0.00572
[2023-08-07 18:39:10,320 INFO misc.py line 115 22900] Train: [1/100][64/156] Data 0.001 (0.001) Batch 4.663 (3.422) Remain 14:46:02 loss: 1.9404 Lr: 0.00575
[2023-08-07 18:39:14,599 INFO misc.py line 115 22900] Train: [1/100][65/156] Data 0.001 (0.001) Batch 4.278 (3.436) Remain 14:49:33 loss: 2.4432 Lr: 0.00577
[2023-08-07 18:39:17,581 INFO misc.py line 115 22900] Train: [1/100][66/156] Data 0.001 (0.001) Batch 2.983 (3.429) Remain 14:47:38 loss: 1.1709 Lr: 0.00579
[2023-08-07 18:39:20,251 INFO misc.py line 115 22900] Train: [1/100][67/156] Data 0.001 (0.001) Batch 2.670 (3.417) Remain 14:44:31 loss: 2.0272 Lr: 0.00582
[2023-08-07 18:39:23,030 INFO misc.py line 115 22900] Train: [1/100][68/156] Data 0.001 (0.001) Batch 2.779 (3.407) Remain 14:41:55 loss: 1.3829 Lr: 0.00584
[2023-08-07 18:39:26,767 INFO misc.py line 115 22900] Train: [1/100][69/156] Data 0.001 (0.001) Batch 3.737 (3.412) Remain 14:43:09 loss: 1.2538 Lr: 0.00587
[2023-08-07 18:39:30,682 INFO misc.py line 115 22900] Train: [1/100][70/156] Data 0.001 (0.001) Batch 3.915 (3.419) Remain 14:45:02 loss: 1.6551 Lr: 0.00589
[2023-08-07 18:39:34,511 INFO misc.py line 115 22900] Train: [1/100][71/156] Data 0.001 (0.001) Batch 3.829 (3.425) Remain 14:46:32 loss: 1.5424 Lr: 0.00592
[2023-08-07 18:39:38,660 INFO misc.py line 115 22900] Train: [1/100][72/156] Data 0.001 (0.001) Batch 4.149 (3.436) Remain 14:49:12 loss: 1.9100 Lr: 0.00594
[2023-08-07 18:39:42,360 INFO misc.py line 115 22900] Train: [1/100][73/156] Data 0.001 (0.001) Batch 3.700 (3.440) Remain 14:50:07 loss: 1.3217 Lr: 0.00597
[2023-08-07 18:39:45,956 INFO misc.py line 115 22900] Train: [1/100][74/156] Data 0.001 (0.001) Batch 3.596 (3.442) Remain 14:50:38 loss: 1.8770 Lr: 0.00599
[2023-08-07 18:39:50,161 INFO misc.py line 115 22900] Train: [1/100][75/156] Data 0.001 (0.001) Batch 4.205 (3.452) Remain 14:53:19 loss: 2.1291 Lr: 0.00602
[2023-08-07 18:39:53,638 INFO misc.py line 115 22900] Train: [1/100][76/156] Data 0.001 (0.001) Batch 3.477 (3.453) Remain 14:53:20 loss: 1.8649 Lr: 0.00605
[2023-08-07 18:39:57,154 INFO misc.py line 115 22900] Train: [1/100][77/156] Data 0.001 (0.001) Batch 3.516 (3.454) Remain 14:53:30 loss: 1.6293 Lr: 0.00608
[2023-08-07 18:40:00,344 INFO misc.py line 115 22900] Train: [1/100][78/156] Data 0.001 (0.001) Batch 3.190 (3.450) Remain 14:52:32 loss: 1.3827 Lr: 0.00610
[2023-08-07 18:40:03,556 INFO misc.py line 115 22900] Train: [1/100][79/156] Data 0.001 (0.001) Batch 3.212 (3.447) Remain 14:51:40 loss: 1.5962 Lr: 0.00613
[2023-08-07 18:40:06,976 INFO misc.py line 115 22900] Train: [1/100][80/156] Data 0.001 (0.001) Batch 3.419 (3.447) Remain 14:51:31 loss: 1.6822 Lr: 0.00616
[2023-08-07 18:40:10,046 INFO misc.py line 115 22900] Train: [1/100][81/156] Data 0.001 (0.001) Batch 3.071 (3.442) Remain 14:50:13 loss: 1.5708 Lr: 0.00619
[2023-08-07 18:40:13,809 INFO misc.py line 115 22900] Train: [1/100][82/156] Data 0.001 (0.001) Batch 3.763 (3.446) Remain 14:51:13 loss: 1.1495 Lr: 0.00622
[2023-08-07 18:40:17,944 INFO misc.py line 115 22900] Train: [1/100][83/156] Data 0.001 (0.001) Batch 4.135 (3.454) Remain 14:53:23 loss: 2.0292 Lr: 0.00625
[2023-08-07 18:40:21,397 INFO misc.py line 115 22900] Train: [1/100][84/156] Data 0.001 (0.001) Batch 3.453 (3.454) Remain 14:53:19 loss: 1.3707 Lr: 0.00628
[2023-08-07 18:40:25,716 INFO misc.py line 115 22900] Train: [1/100][85/156] Data 0.001 (0.001) Batch 4.319 (3.465) Remain 14:55:59 loss: 2.0150 Lr: 0.00631
[2023-08-07 18:40:29,086 INFO misc.py line 115 22900] Train: [1/100][86/156] Data 0.001 (0.001) Batch 3.370 (3.464) Remain 14:55:38 loss: 1.9778 Lr: 0.00634
[2023-08-07 18:40:32,411 INFO misc.py line 115 22900] Train: [1/100][87/156] Data 0.001 (0.001) Batch 3.324 (3.462) Remain 14:55:09 loss: 1.8172 Lr: 0.00637
[2023-08-07 18:40:35,079 INFO misc.py line 115 22900] Train: [1/100][88/156] Data 0.001 (0.001) Batch 2.668 (3.453) Remain 14:52:40 loss: 1.5570 Lr: 0.00640
[2023-08-07 18:40:39,079 INFO misc.py line 115 22900] Train: [1/100][89/156] Data 0.001 (0.001) Batch 4.000 (3.459) Remain 14:54:16 loss: 1.6678 Lr: 0.00643
[2023-08-07 18:40:43,044 INFO misc.py line 115 22900] Train: [1/100][90/156] Data 0.001 (0.001) Batch 3.965 (3.465) Remain 14:55:42 loss: 1.8476 Lr: 0.00647
[2023-08-07 18:40:47,607 INFO misc.py line 115 22900] Train: [1/100][91/156] Data 0.001 (0.001) Batch 4.563 (3.478) Remain 14:58:52 loss: 1.7216 Lr: 0.00650
[2023-08-07 18:40:51,482 INFO misc.py line 115 22900] Train: [1/100][92/156] Data 0.001 (0.001) Batch 3.875 (3.482) Remain 14:59:58 loss: 1.8577 Lr: 0.00653
[2023-08-07 18:40:54,311 INFO misc.py line 115 22900] Train: [1/100][93/156] Data 0.001 (0.001) Batch 2.829 (3.475) Remain 14:58:02 loss: 1.3419 Lr: 0.00656
[2023-08-07 18:40:57,935 INFO misc.py line 115 22900] Train: [1/100][94/156] Data 0.001 (0.001) Batch 3.624 (3.476) Remain 14:58:24 loss: 1.8093 Lr: 0.00660
[2023-08-07 18:41:01,957 INFO misc.py line 115 22900] Train: [1/100][95/156] Data 0.001 (0.001) Batch 4.023 (3.482) Remain 14:59:53 loss: 1.4619 Lr: 0.00663
[2023-08-07 18:41:05,172 INFO misc.py line 115 22900] Train: [1/100][96/156] Data 0.001 (0.001) Batch 3.215 (3.479) Remain 14:59:05 loss: 1.2322 Lr: 0.00667
[2023-08-07 18:41:08,953 INFO misc.py line 115 22900] Train: [1/100][97/156] Data 0.001 (0.001) Batch 3.781 (3.483) Remain 14:59:51 loss: 1.6689 Lr: 0.00670
[2023-08-07 18:41:11,878 INFO misc.py line 115 22900] Train: [1/100][98/156] Data 0.001 (0.001) Batch 2.925 (3.477) Remain 14:58:16 loss: 1.5380 Lr: 0.00673
[2023-08-07 18:41:15,310 INFO misc.py line 115 22900] Train: [1/100][99/156] Data 0.001 (0.001) Batch 3.432 (3.476) Remain 14:58:06 loss: 1.3101 Lr: 0.00677
[2023-08-07 18:41:19,730 INFO misc.py line 115 22900] Train: [1/100][100/156] Data 0.001 (0.001) Batch 4.420 (3.486) Remain 15:00:33 loss: 2.0483 Lr: 0.00681
[2023-08-07 18:41:24,294 INFO misc.py line 115 22900] Train: [1/100][101/156] Data 0.001 (0.001) Batch 4.564 (3.497) Remain 15:03:20 loss: 1.9047 Lr: 0.00684
[2023-08-07 18:41:28,623 INFO misc.py line 115 22900] Train: [1/100][102/156] Data 0.001 (0.001) Batch 4.329 (3.505) Remain 15:05:27 loss: 1.7245 Lr: 0.00688
[2023-08-07 18:41:32,338 INFO misc.py line 115 22900] Train: [1/100][103/156] Data 0.001 (0.001) Batch 3.715 (3.508) Remain 15:05:56 loss: 1.1537 Lr: 0.00691
[2023-08-07 18:41:35,302 INFO misc.py line 115 22900] Train: [1/100][104/156] Data 0.001 (0.001) Batch 2.964 (3.502) Remain 15:04:29 loss: 1.3408 Lr: 0.00695
[2023-08-07 18:41:38,152 INFO misc.py line 115 22900] Train: [1/100][105/156] Data 0.001 (0.001) Batch 2.850 (3.496) Remain 15:02:46 loss: 1.4045 Lr: 0.00699
[2023-08-07 18:41:41,663 INFO misc.py line 115 22900] Train: [1/100][106/156] Data 0.001 (0.001) Batch 3.511 (3.496) Remain 15:02:45 loss: 1.6320 Lr: 0.00702
[2023-08-07 18:41:44,328 INFO misc.py line 115 22900] Train: [1/100][107/156] Data 0.001 (0.001) Batch 2.666 (3.488) Remain 15:00:38 loss: 2.0079 Lr: 0.00706
[2023-08-07 18:41:47,651 INFO misc.py line 115 22900] Train: [1/100][108/156] Data 0.001 (0.001) Batch 3.322 (3.486) Remain 15:00:10 loss: 1.2872 Lr: 0.00710
[2023-08-07 18:41:50,392 INFO misc.py line 115 22900] Train: [1/100][109/156] Data 0.001 (0.001) Batch 2.742 (3.479) Remain 14:58:18 loss: 1.3514 Lr: 0.00714
[2023-08-07 18:41:52,929 INFO misc.py line 115 22900] Train: [1/100][110/156] Data 0.001 (0.001) Batch 2.537 (3.471) Remain 14:55:58 loss: 1.6611 Lr: 0.00718
[2023-08-07 18:41:56,238 INFO misc.py line 115 22900] Train: [1/100][111/156] Data 0.001 (0.001) Batch 3.309 (3.469) Remain 14:55:31 loss: 1.3279 Lr: 0.00722
[2023-08-07 18:41:58,852 INFO misc.py line 115 22900] Train: [1/100][112/156] Data 0.001 (0.001) Batch 2.614 (3.461) Remain 14:53:26 loss: 1.4268 Lr: 0.00726
[2023-08-07 18:42:01,326 INFO misc.py line 115 22900] Train: [1/100][113/156] Data 0.001 (0.001) Batch 2.474 (3.452) Remain 14:51:04 loss: 1.5887 Lr: 0.00730
[2023-08-07 18:42:04,185 INFO misc.py line 115 22900] Train: [1/100][114/156] Data 0.001 (0.001) Batch 2.859 (3.447) Remain 14:49:37 loss: 1.3289 Lr: 0.00734
[2023-08-07 18:42:06,402 INFO misc.py line 115 22900] Train: [1/100][115/156] Data 0.001 (0.001) Batch 2.216 (3.436) Remain 14:46:44 loss: 1.3240 Lr: 0.00738
[2023-08-07 18:42:10,475 INFO misc.py line 115 22900] Train: [1/100][116/156] Data 0.001 (0.001) Batch 4.074 (3.442) Remain 14:48:08 loss: 1.8111 Lr: 0.00742
[2023-08-07 18:42:14,622 INFO misc.py line 115 22900] Train: [1/100][117/156] Data 0.001 (0.001) Batch 4.147 (3.448) Remain 14:49:40 loss: 1.6230 Lr: 0.00746
[2023-08-07 18:42:18,129 INFO misc.py line 115 22900] Train: [1/100][118/156] Data 0.001 (0.001) Batch 3.507 (3.448) Remain 14:49:45 loss: 1.4087 Lr: 0.00750
[2023-08-07 18:42:21,329 INFO misc.py line 115 22900] Train: [1/100][119/156] Data 0.001 (0.001) Batch 3.199 (3.446) Remain 14:49:08 loss: 1.6050 Lr: 0.00754
[2023-08-07 18:42:24,665 INFO misc.py line 115 22900] Train: [1/100][120/156] Data 0.001 (0.001) Batch 3.337 (3.445) Remain 14:48:50 loss: 1.7631 Lr: 0.00758
[2023-08-07 18:42:27,919 INFO misc.py line 115 22900] Train: [1/100][121/156] Data 0.001 (0.001) Batch 3.253 (3.444) Remain 14:48:22 loss: 1.5366 Lr: 0.00763
[2023-08-07 18:42:31,893 INFO misc.py line 115 22900] Train: [1/100][122/156] Data 0.001 (0.001) Batch 3.975 (3.448) Remain 14:49:27 loss: 2.1296 Lr: 0.00767
[2023-08-07 18:42:34,110 INFO misc.py line 115 22900] Train: [1/100][123/156] Data 0.001 (0.001) Batch 2.217 (3.438) Remain 14:46:45 loss: 1.1899 Lr: 0.00771
[2023-08-07 18:42:38,686 INFO misc.py line 115 22900] Train: [1/100][124/156] Data 0.001 (0.001) Batch 4.576 (3.447) Remain 14:49:07 loss: 2.0744 Lr: 0.00776
[2023-08-07 18:42:41,613 INFO misc.py line 115 22900] Train: [1/100][125/156] Data 0.001 (0.001) Batch 2.928 (3.443) Remain 14:47:58 loss: 1.4591 Lr: 0.00780
[2023-08-07 18:42:44,980 INFO misc.py line 115 22900] Train: [1/100][126/156] Data 0.001 (0.001) Batch 3.367 (3.442) Remain 14:47:45 loss: 1.3588 Lr: 0.00784
[2023-08-07 18:42:48,276 INFO misc.py line 115 22900] Train: [1/100][127/156] Data 0.001 (0.001) Batch 3.296 (3.441) Remain 14:47:23 loss: 1.4537 Lr: 0.00789
[2023-08-07 18:42:51,722 INFO misc.py line 115 22900] Train: [1/100][128/156] Data 0.001 (0.001) Batch 3.446 (3.441) Remain 14:47:20 loss: 1.4137 Lr: 0.00793
[2023-08-07 18:42:55,396 INFO misc.py line 115 22900] Train: [1/100][129/156] Data 0.001 (0.001) Batch 3.674 (3.443) Remain 14:47:45 loss: 2.1168 Lr: 0.00798
[2023-08-07 18:42:58,677 INFO misc.py line 115 22900] Train: [1/100][130/156] Data 0.001 (0.001) Batch 3.281 (3.442) Remain 14:47:22 loss: 1.5923 Lr: 0.00802
[2023-08-07 18:43:02,013 INFO misc.py line 115 22900] Train: [1/100][131/156] Data 0.001 (0.001) Batch 3.336 (3.441) Remain 14:47:06 loss: 1.3359 Lr: 0.00807
[2023-08-07 18:43:05,548 INFO misc.py line 115 22900] Train: [1/100][132/156] Data 0.001 (0.001) Batch 3.536 (3.442) Remain 14:47:14 loss: 2.1273 Lr: 0.00811
[2023-08-07 18:43:09,167 INFO misc.py line 115 22900] Train: [1/100][133/156] Data 0.001 (0.001) Batch 3.619 (3.443) Remain 14:47:31 loss: 1.5795 Lr: 0.00816
[2023-08-07 18:43:12,871 INFO misc.py line 115 22900] Train: [1/100][134/156] Data 0.001 (0.001) Batch 3.704 (3.445) Remain 14:47:59 loss: 1.8110 Lr: 0.00821
[2023-08-07 18:43:17,359 INFO misc.py line 115 22900] Train: [1/100][135/156] Data 0.001 (0.001) Batch 4.488 (3.453) Remain 14:49:58 loss: 1.7900 Lr: 0.00825
[2023-08-07 18:43:21,539 INFO misc.py line 115 22900] Train: [1/100][136/156] Data 0.001 (0.001) Batch 4.180 (3.458) Remain 14:51:19 loss: 1.6930 Lr: 0.00830
[2023-08-07 18:43:24,149 INFO misc.py line 115 22900] Train: [1/100][137/156] Data 0.001 (0.001) Batch 2.610 (3.452) Remain 14:49:37 loss: 1.2341 Lr: 0.00835
[2023-08-07 18:43:28,018 INFO misc.py line 115 22900] Train: [1/100][138/156] Data 0.001 (0.001) Batch 3.869 (3.455) Remain 14:50:22 loss: 1.1710 Lr: 0.00840
[2023-08-07 18:43:30,897 INFO misc.py line 115 22900] Train: [1/100][139/156] Data 0.001 (0.001) Batch 2.879 (3.451) Remain 14:49:13 loss: 1.4704 Lr: 0.00844
[2023-08-07 18:43:34,815 INFO misc.py line 115 22900] Train: [1/100][140/156] Data 0.001 (0.001) Batch 3.918 (3.454) Remain 14:50:02 loss: 1.7882 Lr: 0.00849
[2023-08-07 18:43:38,910 INFO misc.py line 115 22900] Train: [1/100][141/156] Data 0.001 (0.001) Batch 4.096 (3.459) Remain 14:51:10 loss: 1.3027 Lr: 0.00854
[2023-08-07 18:43:41,742 INFO misc.py line 115 22900] Train: [1/100][142/156] Data 0.001 (0.001) Batch 2.831 (3.454) Remain 14:49:57 loss: 1.2220 Lr: 0.00859
[2023-08-07 18:43:45,664 INFO misc.py line 115 22900] Train: [1/100][143/156] Data 0.001 (0.001) Batch 3.922 (3.458) Remain 14:50:45 loss: 1.6530 Lr: 0.00864
[2023-08-07 18:43:49,324 INFO misc.py line 115 22900] Train: [1/100][144/156] Data 0.001 (0.001) Batch 3.660 (3.459) Remain 14:51:04 loss: 1.5407 Lr: 0.00869
[2023-08-07 18:43:53,108 INFO misc.py line 115 22900] Train: [1/100][145/156] Data 0.001 (0.001) Batch 3.784 (3.461) Remain 14:51:36 loss: 1.5798 Lr: 0.00874
[2023-08-07 18:43:56,202 INFO misc.py line 115 22900] Train: [1/100][146/156] Data 0.001 (0.001) Batch 3.094 (3.459) Remain 14:50:53 loss: 1.4691 Lr: 0.00879
[2023-08-07 18:44:00,871 INFO misc.py line 115 22900] Train: [1/100][147/156] Data 0.001 (0.001) Batch 4.670 (3.467) Remain 14:52:59 loss: 2.0359 Lr: 0.00884
[2023-08-07 18:44:04,935 INFO misc.py line 115 22900] Train: [1/100][148/156] Data 0.001 (0.001) Batch 4.063 (3.471) Remain 14:53:59 loss: 1.4050 Lr: 0.00889
[2023-08-07 18:44:08,115 INFO misc.py line 115 22900] Train: [1/100][149/156] Data 0.001 (0.001) Batch 3.180 (3.469) Remain 14:53:25 loss: 1.6562 Lr: 0.00894
[2023-08-07 18:44:11,467 INFO misc.py line 115 22900] Train: [1/100][150/156] Data 0.001 (0.001) Batch 3.352 (3.469) Remain 14:53:09 loss: 1.1875 Lr: 0.00899
[2023-08-07 18:44:13,403 INFO misc.py line 115 22900] Train: [1/100][151/156] Data 0.001 (0.001) Batch 1.936 (3.458) Remain 14:50:26 loss: 1.7926 Lr: 0.00904
[2023-08-07 18:44:16,299 INFO misc.py line 115 22900] Train: [1/100][152/156] Data 0.001 (0.001) Batch 2.896 (3.454) Remain 14:49:24 loss: 1.1493 Lr: 0.00910
[2023-08-07 18:44:19,903 INFO misc.py line 115 22900] Train: [1/100][153/156] Data 0.001 (0.001) Batch 3.604 (3.455) Remain 14:49:36 loss: 1.4016 Lr: 0.00915
[2023-08-07 18:44:23,592 INFO misc.py line 115 22900] Train: [1/100][154/156] Data 0.001 (0.001) Batch 3.688 (3.457) Remain 14:49:56 loss: 1.7500 Lr: 0.00920
[2023-08-07 18:44:26,499 INFO misc.py line 115 22900] Train: [1/100][155/156] Data 0.001 (0.001) Batch 2.908 (3.453) Remain 14:48:57 loss: 1.5861 Lr: 0.00925
[2023-08-07 18:44:29,949 INFO misc.py line 115 22900] Train: [1/100][156/156] Data 0.001 (0.001) Batch 3.450 (3.453) Remain 14:48:53 loss: 2.0052 Lr: 0.00931
[2023-08-07 18:44:29,949 INFO misc.py line 129 22900] Train result: loss: 1.7678 
[2023-08-07 18:44:29,949 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 18:44:38,853 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.1291 
[2023-08-07 18:44:39,723 INFO evaluator.py line 122 22900] Test: [2/24] Loss 1.6937 
[2023-08-07 18:44:41,390 INFO evaluator.py line 122 22900] Test: [3/24] Loss 1.6526 
[2023-08-07 18:44:42,911 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.5141 
[2023-08-07 18:44:44,757 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.8165 
[2023-08-07 18:44:46,425 INFO evaluator.py line 122 22900] Test: [6/24] Loss 1.5655 
[2023-08-07 18:44:48,564 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.4206 
[2023-08-07 18:44:50,372 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.1143 
[2023-08-07 18:44:51,657 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.5595 
[2023-08-07 18:44:53,787 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.8508 
[2023-08-07 18:44:54,313 INFO evaluator.py line 122 22900] Test: [11/24] Loss 2.1140 
[2023-08-07 18:44:55,848 INFO evaluator.py line 122 22900] Test: [12/24] Loss 1.6011 
[2023-08-07 18:44:58,562 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.8798 
[2023-08-07 18:45:00,242 INFO evaluator.py line 122 22900] Test: [14/24] Loss 1.6150 
[2023-08-07 18:45:02,270 INFO evaluator.py line 122 22900] Test: [15/24] Loss 1.4200 
[2023-08-07 18:45:04,984 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.9174 
[2023-08-07 18:45:07,692 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.9318 
[2023-08-07 18:45:09,541 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.7086 
[2023-08-07 18:45:10,291 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.6794 
[2023-08-07 18:45:11,178 INFO evaluator.py line 122 22900] Test: [20/24] Loss 1.9518 
[2023-08-07 18:45:13,442 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.8658 
[2023-08-07 18:45:15,411 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.7771 
[2023-08-07 18:45:17,259 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.3318 
[2023-08-07 18:45:19,198 INFO evaluator.py line 122 22900] Test: [24/24] Loss 2.0720 
[2023-08-07 18:45:19,248 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.0532/0.0994/0.5156.
[2023-08-07 18:45:19,248 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.4509/0.9796
[2023-08-07 18:45:19,248 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.5944/0.9890
[2023-08-07 18:45:19,248 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.0000/0.0000
[2023-08-07 18:45:19,249 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.0001/0.0001
[2023-08-07 18:45:19,249 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.0195/0.0198
[2023-08-07 18:45:19,249 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.0000/0.0000
[2023-08-07 18:45:19,249 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.0000/0.0000
[2023-08-07 18:45:19,249 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0000/0.0000
[2023-08-07 18:45:19,249 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.0001/0.0001
[2023-08-07 18:45:19,249 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0000/0.0000
[2023-08-07 18:45:19,249 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 18:45:19,249 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0000/0.0000
[2023-08-07 18:45:19,249 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.0000/0.0000
[2023-08-07 18:45:19,249 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-07 18:45:19,249 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0000/0.0000
[2023-08-07 18:45:19,249 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-07 18:45:19,249 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0000/0.0000
[2023-08-07 18:45:19,249 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.0000/0.0000
[2023-08-07 18:45:19,249 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 18:45:19,249 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 18:45:19,249 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 18:45:19,249 INFO misc.py line 150 22900] Best validation mIoU updated to: 0.0532
[2023-08-07 18:45:19,249 INFO misc.py line 152 22900] Currently Best mIoU: 0.0532
[2023-08-07 18:45:19,250 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 18:45:24,014 INFO misc.py line 115 22900] Train: [2/100][1/156] Data 1.124 (1.124) Batch 4.009 (4.009) Remain 17:11:48 loss: 1.1687 Lr: 0.00936
[2023-08-07 18:45:27,437 INFO misc.py line 115 22900] Train: [2/100][2/156] Data 0.001 (0.001) Batch 3.423 (3.423) Remain 14:40:52 loss: 1.3855 Lr: 0.00942
[2023-08-07 18:45:29,879 INFO misc.py line 115 22900] Train: [2/100][3/156] Data 0.001 (0.001) Batch 2.443 (2.443) Remain 10:28:37 loss: 1.5989 Lr: 0.00947
[2023-08-07 18:45:33,875 INFO misc.py line 115 22900] Train: [2/100][4/156] Data 0.001 (0.001) Batch 3.996 (3.996) Remain 17:08:13 loss: 1.9177 Lr: 0.00952
[2023-08-07 18:45:37,692 INFO misc.py line 115 22900] Train: [2/100][5/156] Data 0.001 (0.001) Batch 3.817 (3.906) Remain 16:45:07 loss: 1.8261 Lr: 0.00958
[2023-08-07 18:45:41,309 INFO misc.py line 115 22900] Train: [2/100][6/156] Data 0.001 (0.001) Batch 3.617 (3.810) Remain 16:20:14 loss: 1.4317 Lr: 0.00963
[2023-08-07 18:45:44,997 INFO misc.py line 115 22900] Train: [2/100][7/156] Data 0.001 (0.001) Batch 3.688 (3.779) Remain 16:12:21 loss: 1.2028 Lr: 0.00969
[2023-08-07 18:45:48,569 INFO misc.py line 115 22900] Train: [2/100][8/156] Data 0.001 (0.001) Batch 3.572 (3.738) Remain 16:01:38 loss: 1.4365 Lr: 0.00974
[2023-08-07 18:45:51,878 INFO misc.py line 115 22900] Train: [2/100][9/156] Data 0.001 (0.001) Batch 3.309 (3.666) Remain 15:43:11 loss: 1.5346 Lr: 0.00980
[2023-08-07 18:45:55,831 INFO misc.py line 115 22900] Train: [2/100][10/156] Data 0.001 (0.001) Batch 3.953 (3.707) Remain 15:53:39 loss: 1.7796 Lr: 0.00986
[2023-08-07 18:45:59,138 INFO misc.py line 115 22900] Train: [2/100][11/156] Data 0.002 (0.001) Batch 3.307 (3.657) Remain 15:40:42 loss: 1.3269 Lr: 0.00991
[2023-08-07 18:46:01,544 INFO misc.py line 115 22900] Train: [2/100][12/156] Data 0.001 (0.001) Batch 2.407 (3.518) Remain 15:04:54 loss: 1.3869 Lr: 0.00997
[2023-08-07 18:46:04,984 INFO misc.py line 115 22900] Train: [2/100][13/156] Data 0.001 (0.001) Batch 3.439 (3.510) Remain 15:02:49 loss: 1.7214 Lr: 0.01003
[2023-08-07 18:46:09,013 INFO misc.py line 115 22900] Train: [2/100][14/156] Data 0.001 (0.001) Batch 4.029 (3.558) Remain 15:14:53 loss: 1.9570 Lr: 0.01008
[2023-08-07 18:46:12,030 INFO misc.py line 115 22900] Train: [2/100][15/156] Data 0.001 (0.001) Batch 3.017 (3.513) Remain 15:03:14 loss: 1.2261 Lr: 0.01014
[2023-08-07 18:46:15,437 INFO misc.py line 115 22900] Train: [2/100][16/156] Data 0.001 (0.001) Batch 3.408 (3.504) Remain 15:01:06 loss: 1.8750 Lr: 0.01020
[2023-08-07 18:46:19,031 INFO misc.py line 115 22900] Train: [2/100][17/156] Data 0.001 (0.001) Batch 3.594 (3.511) Remain 15:02:41 loss: 1.2095 Lr: 0.01026
[2023-08-07 18:46:23,327 INFO misc.py line 115 22900] Train: [2/100][18/156] Data 0.001 (0.001) Batch 4.295 (3.563) Remain 15:16:05 loss: 1.7094 Lr: 0.01032
[2023-08-07 18:46:27,099 INFO misc.py line 115 22900] Train: [2/100][19/156] Data 0.001 (0.001) Batch 3.772 (3.576) Remain 15:19:22 loss: 1.3590 Lr: 0.01037
[2023-08-07 18:46:29,357 INFO misc.py line 115 22900] Train: [2/100][20/156] Data 0.001 (0.001) Batch 2.258 (3.499) Remain 14:59:23 loss: 1.0526 Lr: 0.01043
[2023-08-07 18:46:32,566 INFO misc.py line 115 22900] Train: [2/100][21/156] Data 0.001 (0.001) Batch 3.209 (3.483) Remain 14:55:11 loss: 1.4469 Lr: 0.01049
[2023-08-07 18:46:36,226 INFO misc.py line 115 22900] Train: [2/100][22/156] Data 0.001 (0.001) Batch 3.660 (3.492) Remain 14:57:32 loss: 1.4610 Lr: 0.01055
[2023-08-07 18:46:39,821 INFO misc.py line 115 22900] Train: [2/100][23/156] Data 0.001 (0.001) Batch 3.595 (3.497) Remain 14:58:48 loss: 1.8217 Lr: 0.01061
[2023-08-07 18:46:43,329 INFO misc.py line 115 22900] Train: [2/100][24/156] Data 0.003 (0.001) Batch 3.508 (3.498) Remain 14:58:53 loss: 1.6553 Lr: 0.01067
[2023-08-07 18:46:46,714 INFO misc.py line 115 22900] Train: [2/100][25/156] Data 0.001 (0.001) Batch 3.384 (3.492) Remain 14:57:30 loss: 1.1449 Lr: 0.01073
[2023-08-07 18:46:49,671 INFO misc.py line 115 22900] Train: [2/100][26/156] Data 0.001 (0.001) Batch 2.957 (3.469) Remain 14:51:27 loss: 0.8575 Lr: 0.01079
[2023-08-07 18:46:53,771 INFO misc.py line 115 22900] Train: [2/100][27/156] Data 0.001 (0.001) Batch 4.100 (3.495) Remain 14:58:09 loss: 1.6919 Lr: 0.01085
[2023-08-07 18:46:57,043 INFO misc.py line 115 22900] Train: [2/100][28/156] Data 0.001 (0.001) Batch 3.273 (3.487) Remain 14:55:48 loss: 1.6727 Lr: 0.01092
[2023-08-07 18:46:59,166 INFO misc.py line 115 22900] Train: [2/100][29/156] Data 0.001 (0.001) Batch 2.123 (3.434) Remain 14:42:16 loss: 1.4431 Lr: 0.01098
[2023-08-07 18:47:02,600 INFO misc.py line 115 22900] Train: [2/100][30/156] Data 0.001 (0.001) Batch 3.434 (3.434) Remain 14:42:13 loss: 1.2776 Lr: 0.01104
[2023-08-07 18:47:06,055 INFO misc.py line 115 22900] Train: [2/100][31/156] Data 0.001 (0.001) Batch 3.455 (3.435) Remain 14:42:21 loss: 1.7891 Lr: 0.01110
[2023-08-07 18:47:09,534 INFO misc.py line 115 22900] Train: [2/100][32/156] Data 0.001 (0.001) Batch 3.479 (3.436) Remain 14:42:41 loss: 1.7331 Lr: 0.01116
[2023-08-07 18:47:13,067 INFO misc.py line 115 22900] Train: [2/100][33/156] Data 0.001 (0.001) Batch 3.533 (3.440) Remain 14:43:27 loss: 1.6532 Lr: 0.01123
[2023-08-07 18:47:17,079 INFO misc.py line 115 22900] Train: [2/100][34/156] Data 0.001 (0.001) Batch 4.012 (3.458) Remain 14:48:08 loss: 1.4220 Lr: 0.01129
[2023-08-07 18:47:20,691 INFO misc.py line 115 22900] Train: [2/100][35/156] Data 0.001 (0.001) Batch 3.613 (3.463) Remain 14:49:19 loss: 1.4804 Lr: 0.01135
[2023-08-07 18:47:24,042 INFO misc.py line 115 22900] Train: [2/100][36/156] Data 0.001 (0.001) Batch 3.350 (3.459) Remain 14:48:23 loss: 1.7199 Lr: 0.01141
[2023-08-07 18:47:26,511 INFO misc.py line 115 22900] Train: [2/100][37/156] Data 0.001 (0.001) Batch 2.469 (3.430) Remain 14:40:51 loss: 1.5081 Lr: 0.01148
[2023-08-07 18:47:28,877 INFO misc.py line 115 22900] Train: [2/100][38/156] Data 0.001 (0.001) Batch 2.367 (3.400) Remain 14:32:59 loss: 1.6013 Lr: 0.01154
[2023-08-07 18:47:32,110 INFO misc.py line 115 22900] Train: [2/100][39/156] Data 0.001 (0.001) Batch 3.232 (3.395) Remain 14:31:44 loss: 1.5197 Lr: 0.01161
[2023-08-07 18:47:36,511 INFO misc.py line 115 22900] Train: [2/100][40/156] Data 0.001 (0.001) Batch 4.401 (3.422) Remain 14:38:39 loss: 1.9589 Lr: 0.01167
[2023-08-07 18:47:40,004 INFO misc.py line 115 22900] Train: [2/100][41/156] Data 0.001 (0.001) Batch 3.493 (3.424) Remain 14:39:04 loss: 1.1098 Lr: 0.01174
[2023-08-07 18:47:44,353 INFO misc.py line 115 22900] Train: [2/100][42/156] Data 0.001 (0.001) Batch 4.349 (3.448) Remain 14:45:06 loss: 1.9927 Lr: 0.01180
[2023-08-07 18:47:47,275 INFO misc.py line 115 22900] Train: [2/100][43/156] Data 0.001 (0.001) Batch 2.923 (3.435) Remain 14:41:40 loss: 1.1868 Lr: 0.01187
[2023-08-07 18:47:50,840 INFO misc.py line 115 22900] Train: [2/100][44/156] Data 0.001 (0.001) Batch 3.565 (3.438) Remain 14:42:26 loss: 1.0432 Lr: 0.01193
[2023-08-07 18:47:54,682 INFO misc.py line 115 22900] Train: [2/100][45/156] Data 0.001 (0.001) Batch 3.842 (3.448) Remain 14:44:50 loss: 1.9045 Lr: 0.01200
[2023-08-07 18:47:58,561 INFO misc.py line 115 22900] Train: [2/100][46/156] Data 0.001 (0.001) Batch 3.879 (3.458) Remain 14:47:21 loss: 1.6876 Lr: 0.01206
[2023-08-07 18:48:01,886 INFO misc.py line 115 22900] Train: [2/100][47/156] Data 0.001 (0.001) Batch 3.325 (3.455) Remain 14:46:32 loss: 1.6355 Lr: 0.01213
[2023-08-07 18:48:05,126 INFO misc.py line 115 22900] Train: [2/100][48/156] Data 0.001 (0.001) Batch 3.240 (3.450) Remain 14:45:15 loss: 1.2912 Lr: 0.01219
[2023-08-07 18:48:08,190 INFO misc.py line 115 22900] Train: [2/100][49/156] Data 0.001 (0.001) Batch 3.064 (3.442) Remain 14:43:02 loss: 1.0072 Lr: 0.01226
[2023-08-07 18:48:11,574 INFO misc.py line 115 22900] Train: [2/100][50/156] Data 0.001 (0.001) Batch 3.383 (3.440) Remain 14:42:40 loss: 1.1233 Lr: 0.01233
[2023-08-07 18:48:15,311 INFO misc.py line 115 22900] Train: [2/100][51/156] Data 0.001 (0.001) Batch 3.737 (3.446) Remain 14:44:11 loss: 0.9238 Lr: 0.01240
[2023-08-07 18:48:19,946 INFO misc.py line 115 22900] Train: [2/100][52/156] Data 0.001 (0.001) Batch 4.635 (3.471) Remain 14:50:21 loss: 1.6326 Lr: 0.01246
[2023-08-07 18:48:23,288 INFO misc.py line 115 22900] Train: [2/100][53/156] Data 0.001 (0.001) Batch 3.341 (3.468) Remain 14:49:38 loss: 1.0979 Lr: 0.01253
[2023-08-07 18:48:27,291 INFO misc.py line 115 22900] Train: [2/100][54/156] Data 0.001 (0.001) Batch 4.004 (3.479) Remain 14:52:16 loss: 1.9190 Lr: 0.01260
[2023-08-07 18:48:29,841 INFO misc.py line 115 22900] Train: [2/100][55/156] Data 0.001 (0.001) Batch 2.549 (3.461) Remain 14:47:38 loss: 1.4181 Lr: 0.01267
[2023-08-07 18:48:33,767 INFO misc.py line 115 22900] Train: [2/100][56/156] Data 0.001 (0.001) Batch 3.926 (3.470) Remain 14:49:49 loss: 1.8942 Lr: 0.01273
[2023-08-07 18:48:36,524 INFO misc.py line 115 22900] Train: [2/100][57/156] Data 0.001 (0.001) Batch 2.756 (3.456) Remain 14:46:23 loss: 1.4607 Lr: 0.01280
[2023-08-07 18:48:40,600 INFO misc.py line 115 22900] Train: [2/100][58/156] Data 0.001 (0.001) Batch 4.076 (3.468) Remain 14:49:13 loss: 1.4399 Lr: 0.01287
[2023-08-07 18:48:44,611 INFO misc.py line 115 22900] Train: [2/100][59/156] Data 0.001 (0.001) Batch 4.011 (3.477) Remain 14:51:39 loss: 1.5575 Lr: 0.01294
[2023-08-07 18:48:48,514 INFO misc.py line 115 22900] Train: [2/100][60/156] Data 0.001 (0.001) Batch 3.903 (3.485) Remain 14:53:30 loss: 1.5782 Lr: 0.01301
[2023-08-07 18:48:52,051 INFO misc.py line 115 22900] Train: [2/100][61/156] Data 0.001 (0.001) Batch 3.536 (3.486) Remain 14:53:40 loss: 1.4378 Lr: 0.01308
[2023-08-07 18:48:54,763 INFO misc.py line 115 22900] Train: [2/100][62/156] Data 0.001 (0.001) Batch 2.713 (3.473) Remain 14:50:15 loss: 1.0068 Lr: 0.01315
[2023-08-07 18:48:58,869 INFO misc.py line 115 22900] Train: [2/100][63/156] Data 0.001 (0.001) Batch 4.106 (3.483) Remain 14:52:54 loss: 1.4617 Lr: 0.01322
[2023-08-07 18:49:02,788 INFO misc.py line 115 22900] Train: [2/100][64/156] Data 0.001 (0.001) Batch 3.919 (3.490) Remain 14:54:40 loss: 1.8199 Lr: 0.01329
[2023-08-07 18:49:07,029 INFO misc.py line 115 22900] Train: [2/100][65/156] Data 0.001 (0.001) Batch 4.241 (3.502) Remain 14:57:43 loss: 1.7946 Lr: 0.01336
[2023-08-07 18:49:08,868 INFO misc.py line 115 22900] Train: [2/100][66/156] Data 0.001 (0.001) Batch 1.840 (3.476) Remain 14:50:54 loss: 1.4529 Lr: 0.01343
[2023-08-07 18:49:12,444 INFO misc.py line 115 22900] Train: [2/100][67/156] Data 0.001 (0.001) Batch 3.575 (3.478) Remain 14:51:14 loss: 1.3832 Lr: 0.01350
[2023-08-07 18:49:16,011 INFO misc.py line 115 22900] Train: [2/100][68/156] Data 0.001 (0.001) Batch 3.567 (3.479) Remain 14:51:32 loss: 1.1792 Lr: 0.01357
[2023-08-07 18:49:19,431 INFO misc.py line 115 22900] Train: [2/100][69/156] Data 0.001 (0.001) Batch 3.420 (3.478) Remain 14:51:15 loss: 1.1623 Lr: 0.01364
[2023-08-07 18:49:23,529 INFO misc.py line 115 22900] Train: [2/100][70/156] Data 0.001 (0.001) Batch 4.098 (3.487) Remain 14:53:33 loss: 1.6645 Lr: 0.01372
[2023-08-07 18:49:27,033 INFO misc.py line 115 22900] Train: [2/100][71/156] Data 0.001 (0.001) Batch 3.503 (3.488) Remain 14:53:34 loss: 0.9497 Lr: 0.01379
[2023-08-07 18:49:31,146 INFO misc.py line 115 22900] Train: [2/100][72/156] Data 0.001 (0.001) Batch 4.114 (3.497) Remain 14:55:50 loss: 1.5221 Lr: 0.01386
[2023-08-07 18:49:35,299 INFO misc.py line 115 22900] Train: [2/100][73/156] Data 0.001 (0.001) Batch 4.153 (3.506) Remain 14:58:10 loss: 1.4530 Lr: 0.01393
[2023-08-07 18:49:38,936 INFO misc.py line 115 22900] Train: [2/100][74/156] Data 0.001 (0.001) Batch 3.637 (3.508) Remain 14:58:35 loss: 1.4498 Lr: 0.01400
[2023-08-07 18:49:42,823 INFO misc.py line 115 22900] Train: [2/100][75/156] Data 0.001 (0.001) Batch 3.887 (3.513) Remain 14:59:52 loss: 1.2566 Lr: 0.01408
[2023-08-07 18:49:45,027 INFO misc.py line 115 22900] Train: [2/100][76/156] Data 0.001 (0.001) Batch 2.204 (3.495) Remain 14:55:13 loss: 1.0081 Lr: 0.01415
[2023-08-07 18:49:49,097 INFO misc.py line 115 22900] Train: [2/100][77/156] Data 0.001 (0.001) Batch 4.069 (3.503) Remain 14:57:09 loss: 1.8180 Lr: 0.01422
[2023-08-07 18:49:52,114 INFO misc.py line 115 22900] Train: [2/100][78/156] Data 0.001 (0.001) Batch 3.018 (3.496) Remain 14:55:26 loss: 1.4329 Lr: 0.01430
[2023-08-07 18:49:55,391 INFO misc.py line 115 22900] Train: [2/100][79/156] Data 0.001 (0.001) Batch 3.277 (3.494) Remain 14:54:38 loss: 1.3822 Lr: 0.01437
[2023-08-07 18:49:59,188 INFO misc.py line 115 22900] Train: [2/100][80/156] Data 0.001 (0.001) Batch 3.797 (3.498) Remain 14:55:35 loss: 1.8485 Lr: 0.01444
[2023-08-07 18:50:02,832 INFO misc.py line 115 22900] Train: [2/100][81/156] Data 0.001 (0.001) Batch 3.644 (3.499) Remain 14:56:01 loss: 1.5793 Lr: 0.01452
[2023-08-07 18:50:06,454 INFO misc.py line 115 22900] Train: [2/100][82/156] Data 0.001 (0.001) Batch 3.622 (3.501) Remain 14:56:21 loss: 1.4706 Lr: 0.01459
[2023-08-07 18:50:10,417 INFO misc.py line 115 22900] Train: [2/100][83/156] Data 0.001 (0.001) Batch 3.963 (3.507) Remain 14:57:46 loss: 1.0590 Lr: 0.01467
[2023-08-07 18:50:13,625 INFO misc.py line 115 22900] Train: [2/100][84/156] Data 0.001 (0.001) Batch 3.208 (3.503) Remain 14:56:46 loss: 1.5367 Lr: 0.01474
[2023-08-07 18:50:16,397 INFO misc.py line 115 22900] Train: [2/100][85/156] Data 0.001 (0.001) Batch 2.772 (3.494) Remain 14:54:26 loss: 1.5074 Lr: 0.01482
[2023-08-07 18:50:19,366 INFO misc.py line 115 22900] Train: [2/100][86/156] Data 0.001 (0.001) Batch 2.969 (3.488) Remain 14:52:45 loss: 1.4319 Lr: 0.01489
[2023-08-07 18:50:22,083 INFO misc.py line 115 22900] Train: [2/100][87/156] Data 0.001 (0.001) Batch 2.716 (3.479) Remain 14:50:20 loss: 1.4114 Lr: 0.01497
[2023-08-07 18:50:25,313 INFO misc.py line 115 22900] Train: [2/100][88/156] Data 0.001 (0.001) Batch 3.231 (3.476) Remain 14:49:32 loss: 1.2973 Lr: 0.01504
[2023-08-07 18:50:27,626 INFO misc.py line 115 22900] Train: [2/100][89/156] Data 0.001 (0.001) Batch 2.313 (3.462) Remain 14:46:01 loss: 0.8448 Lr: 0.01512
[2023-08-07 18:50:31,706 INFO misc.py line 115 22900] Train: [2/100][90/156] Data 0.001 (0.001) Batch 4.079 (3.469) Remain 14:47:47 loss: 1.3394 Lr: 0.01519
[2023-08-07 18:50:34,546 INFO misc.py line 115 22900] Train: [2/100][91/156] Data 0.001 (0.001) Batch 2.841 (3.462) Remain 14:45:53 loss: 1.0064 Lr: 0.01527
[2023-08-07 18:50:37,905 INFO misc.py line 115 22900] Train: [2/100][92/156] Data 0.001 (0.001) Batch 3.359 (3.461) Remain 14:45:32 loss: 1.8029 Lr: 0.01535
[2023-08-07 18:50:41,046 INFO misc.py line 115 22900] Train: [2/100][93/156] Data 0.001 (0.001) Batch 3.141 (3.457) Remain 14:44:34 loss: 1.5362 Lr: 0.01542
[2023-08-07 18:50:45,063 INFO misc.py line 115 22900] Train: [2/100][94/156] Data 0.001 (0.001) Batch 4.017 (3.464) Remain 14:46:05 loss: 1.6448 Lr: 0.01550
[2023-08-07 18:50:49,008 INFO misc.py line 115 22900] Train: [2/100][95/156] Data 0.001 (0.001) Batch 3.945 (3.469) Remain 14:47:22 loss: 1.2676 Lr: 0.01558
[2023-08-07 18:50:53,195 INFO misc.py line 115 22900] Train: [2/100][96/156] Data 0.001 (0.001) Batch 4.187 (3.477) Remain 14:49:17 loss: 1.6471 Lr: 0.01565
[2023-08-07 18:50:56,072 INFO misc.py line 115 22900] Train: [2/100][97/156] Data 0.001 (0.001) Batch 2.877 (3.470) Remain 14:47:36 loss: 1.5057 Lr: 0.01573
[2023-08-07 18:51:00,165 INFO misc.py line 115 22900] Train: [2/100][98/156] Data 0.001 (0.001) Batch 4.093 (3.477) Remain 14:49:13 loss: 1.7070 Lr: 0.01581
[2023-08-07 18:51:04,445 INFO misc.py line 115 22900] Train: [2/100][99/156] Data 0.001 (0.001) Batch 4.280 (3.485) Remain 14:51:18 loss: 1.4269 Lr: 0.01589
[2023-08-07 18:51:07,146 INFO misc.py line 115 22900] Train: [2/100][100/156] Data 0.001 (0.001) Batch 2.701 (3.477) Remain 14:49:10 loss: 1.3625 Lr: 0.01596
[2023-08-07 18:51:10,411 INFO misc.py line 115 22900] Train: [2/100][101/156] Data 0.001 (0.001) Batch 3.265 (3.475) Remain 14:48:34 loss: 1.5739 Lr: 0.01604
[2023-08-07 18:51:14,194 INFO misc.py line 115 22900] Train: [2/100][102/156] Data 0.001 (0.001) Batch 3.783 (3.478) Remain 14:49:18 loss: 1.7108 Lr: 0.01612
[2023-08-07 18:51:17,661 INFO misc.py line 115 22900] Train: [2/100][103/156] Data 0.001 (0.001) Batch 3.467 (3.478) Remain 14:49:13 loss: 1.4701 Lr: 0.01620
[2023-08-07 18:51:20,322 INFO misc.py line 115 22900] Train: [2/100][104/156] Data 0.001 (0.001) Batch 2.660 (3.470) Remain 14:47:05 loss: 1.0360 Lr: 0.01628
[2023-08-07 18:51:24,355 INFO misc.py line 115 22900] Train: [2/100][105/156] Data 0.001 (0.001) Batch 4.034 (3.475) Remain 14:48:26 loss: 1.6112 Lr: 0.01635
[2023-08-07 18:51:29,067 INFO misc.py line 115 22900] Train: [2/100][106/156] Data 0.001 (0.001) Batch 4.712 (3.487) Remain 14:51:27 loss: 1.6073 Lr: 0.01643
[2023-08-07 18:51:32,118 INFO misc.py line 115 22900] Train: [2/100][107/156] Data 0.001 (0.001) Batch 3.051 (3.483) Remain 14:50:19 loss: 1.4384 Lr: 0.01651
[2023-08-07 18:51:35,243 INFO misc.py line 115 22900] Train: [2/100][108/156] Data 0.001 (0.001) Batch 3.124 (3.480) Remain 14:49:23 loss: 1.4343 Lr: 0.01659
[2023-08-07 18:51:39,050 INFO misc.py line 115 22900] Train: [2/100][109/156] Data 0.001 (0.001) Batch 3.807 (3.483) Remain 14:50:07 loss: 1.4941 Lr: 0.01667
[2023-08-07 18:51:43,423 INFO misc.py line 115 22900] Train: [2/100][110/156] Data 0.001 (0.001) Batch 4.373 (3.491) Remain 14:52:11 loss: 1.9662 Lr: 0.01675
[2023-08-07 18:51:47,137 INFO misc.py line 115 22900] Train: [2/100][111/156] Data 0.001 (0.001) Batch 3.713 (3.493) Remain 14:52:40 loss: 1.4871 Lr: 0.01683
[2023-08-07 18:51:51,615 INFO misc.py line 115 22900] Train: [2/100][112/156] Data 0.001 (0.001) Batch 4.478 (3.502) Remain 14:54:55 loss: 1.6029 Lr: 0.01691
[2023-08-07 18:51:55,811 INFO misc.py line 115 22900] Train: [2/100][113/156] Data 0.001 (0.001) Batch 4.196 (3.508) Remain 14:56:28 loss: 1.6813 Lr: 0.01699
[2023-08-07 18:52:00,116 INFO misc.py line 115 22900] Train: [2/100][114/156] Data 0.001 (0.001) Batch 4.305 (3.516) Remain 14:58:14 loss: 1.9102 Lr: 0.01707
[2023-08-07 18:52:03,790 INFO misc.py line 115 22900] Train: [2/100][115/156] Data 0.001 (0.001) Batch 3.674 (3.517) Remain 14:58:33 loss: 1.6467 Lr: 0.01715
[2023-08-07 18:52:07,419 INFO misc.py line 115 22900] Train: [2/100][116/156] Data 0.001 (0.001) Batch 3.629 (3.518) Remain 14:58:44 loss: 1.1785 Lr: 0.01723
[2023-08-07 18:52:10,667 INFO misc.py line 115 22900] Train: [2/100][117/156] Data 0.001 (0.001) Batch 3.248 (3.516) Remain 14:58:04 loss: 1.5085 Lr: 0.01731
[2023-08-07 18:52:12,697 INFO misc.py line 115 22900] Train: [2/100][118/156] Data 0.001 (0.001) Batch 2.029 (3.503) Remain 14:54:43 loss: 0.9256 Lr: 0.01739
[2023-08-07 18:52:16,360 INFO misc.py line 115 22900] Train: [2/100][119/156] Data 0.001 (0.001) Batch 3.663 (3.504) Remain 14:55:00 loss: 1.1790 Lr: 0.01748
[2023-08-07 18:52:19,940 INFO misc.py line 115 22900] Train: [2/100][120/156] Data 0.001 (0.001) Batch 3.579 (3.505) Remain 14:55:07 loss: 1.1078 Lr: 0.01756
[2023-08-07 18:52:23,430 INFO misc.py line 115 22900] Train: [2/100][121/156] Data 0.001 (0.001) Batch 3.490 (3.505) Remain 14:55:01 loss: 1.1941 Lr: 0.01764
[2023-08-07 18:52:26,481 INFO misc.py line 115 22900] Train: [2/100][122/156] Data 0.001 (0.001) Batch 3.052 (3.501) Remain 14:54:00 loss: 1.1315 Lr: 0.01772
[2023-08-07 18:52:29,866 INFO misc.py line 115 22900] Train: [2/100][123/156] Data 0.001 (0.001) Batch 3.384 (3.500) Remain 14:53:41 loss: 0.8906 Lr: 0.01780
[2023-08-07 18:52:32,251 INFO misc.py line 115 22900] Train: [2/100][124/156] Data 0.001 (0.001) Batch 2.385 (3.491) Remain 14:51:17 loss: 1.5654 Lr: 0.01788
[2023-08-07 18:52:35,332 INFO misc.py line 115 22900] Train: [2/100][125/156] Data 0.001 (0.001) Batch 3.081 (3.487) Remain 14:50:22 loss: 1.9098 Lr: 0.01797
[2023-08-07 18:52:38,709 INFO misc.py line 115 22900] Train: [2/100][126/156] Data 0.001 (0.001) Batch 3.376 (3.486) Remain 14:50:04 loss: 1.4289 Lr: 0.01805
[2023-08-07 18:52:42,318 INFO misc.py line 115 22900] Train: [2/100][127/156] Data 0.001 (0.001) Batch 3.610 (3.487) Remain 14:50:16 loss: 0.9885 Lr: 0.01813
[2023-08-07 18:52:44,977 INFO misc.py line 115 22900] Train: [2/100][128/156] Data 0.001 (0.001) Batch 2.658 (3.481) Remain 14:48:31 loss: 1.4748 Lr: 0.01821
[2023-08-07 18:52:49,001 INFO misc.py line 115 22900] Train: [2/100][129/156] Data 0.001 (0.001) Batch 4.024 (3.485) Remain 14:49:34 loss: 1.2057 Lr: 0.01830
[2023-08-07 18:52:53,000 INFO misc.py line 115 22900] Train: [2/100][130/156] Data 0.001 (0.001) Batch 3.999 (3.489) Remain 14:50:32 loss: 1.1671 Lr: 0.01838
[2023-08-07 18:52:56,326 INFO misc.py line 115 22900] Train: [2/100][131/156] Data 0.001 (0.001) Batch 3.326 (3.488) Remain 14:50:09 loss: 1.3816 Lr: 0.01846
[2023-08-07 18:53:00,325 INFO misc.py line 115 22900] Train: [2/100][132/156] Data 0.001 (0.001) Batch 3.999 (3.492) Remain 14:51:06 loss: 1.7177 Lr: 0.01855
[2023-08-07 18:53:04,414 INFO misc.py line 115 22900] Train: [2/100][133/156] Data 0.001 (0.001) Batch 4.089 (3.496) Remain 14:52:13 loss: 1.4213 Lr: 0.01863
[2023-08-07 18:53:07,606 INFO misc.py line 115 22900] Train: [2/100][134/156] Data 0.001 (0.001) Batch 3.192 (3.494) Remain 14:51:34 loss: 1.0719 Lr: 0.01871
[2023-08-07 18:53:09,828 INFO misc.py line 115 22900] Train: [2/100][135/156] Data 0.001 (0.001) Batch 2.222 (3.484) Remain 14:49:03 loss: 1.1545 Lr: 0.01880
[2023-08-07 18:53:12,518 INFO misc.py line 115 22900] Train: [2/100][136/156] Data 0.001 (0.001) Batch 2.690 (3.478) Remain 14:47:28 loss: 1.6202 Lr: 0.01888
[2023-08-07 18:53:15,391 INFO misc.py line 115 22900] Train: [2/100][137/156] Data 0.001 (0.001) Batch 2.873 (3.474) Remain 14:46:16 loss: 1.3439 Lr: 0.01896
[2023-08-07 18:53:17,972 INFO misc.py line 115 22900] Train: [2/100][138/156] Data 0.001 (0.001) Batch 2.581 (3.467) Remain 14:44:31 loss: 1.3781 Lr: 0.01905
[2023-08-07 18:53:21,260 INFO misc.py line 115 22900] Train: [2/100][139/156] Data 0.001 (0.001) Batch 3.288 (3.466) Remain 14:44:07 loss: 1.4194 Lr: 0.01913
[2023-08-07 18:53:25,225 INFO misc.py line 115 22900] Train: [2/100][140/156] Data 0.001 (0.001) Batch 3.965 (3.470) Remain 14:44:59 loss: 1.3201 Lr: 0.01922
[2023-08-07 18:53:28,536 INFO misc.py line 115 22900] Train: [2/100][141/156] Data 0.001 (0.001) Batch 3.310 (3.469) Remain 14:44:38 loss: 1.6915 Lr: 0.01930
[2023-08-07 18:53:32,229 INFO misc.py line 115 22900] Train: [2/100][142/156] Data 0.001 (0.001) Batch 3.693 (3.470) Remain 14:45:00 loss: 1.3185 Lr: 0.01938
[2023-08-07 18:53:35,837 INFO misc.py line 115 22900] Train: [2/100][143/156] Data 0.001 (0.001) Batch 3.608 (3.471) Remain 14:45:11 loss: 0.9916 Lr: 0.01947
[2023-08-07 18:53:39,396 INFO misc.py line 115 22900] Train: [2/100][144/156] Data 0.001 (0.001) Batch 3.559 (3.472) Remain 14:45:17 loss: 1.3503 Lr: 0.01955
[2023-08-07 18:53:43,283 INFO misc.py line 115 22900] Train: [2/100][145/156] Data 0.001 (0.001) Batch 3.886 (3.475) Remain 14:45:58 loss: 1.0932 Lr: 0.01964
[2023-08-07 18:53:47,446 INFO misc.py line 115 22900] Train: [2/100][146/156] Data 0.001 (0.001) Batch 4.164 (3.479) Remain 14:47:09 loss: 1.6010 Lr: 0.01972
[2023-08-07 18:53:50,963 INFO misc.py line 115 22900] Train: [2/100][147/156] Data 0.001 (0.001) Batch 3.516 (3.480) Remain 14:47:09 loss: 1.0458 Lr: 0.01981
[2023-08-07 18:53:53,888 INFO misc.py line 115 22900] Train: [2/100][148/156] Data 0.001 (0.001) Batch 2.925 (3.476) Remain 14:46:07 loss: 1.2031 Lr: 0.01989
[2023-08-07 18:53:56,154 INFO misc.py line 115 22900] Train: [2/100][149/156] Data 0.001 (0.001) Batch 2.267 (3.468) Remain 14:43:57 loss: 0.8240 Lr: 0.01998
[2023-08-07 18:53:59,241 INFO misc.py line 115 22900] Train: [2/100][150/156] Data 0.001 (0.001) Batch 3.087 (3.465) Remain 14:43:14 loss: 1.3233 Lr: 0.02007
[2023-08-07 18:54:03,275 INFO misc.py line 115 22900] Train: [2/100][151/156] Data 0.001 (0.001) Batch 4.034 (3.469) Remain 14:44:09 loss: 1.5591 Lr: 0.02015
[2023-08-07 18:54:07,383 INFO misc.py line 115 22900] Train: [2/100][152/156] Data 0.001 (0.001) Batch 4.108 (3.473) Remain 14:45:11 loss: 1.3026 Lr: 0.02024
[2023-08-07 18:54:10,163 INFO misc.py line 115 22900] Train: [2/100][153/156] Data 0.001 (0.001) Batch 2.780 (3.469) Remain 14:43:57 loss: 1.5379 Lr: 0.02032
[2023-08-07 18:54:14,162 INFO misc.py line 115 22900] Train: [2/100][154/156] Data 0.001 (0.001) Batch 3.999 (3.472) Remain 14:44:47 loss: 1.1789 Lr: 0.02041
[2023-08-07 18:54:16,550 INFO misc.py line 115 22900] Train: [2/100][155/156] Data 0.001 (0.001) Batch 2.388 (3.465) Remain 14:42:55 loss: 1.0371 Lr: 0.02050
[2023-08-07 18:54:18,609 INFO misc.py line 115 22900] Train: [2/100][156/156] Data 0.001 (0.001) Batch 2.059 (3.456) Remain 14:40:31 loss: 1.4533 Lr: 0.02058
[2023-08-07 18:54:18,610 INFO misc.py line 129 22900] Train result: loss: 1.4333 
[2023-08-07 18:54:18,610 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 18:54:20,792 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.1910 
[2023-08-07 18:54:21,661 INFO evaluator.py line 122 22900] Test: [2/24] Loss 1.2201 
[2023-08-07 18:54:23,325 INFO evaluator.py line 122 22900] Test: [3/24] Loss 1.6625 
[2023-08-07 18:54:24,848 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.3106 
[2023-08-07 18:54:26,695 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.5472 
[2023-08-07 18:54:28,362 INFO evaluator.py line 122 22900] Test: [6/24] Loss 1.9575 
[2023-08-07 18:54:30,503 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.0183 
[2023-08-07 18:54:32,309 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.1675 
[2023-08-07 18:54:33,592 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.4697 
[2023-08-07 18:54:35,723 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.6718 
[2023-08-07 18:54:36,250 INFO evaluator.py line 122 22900] Test: [11/24] Loss 3.4311 
[2023-08-07 18:54:37,785 INFO evaluator.py line 122 22900] Test: [12/24] Loss 1.4886 
[2023-08-07 18:54:40,498 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.9220 
[2023-08-07 18:54:42,179 INFO evaluator.py line 122 22900] Test: [14/24] Loss 1.5220 
[2023-08-07 18:54:44,207 INFO evaluator.py line 122 22900] Test: [15/24] Loss 1.3279 
[2023-08-07 18:54:46,919 INFO evaluator.py line 122 22900] Test: [16/24] Loss 2.1158 
[2023-08-07 18:54:49,623 INFO evaluator.py line 122 22900] Test: [17/24] Loss 2.0219 
[2023-08-07 18:54:51,472 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.5055 
[2023-08-07 18:54:52,222 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.4926 
[2023-08-07 18:54:53,107 INFO evaluator.py line 122 22900] Test: [20/24] Loss 1.4753 
[2023-08-07 18:54:55,370 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.7766 
[2023-08-07 18:54:57,337 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.5723 
[2023-08-07 18:54:59,185 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.6482 
[2023-08-07 18:55:01,121 INFO evaluator.py line 122 22900] Test: [24/24] Loss 2.0581 
[2023-08-07 18:55:01,174 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.0701/0.1154/0.5404.
[2023-08-07 18:55:01,174 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.4801/0.9717
[2023-08-07 18:55:01,174 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.6438/0.9938
[2023-08-07 18:55:01,174 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.0058/0.0098
[2023-08-07 18:55:01,174 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.0001/0.0001
[2023-08-07 18:55:01,174 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.2010/0.2269
[2023-08-07 18:55:01,174 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.0000/0.0000
[2023-08-07 18:55:01,174 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.0005/0.0005
[2023-08-07 18:55:01,174 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0246/0.0260
[2023-08-07 18:55:01,174 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.0470/0.0793
[2023-08-07 18:55:01,174 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0000/0.0000
[2023-08-07 18:55:01,174 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 18:55:01,174 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0000/0.0000
[2023-08-07 18:55:01,174 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.0000/0.0000
[2023-08-07 18:55:01,174 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-07 18:55:01,174 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0000/0.0000
[2023-08-07 18:55:01,174 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-07 18:55:01,174 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0000/0.0000
[2023-08-07 18:55:01,174 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.0000/0.0000
[2023-08-07 18:55:01,174 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 18:55:01,174 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 18:55:01,174 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 18:55:01,175 INFO misc.py line 150 22900] Best validation mIoU updated to: 0.0701
[2023-08-07 18:55:01,175 INFO misc.py line 152 22900] Currently Best mIoU: 0.0701
[2023-08-07 18:55:01,175 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 18:55:06,334 INFO misc.py line 115 22900] Train: [3/100][1/156] Data 0.751 (0.751) Batch 4.112 (4.112) Remain 17:27:44 loss: 1.1977 Lr: 0.02067
[2023-08-07 18:55:10,468 INFO misc.py line 115 22900] Train: [3/100][2/156] Data 0.001 (0.001) Batch 4.135 (4.135) Remain 17:33:22 loss: 1.7771 Lr: 0.02075
[2023-08-07 18:55:13,107 INFO misc.py line 115 22900] Train: [3/100][3/156] Data 0.001 (0.001) Batch 2.638 (2.638) Remain 11:12:07 loss: 1.0444 Lr: 0.02084
[2023-08-07 18:55:15,891 INFO misc.py line 115 22900] Train: [3/100][4/156] Data 0.001 (0.001) Batch 2.784 (2.784) Remain 11:49:17 loss: 1.2367 Lr: 0.02093
[2023-08-07 18:55:20,343 INFO misc.py line 115 22900] Train: [3/100][5/156] Data 0.001 (0.001) Batch 4.452 (3.618) Remain 15:21:39 loss: 2.1061 Lr: 0.02101
[2023-08-07 18:55:23,993 INFO misc.py line 115 22900] Train: [3/100][6/156] Data 0.001 (0.001) Batch 3.649 (3.629) Remain 15:24:12 loss: 1.2895 Lr: 0.02110
[2023-08-07 18:55:26,698 INFO misc.py line 115 22900] Train: [3/100][7/156] Data 0.001 (0.001) Batch 2.706 (3.398) Remain 14:25:23 loss: 1.2677 Lr: 0.02119
[2023-08-07 18:55:29,767 INFO misc.py line 115 22900] Train: [3/100][8/156] Data 0.001 (0.001) Batch 3.069 (3.332) Remain 14:08:35 loss: 1.2143 Lr: 0.02128
[2023-08-07 18:55:33,190 INFO misc.py line 115 22900] Train: [3/100][9/156] Data 0.001 (0.001) Batch 3.422 (3.347) Remain 14:12:21 loss: 1.6472 Lr: 0.02136
[2023-08-07 18:55:35,960 INFO misc.py line 115 22900] Train: [3/100][10/156] Data 0.001 (0.001) Batch 2.771 (3.265) Remain 13:51:19 loss: 1.0999 Lr: 0.02145
[2023-08-07 18:55:39,444 INFO misc.py line 115 22900] Train: [3/100][11/156] Data 0.001 (0.001) Batch 3.483 (3.292) Remain 13:58:14 loss: 1.0166 Lr: 0.02154
[2023-08-07 18:55:42,906 INFO misc.py line 115 22900] Train: [3/100][12/156] Data 0.001 (0.001) Batch 3.462 (3.311) Remain 14:02:58 loss: 1.6177 Lr: 0.02163
[2023-08-07 18:55:45,922 INFO misc.py line 115 22900] Train: [3/100][13/156] Data 0.002 (0.001) Batch 3.017 (3.282) Remain 13:55:26 loss: 1.0793 Lr: 0.02171
[2023-08-07 18:55:49,243 INFO misc.py line 115 22900] Train: [3/100][14/156] Data 0.001 (0.001) Batch 3.321 (3.285) Remain 13:56:17 loss: 0.8481 Lr: 0.02180
[2023-08-07 18:55:52,474 INFO misc.py line 115 22900] Train: [3/100][15/156] Data 0.001 (0.001) Batch 3.231 (3.281) Remain 13:55:05 loss: 0.9019 Lr: 0.02189
[2023-08-07 18:55:56,237 INFO misc.py line 115 22900] Train: [3/100][16/156] Data 0.001 (0.001) Batch 3.762 (3.318) Remain 14:04:27 loss: 1.3652 Lr: 0.02198
[2023-08-07 18:55:59,065 INFO misc.py line 115 22900] Train: [3/100][17/156] Data 0.001 (0.001) Batch 2.828 (3.283) Remain 13:55:30 loss: 1.0899 Lr: 0.02206
[2023-08-07 18:56:01,990 INFO misc.py line 115 22900] Train: [3/100][18/156] Data 0.001 (0.001) Batch 2.925 (3.259) Remain 13:49:22 loss: 1.2748 Lr: 0.02215
[2023-08-07 18:56:06,188 INFO misc.py line 115 22900] Train: [3/100][19/156] Data 0.001 (0.001) Batch 4.198 (3.318) Remain 14:04:15 loss: 1.6604 Lr: 0.02224
[2023-08-07 18:56:09,884 INFO misc.py line 115 22900] Train: [3/100][20/156] Data 0.001 (0.001) Batch 3.697 (3.340) Remain 14:09:52 loss: 1.3237 Lr: 0.02233
[2023-08-07 18:56:12,949 INFO misc.py line 115 22900] Train: [3/100][21/156] Data 0.001 (0.001) Batch 3.064 (3.325) Remain 14:05:55 loss: 1.1651 Lr: 0.02242
[2023-08-07 18:56:16,297 INFO misc.py line 115 22900] Train: [3/100][22/156] Data 0.001 (0.001) Batch 3.349 (3.326) Remain 14:06:12 loss: 1.6854 Lr: 0.02251
[2023-08-07 18:56:20,293 INFO misc.py line 115 22900] Train: [3/100][23/156] Data 0.001 (0.001) Batch 3.996 (3.359) Remain 14:14:40 loss: 1.3581 Lr: 0.02259
[2023-08-07 18:56:24,340 INFO misc.py line 115 22900] Train: [3/100][24/156] Data 0.001 (0.001) Batch 4.046 (3.392) Remain 14:22:56 loss: 1.3346 Lr: 0.02268
[2023-08-07 18:56:27,179 INFO misc.py line 115 22900] Train: [3/100][25/156] Data 0.001 (0.001) Batch 2.839 (3.367) Remain 14:16:29 loss: 0.9290 Lr: 0.02277
[2023-08-07 18:56:31,231 INFO misc.py line 115 22900] Train: [3/100][26/156] Data 0.001 (0.001) Batch 4.052 (3.397) Remain 14:24:00 loss: 1.4838 Lr: 0.02286
[2023-08-07 18:56:35,108 INFO misc.py line 115 22900] Train: [3/100][27/156] Data 0.002 (0.001) Batch 3.877 (3.417) Remain 14:29:02 loss: 1.1579 Lr: 0.02295
[2023-08-07 18:56:38,699 INFO misc.py line 115 22900] Train: [3/100][28/156] Data 0.001 (0.001) Batch 3.591 (3.424) Remain 14:30:45 loss: 1.3152 Lr: 0.02304
[2023-08-07 18:56:41,730 INFO misc.py line 115 22900] Train: [3/100][29/156] Data 0.001 (0.001) Batch 3.031 (3.409) Remain 14:26:51 loss: 1.6402 Lr: 0.02313
[2023-08-07 18:56:44,894 INFO misc.py line 115 22900] Train: [3/100][30/156] Data 0.001 (0.001) Batch 3.164 (3.400) Remain 14:24:30 loss: 1.7041 Lr: 0.02322
[2023-08-07 18:56:48,185 INFO misc.py line 115 22900] Train: [3/100][31/156] Data 0.001 (0.001) Batch 3.291 (3.396) Remain 14:23:27 loss: 0.8974 Lr: 0.02331
[2023-08-07 18:56:51,599 INFO misc.py line 115 22900] Train: [3/100][32/156] Data 0.001 (0.001) Batch 3.414 (3.396) Remain 14:23:33 loss: 1.2070 Lr: 0.02339
[2023-08-07 18:56:55,582 INFO misc.py line 115 22900] Train: [3/100][33/156] Data 0.001 (0.001) Batch 3.983 (3.416) Remain 14:28:28 loss: 1.9051 Lr: 0.02348
[2023-08-07 18:56:59,858 INFO misc.py line 115 22900] Train: [3/100][34/156] Data 0.001 (0.001) Batch 4.276 (3.444) Remain 14:35:28 loss: 1.4234 Lr: 0.02357
[2023-08-07 18:57:03,014 INFO misc.py line 115 22900] Train: [3/100][35/156] Data 0.001 (0.001) Batch 3.156 (3.435) Remain 14:33:07 loss: 0.8712 Lr: 0.02366
[2023-08-07 18:57:06,388 INFO misc.py line 115 22900] Train: [3/100][36/156] Data 0.001 (0.001) Batch 3.375 (3.433) Remain 14:32:36 loss: 1.2378 Lr: 0.02375
[2023-08-07 18:57:10,512 INFO misc.py line 115 22900] Train: [3/100][37/156] Data 0.001 (0.001) Batch 4.123 (3.453) Remain 14:37:43 loss: 1.1513 Lr: 0.02384
[2023-08-07 18:57:14,547 INFO misc.py line 115 22900] Train: [3/100][38/156] Data 0.001 (0.001) Batch 4.035 (3.470) Remain 14:41:53 loss: 1.5302 Lr: 0.02393
[2023-08-07 18:57:17,931 INFO misc.py line 115 22900] Train: [3/100][39/156] Data 0.001 (0.001) Batch 3.384 (3.467) Remain 14:41:13 loss: 0.9437 Lr: 0.02402
[2023-08-07 18:57:21,193 INFO misc.py line 115 22900] Train: [3/100][40/156] Data 0.001 (0.001) Batch 3.262 (3.462) Remain 14:39:45 loss: 1.0912 Lr: 0.02411
[2023-08-07 18:57:24,448 INFO misc.py line 115 22900] Train: [3/100][41/156] Data 0.001 (0.001) Batch 3.254 (3.456) Remain 14:38:18 loss: 0.7988 Lr: 0.02420
[2023-08-07 18:57:27,921 INFO misc.py line 115 22900] Train: [3/100][42/156] Data 0.001 (0.001) Batch 3.473 (3.457) Remain 14:38:21 loss: 1.3423 Lr: 0.02429
[2023-08-07 18:57:31,054 INFO misc.py line 115 22900] Train: [3/100][43/156] Data 0.001 (0.001) Batch 3.134 (3.449) Remain 14:36:15 loss: 1.0054 Lr: 0.02438
[2023-08-07 18:57:33,259 INFO misc.py line 115 22900] Train: [3/100][44/156] Data 0.001 (0.001) Batch 2.205 (3.418) Remain 14:28:29 loss: 1.4617 Lr: 0.02447
[2023-08-07 18:57:36,846 INFO misc.py line 115 22900] Train: [3/100][45/156] Data 0.001 (0.001) Batch 3.587 (3.422) Remain 14:29:27 loss: 1.1639 Lr: 0.02456
[2023-08-07 18:57:39,240 INFO misc.py line 115 22900] Train: [3/100][46/156] Data 0.001 (0.001) Batch 2.394 (3.398) Remain 14:23:19 loss: 0.5624 Lr: 0.02465
[2023-08-07 18:57:43,156 INFO misc.py line 115 22900] Train: [3/100][47/156] Data 0.001 (0.001) Batch 3.917 (3.410) Remain 14:26:15 loss: 2.2467 Lr: 0.02474
[2023-08-07 18:57:46,158 INFO misc.py line 115 22900] Train: [3/100][48/156] Data 0.001 (0.001) Batch 3.002 (3.401) Remain 14:23:53 loss: 1.7666 Lr: 0.02483
[2023-08-07 18:57:49,494 INFO misc.py line 115 22900] Train: [3/100][49/156] Data 0.001 (0.001) Batch 3.336 (3.400) Remain 14:23:28 loss: 1.3332 Lr: 0.02492
[2023-08-07 18:57:53,597 INFO misc.py line 115 22900] Train: [3/100][50/156] Data 0.001 (0.001) Batch 4.102 (3.415) Remain 14:27:12 loss: 1.8684 Lr: 0.02501
[2023-08-07 18:57:55,845 INFO misc.py line 115 22900] Train: [3/100][51/156] Data 0.001 (0.001) Batch 2.248 (3.390) Remain 14:20:59 loss: 1.5496 Lr: 0.02510
[2023-08-07 18:57:59,058 INFO misc.py line 115 22900] Train: [3/100][52/156] Data 0.001 (0.001) Batch 3.213 (3.387) Remain 14:20:00 loss: 1.5952 Lr: 0.02519
[2023-08-07 18:58:03,286 INFO misc.py line 115 22900] Train: [3/100][53/156] Data 0.001 (0.001) Batch 4.228 (3.404) Remain 14:24:13 loss: 1.4372 Lr: 0.02528
[2023-08-07 18:58:05,463 INFO misc.py line 115 22900] Train: [3/100][54/156] Data 0.001 (0.001) Batch 2.177 (3.380) Remain 14:18:03 loss: 0.8219 Lr: 0.02537
[2023-08-07 18:58:08,928 INFO misc.py line 115 22900] Train: [3/100][55/156] Data 0.001 (0.001) Batch 3.465 (3.381) Remain 14:18:25 loss: 1.3551 Lr: 0.02546
[2023-08-07 18:58:12,407 INFO misc.py line 115 22900] Train: [3/100][56/156] Data 0.001 (0.001) Batch 3.478 (3.383) Remain 14:18:50 loss: 1.8134 Lr: 0.02555
[2023-08-07 18:58:14,545 INFO misc.py line 115 22900] Train: [3/100][57/156] Data 0.001 (0.001) Batch 2.139 (3.360) Remain 14:12:55 loss: 0.6952 Lr: 0.02564
[2023-08-07 18:58:17,810 INFO misc.py line 115 22900] Train: [3/100][58/156] Data 0.001 (0.001) Batch 3.265 (3.358) Remain 14:12:25 loss: 1.1584 Lr: 0.02573
[2023-08-07 18:58:21,064 INFO misc.py line 115 22900] Train: [3/100][59/156] Data 0.001 (0.001) Batch 3.254 (3.356) Remain 14:11:54 loss: 1.2980 Lr: 0.02582
[2023-08-07 18:58:25,072 INFO misc.py line 115 22900] Train: [3/100][60/156] Data 0.001 (0.001) Batch 4.009 (3.368) Remain 14:14:45 loss: 1.1380 Lr: 0.02591
[2023-08-07 18:58:28,511 INFO misc.py line 115 22900] Train: [3/100][61/156] Data 0.001 (0.001) Batch 3.438 (3.369) Remain 14:15:00 loss: 1.1606 Lr: 0.02600
[2023-08-07 18:58:32,048 INFO misc.py line 115 22900] Train: [3/100][62/156] Data 0.001 (0.001) Batch 3.537 (3.372) Remain 14:15:40 loss: 1.9321 Lr: 0.02609
[2023-08-07 18:58:36,014 INFO misc.py line 115 22900] Train: [3/100][63/156] Data 0.001 (0.001) Batch 3.966 (3.382) Remain 14:18:07 loss: 1.5706 Lr: 0.02619
[2023-08-07 18:58:38,724 INFO misc.py line 115 22900] Train: [3/100][64/156] Data 0.001 (0.001) Batch 2.710 (3.371) Remain 14:15:16 loss: 1.1604 Lr: 0.02628
[2023-08-07 18:58:42,725 INFO misc.py line 115 22900] Train: [3/100][65/156] Data 0.001 (0.001) Batch 4.001 (3.381) Remain 14:17:48 loss: 1.3763 Lr: 0.02637
[2023-08-07 18:58:46,975 INFO misc.py line 115 22900] Train: [3/100][66/156] Data 0.001 (0.001) Batch 4.250 (3.395) Remain 14:21:14 loss: 1.5642 Lr: 0.02646
[2023-08-07 18:58:50,636 INFO misc.py line 115 22900] Train: [3/100][67/156] Data 0.001 (0.001) Batch 3.661 (3.399) Remain 14:22:14 loss: 1.5316 Lr: 0.02655
[2023-08-07 18:58:53,776 INFO misc.py line 115 22900] Train: [3/100][68/156] Data 0.001 (0.001) Batch 3.140 (3.395) Remain 14:21:10 loss: 1.1283 Lr: 0.02664
[2023-08-07 18:58:56,974 INFO misc.py line 115 22900] Train: [3/100][69/156] Data 0.001 (0.001) Batch 3.199 (3.392) Remain 14:20:21 loss: 1.0660 Lr: 0.02673
[2023-08-07 18:59:01,021 INFO misc.py line 115 22900] Train: [3/100][70/156] Data 0.001 (0.001) Batch 4.046 (3.402) Remain 14:22:47 loss: 1.3228 Lr: 0.02682
[2023-08-07 18:59:05,269 INFO misc.py line 115 22900] Train: [3/100][71/156] Data 0.001 (0.001) Batch 4.248 (3.414) Remain 14:25:53 loss: 1.6220 Lr: 0.02691
[2023-08-07 18:59:08,136 INFO misc.py line 115 22900] Train: [3/100][72/156] Data 0.001 (0.001) Batch 2.867 (3.406) Remain 14:23:49 loss: 0.8254 Lr: 0.02700
[2023-08-07 18:59:11,994 INFO misc.py line 115 22900] Train: [3/100][73/156] Data 0.001 (0.001) Batch 3.859 (3.413) Remain 14:25:23 loss: 1.0915 Lr: 0.02709
[2023-08-07 18:59:14,024 INFO misc.py line 115 22900] Train: [3/100][74/156] Data 0.001 (0.001) Batch 2.030 (3.393) Remain 14:20:24 loss: 1.6658 Lr: 0.02718
[2023-08-07 18:59:18,712 INFO misc.py line 115 22900] Train: [3/100][75/156] Data 0.002 (0.001) Batch 4.688 (3.411) Remain 14:24:54 loss: 1.9306 Lr: 0.02727
[2023-08-07 18:59:23,240 INFO misc.py line 115 22900] Train: [3/100][76/156] Data 0.001 (0.001) Batch 4.528 (3.426) Remain 14:28:43 loss: 1.5301 Lr: 0.02736
[2023-08-07 18:59:26,887 INFO misc.py line 115 22900] Train: [3/100][77/156] Data 0.001 (0.001) Batch 3.646 (3.429) Remain 14:29:25 loss: 1.3661 Lr: 0.02745
[2023-08-07 18:59:29,837 INFO misc.py line 115 22900] Train: [3/100][78/156] Data 0.001 (0.001) Batch 2.950 (3.423) Remain 14:27:44 loss: 0.9186 Lr: 0.02755
[2023-08-07 18:59:33,568 INFO misc.py line 115 22900] Train: [3/100][79/156] Data 0.001 (0.001) Batch 3.731 (3.427) Remain 14:28:43 loss: 1.7047 Lr: 0.02764
[2023-08-07 18:59:37,566 INFO misc.py line 115 22900] Train: [3/100][80/156] Data 0.001 (0.001) Batch 3.999 (3.435) Remain 14:30:32 loss: 1.0762 Lr: 0.02773
[2023-08-07 18:59:41,555 INFO misc.py line 115 22900] Train: [3/100][81/156] Data 0.001 (0.001) Batch 3.989 (3.442) Remain 14:32:17 loss: 1.5034 Lr: 0.02782
[2023-08-07 18:59:45,267 INFO misc.py line 115 22900] Train: [3/100][82/156] Data 0.001 (0.001) Batch 3.712 (3.445) Remain 14:33:05 loss: 1.5149 Lr: 0.02791
[2023-08-07 18:59:49,233 INFO misc.py line 115 22900] Train: [3/100][83/156] Data 0.001 (0.001) Batch 3.966 (3.452) Remain 14:34:41 loss: 1.1481 Lr: 0.02800
[2023-08-07 18:59:52,448 INFO misc.py line 115 22900] Train: [3/100][84/156] Data 0.001 (0.001) Batch 3.215 (3.449) Remain 14:33:53 loss: 1.1913 Lr: 0.02809
[2023-08-07 18:59:56,465 INFO misc.py line 115 22900] Train: [3/100][85/156] Data 0.001 (0.001) Batch 4.017 (3.456) Remain 14:35:35 loss: 1.5070 Lr: 0.02818
[2023-08-07 18:59:59,018 INFO misc.py line 115 22900] Train: [3/100][86/156] Data 0.001 (0.001) Batch 2.553 (3.445) Remain 14:32:46 loss: 1.1815 Lr: 0.02827
[2023-08-07 19:00:03,200 INFO misc.py line 115 22900] Train: [3/100][87/156] Data 0.001 (0.001) Batch 4.182 (3.453) Remain 14:34:56 loss: 1.4453 Lr: 0.02836
[2023-08-07 19:00:06,872 INFO misc.py line 115 22900] Train: [3/100][88/156] Data 0.001 (0.001) Batch 3.672 (3.456) Remain 14:35:32 loss: 1.1575 Lr: 0.02845
[2023-08-07 19:00:11,156 INFO misc.py line 115 22900] Train: [3/100][89/156] Data 0.001 (0.001) Batch 4.284 (3.466) Remain 14:37:54 loss: 1.7176 Lr: 0.02854
[2023-08-07 19:00:13,844 INFO misc.py line 115 22900] Train: [3/100][90/156] Data 0.001 (0.001) Batch 2.688 (3.457) Remain 14:35:35 loss: 1.2553 Lr: 0.02863
[2023-08-07 19:00:17,059 INFO misc.py line 115 22900] Train: [3/100][91/156] Data 0.001 (0.001) Batch 3.215 (3.454) Remain 14:34:50 loss: 0.8368 Lr: 0.02872
[2023-08-07 19:00:20,514 INFO misc.py line 115 22900] Train: [3/100][92/156] Data 0.001 (0.001) Batch 3.455 (3.454) Remain 14:34:47 loss: 1.1477 Lr: 0.02881
[2023-08-07 19:00:23,706 INFO misc.py line 115 22900] Train: [3/100][93/156] Data 0.001 (0.001) Batch 3.192 (3.451) Remain 14:33:59 loss: 1.2526 Lr: 0.02891
[2023-08-07 19:00:27,307 INFO misc.py line 115 22900] Train: [3/100][94/156] Data 0.001 (0.001) Batch 3.600 (3.453) Remain 14:34:21 loss: 1.5179 Lr: 0.02900
[2023-08-07 19:00:30,796 INFO misc.py line 115 22900] Train: [3/100][95/156] Data 0.001 (0.001) Batch 3.489 (3.453) Remain 14:34:23 loss: 1.4663 Lr: 0.02909
[2023-08-07 19:00:34,764 INFO misc.py line 115 22900] Train: [3/100][96/156] Data 0.001 (0.001) Batch 3.968 (3.459) Remain 14:35:44 loss: 1.3472 Lr: 0.02918
[2023-08-07 19:00:37,691 INFO misc.py line 115 22900] Train: [3/100][97/156] Data 0.001 (0.001) Batch 2.927 (3.453) Remain 14:34:14 loss: 1.3539 Lr: 0.02927
[2023-08-07 19:00:41,085 INFO misc.py line 115 22900] Train: [3/100][98/156] Data 0.001 (0.001) Batch 3.394 (3.452) Remain 14:34:01 loss: 1.1942 Lr: 0.02936
[2023-08-07 19:00:45,276 INFO misc.py line 115 22900] Train: [3/100][99/156] Data 0.001 (0.001) Batch 4.191 (3.460) Remain 14:35:55 loss: 1.2805 Lr: 0.02945
[2023-08-07 19:00:47,753 INFO misc.py line 115 22900] Train: [3/100][100/156] Data 0.001 (0.001) Batch 2.477 (3.450) Remain 14:33:18 loss: 0.9749 Lr: 0.02954
[2023-08-07 19:00:50,910 INFO misc.py line 115 22900] Train: [3/100][101/156] Data 0.001 (0.001) Batch 3.157 (3.447) Remain 14:32:29 loss: 1.2262 Lr: 0.02963
[2023-08-07 19:00:54,336 INFO misc.py line 115 22900] Train: [3/100][102/156] Data 0.001 (0.001) Batch 3.426 (3.447) Remain 14:32:22 loss: 1.6504 Lr: 0.02972
[2023-08-07 19:00:58,438 INFO misc.py line 115 22900] Train: [3/100][103/156] Data 0.001 (0.001) Batch 4.102 (3.453) Remain 14:33:58 loss: 1.1622 Lr: 0.02981
[2023-08-07 19:01:02,945 INFO misc.py line 115 22900] Train: [3/100][104/156] Data 0.001 (0.001) Batch 4.507 (3.464) Remain 14:36:33 loss: 1.3479 Lr: 0.02990
[2023-08-07 19:01:06,623 INFO misc.py line 115 22900] Train: [3/100][105/156] Data 0.001 (0.001) Batch 3.678 (3.466) Remain 14:37:01 loss: 1.5854 Lr: 0.02999
[2023-08-07 19:01:09,983 INFO misc.py line 115 22900] Train: [3/100][106/156] Data 0.001 (0.001) Batch 3.360 (3.465) Remain 14:36:42 loss: 1.5132 Lr: 0.03008
[2023-08-07 19:01:14,058 INFO misc.py line 115 22900] Train: [3/100][107/156] Data 0.001 (0.001) Batch 4.074 (3.471) Remain 14:38:08 loss: 0.8950 Lr: 0.03017
[2023-08-07 19:01:17,650 INFO misc.py line 115 22900] Train: [3/100][108/156] Data 0.001 (0.001) Batch 3.592 (3.472) Remain 14:38:22 loss: 1.3988 Lr: 0.03026
[2023-08-07 19:01:21,725 INFO misc.py line 115 22900] Train: [3/100][109/156] Data 0.001 (0.001) Batch 4.075 (3.478) Remain 14:39:45 loss: 1.3882 Lr: 0.03035
[2023-08-07 19:01:25,451 INFO misc.py line 115 22900] Train: [3/100][110/156] Data 0.001 (0.001) Batch 3.725 (3.480) Remain 14:40:17 loss: 1.3745 Lr: 0.03044
[2023-08-07 19:01:28,832 INFO misc.py line 115 22900] Train: [3/100][111/156] Data 0.001 (0.001) Batch 3.381 (3.479) Remain 14:39:59 loss: 1.1008 Lr: 0.03053
[2023-08-07 19:01:31,535 INFO misc.py line 115 22900] Train: [3/100][112/156] Data 0.001 (0.001) Batch 2.703 (3.472) Remain 14:38:08 loss: 1.2442 Lr: 0.03062
[2023-08-07 19:01:34,718 INFO misc.py line 115 22900] Train: [3/100][113/156] Data 0.001 (0.001) Batch 3.183 (3.469) Remain 14:37:25 loss: 1.3735 Lr: 0.03071
[2023-08-07 19:01:38,794 INFO misc.py line 115 22900] Train: [3/100][114/156] Data 0.001 (0.001) Batch 4.075 (3.475) Remain 14:38:44 loss: 1.4490 Lr: 0.03080
[2023-08-07 19:01:42,507 INFO misc.py line 115 22900] Train: [3/100][115/156] Data 0.001 (0.001) Batch 3.714 (3.477) Remain 14:39:13 loss: 1.2651 Lr: 0.03089
[2023-08-07 19:01:46,094 INFO misc.py line 115 22900] Train: [3/100][116/156] Data 0.001 (0.001) Batch 3.587 (3.478) Remain 14:39:24 loss: 1.4839 Lr: 0.03098
[2023-08-07 19:01:49,472 INFO misc.py line 115 22900] Train: [3/100][117/156] Data 0.001 (0.001) Batch 3.378 (3.477) Remain 14:39:07 loss: 0.9903 Lr: 0.03107
[2023-08-07 19:01:53,337 INFO misc.py line 115 22900] Train: [3/100][118/156] Data 0.001 (0.001) Batch 3.865 (3.480) Remain 14:39:55 loss: 1.6265 Lr: 0.03116
[2023-08-07 19:01:56,096 INFO misc.py line 115 22900] Train: [3/100][119/156] Data 0.001 (0.001) Batch 2.759 (3.474) Remain 14:38:17 loss: 1.4373 Lr: 0.03125
[2023-08-07 19:01:59,595 INFO misc.py line 115 22900] Train: [3/100][120/156] Data 0.001 (0.001) Batch 3.499 (3.474) Remain 14:38:17 loss: 1.2549 Lr: 0.03134
[2023-08-07 19:02:03,680 INFO misc.py line 115 22900] Train: [3/100][121/156] Data 0.001 (0.001) Batch 4.085 (3.479) Remain 14:39:32 loss: 1.6662 Lr: 0.03143
[2023-08-07 19:02:07,734 INFO misc.py line 115 22900] Train: [3/100][122/156] Data 0.001 (0.001) Batch 4.054 (3.484) Remain 14:40:42 loss: 1.3320 Lr: 0.03152
[2023-08-07 19:02:09,953 INFO misc.py line 115 22900] Train: [3/100][123/156] Data 0.001 (0.001) Batch 2.219 (3.474) Remain 14:37:58 loss: 0.7943 Lr: 0.03161
[2023-08-07 19:02:13,993 INFO misc.py line 115 22900] Train: [3/100][124/156] Data 0.001 (0.001) Batch 4.040 (3.478) Remain 14:39:06 loss: 1.6272 Lr: 0.03169
[2023-08-07 19:02:17,996 INFO misc.py line 115 22900] Train: [3/100][125/156] Data 0.001 (0.001) Batch 4.003 (3.483) Remain 14:40:08 loss: 1.1467 Lr: 0.03178
[2023-08-07 19:02:21,555 INFO misc.py line 115 22900] Train: [3/100][126/156] Data 0.001 (0.001) Batch 3.559 (3.483) Remain 14:40:14 loss: 1.1715 Lr: 0.03187
[2023-08-07 19:02:24,419 INFO misc.py line 115 22900] Train: [3/100][127/156] Data 0.001 (0.001) Batch 2.863 (3.478) Remain 14:38:54 loss: 1.0502 Lr: 0.03196
[2023-08-07 19:02:27,776 INFO misc.py line 115 22900] Train: [3/100][128/156] Data 0.001 (0.001) Batch 3.357 (3.477) Remain 14:38:36 loss: 1.1611 Lr: 0.03205
[2023-08-07 19:02:31,268 INFO misc.py line 115 22900] Train: [3/100][129/156] Data 0.001 (0.001) Batch 3.492 (3.477) Remain 14:38:34 loss: 1.4387 Lr: 0.03214
[2023-08-07 19:02:34,119 INFO misc.py line 115 22900] Train: [3/100][130/156] Data 0.001 (0.001) Batch 2.851 (3.473) Remain 14:37:16 loss: 1.2153 Lr: 0.03223
[2023-08-07 19:02:38,197 INFO misc.py line 115 22900] Train: [3/100][131/156] Data 0.001 (0.001) Batch 4.078 (3.477) Remain 14:38:24 loss: 1.3897 Lr: 0.03232
[2023-08-07 19:02:42,158 INFO misc.py line 115 22900] Train: [3/100][132/156] Data 0.001 (0.001) Batch 3.961 (3.481) Remain 14:39:18 loss: 1.1308 Lr: 0.03241
[2023-08-07 19:02:45,763 INFO misc.py line 115 22900] Train: [3/100][133/156] Data 0.001 (0.001) Batch 3.605 (3.482) Remain 14:39:29 loss: 1.0524 Lr: 0.03249
[2023-08-07 19:02:48,324 INFO misc.py line 115 22900] Train: [3/100][134/156] Data 0.001 (0.001) Batch 2.561 (3.475) Remain 14:37:39 loss: 1.2856 Lr: 0.03258
[2023-08-07 19:02:52,055 INFO misc.py line 115 22900] Train: [3/100][135/156] Data 0.001 (0.001) Batch 3.731 (3.477) Remain 14:38:05 loss: 1.0041 Lr: 0.03267
[2023-08-07 19:02:56,054 INFO misc.py line 115 22900] Train: [3/100][136/156] Data 0.001 (0.001) Batch 3.999 (3.481) Remain 14:39:01 loss: 0.8894 Lr: 0.03276
[2023-08-07 19:02:58,695 INFO misc.py line 115 22900] Train: [3/100][137/156] Data 0.001 (0.001) Batch 2.641 (3.475) Remain 14:37:22 loss: 1.3468 Lr: 0.03285
[2023-08-07 19:03:02,086 INFO misc.py line 115 22900] Train: [3/100][138/156] Data 0.001 (0.001) Batch 3.390 (3.474) Remain 14:37:09 loss: 0.8381 Lr: 0.03294
[2023-08-07 19:03:04,863 INFO misc.py line 115 22900] Train: [3/100][139/156] Data 0.001 (0.001) Batch 2.777 (3.469) Remain 14:35:48 loss: 1.2275 Lr: 0.03302
[2023-08-07 19:03:07,927 INFO misc.py line 115 22900] Train: [3/100][140/156] Data 0.001 (0.001) Batch 3.064 (3.466) Remain 14:35:00 loss: 1.2478 Lr: 0.03311
[2023-08-07 19:03:11,981 INFO misc.py line 115 22900] Train: [3/100][141/156] Data 0.001 (0.001) Batch 4.054 (3.470) Remain 14:36:01 loss: 1.4819 Lr: 0.03320
[2023-08-07 19:03:14,844 INFO misc.py line 115 22900] Train: [3/100][142/156] Data 0.001 (0.001) Batch 2.863 (3.466) Remain 14:34:52 loss: 1.1475 Lr: 0.03329
[2023-08-07 19:03:18,847 INFO misc.py line 115 22900] Train: [3/100][143/156] Data 0.001 (0.001) Batch 4.003 (3.470) Remain 14:35:46 loss: 1.0986 Lr: 0.03337
[2023-08-07 19:03:22,921 INFO misc.py line 115 22900] Train: [3/100][144/156] Data 0.001 (0.001) Batch 4.074 (3.474) Remain 14:36:48 loss: 1.4421 Lr: 0.03346
[2023-08-07 19:03:26,427 INFO misc.py line 115 22900] Train: [3/100][145/156] Data 0.001 (0.001) Batch 3.506 (3.474) Remain 14:36:48 loss: 1.6086 Lr: 0.03355
[2023-08-07 19:03:28,557 INFO misc.py line 115 22900] Train: [3/100][146/156] Data 0.001 (0.001) Batch 2.129 (3.465) Remain 14:34:22 loss: 1.7498 Lr: 0.03364
[2023-08-07 19:03:32,699 INFO misc.py line 115 22900] Train: [3/100][147/156] Data 0.001 (0.001) Batch 4.142 (3.469) Remain 14:35:30 loss: 1.5071 Lr: 0.03372
[2023-08-07 19:03:35,966 INFO misc.py line 115 22900] Train: [3/100][148/156] Data 0.001 (0.001) Batch 3.267 (3.468) Remain 14:35:05 loss: 1.2072 Lr: 0.03381
[2023-08-07 19:03:39,852 INFO misc.py line 115 22900] Train: [3/100][149/156] Data 0.001 (0.001) Batch 3.887 (3.471) Remain 14:35:45 loss: 1.0444 Lr: 0.03390
[2023-08-07 19:03:42,751 INFO misc.py line 115 22900] Train: [3/100][150/156] Data 0.001 (0.001) Batch 2.898 (3.467) Remain 14:34:42 loss: 1.3182 Lr: 0.03399
[2023-08-07 19:03:46,774 INFO misc.py line 115 22900] Train: [3/100][151/156] Data 0.001 (0.001) Batch 4.023 (3.471) Remain 14:35:36 loss: 1.3890 Lr: 0.03407
[2023-08-07 19:03:50,768 INFO misc.py line 115 22900] Train: [3/100][152/156] Data 0.001 (0.001) Batch 3.994 (3.474) Remain 14:36:26 loss: 1.1930 Lr: 0.03416
[2023-08-07 19:03:53,246 INFO misc.py line 115 22900] Train: [3/100][153/156] Data 0.001 (0.001) Batch 2.478 (3.468) Remain 14:34:42 loss: 1.2629 Lr: 0.03425
[2023-08-07 19:03:55,851 INFO misc.py line 115 22900] Train: [3/100][154/156] Data 0.001 (0.001) Batch 2.605 (3.462) Remain 14:33:12 loss: 1.2661 Lr: 0.03433
[2023-08-07 19:03:58,419 INFO misc.py line 115 22900] Train: [3/100][155/156] Data 0.001 (0.001) Batch 2.568 (3.456) Remain 14:31:39 loss: 0.8794 Lr: 0.03442
[2023-08-07 19:04:02,415 INFO misc.py line 115 22900] Train: [3/100][156/156] Data 0.001 (0.001) Batch 3.996 (3.460) Remain 14:32:29 loss: 1.6882 Lr: 0.03450
[2023-08-07 19:04:02,415 INFO misc.py line 129 22900] Train result: loss: 1.3084 
[2023-08-07 19:04:02,416 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 19:04:04,550 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.1267 
[2023-08-07 19:04:05,420 INFO evaluator.py line 122 22900] Test: [2/24] Loss 1.2466 
[2023-08-07 19:04:07,084 INFO evaluator.py line 122 22900] Test: [3/24] Loss 1.3833 
[2023-08-07 19:04:08,608 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.2906 
[2023-08-07 19:04:10,454 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.8779 
[2023-08-07 19:04:12,117 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.8391 
[2023-08-07 19:04:14,253 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.9404 
[2023-08-07 19:04:16,059 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.1195 
[2023-08-07 19:04:17,342 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.3794 
[2023-08-07 19:04:19,471 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3621 
[2023-08-07 19:04:19,996 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.9865 
[2023-08-07 19:04:21,528 INFO evaluator.py line 122 22900] Test: [12/24] Loss 1.1279 
[2023-08-07 19:04:24,249 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.1167 
[2023-08-07 19:04:25,930 INFO evaluator.py line 122 22900] Test: [14/24] Loss 1.2818 
[2023-08-07 19:04:27,952 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.6576 
[2023-08-07 19:04:30,663 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.4519 
[2023-08-07 19:04:33,367 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.2389 
[2023-08-07 19:04:35,214 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.6416 
[2023-08-07 19:04:35,964 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.4028 
[2023-08-07 19:04:36,848 INFO evaluator.py line 122 22900] Test: [20/24] Loss 2.2825 
[2023-08-07 19:04:39,110 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.4393 
[2023-08-07 19:04:41,077 INFO evaluator.py line 122 22900] Test: [22/24] Loss 2.0367 
[2023-08-07 19:04:42,924 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.3962 
[2023-08-07 19:04:44,860 INFO evaluator.py line 122 22900] Test: [24/24] Loss 2.3653 
[2023-08-07 19:04:44,910 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.1166/0.1758/0.6142.
[2023-08-07 19:04:44,911 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.5971/0.9242
[2023-08-07 19:04:44,911 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.8932/0.9549
[2023-08-07 19:04:44,911 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.0030/0.0033
[2023-08-07 19:04:44,911 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.0400/0.0954
[2023-08-07 19:04:44,911 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.3305/0.7728
[2023-08-07 19:04:44,911 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:04:44,911 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.3282/0.5590
[2023-08-07 19:04:44,911 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0469/0.0493
[2023-08-07 19:04:44,911 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.0848/0.1480
[2023-08-07 19:04:44,911 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0002/0.0002
[2023-08-07 19:04:44,911 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:04:44,911 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:04:44,911 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:04:44,911 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:04:44,911 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:04:44,911 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:04:44,911 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:04:44,911 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.0072/0.0084
[2023-08-07 19:04:44,911 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:04:44,911 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:04:44,911 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 19:04:44,911 INFO misc.py line 150 22900] Best validation mIoU updated to: 0.1166
[2023-08-07 19:04:44,912 INFO misc.py line 152 22900] Currently Best mIoU: 0.1166
[2023-08-07 19:04:44,912 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 19:04:51,019 INFO misc.py line 115 22900] Train: [4/100][1/156] Data 1.246 (1.246) Batch 5.046 (5.046) Remain 21:12:38 loss: 1.2769 Lr: 0.03459
[2023-08-07 19:04:54,128 INFO misc.py line 115 22900] Train: [4/100][2/156] Data 0.001 (0.001) Batch 3.109 (3.109) Remain 13:03:53 loss: 1.6528 Lr: 0.03468
[2023-08-07 19:04:57,639 INFO misc.py line 115 22900] Train: [4/100][3/156] Data 0.001 (0.001) Batch 3.511 (3.511) Remain 14:45:20 loss: 1.3022 Lr: 0.03476
[2023-08-07 19:05:00,723 INFO misc.py line 115 22900] Train: [4/100][4/156] Data 0.001 (0.001) Batch 3.084 (3.084) Remain 12:57:33 loss: 1.2231 Lr: 0.03485
[2023-08-07 19:05:04,752 INFO misc.py line 115 22900] Train: [4/100][5/156] Data 0.001 (0.001) Batch 4.029 (3.556) Remain 14:56:36 loss: 1.4034 Lr: 0.03493
[2023-08-07 19:05:08,791 INFO misc.py line 115 22900] Train: [4/100][6/156] Data 0.001 (0.001) Batch 4.039 (3.717) Remain 15:37:05 loss: 1.3284 Lr: 0.03502
[2023-08-07 19:05:11,898 INFO misc.py line 115 22900] Train: [4/100][7/156] Data 0.001 (0.001) Batch 3.107 (3.565) Remain 14:58:34 loss: 0.9781 Lr: 0.03511
[2023-08-07 19:05:14,818 INFO misc.py line 115 22900] Train: [4/100][8/156] Data 0.001 (0.001) Batch 2.921 (3.436) Remain 14:26:03 loss: 1.0752 Lr: 0.03519
[2023-08-07 19:05:17,634 INFO misc.py line 115 22900] Train: [4/100][9/156] Data 0.001 (0.001) Batch 2.816 (3.332) Remain 13:59:57 loss: 1.6503 Lr: 0.03528
[2023-08-07 19:05:21,716 INFO misc.py line 115 22900] Train: [4/100][10/156] Data 0.001 (0.001) Batch 4.082 (3.439) Remain 14:26:52 loss: 1.0274 Lr: 0.03536
[2023-08-07 19:05:25,789 INFO misc.py line 115 22900] Train: [4/100][11/156] Data 0.001 (0.001) Batch 4.074 (3.519) Remain 14:46:47 loss: 1.2002 Lr: 0.03545
[2023-08-07 19:05:28,464 INFO misc.py line 115 22900] Train: [4/100][12/156] Data 0.001 (0.001) Batch 2.675 (3.425) Remain 14:23:05 loss: 1.0881 Lr: 0.03553
[2023-08-07 19:05:32,142 INFO misc.py line 115 22900] Train: [4/100][13/156] Data 0.001 (0.001) Batch 3.678 (3.450) Remain 14:29:24 loss: 0.8844 Lr: 0.03562
[2023-08-07 19:05:36,126 INFO misc.py line 115 22900] Train: [4/100][14/156] Data 0.001 (0.001) Batch 3.984 (3.499) Remain 14:41:34 loss: 1.4936 Lr: 0.03570
[2023-08-07 19:05:39,588 INFO misc.py line 115 22900] Train: [4/100][15/156] Data 0.001 (0.001) Batch 3.463 (3.496) Remain 14:40:45 loss: 1.6056 Lr: 0.03578
[2023-08-07 19:05:42,980 INFO misc.py line 115 22900] Train: [4/100][16/156] Data 0.001 (0.001) Batch 3.392 (3.488) Remain 14:38:41 loss: 1.3092 Lr: 0.03587
[2023-08-07 19:05:45,447 INFO misc.py line 115 22900] Train: [4/100][17/156] Data 0.001 (0.001) Batch 2.467 (3.415) Remain 14:20:15 loss: 1.2126 Lr: 0.03595
[2023-08-07 19:05:48,863 INFO misc.py line 115 22900] Train: [4/100][18/156] Data 0.001 (0.001) Batch 3.416 (3.415) Remain 14:20:13 loss: 1.5094 Lr: 0.03604
[2023-08-07 19:05:51,818 INFO misc.py line 115 22900] Train: [4/100][19/156] Data 0.001 (0.001) Batch 2.955 (3.386) Remain 14:12:54 loss: 0.9645 Lr: 0.03612
[2023-08-07 19:05:55,805 INFO misc.py line 115 22900] Train: [4/100][20/156] Data 0.001 (0.001) Batch 3.987 (3.422) Remain 14:21:45 loss: 1.4224 Lr: 0.03620
[2023-08-07 19:05:59,186 INFO misc.py line 115 22900] Train: [4/100][21/156] Data 0.001 (0.001) Batch 3.381 (3.419) Remain 14:21:08 loss: 1.1946 Lr: 0.03629
[2023-08-07 19:06:02,052 INFO misc.py line 115 22900] Train: [4/100][22/156] Data 0.001 (0.001) Batch 2.866 (3.390) Remain 14:13:45 loss: 1.1392 Lr: 0.03637
[2023-08-07 19:06:05,646 INFO misc.py line 115 22900] Train: [4/100][23/156] Data 0.001 (0.001) Batch 3.593 (3.400) Remain 14:16:15 loss: 1.4336 Lr: 0.03645
[2023-08-07 19:06:08,885 INFO misc.py line 115 22900] Train: [4/100][24/156] Data 0.001 (0.001) Batch 3.239 (3.393) Remain 14:14:15 loss: 1.2693 Lr: 0.03654
[2023-08-07 19:06:12,237 INFO misc.py line 115 22900] Train: [4/100][25/156] Data 0.001 (0.001) Batch 3.353 (3.391) Remain 14:13:45 loss: 1.4171 Lr: 0.03662
[2023-08-07 19:06:16,278 INFO misc.py line 115 22900] Train: [4/100][26/156] Data 0.001 (0.001) Batch 4.041 (3.419) Remain 14:20:48 loss: 1.2737 Lr: 0.03670
[2023-08-07 19:06:20,318 INFO misc.py line 115 22900] Train: [4/100][27/156] Data 0.001 (0.001) Batch 4.039 (3.445) Remain 14:27:15 loss: 1.6157 Lr: 0.03679
[2023-08-07 19:06:23,030 INFO misc.py line 115 22900] Train: [4/100][28/156] Data 0.001 (0.001) Batch 2.712 (3.416) Remain 14:19:49 loss: 1.5530 Lr: 0.03687
[2023-08-07 19:06:26,562 INFO misc.py line 115 22900] Train: [4/100][29/156] Data 0.001 (0.001) Batch 3.532 (3.420) Remain 14:20:53 loss: 1.1168 Lr: 0.03695
[2023-08-07 19:06:30,230 INFO misc.py line 115 22900] Train: [4/100][30/156] Data 0.001 (0.001) Batch 3.668 (3.429) Remain 14:23:09 loss: 0.9420 Lr: 0.03703
[2023-08-07 19:06:33,735 INFO misc.py line 115 22900] Train: [4/100][31/156] Data 0.001 (0.001) Batch 3.505 (3.432) Remain 14:23:46 loss: 1.2500 Lr: 0.03712
[2023-08-07 19:06:36,838 INFO misc.py line 115 22900] Train: [4/100][32/156] Data 0.001 (0.001) Batch 3.102 (3.421) Remain 14:20:51 loss: 1.0045 Lr: 0.03720
[2023-08-07 19:06:40,171 INFO misc.py line 115 22900] Train: [4/100][33/156] Data 0.001 (0.001) Batch 3.333 (3.418) Remain 14:20:04 loss: 1.0586 Lr: 0.03728
[2023-08-07 19:06:43,622 INFO misc.py line 115 22900] Train: [4/100][34/156] Data 0.001 (0.001) Batch 3.451 (3.419) Remain 14:20:16 loss: 0.9679 Lr: 0.03736
[2023-08-07 19:06:46,186 INFO misc.py line 115 22900] Train: [4/100][35/156] Data 0.001 (0.001) Batch 2.564 (3.392) Remain 14:13:30 loss: 1.3676 Lr: 0.03744
[2023-08-07 19:06:49,825 INFO misc.py line 115 22900] Train: [4/100][36/156] Data 0.001 (0.001) Batch 3.639 (3.400) Remain 14:15:19 loss: 0.8741 Lr: 0.03752
[2023-08-07 19:06:53,931 INFO misc.py line 115 22900] Train: [4/100][37/156] Data 0.001 (0.001) Batch 4.107 (3.420) Remain 14:20:30 loss: 1.1125 Lr: 0.03761
[2023-08-07 19:06:56,539 INFO misc.py line 115 22900] Train: [4/100][38/156] Data 0.001 (0.001) Batch 2.608 (3.397) Remain 14:14:36 loss: 1.5924 Lr: 0.03769
[2023-08-07 19:06:58,845 INFO misc.py line 115 22900] Train: [4/100][39/156] Data 0.001 (0.001) Batch 2.305 (3.367) Remain 14:06:55 loss: 0.6867 Lr: 0.03777
[2023-08-07 19:07:02,458 INFO misc.py line 115 22900] Train: [4/100][40/156] Data 0.001 (0.001) Batch 3.613 (3.373) Remain 14:08:32 loss: 1.1327 Lr: 0.03785
[2023-08-07 19:07:06,798 INFO misc.py line 115 22900] Train: [4/100][41/156] Data 0.001 (0.001) Batch 4.340 (3.399) Remain 14:14:52 loss: 1.6536 Lr: 0.03793
[2023-08-07 19:07:10,424 INFO misc.py line 115 22900] Train: [4/100][42/156] Data 0.001 (0.001) Batch 3.626 (3.405) Remain 14:16:17 loss: 1.3070 Lr: 0.03801
[2023-08-07 19:07:14,367 INFO misc.py line 115 22900] Train: [4/100][43/156] Data 0.001 (0.001) Batch 3.943 (3.418) Remain 14:19:37 loss: 1.3562 Lr: 0.03809
[2023-08-07 19:07:17,660 INFO misc.py line 115 22900] Train: [4/100][44/156] Data 0.001 (0.001) Batch 3.293 (3.415) Remain 14:18:47 loss: 1.0971 Lr: 0.03817
[2023-08-07 19:07:21,415 INFO misc.py line 115 22900] Train: [4/100][45/156] Data 0.001 (0.001) Batch 3.755 (3.423) Remain 14:20:46 loss: 1.5597 Lr: 0.03825
[2023-08-07 19:07:25,872 INFO misc.py line 115 22900] Train: [4/100][46/156] Data 0.001 (0.001) Batch 4.458 (3.447) Remain 14:26:45 loss: 1.6998 Lr: 0.03833
[2023-08-07 19:07:29,204 INFO misc.py line 115 22900] Train: [4/100][47/156] Data 0.001 (0.001) Batch 3.332 (3.445) Remain 14:26:02 loss: 1.3207 Lr: 0.03841
[2023-08-07 19:07:33,055 INFO misc.py line 115 22900] Train: [4/100][48/156] Data 0.001 (0.001) Batch 3.851 (3.454) Remain 14:28:15 loss: 1.1511 Lr: 0.03849
[2023-08-07 19:07:36,278 INFO misc.py line 115 22900] Train: [4/100][49/156] Data 0.001 (0.001) Batch 3.222 (3.449) Remain 14:26:56 loss: 1.1499 Lr: 0.03857
[2023-08-07 19:07:39,495 INFO misc.py line 115 22900] Train: [4/100][50/156] Data 0.001 (0.001) Batch 3.217 (3.444) Remain 14:25:38 loss: 1.1984 Lr: 0.03865
[2023-08-07 19:07:43,059 INFO misc.py line 115 22900] Train: [4/100][51/156] Data 0.001 (0.001) Batch 3.564 (3.446) Remain 14:26:12 loss: 1.1860 Lr: 0.03872
[2023-08-07 19:07:46,311 INFO misc.py line 115 22900] Train: [4/100][52/156] Data 0.001 (0.001) Batch 3.251 (3.442) Remain 14:25:09 loss: 0.7830 Lr: 0.03880
[2023-08-07 19:07:49,644 INFO misc.py line 115 22900] Train: [4/100][53/156] Data 0.001 (0.001) Batch 3.333 (3.440) Remain 14:24:33 loss: 1.0350 Lr: 0.03888
[2023-08-07 19:07:52,291 INFO misc.py line 115 22900] Train: [4/100][54/156] Data 0.001 (0.001) Batch 2.647 (3.425) Remain 14:20:35 loss: 1.0855 Lr: 0.03896
[2023-08-07 19:07:54,941 INFO misc.py line 115 22900] Train: [4/100][55/156] Data 0.001 (0.001) Batch 2.651 (3.410) Remain 14:16:47 loss: 1.5676 Lr: 0.03904
[2023-08-07 19:07:58,464 INFO misc.py line 115 22900] Train: [4/100][56/156] Data 0.001 (0.001) Batch 3.522 (3.412) Remain 14:17:15 loss: 1.6420 Lr: 0.03911
[2023-08-07 19:08:01,563 INFO misc.py line 115 22900] Train: [4/100][57/156] Data 0.001 (0.001) Batch 3.100 (3.406) Remain 14:15:45 loss: 1.4024 Lr: 0.03919
[2023-08-07 19:08:05,246 INFO misc.py line 115 22900] Train: [4/100][58/156] Data 0.001 (0.001) Batch 3.683 (3.411) Remain 14:16:57 loss: 0.8929 Lr: 0.03927
[2023-08-07 19:08:08,141 INFO misc.py line 115 22900] Train: [4/100][59/156] Data 0.001 (0.001) Batch 2.895 (3.402) Remain 14:14:35 loss: 0.8933 Lr: 0.03935
[2023-08-07 19:08:12,121 INFO misc.py line 115 22900] Train: [4/100][60/156] Data 0.001 (0.001) Batch 3.980 (3.412) Remain 14:17:05 loss: 1.2173 Lr: 0.03942
[2023-08-07 19:08:16,155 INFO misc.py line 115 22900] Train: [4/100][61/156] Data 0.001 (0.001) Batch 4.033 (3.423) Remain 14:19:43 loss: 1.4854 Lr: 0.03950
[2023-08-07 19:08:18,687 INFO misc.py line 115 22900] Train: [4/100][62/156] Data 0.001 (0.001) Batch 2.532 (3.408) Remain 14:15:52 loss: 0.9359 Lr: 0.03958
[2023-08-07 19:08:21,684 INFO misc.py line 115 22900] Train: [4/100][63/156] Data 0.001 (0.001) Batch 2.997 (3.401) Remain 14:14:05 loss: 1.1501 Lr: 0.03965
[2023-08-07 19:08:25,177 INFO misc.py line 115 22900] Train: [4/100][64/156] Data 0.001 (0.001) Batch 3.492 (3.402) Remain 14:14:25 loss: 0.9646 Lr: 0.03973
[2023-08-07 19:08:28,618 INFO misc.py line 115 22900] Train: [4/100][65/156] Data 0.001 (0.001) Batch 3.441 (3.403) Remain 14:14:31 loss: 0.8882 Lr: 0.03981
[2023-08-07 19:08:32,462 INFO misc.py line 115 22900] Train: [4/100][66/156] Data 0.001 (0.001) Batch 3.844 (3.410) Remain 14:16:13 loss: 1.1762 Lr: 0.03988
[2023-08-07 19:08:36,631 INFO misc.py line 115 22900] Train: [4/100][67/156] Data 0.001 (0.001) Batch 4.169 (3.422) Remain 14:19:08 loss: 1.3055 Lr: 0.03996
[2023-08-07 19:08:40,446 INFO misc.py line 115 22900] Train: [4/100][68/156] Data 0.001 (0.001) Batch 3.814 (3.428) Remain 14:20:36 loss: 1.1761 Lr: 0.04003
[2023-08-07 19:08:44,517 INFO misc.py line 115 22900] Train: [4/100][69/156] Data 0.001 (0.001) Batch 4.072 (3.438) Remain 14:22:59 loss: 1.0830 Lr: 0.04011
[2023-08-07 19:08:48,164 INFO misc.py line 115 22900] Train: [4/100][70/156] Data 0.001 (0.001) Batch 3.647 (3.441) Remain 14:23:43 loss: 1.4009 Lr: 0.04018
[2023-08-07 19:08:50,012 INFO misc.py line 115 22900] Train: [4/100][71/156] Data 0.001 (0.001) Batch 1.848 (3.417) Remain 14:17:47 loss: 1.2356 Lr: 0.04026
[2023-08-07 19:08:54,270 INFO misc.py line 115 22900] Train: [4/100][72/156] Data 0.001 (0.001) Batch 4.259 (3.429) Remain 14:20:47 loss: 1.6259 Lr: 0.04033
[2023-08-07 19:08:58,699 INFO misc.py line 115 22900] Train: [4/100][73/156] Data 0.001 (0.001) Batch 4.429 (3.444) Remain 14:24:18 loss: 1.5278 Lr: 0.04041
[2023-08-07 19:09:01,256 INFO misc.py line 115 22900] Train: [4/100][74/156] Data 0.001 (0.001) Batch 2.557 (3.431) Remain 14:21:07 loss: 0.9523 Lr: 0.04048
[2023-08-07 19:09:03,119 INFO misc.py line 115 22900] Train: [4/100][75/156] Data 0.001 (0.001) Batch 1.862 (3.409) Remain 14:15:35 loss: 0.9105 Lr: 0.04056
[2023-08-07 19:09:06,581 INFO misc.py line 115 22900] Train: [4/100][76/156] Data 0.001 (0.001) Batch 3.463 (3.410) Remain 14:15:43 loss: 1.5102 Lr: 0.04063
[2023-08-07 19:09:09,793 INFO misc.py line 115 22900] Train: [4/100][77/156] Data 0.001 (0.001) Batch 3.212 (3.407) Remain 14:14:59 loss: 1.1401 Lr: 0.04070
[2023-08-07 19:09:13,311 INFO misc.py line 115 22900] Train: [4/100][78/156] Data 0.001 (0.001) Batch 3.517 (3.409) Remain 14:15:18 loss: 1.4745 Lr: 0.04078
[2023-08-07 19:09:17,603 INFO misc.py line 115 22900] Train: [4/100][79/156] Data 0.001 (0.001) Batch 4.292 (3.421) Remain 14:18:09 loss: 1.4018 Lr: 0.04085
[2023-08-07 19:09:21,944 INFO misc.py line 115 22900] Train: [4/100][80/156] Data 0.001 (0.001) Batch 4.341 (3.433) Remain 14:21:06 loss: 1.3770 Lr: 0.04092
[2023-08-07 19:09:25,238 INFO misc.py line 115 22900] Train: [4/100][81/156] Data 0.001 (0.001) Batch 3.294 (3.431) Remain 14:20:36 loss: 1.0816 Lr: 0.04100
[2023-08-07 19:09:27,864 INFO misc.py line 115 22900] Train: [4/100][82/156] Data 0.001 (0.001) Batch 2.626 (3.421) Remain 14:17:59 loss: 1.3121 Lr: 0.04107
[2023-08-07 19:09:31,450 INFO misc.py line 115 22900] Train: [4/100][83/156] Data 0.001 (0.001) Batch 3.586 (3.423) Remain 14:18:27 loss: 1.1688 Lr: 0.04114
[2023-08-07 19:09:35,245 INFO misc.py line 115 22900] Train: [4/100][84/156] Data 0.001 (0.001) Batch 3.795 (3.427) Remain 14:19:32 loss: 1.0413 Lr: 0.04121
[2023-08-07 19:09:39,328 INFO misc.py line 115 22900] Train: [4/100][85/156] Data 0.001 (0.001) Batch 4.082 (3.435) Remain 14:21:29 loss: 1.1125 Lr: 0.04128
[2023-08-07 19:09:42,233 INFO misc.py line 115 22900] Train: [4/100][86/156] Data 0.001 (0.001) Batch 2.906 (3.429) Remain 14:19:50 loss: 1.2205 Lr: 0.04136
[2023-08-07 19:09:46,224 INFO misc.py line 115 22900] Train: [4/100][87/156] Data 0.001 (0.001) Batch 3.991 (3.436) Remain 14:21:27 loss: 0.9546 Lr: 0.04143
[2023-08-07 19:09:49,848 INFO misc.py line 115 22900] Train: [4/100][88/156] Data 0.002 (0.001) Batch 3.624 (3.438) Remain 14:21:57 loss: 1.0805 Lr: 0.04150
[2023-08-07 19:09:53,025 INFO misc.py line 115 22900] Train: [4/100][89/156] Data 0.002 (0.001) Batch 3.177 (3.435) Remain 14:21:08 loss: 0.9989 Lr: 0.04157
[2023-08-07 19:09:57,369 INFO misc.py line 115 22900] Train: [4/100][90/156] Data 0.001 (0.001) Batch 4.344 (3.445) Remain 14:23:42 loss: 1.2196 Lr: 0.04164
[2023-08-07 19:10:01,417 INFO misc.py line 115 22900] Train: [4/100][91/156] Data 0.001 (0.001) Batch 4.048 (3.452) Remain 14:25:21 loss: 1.4093 Lr: 0.04171
[2023-08-07 19:10:04,318 INFO misc.py line 115 22900] Train: [4/100][92/156] Data 0.001 (0.001) Batch 2.901 (3.446) Remain 14:23:45 loss: 0.9346 Lr: 0.04178
[2023-08-07 19:10:06,890 INFO misc.py line 115 22900] Train: [4/100][93/156] Data 0.001 (0.001) Batch 2.571 (3.436) Remain 14:21:15 loss: 0.8511 Lr: 0.04185
[2023-08-07 19:10:11,002 INFO misc.py line 115 22900] Train: [4/100][94/156] Data 0.001 (0.001) Batch 4.112 (3.444) Remain 14:23:03 loss: 1.5963 Lr: 0.04192
[2023-08-07 19:10:13,620 INFO misc.py line 115 22900] Train: [4/100][95/156] Data 0.001 (0.001) Batch 2.619 (3.435) Remain 14:20:45 loss: 1.2169 Lr: 0.04199
[2023-08-07 19:10:16,383 INFO misc.py line 115 22900] Train: [4/100][96/156] Data 0.001 (0.001) Batch 2.762 (3.427) Remain 14:18:53 loss: 0.9763 Lr: 0.04206
[2023-08-07 19:10:20,552 INFO misc.py line 115 22900] Train: [4/100][97/156] Data 0.001 (0.001) Batch 4.170 (3.435) Remain 14:20:48 loss: 1.5094 Lr: 0.04213
[2023-08-07 19:10:23,402 INFO misc.py line 115 22900] Train: [4/100][98/156] Data 0.001 (0.001) Batch 2.850 (3.429) Remain 14:19:12 loss: 1.4291 Lr: 0.04220
[2023-08-07 19:10:25,661 INFO misc.py line 115 22900] Train: [4/100][99/156] Data 0.001 (0.001) Batch 2.259 (3.417) Remain 14:16:06 loss: 1.2284 Lr: 0.04227
[2023-08-07 19:10:28,488 INFO misc.py line 115 22900] Train: [4/100][100/156] Data 0.001 (0.001) Batch 2.827 (3.411) Remain 14:14:31 loss: 0.9840 Lr: 0.04233
[2023-08-07 19:10:31,705 INFO misc.py line 115 22900] Train: [4/100][101/156] Data 0.001 (0.001) Batch 3.217 (3.409) Remain 14:13:58 loss: 0.9335 Lr: 0.04240
[2023-08-07 19:10:35,693 INFO misc.py line 115 22900] Train: [4/100][102/156] Data 0.001 (0.001) Batch 3.988 (3.415) Remain 14:15:22 loss: 1.5479 Lr: 0.04247
[2023-08-07 19:10:38,732 INFO misc.py line 115 22900] Train: [4/100][103/156] Data 0.001 (0.001) Batch 3.038 (3.411) Remain 14:14:22 loss: 0.7843 Lr: 0.04254
[2023-08-07 19:10:41,405 INFO misc.py line 115 22900] Train: [4/100][104/156] Data 0.001 (0.001) Batch 2.673 (3.404) Remain 14:12:29 loss: 1.3584 Lr: 0.04260
[2023-08-07 19:10:45,818 INFO misc.py line 115 22900] Train: [4/100][105/156] Data 0.001 (0.001) Batch 4.413 (3.414) Remain 14:14:54 loss: 1.3164 Lr: 0.04267
[2023-08-07 19:10:49,035 INFO misc.py line 115 22900] Train: [4/100][106/156] Data 0.001 (0.001) Batch 3.217 (3.412) Remain 14:14:22 loss: 0.9644 Lr: 0.04274
[2023-08-07 19:10:52,343 INFO misc.py line 115 22900] Train: [4/100][107/156] Data 0.001 (0.001) Batch 3.308 (3.411) Remain 14:14:04 loss: 0.9929 Lr: 0.04281
[2023-08-07 19:10:56,081 INFO misc.py line 115 22900] Train: [4/100][108/156] Data 0.001 (0.001) Batch 3.738 (3.414) Remain 14:14:47 loss: 1.2023 Lr: 0.04287
[2023-08-07 19:11:00,173 INFO misc.py line 115 22900] Train: [4/100][109/156] Data 0.001 (0.001) Batch 4.092 (3.420) Remain 14:16:20 loss: 1.2240 Lr: 0.04294
[2023-08-07 19:11:03,275 INFO misc.py line 115 22900] Train: [4/100][110/156] Data 0.001 (0.001) Batch 3.101 (3.417) Remain 14:15:32 loss: 1.1665 Lr: 0.04300
[2023-08-07 19:11:06,999 INFO misc.py line 115 22900] Train: [4/100][111/156] Data 0.001 (0.001) Batch 3.724 (3.420) Remain 14:16:11 loss: 1.2559 Lr: 0.04307
[2023-08-07 19:11:10,035 INFO misc.py line 115 22900] Train: [4/100][112/156] Data 0.001 (0.001) Batch 3.036 (3.416) Remain 14:15:15 loss: 1.5577 Lr: 0.04313
[2023-08-07 19:11:14,221 INFO misc.py line 115 22900] Train: [4/100][113/156] Data 0.001 (0.001) Batch 4.187 (3.423) Remain 14:16:57 loss: 1.1377 Lr: 0.04320
[2023-08-07 19:11:16,665 INFO misc.py line 115 22900] Train: [4/100][114/156] Data 0.001 (0.001) Batch 2.444 (3.415) Remain 14:14:41 loss: 1.1948 Lr: 0.04326
[2023-08-07 19:11:20,686 INFO misc.py line 115 22900] Train: [4/100][115/156] Data 0.001 (0.001) Batch 4.020 (3.420) Remain 14:15:58 loss: 1.4408 Lr: 0.04333
[2023-08-07 19:11:24,245 INFO misc.py line 115 22900] Train: [4/100][116/156] Data 0.001 (0.001) Batch 3.560 (3.421) Remain 14:16:14 loss: 1.1017 Lr: 0.04339
[2023-08-07 19:11:27,670 INFO misc.py line 115 22900] Train: [4/100][117/156] Data 0.001 (0.001) Batch 3.425 (3.421) Remain 14:16:11 loss: 1.1801 Lr: 0.04346
[2023-08-07 19:11:30,829 INFO misc.py line 115 22900] Train: [4/100][118/156] Data 0.001 (0.001) Batch 3.159 (3.419) Remain 14:15:33 loss: 0.8989 Lr: 0.04352
[2023-08-07 19:11:34,488 INFO misc.py line 115 22900] Train: [4/100][119/156] Data 0.001 (0.001) Batch 3.659 (3.421) Remain 14:16:01 loss: 1.1005 Lr: 0.04359
[2023-08-07 19:11:38,780 INFO misc.py line 115 22900] Train: [4/100][120/156] Data 0.001 (0.001) Batch 4.291 (3.429) Remain 14:17:49 loss: 1.5927 Lr: 0.04365
[2023-08-07 19:11:42,021 INFO misc.py line 115 22900] Train: [4/100][121/156] Data 0.001 (0.001) Batch 3.242 (3.427) Remain 14:17:22 loss: 1.3903 Lr: 0.04371
[2023-08-07 19:11:46,012 INFO misc.py line 115 22900] Train: [4/100][122/156] Data 0.001 (0.001) Batch 3.991 (3.432) Remain 14:18:29 loss: 1.1212 Lr: 0.04377
[2023-08-07 19:11:50,054 INFO misc.py line 115 22900] Train: [4/100][123/156] Data 0.001 (0.001) Batch 4.041 (3.437) Remain 14:19:42 loss: 1.2155 Lr: 0.04384
[2023-08-07 19:11:53,734 INFO misc.py line 115 22900] Train: [4/100][124/156] Data 0.001 (0.001) Batch 3.680 (3.439) Remain 14:20:09 loss: 1.1154 Lr: 0.04390
[2023-08-07 19:11:57,393 INFO misc.py line 115 22900] Train: [4/100][125/156] Data 0.001 (0.001) Batch 3.659 (3.441) Remain 14:20:33 loss: 1.0332 Lr: 0.04396
[2023-08-07 19:12:00,685 INFO misc.py line 115 22900] Train: [4/100][126/156] Data 0.001 (0.001) Batch 3.292 (3.439) Remain 14:20:11 loss: 1.2972 Lr: 0.04402
[2023-08-07 19:12:04,330 INFO misc.py line 115 22900] Train: [4/100][127/156] Data 0.001 (0.001) Batch 3.645 (3.441) Remain 14:20:32 loss: 0.7938 Lr: 0.04408
[2023-08-07 19:12:08,842 INFO misc.py line 115 22900] Train: [4/100][128/156] Data 0.001 (0.001) Batch 4.513 (3.450) Remain 14:22:38 loss: 1.5694 Lr: 0.04415
[2023-08-07 19:12:11,259 INFO misc.py line 115 22900] Train: [4/100][129/156] Data 0.001 (0.001) Batch 2.417 (3.441) Remain 14:20:31 loss: 1.0094 Lr: 0.04421
[2023-08-07 19:12:13,456 INFO misc.py line 115 22900] Train: [4/100][130/156] Data 0.001 (0.001) Batch 2.197 (3.432) Remain 14:18:01 loss: 0.9786 Lr: 0.04427
[2023-08-07 19:12:17,461 INFO misc.py line 115 22900] Train: [4/100][131/156] Data 0.001 (0.001) Batch 4.005 (3.436) Remain 14:19:04 loss: 1.3745 Lr: 0.04433
[2023-08-07 19:12:21,615 INFO misc.py line 115 22900] Train: [4/100][132/156] Data 0.001 (0.001) Batch 4.155 (3.442) Remain 14:20:25 loss: 1.3852 Lr: 0.04439
[2023-08-07 19:12:24,068 INFO misc.py line 115 22900] Train: [4/100][133/156] Data 0.001 (0.001) Batch 2.452 (3.434) Remain 14:18:27 loss: 0.8565 Lr: 0.04445
[2023-08-07 19:12:27,813 INFO misc.py line 115 22900] Train: [4/100][134/156] Data 0.001 (0.001) Batch 3.745 (3.436) Remain 14:18:59 loss: 1.2328 Lr: 0.04451
[2023-08-07 19:12:31,967 INFO misc.py line 115 22900] Train: [4/100][135/156] Data 0.001 (0.001) Batch 4.154 (3.442) Remain 14:20:17 loss: 1.3128 Lr: 0.04457
[2023-08-07 19:12:35,697 INFO misc.py line 115 22900] Train: [4/100][136/156] Data 0.001 (0.001) Batch 3.731 (3.444) Remain 14:20:46 loss: 1.1190 Lr: 0.04463
[2023-08-07 19:12:39,108 INFO misc.py line 115 22900] Train: [4/100][137/156] Data 0.001 (0.001) Batch 3.411 (3.444) Remain 14:20:39 loss: 1.0350 Lr: 0.04468
[2023-08-07 19:12:42,596 INFO misc.py line 115 22900] Train: [4/100][138/156] Data 0.001 (0.001) Batch 3.487 (3.444) Remain 14:20:41 loss: 1.0609 Lr: 0.04474
[2023-08-07 19:12:46,494 INFO misc.py line 115 22900] Train: [4/100][139/156] Data 0.001 (0.001) Batch 3.899 (3.447) Remain 14:21:27 loss: 1.6488 Lr: 0.04480
[2023-08-07 19:12:49,084 INFO misc.py line 115 22900] Train: [4/100][140/156] Data 0.001 (0.001) Batch 2.590 (3.441) Remain 14:19:50 loss: 0.7266 Lr: 0.04486
[2023-08-07 19:12:53,167 INFO misc.py line 115 22900] Train: [4/100][141/156] Data 0.001 (0.001) Batch 4.083 (3.446) Remain 14:20:56 loss: 1.4205 Lr: 0.04492
[2023-08-07 19:12:56,564 INFO misc.py line 115 22900] Train: [4/100][142/156] Data 0.001 (0.001) Batch 3.397 (3.446) Remain 14:20:48 loss: 0.8443 Lr: 0.04497
[2023-08-07 19:13:00,082 INFO misc.py line 115 22900] Train: [4/100][143/156] Data 0.001 (0.001) Batch 3.518 (3.446) Remain 14:20:52 loss: 1.5541 Lr: 0.04503
[2023-08-07 19:13:03,253 INFO misc.py line 115 22900] Train: [4/100][144/156] Data 0.001 (0.001) Batch 3.170 (3.444) Remain 14:20:19 loss: 1.1311 Lr: 0.04509
[2023-08-07 19:13:07,336 INFO misc.py line 115 22900] Train: [4/100][145/156] Data 0.001 (0.001) Batch 4.083 (3.449) Remain 14:21:23 loss: 1.0353 Lr: 0.04514
[2023-08-07 19:13:10,355 INFO misc.py line 115 22900] Train: [4/100][146/156] Data 0.001 (0.001) Batch 3.019 (3.446) Remain 14:20:35 loss: 1.1650 Lr: 0.04520
[2023-08-07 19:13:13,489 INFO misc.py line 115 22900] Train: [4/100][147/156] Data 0.001 (0.001) Batch 3.134 (3.443) Remain 14:19:59 loss: 0.9414 Lr: 0.04526
[2023-08-07 19:13:16,159 INFO misc.py line 115 22900] Train: [4/100][148/156] Data 0.001 (0.001) Batch 2.671 (3.438) Remain 14:18:36 loss: 0.8950 Lr: 0.04531
[2023-08-07 19:13:19,620 INFO misc.py line 115 22900] Train: [4/100][149/156] Data 0.001 (0.001) Batch 3.461 (3.438) Remain 14:18:34 loss: 0.8870 Lr: 0.04537
[2023-08-07 19:13:22,402 INFO misc.py line 115 22900] Train: [4/100][150/156] Data 0.001 (0.001) Batch 2.782 (3.434) Remain 14:17:24 loss: 1.0075 Lr: 0.04542
[2023-08-07 19:13:25,908 INFO misc.py line 115 22900] Train: [4/100][151/156] Data 0.001 (0.001) Batch 3.505 (3.434) Remain 14:17:28 loss: 1.7871 Lr: 0.04548
[2023-08-07 19:13:30,223 INFO misc.py line 115 22900] Train: [4/100][152/156] Data 0.001 (0.001) Batch 4.316 (3.440) Remain 14:18:53 loss: 1.5670 Lr: 0.04553
[2023-08-07 19:13:33,614 INFO misc.py line 115 22900] Train: [4/100][153/156] Data 0.001 (0.001) Batch 3.391 (3.440) Remain 14:18:45 loss: 0.7554 Lr: 0.04558
[2023-08-07 19:13:36,978 INFO misc.py line 115 22900] Train: [4/100][154/156] Data 0.001 (0.001) Batch 3.363 (3.439) Remain 14:18:34 loss: 1.3637 Lr: 0.04564
[2023-08-07 19:13:40,591 INFO misc.py line 115 22900] Train: [4/100][155/156] Data 0.001 (0.001) Batch 3.613 (3.440) Remain 14:18:47 loss: 1.3877 Lr: 0.04569
[2023-08-07 19:13:45,160 INFO misc.py line 115 22900] Train: [4/100][156/156] Data 0.001 (0.001) Batch 4.570 (3.448) Remain 14:20:34 loss: 1.3468 Lr: 0.04575
[2023-08-07 19:13:45,161 INFO misc.py line 129 22900] Train result: loss: 1.2126 
[2023-08-07 19:13:45,161 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 19:13:47,251 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.9793 
[2023-08-07 19:13:48,119 INFO evaluator.py line 122 22900] Test: [2/24] Loss 1.0221 
[2023-08-07 19:13:49,781 INFO evaluator.py line 122 22900] Test: [3/24] Loss 1.1484 
[2023-08-07 19:13:51,302 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.1438 
[2023-08-07 19:13:53,147 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.4322 
[2023-08-07 19:13:54,813 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.9862 
[2023-08-07 19:13:56,950 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.3069 
[2023-08-07 19:13:58,754 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.9561 
[2023-08-07 19:14:00,038 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.2353 
[2023-08-07 19:14:02,168 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.2713 
[2023-08-07 19:14:02,695 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.7290 
[2023-08-07 19:14:04,228 INFO evaluator.py line 122 22900] Test: [12/24] Loss 1.0266 
[2023-08-07 19:14:06,938 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.0889 
[2023-08-07 19:14:08,618 INFO evaluator.py line 122 22900] Test: [14/24] Loss 1.1732 
[2023-08-07 19:14:10,639 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.6356 
[2023-08-07 19:14:13,350 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.1728 
[2023-08-07 19:14:16,057 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.1704 
[2023-08-07 19:14:17,906 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.4312 
[2023-08-07 19:14:18,655 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.4054 
[2023-08-07 19:14:19,540 INFO evaluator.py line 122 22900] Test: [20/24] Loss 1.3547 
[2023-08-07 19:14:21,802 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.2790 
[2023-08-07 19:14:23,767 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.4517 
[2023-08-07 19:14:25,613 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.2056 
[2023-08-07 19:14:27,549 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.8765 
[2023-08-07 19:14:27,602 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.1292/0.1991/0.6257.
[2023-08-07 19:14:27,602 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.5957/0.9107
[2023-08-07 19:14:27,602 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.8947/0.9939
[2023-08-07 19:14:27,602 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.0442/0.0950
[2023-08-07 19:14:27,602 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.0235/0.0462
[2023-08-07 19:14:27,602 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.4461/0.8158
[2023-08-07 19:14:27,602 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:14:27,602 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.3562/0.4316
[2023-08-07 19:14:27,602 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0881/0.1161
[2023-08-07 19:14:27,602 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.0627/0.1321
[2023-08-07 19:14:27,602 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0175/0.0179
[2023-08-07 19:14:27,602 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:14:27,602 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:14:27,602 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:14:27,602 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:14:27,602 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:14:27,602 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:14:27,602 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:14:27,602 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.0382/0.4015
[2023-08-07 19:14:27,602 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:14:27,602 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0172/0.0215
[2023-08-07 19:14:27,602 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 19:14:27,602 INFO misc.py line 150 22900] Best validation mIoU updated to: 0.1292
[2023-08-07 19:14:27,603 INFO misc.py line 152 22900] Currently Best mIoU: 0.1292
[2023-08-07 19:14:27,603 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 19:14:34,741 INFO misc.py line 115 22900] Train: [5/100][1/156] Data 1.909 (1.909) Batch 6.084 (6.084) Remain 25:18:30 loss: 0.8799 Lr: 0.04580
[2023-08-07 19:14:38,047 INFO misc.py line 115 22900] Train: [5/100][2/156] Data 0.001 (0.001) Batch 3.306 (3.306) Remain 13:45:05 loss: 1.1493 Lr: 0.04585
[2023-08-07 19:14:41,255 INFO misc.py line 115 22900] Train: [5/100][3/156] Data 0.001 (0.001) Batch 3.208 (3.208) Remain 13:20:34 loss: 1.2962 Lr: 0.04590
[2023-08-07 19:14:44,452 INFO misc.py line 115 22900] Train: [5/100][4/156] Data 0.001 (0.001) Batch 3.197 (3.197) Remain 13:17:41 loss: 0.7440 Lr: 0.04596
[2023-08-07 19:14:47,497 INFO misc.py line 115 22900] Train: [5/100][5/156] Data 0.001 (0.001) Batch 3.045 (3.121) Remain 12:58:39 loss: 0.7757 Lr: 0.04601
[2023-08-07 19:14:50,531 INFO misc.py line 115 22900] Train: [5/100][6/156] Data 0.002 (0.001) Batch 3.034 (3.092) Remain 12:51:24 loss: 1.0829 Lr: 0.04606
[2023-08-07 19:14:53,831 INFO misc.py line 115 22900] Train: [5/100][7/156] Data 0.001 (0.001) Batch 3.300 (3.144) Remain 13:04:19 loss: 1.2408 Lr: 0.04611
[2023-08-07 19:14:57,481 INFO misc.py line 115 22900] Train: [5/100][8/156] Data 0.001 (0.001) Batch 3.650 (3.245) Remain 13:29:31 loss: 1.0412 Lr: 0.04616
[2023-08-07 19:15:00,647 INFO misc.py line 115 22900] Train: [5/100][9/156] Data 0.001 (0.001) Batch 3.166 (3.232) Remain 13:26:11 loss: 0.9017 Lr: 0.04621
[2023-08-07 19:15:03,883 INFO misc.py line 115 22900] Train: [5/100][10/156] Data 0.001 (0.001) Batch 3.236 (3.232) Remain 13:26:17 loss: 1.2463 Lr: 0.04626
[2023-08-07 19:15:07,895 INFO misc.py line 115 22900] Train: [5/100][11/156] Data 0.001 (0.001) Batch 4.012 (3.330) Remain 13:50:33 loss: 1.4827 Lr: 0.04631
[2023-08-07 19:15:11,390 INFO misc.py line 115 22900] Train: [5/100][12/156] Data 0.001 (0.001) Batch 3.495 (3.348) Remain 13:55:03 loss: 0.8145 Lr: 0.04636
[2023-08-07 19:15:14,905 INFO misc.py line 115 22900] Train: [5/100][13/156] Data 0.001 (0.001) Batch 3.515 (3.365) Remain 13:59:10 loss: 1.0321 Lr: 0.04641
[2023-08-07 19:15:19,104 INFO misc.py line 115 22900] Train: [5/100][14/156] Data 0.001 (0.001) Batch 4.198 (3.441) Remain 14:18:00 loss: 1.7845 Lr: 0.04646
[2023-08-07 19:15:22,359 INFO misc.py line 115 22900] Train: [5/100][15/156] Data 0.001 (0.001) Batch 3.255 (3.425) Remain 14:14:05 loss: 1.2156 Lr: 0.04651
[2023-08-07 19:15:25,586 INFO misc.py line 115 22900] Train: [5/100][16/156] Data 0.001 (0.001) Batch 3.227 (3.410) Remain 14:10:13 loss: 1.1428 Lr: 0.04656
[2023-08-07 19:15:28,477 INFO misc.py line 115 22900] Train: [5/100][17/156] Data 0.001 (0.001) Batch 2.891 (3.373) Remain 14:00:55 loss: 0.8875 Lr: 0.04660
[2023-08-07 19:15:31,646 INFO misc.py line 115 22900] Train: [5/100][18/156] Data 0.001 (0.001) Batch 3.169 (3.359) Remain 13:57:29 loss: 1.0581 Lr: 0.04665
[2023-08-07 19:15:35,685 INFO misc.py line 115 22900] Train: [5/100][19/156] Data 0.001 (0.001) Batch 4.040 (3.402) Remain 14:08:01 loss: 1.1514 Lr: 0.04670
[2023-08-07 19:15:39,634 INFO misc.py line 115 22900] Train: [5/100][20/156] Data 0.001 (0.001) Batch 3.948 (3.434) Remain 14:15:59 loss: 1.3685 Lr: 0.04675
[2023-08-07 19:15:43,049 INFO misc.py line 115 22900] Train: [5/100][21/156] Data 0.001 (0.001) Batch 3.416 (3.433) Remain 14:15:40 loss: 0.8291 Lr: 0.04679
[2023-08-07 19:15:46,917 INFO misc.py line 115 22900] Train: [5/100][22/156] Data 0.001 (0.001) Batch 3.867 (3.456) Remain 14:21:18 loss: 1.4031 Lr: 0.04684
[2023-08-07 19:15:50,621 INFO misc.py line 115 22900] Train: [5/100][23/156] Data 0.001 (0.001) Batch 3.704 (3.468) Remain 14:24:21 loss: 1.0553 Lr: 0.04689
[2023-08-07 19:15:53,371 INFO misc.py line 115 22900] Train: [5/100][24/156] Data 0.001 (0.001) Batch 2.750 (3.434) Remain 14:15:45 loss: 1.0893 Lr: 0.04693
[2023-08-07 19:15:56,558 INFO misc.py line 115 22900] Train: [5/100][25/156] Data 0.001 (0.001) Batch 3.188 (3.423) Remain 14:12:55 loss: 0.8787 Lr: 0.04698
[2023-08-07 19:16:00,042 INFO misc.py line 115 22900] Train: [5/100][26/156] Data 0.001 (0.001) Batch 3.483 (3.425) Remain 14:13:30 loss: 1.1010 Lr: 0.04702
[2023-08-07 19:16:03,350 INFO misc.py line 115 22900] Train: [5/100][27/156] Data 0.001 (0.001) Batch 3.308 (3.421) Remain 14:12:14 loss: 1.2474 Lr: 0.04707
[2023-08-07 19:16:07,461 INFO misc.py line 115 22900] Train: [5/100][28/156] Data 0.001 (0.001) Batch 4.111 (3.448) Remain 14:19:03 loss: 1.2800 Lr: 0.04711
[2023-08-07 19:16:11,431 INFO misc.py line 115 22900] Train: [5/100][29/156] Data 0.001 (0.001) Batch 3.970 (3.468) Remain 14:24:00 loss: 1.1248 Lr: 0.04716
[2023-08-07 19:16:15,463 INFO misc.py line 115 22900] Train: [5/100][30/156] Data 0.001 (0.001) Batch 4.032 (3.489) Remain 14:29:09 loss: 1.2593 Lr: 0.04720
[2023-08-07 19:16:19,259 INFO misc.py line 115 22900] Train: [5/100][31/156] Data 0.001 (0.001) Batch 3.796 (3.500) Remain 14:31:49 loss: 1.3461 Lr: 0.04724
[2023-08-07 19:16:21,974 INFO misc.py line 115 22900] Train: [5/100][32/156] Data 0.001 (0.001) Batch 2.715 (3.473) Remain 14:25:01 loss: 0.9798 Lr: 0.04729
[2023-08-07 19:16:25,137 INFO misc.py line 115 22900] Train: [5/100][33/156] Data 0.001 (0.001) Batch 3.162 (3.463) Remain 14:22:23 loss: 0.7432 Lr: 0.04733
[2023-08-07 19:16:28,747 INFO misc.py line 115 22900] Train: [5/100][34/156] Data 0.001 (0.001) Batch 3.610 (3.467) Remain 14:23:30 loss: 1.1216 Lr: 0.04737
[2023-08-07 19:16:31,985 INFO misc.py line 115 22900] Train: [5/100][35/156] Data 0.001 (0.001) Batch 3.238 (3.460) Remain 14:21:40 loss: 1.2527 Lr: 0.04742
[2023-08-07 19:16:35,648 INFO misc.py line 115 22900] Train: [5/100][36/156] Data 0.001 (0.001) Batch 3.663 (3.466) Remain 14:23:08 loss: 1.0304 Lr: 0.04746
[2023-08-07 19:16:39,595 INFO misc.py line 115 22900] Train: [5/100][37/156] Data 0.001 (0.001) Batch 3.947 (3.481) Remain 14:26:36 loss: 1.0785 Lr: 0.04750
[2023-08-07 19:16:43,612 INFO misc.py line 115 22900] Train: [5/100][38/156] Data 0.001 (0.001) Batch 4.017 (3.496) Remain 14:30:21 loss: 1.1603 Lr: 0.04754
[2023-08-07 19:16:46,796 INFO misc.py line 115 22900] Train: [5/100][39/156] Data 0.001 (0.001) Batch 3.185 (3.487) Remain 14:28:09 loss: 0.8868 Lr: 0.04758
[2023-08-07 19:16:50,398 INFO misc.py line 115 22900] Train: [5/100][40/156] Data 0.001 (0.001) Batch 3.601 (3.490) Remain 14:28:51 loss: 0.8617 Lr: 0.04762
[2023-08-07 19:16:54,014 INFO misc.py line 115 22900] Train: [5/100][41/156] Data 0.001 (0.001) Batch 3.617 (3.494) Remain 14:29:37 loss: 1.0340 Lr: 0.04766
[2023-08-07 19:16:57,310 INFO misc.py line 115 22900] Train: [5/100][42/156] Data 0.001 (0.001) Batch 3.296 (3.489) Remain 14:28:18 loss: 1.0769 Lr: 0.04770
[2023-08-07 19:16:59,966 INFO misc.py line 115 22900] Train: [5/100][43/156] Data 0.001 (0.001) Batch 2.655 (3.468) Remain 14:23:03 loss: 1.9312 Lr: 0.04774
[2023-08-07 19:17:03,960 INFO misc.py line 115 22900] Train: [5/100][44/156] Data 0.001 (0.001) Batch 3.995 (3.481) Remain 14:26:12 loss: 1.1326 Lr: 0.04778
[2023-08-07 19:17:07,893 INFO misc.py line 115 22900] Train: [5/100][45/156] Data 0.001 (0.001) Batch 3.933 (3.491) Remain 14:28:49 loss: 1.0028 Lr: 0.04782
[2023-08-07 19:17:11,284 INFO misc.py line 115 22900] Train: [5/100][46/156] Data 0.001 (0.001) Batch 3.390 (3.489) Remain 14:28:11 loss: 1.3471 Lr: 0.04786
[2023-08-07 19:17:13,804 INFO misc.py line 115 22900] Train: [5/100][47/156] Data 0.001 (0.001) Batch 2.520 (3.467) Remain 14:22:38 loss: 1.0548 Lr: 0.04790
[2023-08-07 19:17:16,420 INFO misc.py line 115 22900] Train: [5/100][48/156] Data 0.001 (0.001) Batch 2.616 (3.448) Remain 14:17:53 loss: 1.0653 Lr: 0.04794
[2023-08-07 19:17:18,818 INFO misc.py line 115 22900] Train: [5/100][49/156] Data 0.001 (0.001) Batch 2.398 (3.425) Remain 14:12:08 loss: 0.5122 Lr: 0.04798
[2023-08-07 19:17:22,579 INFO misc.py line 115 22900] Train: [5/100][50/156] Data 0.001 (0.001) Batch 3.761 (3.432) Remain 14:13:52 loss: 1.0913 Lr: 0.04801
[2023-08-07 19:17:25,626 INFO misc.py line 115 22900] Train: [5/100][51/156] Data 0.001 (0.001) Batch 3.047 (3.424) Remain 14:11:48 loss: 0.9345 Lr: 0.04805
[2023-08-07 19:17:27,667 INFO misc.py line 115 22900] Train: [5/100][52/156] Data 0.001 (0.001) Batch 2.041 (3.396) Remain 14:04:44 loss: 0.4768 Lr: 0.04809
[2023-08-07 19:17:31,628 INFO misc.py line 115 22900] Train: [5/100][53/156] Data 0.001 (0.001) Batch 3.961 (3.407) Remain 14:07:29 loss: 1.3913 Lr: 0.04812
[2023-08-07 19:17:35,553 INFO misc.py line 115 22900] Train: [5/100][54/156] Data 0.001 (0.001) Batch 3.926 (3.418) Remain 14:09:57 loss: 1.2847 Lr: 0.04816
[2023-08-07 19:17:39,432 INFO misc.py line 115 22900] Train: [5/100][55/156] Data 0.001 (0.001) Batch 3.879 (3.426) Remain 14:12:06 loss: 0.9144 Lr: 0.04819
[2023-08-07 19:17:41,250 INFO misc.py line 115 22900] Train: [5/100][56/156] Data 0.001 (0.001) Batch 1.818 (3.396) Remain 14:04:30 loss: 1.0697 Lr: 0.04823
[2023-08-07 19:17:44,992 INFO misc.py line 115 22900] Train: [5/100][57/156] Data 0.001 (0.001) Batch 3.743 (3.403) Remain 14:06:02 loss: 1.6183 Lr: 0.04827
[2023-08-07 19:17:47,240 INFO misc.py line 115 22900] Train: [5/100][58/156] Data 0.001 (0.001) Batch 2.248 (3.382) Remain 14:00:45 loss: 0.5708 Lr: 0.04830
[2023-08-07 19:17:51,304 INFO misc.py line 115 22900] Train: [5/100][59/156] Data 0.001 (0.001) Batch 4.064 (3.394) Remain 14:03:44 loss: 0.9632 Lr: 0.04833
[2023-08-07 19:17:55,398 INFO misc.py line 115 22900] Train: [5/100][60/156] Data 0.001 (0.001) Batch 4.094 (3.406) Remain 14:06:44 loss: 2.1348 Lr: 0.04837
[2023-08-07 19:17:58,504 INFO misc.py line 115 22900] Train: [5/100][61/156] Data 0.001 (0.001) Batch 3.106 (3.401) Remain 14:05:23 loss: 1.2351 Lr: 0.04840
[2023-08-07 19:18:01,306 INFO misc.py line 115 22900] Train: [5/100][62/156] Data 0.001 (0.001) Batch 2.802 (3.391) Remain 14:02:48 loss: 0.8164 Lr: 0.04844
[2023-08-07 19:18:03,738 INFO misc.py line 115 22900] Train: [5/100][63/156] Data 0.001 (0.001) Batch 2.432 (3.375) Remain 13:58:46 loss: 1.3041 Lr: 0.04847
[2023-08-07 19:18:05,748 INFO misc.py line 115 22900] Train: [5/100][64/156] Data 0.001 (0.001) Batch 2.011 (3.352) Remain 13:53:10 loss: 0.8610 Lr: 0.04850
[2023-08-07 19:18:08,800 INFO misc.py line 115 22900] Train: [5/100][65/156] Data 0.001 (0.001) Batch 3.051 (3.347) Remain 13:51:54 loss: 1.4559 Lr: 0.04853
[2023-08-07 19:18:12,810 INFO misc.py line 115 22900] Train: [5/100][66/156] Data 0.001 (0.001) Batch 4.010 (3.358) Remain 13:54:27 loss: 1.2102 Lr: 0.04857
[2023-08-07 19:18:16,320 INFO misc.py line 115 22900] Train: [5/100][67/156] Data 0.001 (0.001) Batch 3.509 (3.360) Remain 13:54:59 loss: 0.7994 Lr: 0.04860
[2023-08-07 19:18:20,347 INFO misc.py line 115 22900] Train: [5/100][68/156] Data 0.001 (0.001) Batch 4.027 (3.371) Remain 13:57:29 loss: 1.2953 Lr: 0.04863
[2023-08-07 19:18:24,495 INFO misc.py line 115 22900] Train: [5/100][69/156] Data 0.001 (0.001) Batch 4.148 (3.382) Remain 14:00:21 loss: 1.2132 Lr: 0.04866
[2023-08-07 19:18:28,577 INFO misc.py line 115 22900] Train: [5/100][70/156] Data 0.001 (0.001) Batch 4.082 (3.393) Remain 14:02:53 loss: 1.1305 Lr: 0.04869
[2023-08-07 19:18:31,701 INFO misc.py line 115 22900] Train: [5/100][71/156] Data 0.001 (0.001) Batch 3.124 (3.389) Remain 14:01:51 loss: 0.9103 Lr: 0.04872
[2023-08-07 19:18:35,816 INFO misc.py line 115 22900] Train: [5/100][72/156] Data 0.001 (0.001) Batch 4.115 (3.399) Remain 14:04:25 loss: 1.6540 Lr: 0.04875
[2023-08-07 19:18:39,889 INFO misc.py line 115 22900] Train: [5/100][73/156] Data 0.001 (0.001) Batch 4.073 (3.409) Remain 14:06:45 loss: 1.2676 Lr: 0.04878
[2023-08-07 19:18:44,279 INFO misc.py line 115 22900] Train: [5/100][74/156] Data 0.001 (0.001) Batch 4.390 (3.423) Remain 14:10:07 loss: 1.3953 Lr: 0.04881
[2023-08-07 19:18:47,719 INFO misc.py line 115 22900] Train: [5/100][75/156] Data 0.001 (0.001) Batch 3.440 (3.423) Remain 14:10:07 loss: 0.9675 Lr: 0.04884
[2023-08-07 19:18:51,805 INFO misc.py line 115 22900] Train: [5/100][76/156] Data 0.001 (0.001) Batch 4.086 (3.432) Remain 14:12:19 loss: 1.1978 Lr: 0.04887
[2023-08-07 19:18:55,546 INFO misc.py line 115 22900] Train: [5/100][77/156] Data 0.001 (0.001) Batch 3.741 (3.436) Remain 14:13:18 loss: 1.4460 Lr: 0.04890
[2023-08-07 19:18:59,717 INFO misc.py line 115 22900] Train: [5/100][78/156] Data 0.001 (0.001) Batch 4.171 (3.446) Remain 14:15:40 loss: 1.2404 Lr: 0.04892
[2023-08-07 19:19:02,557 INFO misc.py line 115 22900] Train: [5/100][79/156] Data 0.001 (0.001) Batch 2.840 (3.438) Remain 14:13:38 loss: 1.4419 Lr: 0.04895
[2023-08-07 19:19:06,520 INFO misc.py line 115 22900] Train: [5/100][80/156] Data 0.001 (0.001) Batch 3.964 (3.445) Remain 14:15:16 loss: 1.3816 Lr: 0.04898
[2023-08-07 19:19:10,188 INFO misc.py line 115 22900] Train: [5/100][81/156] Data 0.001 (0.001) Batch 3.668 (3.448) Remain 14:15:55 loss: 0.9488 Lr: 0.04901
[2023-08-07 19:19:14,089 INFO misc.py line 115 22900] Train: [5/100][82/156] Data 0.001 (0.001) Batch 3.901 (3.454) Remain 14:17:17 loss: 1.1260 Lr: 0.04903
[2023-08-07 19:19:17,949 INFO misc.py line 115 22900] Train: [5/100][83/156] Data 0.001 (0.001) Batch 3.860 (3.459) Remain 14:18:29 loss: 1.1925 Lr: 0.04906
[2023-08-07 19:19:21,722 INFO misc.py line 115 22900] Train: [5/100][84/156] Data 0.001 (0.001) Batch 3.772 (3.463) Remain 14:19:24 loss: 0.9031 Lr: 0.04908
[2023-08-07 19:19:25,717 INFO misc.py line 115 22900] Train: [5/100][85/156] Data 0.001 (0.001) Batch 3.996 (3.469) Remain 14:20:57 loss: 1.3237 Lr: 0.04911
[2023-08-07 19:19:28,476 INFO misc.py line 115 22900] Train: [5/100][86/156] Data 0.001 (0.001) Batch 2.759 (3.460) Remain 14:18:46 loss: 0.8191 Lr: 0.04913
[2023-08-07 19:19:32,840 INFO misc.py line 115 22900] Train: [5/100][87/156] Data 0.001 (0.001) Batch 4.364 (3.471) Remain 14:21:23 loss: 1.3155 Lr: 0.04916
[2023-08-07 19:19:36,045 INFO misc.py line 115 22900] Train: [5/100][88/156] Data 0.001 (0.001) Batch 3.205 (3.468) Remain 14:20:33 loss: 1.2564 Lr: 0.04918
[2023-08-07 19:19:40,017 INFO misc.py line 115 22900] Train: [5/100][89/156] Data 0.001 (0.001) Batch 3.972 (3.474) Remain 14:21:56 loss: 1.6049 Lr: 0.04921
[2023-08-07 19:19:43,763 INFO misc.py line 115 22900] Train: [5/100][90/156] Data 0.001 (0.001) Batch 3.746 (3.477) Remain 14:22:39 loss: 1.0711 Lr: 0.04923
[2023-08-07 19:19:46,727 INFO misc.py line 115 22900] Train: [5/100][91/156] Data 0.001 (0.001) Batch 2.965 (3.471) Remain 14:21:09 loss: 1.0203 Lr: 0.04925
[2023-08-07 19:19:50,663 INFO misc.py line 115 22900] Train: [5/100][92/156] Data 0.001 (0.001) Batch 3.935 (3.476) Remain 14:22:23 loss: 1.1917 Lr: 0.04928
[2023-08-07 19:19:55,131 INFO misc.py line 115 22900] Train: [5/100][93/156] Data 0.001 (0.001) Batch 4.468 (3.488) Remain 14:25:04 loss: 1.2887 Lr: 0.04930
[2023-08-07 19:19:58,650 INFO misc.py line 115 22900] Train: [5/100][94/156] Data 0.001 (0.001) Batch 3.519 (3.488) Remain 14:25:06 loss: 1.3091 Lr: 0.04932
[2023-08-07 19:20:02,918 INFO misc.py line 115 22900] Train: [5/100][95/156] Data 0.001 (0.001) Batch 4.268 (3.496) Remain 14:27:08 loss: 1.6123 Lr: 0.04934
[2023-08-07 19:20:06,069 INFO misc.py line 115 22900] Train: [5/100][96/156] Data 0.001 (0.001) Batch 3.151 (3.493) Remain 14:26:10 loss: 0.8931 Lr: 0.04937
[2023-08-07 19:20:09,112 INFO misc.py line 115 22900] Train: [5/100][97/156] Data 0.001 (0.001) Batch 3.044 (3.488) Remain 14:24:55 loss: 1.0948 Lr: 0.04939
[2023-08-07 19:20:13,181 INFO misc.py line 115 22900] Train: [5/100][98/156] Data 0.001 (0.001) Batch 4.068 (3.494) Remain 14:26:22 loss: 1.0945 Lr: 0.04941
[2023-08-07 19:20:15,567 INFO misc.py line 115 22900] Train: [5/100][99/156] Data 0.001 (0.001) Batch 2.386 (3.482) Remain 14:23:27 loss: 1.0235 Lr: 0.04943
[2023-08-07 19:20:19,356 INFO misc.py line 115 22900] Train: [5/100][100/156] Data 0.001 (0.001) Batch 3.789 (3.486) Remain 14:24:11 loss: 1.0400 Lr: 0.04945
[2023-08-07 19:20:23,958 INFO misc.py line 115 22900] Train: [5/100][101/156] Data 0.001 (0.001) Batch 4.603 (3.497) Remain 14:26:57 loss: 1.2903 Lr: 0.04947
[2023-08-07 19:20:26,579 INFO misc.py line 115 22900] Train: [5/100][102/156] Data 0.001 (0.001) Batch 2.620 (3.488) Remain 14:24:42 loss: 0.8858 Lr: 0.04949
[2023-08-07 19:20:28,821 INFO misc.py line 115 22900] Train: [5/100][103/156] Data 0.001 (0.001) Batch 2.242 (3.476) Remain 14:21:33 loss: 1.2882 Lr: 0.04951
[2023-08-07 19:20:32,540 INFO misc.py line 115 22900] Train: [5/100][104/156] Data 0.001 (0.001) Batch 3.719 (3.478) Remain 14:22:05 loss: 1.4073 Lr: 0.04953
[2023-08-07 19:20:36,143 INFO misc.py line 115 22900] Train: [5/100][105/156] Data 0.001 (0.001) Batch 3.603 (3.479) Remain 14:22:20 loss: 1.1511 Lr: 0.04954
[2023-08-07 19:20:39,789 INFO misc.py line 115 22900] Train: [5/100][106/156] Data 0.001 (0.001) Batch 3.646 (3.481) Remain 14:22:41 loss: 1.3387 Lr: 0.04956
[2023-08-07 19:20:43,321 INFO misc.py line 115 22900] Train: [5/100][107/156] Data 0.001 (0.001) Batch 3.532 (3.481) Remain 14:22:44 loss: 1.1384 Lr: 0.04958
[2023-08-07 19:20:45,880 INFO misc.py line 115 22900] Train: [5/100][108/156] Data 0.001 (0.001) Batch 2.559 (3.473) Remain 14:20:30 loss: 0.5966 Lr: 0.04960
[2023-08-07 19:20:49,520 INFO misc.py line 115 22900] Train: [5/100][109/156] Data 0.001 (0.001) Batch 3.640 (3.474) Remain 14:20:50 loss: 1.2365 Lr: 0.04961
[2023-08-07 19:20:52,358 INFO misc.py line 115 22900] Train: [5/100][110/156] Data 0.001 (0.001) Batch 2.838 (3.468) Remain 14:19:19 loss: 0.9648 Lr: 0.04963
[2023-08-07 19:20:55,794 INFO misc.py line 115 22900] Train: [5/100][111/156] Data 0.001 (0.001) Batch 3.436 (3.468) Remain 14:19:11 loss: 1.0173 Lr: 0.04965
[2023-08-07 19:20:58,843 INFO misc.py line 115 22900] Train: [5/100][112/156] Data 0.001 (0.001) Batch 3.049 (3.464) Remain 14:18:10 loss: 0.9575 Lr: 0.04966
[2023-08-07 19:21:02,768 INFO misc.py line 115 22900] Train: [5/100][113/156] Data 0.001 (0.001) Batch 3.925 (3.468) Remain 14:19:09 loss: 1.1142 Lr: 0.04968
[2023-08-07 19:21:06,640 INFO misc.py line 115 22900] Train: [5/100][114/156] Data 0.001 (0.001) Batch 3.872 (3.472) Remain 14:19:59 loss: 1.4000 Lr: 0.04969
[2023-08-07 19:21:11,165 INFO misc.py line 115 22900] Train: [5/100][115/156] Data 0.001 (0.001) Batch 4.525 (3.481) Remain 14:22:16 loss: 1.5312 Lr: 0.04971
[2023-08-07 19:21:13,379 INFO misc.py line 115 22900] Train: [5/100][116/156] Data 0.001 (0.001) Batch 2.214 (3.470) Remain 14:19:26 loss: 0.8322 Lr: 0.04972
[2023-08-07 19:21:16,267 INFO misc.py line 115 22900] Train: [5/100][117/156] Data 0.001 (0.001) Batch 2.887 (3.465) Remain 14:18:06 loss: 0.8135 Lr: 0.04974
[2023-08-07 19:21:19,630 INFO misc.py line 115 22900] Train: [5/100][118/156] Data 0.001 (0.001) Batch 3.363 (3.464) Remain 14:17:49 loss: 1.1457 Lr: 0.04975
[2023-08-07 19:21:23,724 INFO misc.py line 115 22900] Train: [5/100][119/156] Data 0.001 (0.001) Batch 4.094 (3.470) Remain 14:19:07 loss: 1.3943 Lr: 0.04976
[2023-08-07 19:21:26,564 INFO misc.py line 115 22900] Train: [5/100][120/156] Data 0.001 (0.001) Batch 2.840 (3.464) Remain 14:17:43 loss: 1.4153 Lr: 0.04978
[2023-08-07 19:21:29,981 INFO misc.py line 115 22900] Train: [5/100][121/156] Data 0.001 (0.001) Batch 3.417 (3.464) Remain 14:17:34 loss: 0.8476 Lr: 0.04979
[2023-08-07 19:21:32,944 INFO misc.py line 115 22900] Train: [5/100][122/156] Data 0.001 (0.001) Batch 2.963 (3.460) Remain 14:16:28 loss: 1.1360 Lr: 0.04980
[2023-08-07 19:21:36,092 INFO misc.py line 115 22900] Train: [5/100][123/156] Data 0.001 (0.001) Batch 3.148 (3.457) Remain 14:15:46 loss: 0.8462 Lr: 0.04981
[2023-08-07 19:21:39,341 INFO misc.py line 115 22900] Train: [5/100][124/156] Data 0.001 (0.001) Batch 3.249 (3.455) Remain 14:15:17 loss: 1.0497 Lr: 0.04982
[2023-08-07 19:21:42,565 INFO misc.py line 115 22900] Train: [5/100][125/156] Data 0.001 (0.001) Batch 3.224 (3.453) Remain 14:14:45 loss: 1.3006 Lr: 0.04984
[2023-08-07 19:21:47,027 INFO misc.py line 115 22900] Train: [5/100][126/156] Data 0.001 (0.001) Batch 4.462 (3.462) Remain 14:16:44 loss: 1.2077 Lr: 0.04985
[2023-08-07 19:21:51,137 INFO misc.py line 115 22900] Train: [5/100][127/156] Data 0.001 (0.001) Batch 4.110 (3.467) Remain 14:17:58 loss: 1.5096 Lr: 0.04986
[2023-08-07 19:21:55,158 INFO misc.py line 115 22900] Train: [5/100][128/156] Data 0.001 (0.001) Batch 4.021 (3.471) Remain 14:19:00 loss: 1.0142 Lr: 0.04987
[2023-08-07 19:21:58,644 INFO misc.py line 115 22900] Train: [5/100][129/156] Data 0.001 (0.001) Batch 3.486 (3.471) Remain 14:18:58 loss: 1.0015 Lr: 0.04988
[2023-08-07 19:22:02,648 INFO misc.py line 115 22900] Train: [5/100][130/156] Data 0.001 (0.001) Batch 4.004 (3.476) Remain 14:19:57 loss: 1.4896 Lr: 0.04989
[2023-08-07 19:22:06,759 INFO misc.py line 115 22900] Train: [5/100][131/156] Data 0.001 (0.001) Batch 4.111 (3.480) Remain 14:21:07 loss: 1.3469 Lr: 0.04989
[2023-08-07 19:22:09,329 INFO misc.py line 115 22900] Train: [5/100][132/156] Data 0.001 (0.001) Batch 2.570 (3.473) Remain 14:19:19 loss: 1.0032 Lr: 0.04990
[2023-08-07 19:22:12,951 INFO misc.py line 115 22900] Train: [5/100][133/156] Data 0.001 (0.001) Batch 3.622 (3.475) Remain 14:19:33 loss: 1.0116 Lr: 0.04991
[2023-08-07 19:22:16,348 INFO misc.py line 115 22900] Train: [5/100][134/156] Data 0.001 (0.001) Batch 3.396 (3.474) Remain 14:19:20 loss: 1.0145 Lr: 0.04992
[2023-08-07 19:22:20,415 INFO misc.py line 115 22900] Train: [5/100][135/156] Data 0.001 (0.001) Batch 4.068 (3.478) Remain 14:20:24 loss: 0.8857 Lr: 0.04993
[2023-08-07 19:22:23,323 INFO misc.py line 115 22900] Train: [5/100][136/156] Data 0.001 (0.001) Batch 2.908 (3.474) Remain 14:19:17 loss: 0.9431 Lr: 0.04993
[2023-08-07 19:22:26,173 INFO misc.py line 115 22900] Train: [5/100][137/156] Data 0.001 (0.001) Batch 2.850 (3.470) Remain 14:18:04 loss: 0.5808 Lr: 0.04994
[2023-08-07 19:22:29,476 INFO misc.py line 115 22900] Train: [5/100][138/156] Data 0.001 (0.001) Batch 3.303 (3.468) Remain 14:17:42 loss: 1.0043 Lr: 0.04995
[2023-08-07 19:22:33,385 INFO misc.py line 115 22900] Train: [5/100][139/156] Data 0.001 (0.001) Batch 3.909 (3.472) Remain 14:18:27 loss: 1.3363 Lr: 0.04995
[2023-08-07 19:22:36,622 INFO misc.py line 115 22900] Train: [5/100][140/156] Data 0.001 (0.001) Batch 3.237 (3.470) Remain 14:17:58 loss: 1.1373 Lr: 0.04996
[2023-08-07 19:22:39,218 INFO misc.py line 115 22900] Train: [5/100][141/156] Data 0.001 (0.001) Batch 2.596 (3.463) Remain 14:16:20 loss: 0.8582 Lr: 0.04996
[2023-08-07 19:22:42,968 INFO misc.py line 115 22900] Train: [5/100][142/156] Data 0.001 (0.001) Batch 3.750 (3.466) Remain 14:16:48 loss: 1.1873 Lr: 0.04997
[2023-08-07 19:22:46,051 INFO misc.py line 115 22900] Train: [5/100][143/156] Data 0.001 (0.001) Batch 3.083 (3.463) Remain 14:16:04 loss: 0.8165 Lr: 0.04997
[2023-08-07 19:22:49,373 INFO misc.py line 115 22900] Train: [5/100][144/156] Data 0.001 (0.001) Batch 3.322 (3.462) Remain 14:15:45 loss: 0.9171 Lr: 0.04998
[2023-08-07 19:22:53,161 INFO misc.py line 115 22900] Train: [5/100][145/156] Data 0.001 (0.001) Batch 3.789 (3.464) Remain 14:16:16 loss: 1.3911 Lr: 0.04998
[2023-08-07 19:22:55,966 INFO misc.py line 115 22900] Train: [5/100][146/156] Data 0.001 (0.001) Batch 2.805 (3.460) Remain 14:15:04 loss: 0.9645 Lr: 0.04999
[2023-08-07 19:22:59,845 INFO misc.py line 115 22900] Train: [5/100][147/156] Data 0.001 (0.001) Batch 3.880 (3.462) Remain 14:15:44 loss: 1.0469 Lr: 0.04999
[2023-08-07 19:23:03,291 INFO misc.py line 115 22900] Train: [5/100][148/156] Data 0.001 (0.001) Batch 3.446 (3.462) Remain 14:15:39 loss: 0.8801 Lr: 0.04999
[2023-08-07 19:23:05,821 INFO misc.py line 115 22900] Train: [5/100][149/156] Data 0.001 (0.001) Batch 2.530 (3.456) Remain 14:14:01 loss: 0.9119 Lr: 0.04999
[2023-08-07 19:23:08,458 INFO misc.py line 115 22900] Train: [5/100][150/156] Data 0.001 (0.001) Batch 2.637 (3.450) Remain 14:12:34 loss: 1.0932 Lr: 0.05000
[2023-08-07 19:23:12,688 INFO misc.py line 115 22900] Train: [5/100][151/156] Data 0.001 (0.001) Batch 4.230 (3.456) Remain 14:13:49 loss: 1.5275 Lr: 0.05000
[2023-08-07 19:23:16,496 INFO misc.py line 115 22900] Train: [5/100][152/156] Data 0.001 (0.001) Batch 3.808 (3.458) Remain 14:14:21 loss: 0.7467 Lr: 0.05000
[2023-08-07 19:23:20,502 INFO misc.py line 115 22900] Train: [5/100][153/156] Data 0.001 (0.001) Batch 4.006 (3.462) Remain 14:15:11 loss: 1.2605 Lr: 0.05000
[2023-08-07 19:23:23,874 INFO misc.py line 115 22900] Train: [5/100][154/156] Data 0.001 (0.001) Batch 3.372 (3.461) Remain 14:14:59 loss: 0.8483 Lr: 0.05000
[2023-08-07 19:23:26,932 INFO misc.py line 115 22900] Train: [5/100][155/156] Data 0.001 (0.001) Batch 3.058 (3.458) Remain 14:14:16 loss: 1.4807 Lr: 0.05000
[2023-08-07 19:23:29,990 INFO misc.py line 115 22900] Train: [5/100][156/156] Data 0.001 (0.001) Batch 3.058 (3.456) Remain 14:13:34 loss: 1.3860 Lr: 0.05000
[2023-08-07 19:23:29,991 INFO misc.py line 129 22900] Train result: loss: 1.1275 
[2023-08-07 19:23:29,991 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 19:23:32,105 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.9348 
[2023-08-07 19:23:32,973 INFO evaluator.py line 122 22900] Test: [2/24] Loss 1.0623 
[2023-08-07 19:23:34,635 INFO evaluator.py line 122 22900] Test: [3/24] Loss 1.1391 
[2023-08-07 19:23:36,157 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.2384 
[2023-08-07 19:23:37,997 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.4652 
[2023-08-07 19:23:39,663 INFO evaluator.py line 122 22900] Test: [6/24] Loss 1.0188 
[2023-08-07 19:23:41,803 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.4552 
[2023-08-07 19:23:43,610 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.9316 
[2023-08-07 19:23:44,892 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.3296 
[2023-08-07 19:23:47,020 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3298 
[2023-08-07 19:23:47,547 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.6719 
[2023-08-07 19:23:49,081 INFO evaluator.py line 122 22900] Test: [12/24] Loss 1.0519 
[2023-08-07 19:23:51,790 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.1693 
[2023-08-07 19:23:53,467 INFO evaluator.py line 122 22900] Test: [14/24] Loss 1.1168 
[2023-08-07 19:23:55,489 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.6215 
[2023-08-07 19:23:58,199 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.3169 
[2023-08-07 19:24:00,906 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.3257 
[2023-08-07 19:24:02,752 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.4791 
[2023-08-07 19:24:03,501 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.4810 
[2023-08-07 19:24:04,384 INFO evaluator.py line 122 22900] Test: [20/24] Loss 1.1847 
[2023-08-07 19:24:06,646 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3534 
[2023-08-07 19:24:08,611 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.4996 
[2023-08-07 19:24:10,458 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.2214 
[2023-08-07 19:24:12,393 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.7652 
[2023-08-07 19:24:12,439 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.1290/0.1914/0.5954.
[2023-08-07 19:24:12,439 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.5356/0.8308
[2023-08-07 19:24:12,439 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9281/0.9907
[2023-08-07 19:24:12,439 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.0410/0.0908
[2023-08-07 19:24:12,439 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.0615/0.1668
[2023-08-07 19:24:12,439 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.4957/0.8192
[2023-08-07 19:24:12,439 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.0101/0.0106
[2023-08-07 19:24:12,439 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.1158/0.1201
[2023-08-07 19:24:12,439 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1120/0.1795
[2023-08-07 19:24:12,439 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.0796/0.1583
[2023-08-07 19:24:12,439 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0056/0.0056
[2023-08-07 19:24:12,439 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:24:12,439 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:24:12,439 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.0678/0.1843
[2023-08-07 19:24:12,439 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0505/0.1061
[2023-08-07 19:24:12,439 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:24:12,439 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:24:12,439 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:24:12,439 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.0570/0.1412
[2023-08-07 19:24:12,440 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:24:12,440 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0201/0.0242
[2023-08-07 19:24:12,440 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 19:24:12,440 INFO misc.py line 152 22900] Currently Best mIoU: 0.1292
[2023-08-07 19:24:12,440 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 19:24:17,707 INFO misc.py line 115 22900] Train: [6/100][1/156] Data 0.994 (0.994) Batch 4.468 (4.468) Remain 18:23:33 loss: 0.9543 Lr: 0.05000
[2023-08-07 19:24:20,649 INFO misc.py line 115 22900] Train: [6/100][2/156] Data 0.001 (0.001) Batch 2.942 (2.942) Remain 12:06:29 loss: 1.2145 Lr: 0.05000
[2023-08-07 19:24:25,597 INFO misc.py line 115 22900] Train: [6/100][3/156] Data 0.001 (0.001) Batch 4.948 (4.948) Remain 20:21:54 loss: 1.6910 Lr: 0.05000
[2023-08-07 19:24:28,884 INFO misc.py line 115 22900] Train: [6/100][4/156] Data 0.001 (0.001) Batch 3.288 (3.288) Remain 13:31:52 loss: 1.3803 Lr: 0.05000
[2023-08-07 19:24:32,877 INFO misc.py line 115 22900] Train: [6/100][5/156] Data 0.001 (0.001) Batch 3.993 (3.640) Remain 14:58:49 loss: 1.4384 Lr: 0.05000
[2023-08-07 19:24:36,529 INFO misc.py line 115 22900] Train: [6/100][6/156] Data 0.001 (0.001) Batch 3.652 (3.644) Remain 14:59:46 loss: 0.9673 Lr: 0.05000
[2023-08-07 19:24:40,499 INFO misc.py line 115 22900] Train: [6/100][7/156] Data 0.001 (0.001) Batch 3.969 (3.726) Remain 15:19:47 loss: 1.2230 Lr: 0.05000
[2023-08-07 19:24:44,519 INFO misc.py line 115 22900] Train: [6/100][8/156] Data 0.001 (0.001) Batch 4.020 (3.784) Remain 15:34:15 loss: 1.2192 Lr: 0.05000
[2023-08-07 19:24:48,712 INFO misc.py line 115 22900] Train: [6/100][9/156] Data 0.001 (0.001) Batch 4.193 (3.853) Remain 15:50:59 loss: 1.0518 Lr: 0.05000
[2023-08-07 19:24:51,992 INFO misc.py line 115 22900] Train: [6/100][10/156] Data 0.001 (0.001) Batch 3.280 (3.771) Remain 15:30:44 loss: 0.9928 Lr: 0.05000
[2023-08-07 19:24:54,444 INFO misc.py line 115 22900] Train: [6/100][11/156] Data 0.001 (0.001) Batch 2.453 (3.606) Remain 14:50:00 loss: 0.6769 Lr: 0.05000
[2023-08-07 19:24:57,207 INFO misc.py line 115 22900] Train: [6/100][12/156] Data 0.001 (0.001) Batch 2.763 (3.512) Remain 14:26:50 loss: 1.0870 Lr: 0.05000
[2023-08-07 19:25:00,644 INFO misc.py line 115 22900] Train: [6/100][13/156] Data 0.001 (0.001) Batch 3.437 (3.505) Remain 14:24:54 loss: 0.9065 Lr: 0.05000
[2023-08-07 19:25:04,493 INFO misc.py line 115 22900] Train: [6/100][14/156] Data 0.001 (0.001) Batch 3.849 (3.536) Remain 14:32:34 loss: 1.0435 Lr: 0.05000
[2023-08-07 19:25:08,506 INFO misc.py line 115 22900] Train: [6/100][15/156] Data 0.001 (0.001) Batch 4.013 (3.576) Remain 14:42:19 loss: 1.4400 Lr: 0.05000
[2023-08-07 19:25:10,833 INFO misc.py line 115 22900] Train: [6/100][16/156] Data 0.001 (0.001) Batch 2.328 (3.480) Remain 14:18:34 loss: 1.0280 Lr: 0.05000
[2023-08-07 19:25:14,843 INFO misc.py line 115 22900] Train: [6/100][17/156] Data 0.001 (0.001) Batch 4.009 (3.518) Remain 14:27:50 loss: 1.4335 Lr: 0.05000
[2023-08-07 19:25:18,096 INFO misc.py line 115 22900] Train: [6/100][18/156] Data 0.001 (0.001) Batch 3.254 (3.500) Remain 14:23:26 loss: 1.0113 Lr: 0.05000
[2023-08-07 19:25:20,808 INFO misc.py line 115 22900] Train: [6/100][19/156] Data 0.001 (0.001) Batch 2.712 (3.451) Remain 14:11:14 loss: 0.9581 Lr: 0.05000
[2023-08-07 19:25:24,844 INFO misc.py line 115 22900] Train: [6/100][20/156] Data 0.001 (0.001) Batch 4.036 (3.485) Remain 14:19:39 loss: 1.0852 Lr: 0.05000
[2023-08-07 19:25:28,252 INFO misc.py line 115 22900] Train: [6/100][21/156] Data 0.001 (0.001) Batch 3.409 (3.481) Remain 14:18:33 loss: 1.0417 Lr: 0.05000
[2023-08-07 19:25:30,566 INFO misc.py line 115 22900] Train: [6/100][22/156] Data 0.001 (0.001) Batch 2.314 (3.419) Remain 14:03:21 loss: 1.0315 Lr: 0.05000
[2023-08-07 19:25:34,370 INFO misc.py line 115 22900] Train: [6/100][23/156] Data 0.001 (0.001) Batch 3.804 (3.439) Remain 14:08:02 loss: 1.2109 Lr: 0.05000
[2023-08-07 19:25:37,625 INFO misc.py line 115 22900] Train: [6/100][24/156] Data 0.001 (0.001) Batch 3.254 (3.430) Remain 14:05:49 loss: 1.1841 Lr: 0.05000
[2023-08-07 19:25:40,764 INFO misc.py line 115 22900] Train: [6/100][25/156] Data 0.001 (0.001) Batch 3.139 (3.417) Remain 14:02:29 loss: 0.9219 Lr: 0.05000
[2023-08-07 19:25:43,368 INFO misc.py line 115 22900] Train: [6/100][26/156] Data 0.001 (0.001) Batch 2.604 (3.381) Remain 13:53:43 loss: 0.5934 Lr: 0.05000
[2023-08-07 19:25:47,986 INFO misc.py line 115 22900] Train: [6/100][27/156] Data 0.001 (0.001) Batch 4.619 (3.433) Remain 14:06:22 loss: 1.7513 Lr: 0.05000
[2023-08-07 19:25:51,487 INFO misc.py line 115 22900] Train: [6/100][28/156] Data 0.001 (0.001) Batch 3.501 (3.436) Remain 14:06:59 loss: 1.3582 Lr: 0.05000
[2023-08-07 19:25:55,487 INFO misc.py line 115 22900] Train: [6/100][29/156] Data 0.001 (0.001) Batch 4.000 (3.457) Remain 14:12:17 loss: 0.9335 Lr: 0.05000
[2023-08-07 19:25:58,075 INFO misc.py line 115 22900] Train: [6/100][30/156] Data 0.001 (0.001) Batch 2.588 (3.425) Remain 14:04:17 loss: 0.9936 Lr: 0.05000
[2023-08-07 19:26:01,411 INFO misc.py line 115 22900] Train: [6/100][31/156] Data 0.001 (0.001) Batch 3.337 (3.422) Remain 14:03:27 loss: 0.9000 Lr: 0.05000
[2023-08-07 19:26:04,619 INFO misc.py line 115 22900] Train: [6/100][32/156] Data 0.001 (0.001) Batch 3.208 (3.415) Remain 14:01:34 loss: 1.0878 Lr: 0.05000
[2023-08-07 19:26:08,655 INFO misc.py line 115 22900] Train: [6/100][33/156] Data 0.001 (0.001) Batch 4.036 (3.435) Remain 14:06:37 loss: 0.8496 Lr: 0.05000
[2023-08-07 19:26:12,141 INFO misc.py line 115 22900] Train: [6/100][34/156] Data 0.001 (0.001) Batch 3.486 (3.437) Remain 14:06:58 loss: 0.7854 Lr: 0.05000
[2023-08-07 19:26:16,528 INFO misc.py line 115 22900] Train: [6/100][35/156] Data 0.001 (0.001) Batch 4.387 (3.467) Remain 14:14:13 loss: 1.2524 Lr: 0.05000
[2023-08-07 19:26:19,192 INFO misc.py line 115 22900] Train: [6/100][36/156] Data 0.001 (0.001) Batch 2.664 (3.442) Remain 14:08:10 loss: 1.0758 Lr: 0.05000
[2023-08-07 19:26:22,660 INFO misc.py line 115 22900] Train: [6/100][37/156] Data 0.001 (0.001) Batch 3.469 (3.443) Remain 14:08:18 loss: 1.0335 Lr: 0.05000
[2023-08-07 19:26:26,380 INFO misc.py line 115 22900] Train: [6/100][38/156] Data 0.001 (0.001) Batch 3.719 (3.451) Remain 14:10:11 loss: 1.2444 Lr: 0.05000
[2023-08-07 19:26:29,466 INFO misc.py line 115 22900] Train: [6/100][39/156] Data 0.001 (0.001) Batch 3.086 (3.441) Remain 14:07:38 loss: 0.9423 Lr: 0.05000
[2023-08-07 19:26:32,531 INFO misc.py line 115 22900] Train: [6/100][40/156] Data 0.001 (0.001) Batch 3.065 (3.431) Remain 14:05:05 loss: 0.8406 Lr: 0.05000
[2023-08-07 19:26:36,058 INFO misc.py line 115 22900] Train: [6/100][41/156] Data 0.001 (0.001) Batch 3.527 (3.433) Remain 14:05:39 loss: 1.4295 Lr: 0.05000
[2023-08-07 19:26:40,021 INFO misc.py line 115 22900] Train: [6/100][42/156] Data 0.001 (0.001) Batch 3.963 (3.447) Remain 14:08:56 loss: 1.3341 Lr: 0.05000
[2023-08-07 19:26:42,656 INFO misc.py line 115 22900] Train: [6/100][43/156] Data 0.001 (0.001) Batch 2.635 (3.426) Remain 14:03:53 loss: 0.8897 Lr: 0.05000
[2023-08-07 19:26:44,995 INFO misc.py line 115 22900] Train: [6/100][44/156] Data 0.001 (0.001) Batch 2.339 (3.400) Remain 13:57:17 loss: 0.5512 Lr: 0.05000
[2023-08-07 19:26:48,709 INFO misc.py line 115 22900] Train: [6/100][45/156] Data 0.001 (0.001) Batch 3.714 (3.407) Remain 13:59:04 loss: 1.2060 Lr: 0.05000
[2023-08-07 19:26:51,758 INFO misc.py line 115 22900] Train: [6/100][46/156] Data 0.001 (0.001) Batch 3.049 (3.399) Remain 13:56:58 loss: 1.0453 Lr: 0.05000
[2023-08-07 19:26:55,194 INFO misc.py line 115 22900] Train: [6/100][47/156] Data 0.001 (0.001) Batch 3.436 (3.400) Remain 13:57:07 loss: 1.3738 Lr: 0.05000
[2023-08-07 19:26:59,225 INFO misc.py line 115 22900] Train: [6/100][48/156] Data 0.001 (0.001) Batch 4.031 (3.414) Remain 14:00:31 loss: 1.0411 Lr: 0.05000
[2023-08-07 19:27:03,125 INFO misc.py line 115 22900] Train: [6/100][49/156] Data 0.001 (0.001) Batch 3.899 (3.425) Remain 14:03:03 loss: 0.9902 Lr: 0.05000
[2023-08-07 19:27:05,892 INFO misc.py line 115 22900] Train: [6/100][50/156] Data 0.001 (0.001) Batch 2.767 (3.411) Remain 13:59:33 loss: 0.9509 Lr: 0.05000
[2023-08-07 19:27:09,215 INFO misc.py line 115 22900] Train: [6/100][51/156] Data 0.001 (0.001) Batch 3.323 (3.409) Remain 13:59:03 loss: 0.7930 Lr: 0.05000
[2023-08-07 19:27:12,226 INFO misc.py line 115 22900] Train: [6/100][52/156] Data 0.001 (0.001) Batch 3.011 (3.401) Remain 13:56:59 loss: 0.9849 Lr: 0.05000
[2023-08-07 19:27:16,248 INFO misc.py line 115 22900] Train: [6/100][53/156] Data 0.001 (0.001) Batch 4.022 (3.413) Remain 14:00:00 loss: 1.1396 Lr: 0.05000
[2023-08-07 19:27:19,843 INFO misc.py line 115 22900] Train: [6/100][54/156] Data 0.001 (0.001) Batch 3.596 (3.417) Remain 14:00:49 loss: 0.7363 Lr: 0.05000
[2023-08-07 19:27:22,757 INFO misc.py line 115 22900] Train: [6/100][55/156] Data 0.001 (0.001) Batch 2.914 (3.407) Remain 13:58:23 loss: 0.9784 Lr: 0.05000
[2023-08-07 19:27:26,145 INFO misc.py line 115 22900] Train: [6/100][56/156] Data 0.001 (0.001) Batch 3.388 (3.407) Remain 13:58:14 loss: 0.9432 Lr: 0.05000
[2023-08-07 19:27:29,217 INFO misc.py line 115 22900] Train: [6/100][57/156] Data 0.001 (0.001) Batch 3.072 (3.400) Remain 13:56:39 loss: 1.1814 Lr: 0.05000
[2023-08-07 19:27:32,555 INFO misc.py line 115 22900] Train: [6/100][58/156] Data 0.001 (0.001) Batch 3.338 (3.399) Remain 13:56:19 loss: 0.8633 Lr: 0.05000
[2023-08-07 19:27:36,665 INFO misc.py line 115 22900] Train: [6/100][59/156] Data 0.001 (0.001) Batch 4.109 (3.412) Remain 13:59:23 loss: 0.8771 Lr: 0.05000
[2023-08-07 19:27:39,576 INFO misc.py line 115 22900] Train: [6/100][60/156] Data 0.001 (0.001) Batch 2.911 (3.403) Remain 13:57:10 loss: 0.9785 Lr: 0.05000
[2023-08-07 19:27:43,681 INFO misc.py line 115 22900] Train: [6/100][61/156] Data 0.001 (0.001) Batch 4.105 (3.415) Remain 14:00:05 loss: 1.1455 Lr: 0.05000
[2023-08-07 19:27:46,830 INFO misc.py line 115 22900] Train: [6/100][62/156] Data 0.001 (0.001) Batch 3.149 (3.411) Remain 13:58:55 loss: 0.7223 Lr: 0.05000
[2023-08-07 19:27:50,540 INFO misc.py line 115 22900] Train: [6/100][63/156] Data 0.001 (0.001) Batch 3.710 (3.416) Remain 14:00:05 loss: 1.2520 Lr: 0.05000
[2023-08-07 19:27:54,148 INFO misc.py line 115 22900] Train: [6/100][64/156] Data 0.001 (0.001) Batch 3.608 (3.419) Remain 14:00:48 loss: 1.0548 Lr: 0.05000
[2023-08-07 19:27:57,458 INFO misc.py line 115 22900] Train: [6/100][65/156] Data 0.001 (0.001) Batch 3.310 (3.417) Remain 14:00:19 loss: 0.7787 Lr: 0.05000
[2023-08-07 19:28:01,573 INFO misc.py line 115 22900] Train: [6/100][66/156] Data 0.001 (0.001) Batch 4.115 (3.428) Remain 14:02:59 loss: 1.5723 Lr: 0.05000
[2023-08-07 19:28:04,215 INFO misc.py line 115 22900] Train: [6/100][67/156] Data 0.001 (0.001) Batch 2.642 (3.416) Remain 13:59:54 loss: 1.0785 Lr: 0.05000
[2023-08-07 19:28:07,511 INFO misc.py line 115 22900] Train: [6/100][68/156] Data 0.001 (0.001) Batch 3.296 (3.414) Remain 13:59:24 loss: 1.3886 Lr: 0.05000
[2023-08-07 19:28:11,153 INFO misc.py line 115 22900] Train: [6/100][69/156] Data 0.001 (0.001) Batch 3.642 (3.418) Remain 14:00:11 loss: 1.1470 Lr: 0.05000
[2023-08-07 19:28:13,660 INFO misc.py line 115 22900] Train: [6/100][70/156] Data 0.001 (0.001) Batch 2.507 (3.404) Remain 13:56:47 loss: 0.6447 Lr: 0.05000
[2023-08-07 19:28:17,676 INFO misc.py line 115 22900] Train: [6/100][71/156] Data 0.001 (0.001) Batch 4.016 (3.413) Remain 13:58:57 loss: 0.8435 Lr: 0.05000
[2023-08-07 19:28:20,790 INFO misc.py line 115 22900] Train: [6/100][72/156] Data 0.001 (0.001) Batch 3.114 (3.409) Remain 13:57:50 loss: 1.0460 Lr: 0.05000
[2023-08-07 19:28:23,510 INFO misc.py line 115 22900] Train: [6/100][73/156] Data 0.001 (0.001) Batch 2.720 (3.399) Remain 13:55:21 loss: 1.1336 Lr: 0.05000
[2023-08-07 19:28:27,154 INFO misc.py line 115 22900] Train: [6/100][74/156] Data 0.001 (0.001) Batch 3.644 (3.402) Remain 13:56:09 loss: 0.8822 Lr: 0.05000
[2023-08-07 19:28:30,607 INFO misc.py line 115 22900] Train: [6/100][75/156] Data 0.001 (0.001) Batch 3.453 (3.403) Remain 13:56:16 loss: 1.1293 Lr: 0.05000
[2023-08-07 19:28:33,662 INFO misc.py line 115 22900] Train: [6/100][76/156] Data 0.001 (0.001) Batch 3.055 (3.398) Remain 13:55:02 loss: 0.9831 Lr: 0.05000
[2023-08-07 19:28:37,821 INFO misc.py line 115 22900] Train: [6/100][77/156] Data 0.001 (0.001) Batch 4.159 (3.408) Remain 13:57:30 loss: 1.4200 Lr: 0.05000
[2023-08-07 19:28:41,298 INFO misc.py line 115 22900] Train: [6/100][78/156] Data 0.001 (0.001) Batch 3.476 (3.409) Remain 13:57:40 loss: 1.0171 Lr: 0.05000
[2023-08-07 19:28:45,178 INFO misc.py line 115 22900] Train: [6/100][79/156] Data 0.001 (0.001) Batch 3.880 (3.416) Remain 13:59:08 loss: 0.9890 Lr: 0.05000
[2023-08-07 19:28:48,393 INFO misc.py line 115 22900] Train: [6/100][80/156] Data 0.001 (0.001) Batch 3.215 (3.413) Remain 13:58:26 loss: 0.5986 Lr: 0.05000
[2023-08-07 19:28:53,174 INFO misc.py line 115 22900] Train: [6/100][81/156] Data 0.001 (0.001) Batch 4.781 (3.430) Remain 14:02:41 loss: 1.7017 Lr: 0.05000
[2023-08-07 19:28:55,796 INFO misc.py line 115 22900] Train: [6/100][82/156] Data 0.001 (0.001) Batch 2.623 (3.420) Remain 14:00:07 loss: 1.4180 Lr: 0.05000
[2023-08-07 19:28:59,478 INFO misc.py line 115 22900] Train: [6/100][83/156] Data 0.001 (0.001) Batch 3.682 (3.424) Remain 14:00:52 loss: 0.8596 Lr: 0.05000
[2023-08-07 19:29:02,808 INFO misc.py line 115 22900] Train: [6/100][84/156] Data 0.001 (0.001) Batch 3.329 (3.422) Remain 14:00:31 loss: 0.8757 Lr: 0.05000
[2023-08-07 19:29:06,586 INFO misc.py line 115 22900] Train: [6/100][85/156] Data 0.001 (0.001) Batch 3.778 (3.427) Remain 14:01:32 loss: 1.1616 Lr: 0.05000
[2023-08-07 19:29:10,436 INFO misc.py line 115 22900] Train: [6/100][86/156] Data 0.001 (0.001) Batch 3.850 (3.432) Remain 14:02:44 loss: 0.7737 Lr: 0.05000
[2023-08-07 19:29:15,193 INFO misc.py line 115 22900] Train: [6/100][87/156] Data 0.001 (0.001) Batch 4.757 (3.448) Remain 14:06:33 loss: 1.4687 Lr: 0.05000
[2023-08-07 19:29:19,557 INFO misc.py line 115 22900] Train: [6/100][88/156] Data 0.001 (0.001) Batch 4.364 (3.458) Remain 14:09:08 loss: 1.3335 Lr: 0.05000
[2023-08-07 19:29:24,189 INFO misc.py line 115 22900] Train: [6/100][89/156] Data 0.001 (0.001) Batch 4.632 (3.472) Remain 14:12:26 loss: 1.5787 Lr: 0.05000
[2023-08-07 19:29:27,264 INFO misc.py line 115 22900] Train: [6/100][90/156] Data 0.001 (0.001) Batch 3.075 (3.467) Remain 14:11:15 loss: 1.3436 Lr: 0.05000
[2023-08-07 19:29:30,748 INFO misc.py line 115 22900] Train: [6/100][91/156] Data 0.001 (0.001) Batch 3.484 (3.468) Remain 14:11:14 loss: 0.9266 Lr: 0.05000
[2023-08-07 19:29:34,384 INFO misc.py line 115 22900] Train: [6/100][92/156] Data 0.001 (0.001) Batch 3.636 (3.470) Remain 14:11:39 loss: 0.9944 Lr: 0.05000
[2023-08-07 19:29:37,813 INFO misc.py line 115 22900] Train: [6/100][93/156] Data 0.001 (0.001) Batch 3.429 (3.469) Remain 14:11:29 loss: 0.8778 Lr: 0.05000
[2023-08-07 19:29:41,833 INFO misc.py line 115 22900] Train: [6/100][94/156] Data 0.001 (0.001) Batch 4.020 (3.475) Remain 14:12:54 loss: 1.1088 Lr: 0.04999
[2023-08-07 19:29:44,657 INFO misc.py line 115 22900] Train: [6/100][95/156] Data 0.001 (0.001) Batch 2.824 (3.468) Remain 14:11:06 loss: 0.8342 Lr: 0.04999
[2023-08-07 19:29:48,262 INFO misc.py line 115 22900] Train: [6/100][96/156] Data 0.001 (0.001) Batch 3.605 (3.470) Remain 14:11:25 loss: 0.6412 Lr: 0.04999
[2023-08-07 19:29:51,490 INFO misc.py line 115 22900] Train: [6/100][97/156] Data 0.001 (0.001) Batch 3.228 (3.467) Remain 14:10:43 loss: 0.7776 Lr: 0.04999
[2023-08-07 19:29:55,576 INFO misc.py line 115 22900] Train: [6/100][98/156] Data 0.001 (0.001) Batch 4.085 (3.473) Remain 14:12:16 loss: 1.3190 Lr: 0.04999
[2023-08-07 19:29:59,033 INFO misc.py line 115 22900] Train: [6/100][99/156] Data 0.001 (0.001) Batch 3.458 (3.473) Remain 14:12:10 loss: 0.6565 Lr: 0.04999
[2023-08-07 19:30:02,544 INFO misc.py line 115 22900] Train: [6/100][100/156] Data 0.001 (0.001) Batch 3.511 (3.474) Remain 14:12:12 loss: 1.2379 Lr: 0.04999
[2023-08-07 19:30:06,147 INFO misc.py line 115 22900] Train: [6/100][101/156] Data 0.001 (0.001) Batch 3.603 (3.475) Remain 14:12:28 loss: 1.0962 Lr: 0.04999
[2023-08-07 19:30:09,399 INFO misc.py line 115 22900] Train: [6/100][102/156] Data 0.001 (0.001) Batch 3.252 (3.473) Remain 14:11:51 loss: 0.7773 Lr: 0.04999
[2023-08-07 19:30:12,345 INFO misc.py line 115 22900] Train: [6/100][103/156] Data 0.001 (0.001) Batch 2.946 (3.467) Remain 14:10:30 loss: 0.8035 Lr: 0.04999
[2023-08-07 19:30:15,551 INFO misc.py line 115 22900] Train: [6/100][104/156] Data 0.001 (0.001) Batch 3.206 (3.465) Remain 14:09:49 loss: 0.7808 Lr: 0.04999
[2023-08-07 19:30:19,122 INFO misc.py line 115 22900] Train: [6/100][105/156] Data 0.001 (0.001) Batch 3.571 (3.466) Remain 14:10:01 loss: 1.0652 Lr: 0.04999
[2023-08-07 19:30:21,752 INFO misc.py line 115 22900] Train: [6/100][106/156] Data 0.001 (0.001) Batch 2.629 (3.458) Remain 14:07:58 loss: 0.9307 Lr: 0.04999
[2023-08-07 19:30:24,014 INFO misc.py line 115 22900] Train: [6/100][107/156] Data 0.001 (0.001) Batch 2.263 (3.446) Remain 14:05:05 loss: 0.8253 Lr: 0.04999
[2023-08-07 19:30:27,699 INFO misc.py line 115 22900] Train: [6/100][108/156] Data 0.001 (0.001) Batch 3.685 (3.449) Remain 14:05:35 loss: 1.3746 Lr: 0.04999
[2023-08-07 19:30:31,090 INFO misc.py line 115 22900] Train: [6/100][109/156] Data 0.001 (0.001) Batch 3.391 (3.448) Remain 14:05:24 loss: 0.8168 Lr: 0.04999
[2023-08-07 19:30:35,147 INFO misc.py line 115 22900] Train: [6/100][110/156] Data 0.001 (0.001) Batch 4.056 (3.454) Remain 14:06:44 loss: 1.0102 Lr: 0.04999
[2023-08-07 19:30:38,237 INFO misc.py line 115 22900] Train: [6/100][111/156] Data 0.001 (0.001) Batch 3.091 (3.450) Remain 14:05:51 loss: 0.9092 Lr: 0.04999
[2023-08-07 19:30:42,210 INFO misc.py line 115 22900] Train: [6/100][112/156] Data 0.001 (0.001) Batch 3.973 (3.455) Remain 14:06:58 loss: 1.0374 Lr: 0.04999
[2023-08-07 19:30:45,995 INFO misc.py line 115 22900] Train: [6/100][113/156] Data 0.001 (0.001) Batch 3.785 (3.458) Remain 14:07:39 loss: 0.9168 Lr: 0.04999
[2023-08-07 19:30:49,582 INFO misc.py line 115 22900] Train: [6/100][114/156] Data 0.001 (0.001) Batch 3.588 (3.459) Remain 14:07:52 loss: 1.2362 Lr: 0.04999
[2023-08-07 19:30:53,770 INFO misc.py line 115 22900] Train: [6/100][115/156] Data 0.001 (0.001) Batch 4.187 (3.466) Remain 14:09:25 loss: 1.0109 Lr: 0.04999
[2023-08-07 19:30:57,694 INFO misc.py line 115 22900] Train: [6/100][116/156] Data 0.001 (0.001) Batch 3.924 (3.470) Remain 14:10:21 loss: 1.6425 Lr: 0.04999
[2023-08-07 19:31:01,164 INFO misc.py line 115 22900] Train: [6/100][117/156] Data 0.001 (0.001) Batch 3.470 (3.470) Remain 14:10:17 loss: 0.9230 Lr: 0.04999
[2023-08-07 19:31:03,788 INFO misc.py line 115 22900] Train: [6/100][118/156] Data 0.001 (0.001) Batch 2.624 (3.463) Remain 14:08:26 loss: 1.4032 Lr: 0.04999
[2023-08-07 19:31:08,169 INFO misc.py line 115 22900] Train: [6/100][119/156] Data 0.001 (0.001) Batch 4.381 (3.470) Remain 14:10:19 loss: 1.4891 Lr: 0.04999
[2023-08-07 19:31:12,765 INFO misc.py line 115 22900] Train: [6/100][120/156] Data 0.001 (0.001) Batch 4.596 (3.480) Remain 14:12:37 loss: 1.3092 Lr: 0.04999
[2023-08-07 19:31:15,991 INFO misc.py line 115 22900] Train: [6/100][121/156] Data 0.001 (0.001) Batch 3.226 (3.478) Remain 14:12:01 loss: 1.3711 Lr: 0.04999
[2023-08-07 19:31:19,411 INFO misc.py line 115 22900] Train: [6/100][122/156] Data 0.001 (0.001) Batch 3.420 (3.477) Remain 14:11:51 loss: 0.9725 Lr: 0.04999
[2023-08-07 19:31:22,597 INFO misc.py line 115 22900] Train: [6/100][123/156] Data 0.001 (0.001) Batch 3.187 (3.475) Remain 14:11:12 loss: 0.8625 Lr: 0.04999
[2023-08-07 19:31:26,623 INFO misc.py line 115 22900] Train: [6/100][124/156] Data 0.001 (0.001) Batch 4.026 (3.480) Remain 14:12:15 loss: 0.8473 Lr: 0.04999
[2023-08-07 19:31:29,680 INFO misc.py line 115 22900] Train: [6/100][125/156] Data 0.001 (0.001) Batch 3.057 (3.476) Remain 14:11:21 loss: 1.2149 Lr: 0.04999
[2023-08-07 19:31:34,205 INFO misc.py line 115 22900] Train: [6/100][126/156] Data 0.001 (0.001) Batch 4.525 (3.485) Remain 14:13:23 loss: 1.5966 Lr: 0.04999
[2023-08-07 19:31:37,811 INFO misc.py line 115 22900] Train: [6/100][127/156] Data 0.001 (0.001) Batch 3.606 (3.486) Remain 14:13:33 loss: 0.8758 Lr: 0.04999
[2023-08-07 19:31:41,334 INFO misc.py line 115 22900] Train: [6/100][128/156] Data 0.001 (0.001) Batch 3.523 (3.486) Remain 14:13:34 loss: 0.9957 Lr: 0.04999
[2023-08-07 19:31:43,970 INFO misc.py line 115 22900] Train: [6/100][129/156] Data 0.001 (0.001) Batch 2.636 (3.479) Remain 14:11:52 loss: 1.0870 Lr: 0.04999
[2023-08-07 19:31:48,021 INFO misc.py line 115 22900] Train: [6/100][130/156] Data 0.001 (0.001) Batch 4.052 (3.484) Remain 14:12:54 loss: 1.0217 Lr: 0.04999
[2023-08-07 19:31:51,801 INFO misc.py line 115 22900] Train: [6/100][131/156] Data 0.001 (0.001) Batch 3.779 (3.486) Remain 14:13:25 loss: 1.3715 Lr: 0.04999
[2023-08-07 19:31:55,920 INFO misc.py line 115 22900] Train: [6/100][132/156] Data 0.001 (0.001) Batch 4.119 (3.491) Remain 14:14:33 loss: 0.9664 Lr: 0.04999
[2023-08-07 19:31:58,555 INFO misc.py line 115 22900] Train: [6/100][133/156] Data 0.001 (0.001) Batch 2.635 (3.484) Remain 14:12:53 loss: 1.2954 Lr: 0.04999
[2023-08-07 19:32:02,351 INFO misc.py line 115 22900] Train: [6/100][134/156] Data 0.001 (0.001) Batch 3.796 (3.487) Remain 14:13:25 loss: 0.9308 Lr: 0.04999
[2023-08-07 19:32:05,106 INFO misc.py line 115 22900] Train: [6/100][135/156] Data 0.001 (0.001) Batch 2.755 (3.481) Remain 14:12:00 loss: 1.4008 Lr: 0.04999
[2023-08-07 19:32:09,451 INFO misc.py line 115 22900] Train: [6/100][136/156] Data 0.001 (0.001) Batch 4.345 (3.488) Remain 14:13:32 loss: 1.1171 Lr: 0.04999
[2023-08-07 19:32:12,704 INFO misc.py line 115 22900] Train: [6/100][137/156] Data 0.001 (0.001) Batch 3.253 (3.486) Remain 14:13:03 loss: 0.8822 Lr: 0.04999
[2023-08-07 19:32:16,215 INFO misc.py line 115 22900] Train: [6/100][138/156] Data 0.001 (0.001) Batch 3.511 (3.486) Remain 14:13:02 loss: 0.8059 Lr: 0.04999
[2023-08-07 19:32:19,405 INFO misc.py line 115 22900] Train: [6/100][139/156] Data 0.001 (0.001) Batch 3.190 (3.484) Remain 14:12:26 loss: 1.0030 Lr: 0.04999
[2023-08-07 19:32:23,425 INFO misc.py line 115 22900] Train: [6/100][140/156] Data 0.001 (0.001) Batch 4.021 (3.488) Remain 14:13:20 loss: 1.5802 Lr: 0.04999
[2023-08-07 19:32:26,022 INFO misc.py line 115 22900] Train: [6/100][141/156] Data 0.001 (0.001) Batch 2.596 (3.481) Remain 14:11:42 loss: 0.8470 Lr: 0.04999
[2023-08-07 19:32:28,632 INFO misc.py line 115 22900] Train: [6/100][142/156] Data 0.001 (0.001) Batch 2.611 (3.475) Remain 14:10:07 loss: 0.9304 Lr: 0.04999
[2023-08-07 19:32:32,065 INFO misc.py line 115 22900] Train: [6/100][143/156] Data 0.001 (0.001) Batch 3.432 (3.475) Remain 14:09:59 loss: 1.2855 Lr: 0.04999
[2023-08-07 19:32:34,544 INFO misc.py line 115 22900] Train: [6/100][144/156] Data 0.001 (0.001) Batch 2.479 (3.468) Remain 14:08:12 loss: 0.7095 Lr: 0.04999
[2023-08-07 19:32:37,017 INFO misc.py line 115 22900] Train: [6/100][145/156] Data 0.001 (0.001) Batch 2.473 (3.461) Remain 14:06:25 loss: 1.0134 Lr: 0.04999
[2023-08-07 19:32:38,989 INFO misc.py line 115 22900] Train: [6/100][146/156] Data 0.001 (0.001) Batch 1.972 (3.450) Remain 14:03:49 loss: 1.1945 Lr: 0.04999
[2023-08-07 19:32:42,960 INFO misc.py line 115 22900] Train: [6/100][147/156] Data 0.001 (0.001) Batch 3.970 (3.454) Remain 14:04:39 loss: 1.3069 Lr: 0.04999
[2023-08-07 19:32:46,939 INFO misc.py line 115 22900] Train: [6/100][148/156] Data 0.001 (0.001) Batch 3.980 (3.458) Remain 14:05:28 loss: 1.2458 Lr: 0.04999
[2023-08-07 19:32:51,014 INFO misc.py line 115 22900] Train: [6/100][149/156] Data 0.001 (0.001) Batch 4.075 (3.462) Remain 14:06:27 loss: 1.5344 Lr: 0.04999
[2023-08-07 19:32:55,380 INFO misc.py line 115 22900] Train: [6/100][150/156] Data 0.001 (0.001) Batch 4.365 (3.468) Remain 14:07:54 loss: 1.2754 Lr: 0.04999
[2023-08-07 19:32:58,741 INFO misc.py line 115 22900] Train: [6/100][151/156] Data 0.001 (0.001) Batch 3.361 (3.467) Remain 14:07:40 loss: 1.1049 Lr: 0.04999
[2023-08-07 19:33:00,738 INFO misc.py line 115 22900] Train: [6/100][152/156] Data 0.001 (0.001) Batch 1.997 (3.457) Remain 14:05:12 loss: 0.9767 Lr: 0.04999
[2023-08-07 19:33:03,424 INFO misc.py line 115 22900] Train: [6/100][153/156] Data 0.001 (0.001) Batch 2.686 (3.452) Remain 14:03:53 loss: 0.8157 Lr: 0.04999
[2023-08-07 19:33:06,935 INFO misc.py line 115 22900] Train: [6/100][154/156] Data 0.001 (0.001) Batch 3.510 (3.453) Remain 14:03:55 loss: 0.9132 Lr: 0.04999
[2023-08-07 19:33:10,968 INFO misc.py line 115 22900] Train: [6/100][155/156] Data 0.001 (0.001) Batch 4.034 (3.456) Remain 14:04:47 loss: 0.8573 Lr: 0.04999
[2023-08-07 19:33:14,261 INFO misc.py line 115 22900] Train: [6/100][156/156] Data 0.001 (0.001) Batch 3.293 (3.455) Remain 14:04:28 loss: 1.0772 Lr: 0.04999
[2023-08-07 19:33:14,262 INFO misc.py line 129 22900] Train result: loss: 1.0736 
[2023-08-07 19:33:14,262 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 19:33:16,348 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.9116 
[2023-08-07 19:33:17,218 INFO evaluator.py line 122 22900] Test: [2/24] Loss 1.1042 
[2023-08-07 19:33:18,881 INFO evaluator.py line 122 22900] Test: [3/24] Loss 1.2962 
[2023-08-07 19:33:20,402 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.0732 
[2023-08-07 19:33:22,248 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.7876 
[2023-08-07 19:33:23,912 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.7637 
[2023-08-07 19:33:26,048 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.8429 
[2023-08-07 19:33:27,856 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.8699 
[2023-08-07 19:33:29,139 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.1934 
[2023-08-07 19:33:31,269 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3009 
[2023-08-07 19:33:31,794 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.7935 
[2023-08-07 19:33:33,329 INFO evaluator.py line 122 22900] Test: [12/24] Loss 1.1501 
[2023-08-07 19:33:36,042 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.1245 
[2023-08-07 19:33:37,723 INFO evaluator.py line 122 22900] Test: [14/24] Loss 1.2603 
[2023-08-07 19:33:39,746 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.5867 
[2023-08-07 19:33:42,457 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.1788 
[2023-08-07 19:33:45,163 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.3285 
[2023-08-07 19:33:47,010 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.8255 
[2023-08-07 19:33:47,760 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.3537 
[2023-08-07 19:33:48,645 INFO evaluator.py line 122 22900] Test: [20/24] Loss 1.4260 
[2023-08-07 19:33:50,907 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.2249 
[2023-08-07 19:33:52,873 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.7784 
[2023-08-07 19:33:54,719 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.4486 
[2023-08-07 19:33:56,655 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.7819 
[2023-08-07 19:33:56,702 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.1299/0.1986/0.6299.
[2023-08-07 19:33:56,702 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6365/0.9445
[2023-08-07 19:33:56,702 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9306/0.9911
[2023-08-07 19:33:56,702 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.0313/0.0750
[2023-08-07 19:33:56,702 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.0669/0.3202
[2023-08-07 19:33:56,702 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.4564/0.8712
[2023-08-07 19:33:56,702 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.0019/0.0019
[2023-08-07 19:33:56,702 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.2975/0.3822
[2023-08-07 19:33:56,702 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:33:56,702 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.0496/0.0872
[2023-08-07 19:33:56,702 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0134/0.0137
[2023-08-07 19:33:56,702 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:33:56,703 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:33:56,703 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.0001/0.0001
[2023-08-07 19:33:56,703 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0086/0.0091
[2023-08-07 19:33:56,703 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0019/0.0035
[2023-08-07 19:33:56,703 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:33:56,703 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:33:56,703 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.0729/0.2321
[2023-08-07 19:33:56,703 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:33:56,703 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0301/0.0406
[2023-08-07 19:33:56,703 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 19:33:56,703 INFO misc.py line 150 22900] Best validation mIoU updated to: 0.1299
[2023-08-07 19:33:56,703 INFO misc.py line 152 22900] Currently Best mIoU: 0.1299
[2023-08-07 19:33:56,703 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 19:34:02,270 INFO misc.py line 115 22900] Train: [7/100][1/156] Data 0.752 (0.752) Batch 4.492 (4.492) Remain 18:17:44 loss: 0.7602 Lr: 0.04999
[2023-08-07 19:34:05,235 INFO misc.py line 115 22900] Train: [7/100][2/156] Data 0.001 (0.001) Batch 2.965 (2.965) Remain 12:04:36 loss: 0.8516 Lr: 0.04999
[2023-08-07 19:34:08,993 INFO misc.py line 115 22900] Train: [7/100][3/156] Data 0.001 (0.001) Batch 3.758 (3.758) Remain 15:18:22 loss: 0.8807 Lr: 0.04999
[2023-08-07 19:34:13,101 INFO misc.py line 115 22900] Train: [7/100][4/156] Data 0.001 (0.001) Batch 4.108 (4.108) Remain 16:43:45 loss: 1.1124 Lr: 0.04999
[2023-08-07 19:34:16,709 INFO misc.py line 115 22900] Train: [7/100][5/156] Data 0.001 (0.001) Batch 3.608 (3.858) Remain 15:42:36 loss: 1.1368 Lr: 0.04999
[2023-08-07 19:34:20,444 INFO misc.py line 115 22900] Train: [7/100][6/156] Data 0.001 (0.001) Batch 3.735 (3.817) Remain 15:32:29 loss: 0.9209 Lr: 0.04999
[2023-08-07 19:34:22,807 INFO misc.py line 115 22900] Train: [7/100][7/156] Data 0.001 (0.001) Batch 2.363 (3.453) Remain 14:03:37 loss: 0.8824 Lr: 0.04998
[2023-08-07 19:34:26,779 INFO misc.py line 115 22900] Train: [7/100][8/156] Data 0.001 (0.001) Batch 3.972 (3.557) Remain 14:28:54 loss: 1.0443 Lr: 0.04998
[2023-08-07 19:34:31,162 INFO misc.py line 115 22900] Train: [7/100][9/156] Data 0.001 (0.001) Batch 4.383 (3.695) Remain 15:02:27 loss: 1.3510 Lr: 0.04998
[2023-08-07 19:34:35,293 INFO misc.py line 115 22900] Train: [7/100][10/156] Data 0.001 (0.001) Batch 4.131 (3.757) Remain 15:17:36 loss: 1.2018 Lr: 0.04998
[2023-08-07 19:34:39,065 INFO misc.py line 115 22900] Train: [7/100][11/156] Data 0.001 (0.001) Batch 3.772 (3.759) Remain 15:17:59 loss: 0.8376 Lr: 0.04998
[2023-08-07 19:34:42,696 INFO misc.py line 115 22900] Train: [7/100][12/156] Data 0.001 (0.001) Batch 3.631 (3.745) Remain 15:14:27 loss: 0.9568 Lr: 0.04998
[2023-08-07 19:34:47,023 INFO misc.py line 115 22900] Train: [7/100][13/156] Data 0.001 (0.001) Batch 4.327 (3.803) Remain 15:28:36 loss: 1.2531 Lr: 0.04998
[2023-08-07 19:34:50,334 INFO misc.py line 115 22900] Train: [7/100][14/156] Data 0.001 (0.001) Batch 3.311 (3.758) Remain 15:17:37 loss: 0.9876 Lr: 0.04998
[2023-08-07 19:34:53,708 INFO misc.py line 115 22900] Train: [7/100][15/156] Data 0.001 (0.001) Batch 3.375 (3.726) Remain 15:09:45 loss: 1.1272 Lr: 0.04998
[2023-08-07 19:34:58,105 INFO misc.py line 115 22900] Train: [7/100][16/156] Data 0.001 (0.001) Batch 4.396 (3.778) Remain 15:22:16 loss: 1.1030 Lr: 0.04998
[2023-08-07 19:35:01,538 INFO misc.py line 115 22900] Train: [7/100][17/156] Data 0.001 (0.001) Batch 3.433 (3.753) Remain 15:16:12 loss: 0.9256 Lr: 0.04998
[2023-08-07 19:35:05,195 INFO misc.py line 115 22900] Train: [7/100][18/156] Data 0.001 (0.001) Batch 3.658 (3.747) Remain 15:14:35 loss: 0.9416 Lr: 0.04998
[2023-08-07 19:35:09,334 INFO misc.py line 115 22900] Train: [7/100][19/156] Data 0.001 (0.001) Batch 4.139 (3.771) Remain 15:20:30 loss: 0.8798 Lr: 0.04998
[2023-08-07 19:35:12,447 INFO misc.py line 115 22900] Train: [7/100][20/156] Data 0.001 (0.001) Batch 3.113 (3.733) Remain 15:10:59 loss: 1.1506 Lr: 0.04998
[2023-08-07 19:35:14,999 INFO misc.py line 115 22900] Train: [7/100][21/156] Data 0.001 (0.001) Batch 2.552 (3.667) Remain 14:54:55 loss: 0.6924 Lr: 0.04998
[2023-08-07 19:35:19,116 INFO misc.py line 115 22900] Train: [7/100][22/156] Data 0.001 (0.001) Batch 4.117 (3.691) Remain 15:00:38 loss: 1.3419 Lr: 0.04998
[2023-08-07 19:35:21,540 INFO misc.py line 115 22900] Train: [7/100][23/156] Data 0.001 (0.001) Batch 2.424 (3.627) Remain 14:45:07 loss: 0.8141 Lr: 0.04998
[2023-08-07 19:35:23,428 INFO misc.py line 115 22900] Train: [7/100][24/156] Data 0.001 (0.001) Batch 1.888 (3.544) Remain 14:24:51 loss: 0.8431 Lr: 0.04998
[2023-08-07 19:35:26,588 INFO misc.py line 115 22900] Train: [7/100][25/156] Data 0.001 (0.001) Batch 3.160 (3.527) Remain 14:20:32 loss: 1.2517 Lr: 0.04998
[2023-08-07 19:35:31,002 INFO misc.py line 115 22900] Train: [7/100][26/156] Data 0.001 (0.001) Batch 4.414 (3.566) Remain 14:29:53 loss: 1.0758 Lr: 0.04998
[2023-08-07 19:35:33,909 INFO misc.py line 115 22900] Train: [7/100][27/156] Data 0.001 (0.001) Batch 2.907 (3.538) Remain 14:23:07 loss: 0.7208 Lr: 0.04998
[2023-08-07 19:35:37,365 INFO misc.py line 115 22900] Train: [7/100][28/156] Data 0.001 (0.001) Batch 3.456 (3.535) Remain 14:22:16 loss: 1.0295 Lr: 0.04998
[2023-08-07 19:35:39,547 INFO misc.py line 115 22900] Train: [7/100][29/156] Data 0.001 (0.001) Batch 2.183 (3.483) Remain 14:09:31 loss: 0.8120 Lr: 0.04998
[2023-08-07 19:35:43,397 INFO misc.py line 115 22900] Train: [7/100][30/156] Data 0.001 (0.001) Batch 3.850 (3.496) Remain 14:12:46 loss: 1.2667 Lr: 0.04998
[2023-08-07 19:35:47,028 INFO misc.py line 115 22900] Train: [7/100][31/156] Data 0.001 (0.001) Batch 3.631 (3.501) Remain 14:13:53 loss: 1.1113 Lr: 0.04998
[2023-08-07 19:35:51,600 INFO misc.py line 115 22900] Train: [7/100][32/156] Data 0.001 (0.001) Batch 4.572 (3.538) Remain 14:22:50 loss: 1.5291 Lr: 0.04998
[2023-08-07 19:35:55,413 INFO misc.py line 115 22900] Train: [7/100][33/156] Data 0.001 (0.001) Batch 3.813 (3.547) Remain 14:25:00 loss: 1.2312 Lr: 0.04998
[2023-08-07 19:35:59,402 INFO misc.py line 115 22900] Train: [7/100][34/156] Data 0.001 (0.001) Batch 3.989 (3.562) Remain 14:28:25 loss: 1.2827 Lr: 0.04998
[2023-08-07 19:36:02,943 INFO misc.py line 115 22900] Train: [7/100][35/156] Data 0.001 (0.001) Batch 3.542 (3.561) Remain 14:28:13 loss: 1.2600 Lr: 0.04998
[2023-08-07 19:36:06,817 INFO misc.py line 115 22900] Train: [7/100][36/156] Data 0.001 (0.001) Batch 3.874 (3.570) Remain 14:30:28 loss: 1.0763 Lr: 0.04998
[2023-08-07 19:36:09,834 INFO misc.py line 115 22900] Train: [7/100][37/156] Data 0.001 (0.001) Batch 3.016 (3.554) Remain 14:26:26 loss: 0.8870 Lr: 0.04998
[2023-08-07 19:36:13,291 INFO misc.py line 115 22900] Train: [7/100][38/156] Data 0.001 (0.001) Batch 3.457 (3.551) Remain 14:25:42 loss: 1.1518 Lr: 0.04998
[2023-08-07 19:36:16,760 INFO misc.py line 115 22900] Train: [7/100][39/156] Data 0.001 (0.001) Batch 3.470 (3.549) Remain 14:25:05 loss: 0.9654 Lr: 0.04998
[2023-08-07 19:36:20,064 INFO misc.py line 115 22900] Train: [7/100][40/156] Data 0.001 (0.001) Batch 3.304 (3.542) Remain 14:23:24 loss: 1.0599 Lr: 0.04998
[2023-08-07 19:36:23,130 INFO misc.py line 115 22900] Train: [7/100][41/156] Data 0.001 (0.001) Batch 3.066 (3.530) Remain 14:20:18 loss: 0.6579 Lr: 0.04998
[2023-08-07 19:36:27,200 INFO misc.py line 115 22900] Train: [7/100][42/156] Data 0.001 (0.001) Batch 4.070 (3.544) Remain 14:23:37 loss: 1.0811 Lr: 0.04998
[2023-08-07 19:36:30,602 INFO misc.py line 115 22900] Train: [7/100][43/156] Data 0.001 (0.001) Batch 3.402 (3.540) Remain 14:22:41 loss: 1.0906 Lr: 0.04998
[2023-08-07 19:36:34,260 INFO misc.py line 115 22900] Train: [7/100][44/156] Data 0.001 (0.001) Batch 3.658 (3.543) Remain 14:23:19 loss: 0.8866 Lr: 0.04998
[2023-08-07 19:36:38,315 INFO misc.py line 115 22900] Train: [7/100][45/156] Data 0.001 (0.001) Batch 4.055 (3.555) Remain 14:26:14 loss: 1.2382 Lr: 0.04998
[2023-08-07 19:36:41,979 INFO misc.py line 115 22900] Train: [7/100][46/156] Data 0.001 (0.001) Batch 3.664 (3.558) Remain 14:26:48 loss: 1.3127 Lr: 0.04998
[2023-08-07 19:36:45,026 INFO misc.py line 115 22900] Train: [7/100][47/156] Data 0.001 (0.001) Batch 3.047 (3.546) Remain 14:23:54 loss: 0.9583 Lr: 0.04998
[2023-08-07 19:36:48,473 INFO misc.py line 115 22900] Train: [7/100][48/156] Data 0.001 (0.001) Batch 3.447 (3.544) Remain 14:23:19 loss: 1.2607 Lr: 0.04998
[2023-08-07 19:36:51,870 INFO misc.py line 115 22900] Train: [7/100][49/156] Data 0.001 (0.001) Batch 3.396 (3.541) Remain 14:22:28 loss: 1.1088 Lr: 0.04998
[2023-08-07 19:36:54,480 INFO misc.py line 115 22900] Train: [7/100][50/156] Data 0.001 (0.001) Batch 2.610 (3.521) Remain 14:17:35 loss: 0.6756 Lr: 0.04998
[2023-08-07 19:36:58,180 INFO misc.py line 115 22900] Train: [7/100][51/156] Data 0.001 (0.001) Batch 3.700 (3.525) Remain 14:18:26 loss: 1.0819 Lr: 0.04998
[2023-08-07 19:37:01,441 INFO misc.py line 115 22900] Train: [7/100][52/156] Data 0.001 (0.001) Batch 3.261 (3.519) Remain 14:17:04 loss: 0.7794 Lr: 0.04998
[2023-08-07 19:37:04,924 INFO misc.py line 115 22900] Train: [7/100][53/156] Data 0.001 (0.001) Batch 3.483 (3.519) Remain 14:16:50 loss: 1.1896 Lr: 0.04998
[2023-08-07 19:37:08,071 INFO misc.py line 115 22900] Train: [7/100][54/156] Data 0.001 (0.001) Batch 3.146 (3.511) Remain 14:15:00 loss: 0.8043 Lr: 0.04997
[2023-08-07 19:37:10,969 INFO misc.py line 115 22900] Train: [7/100][55/156] Data 0.001 (0.001) Batch 2.898 (3.500) Remain 14:12:04 loss: 0.6532 Lr: 0.04997
[2023-08-07 19:37:14,109 INFO misc.py line 115 22900] Train: [7/100][56/156] Data 0.001 (0.001) Batch 3.139 (3.493) Remain 14:10:21 loss: 1.2626 Lr: 0.04997
[2023-08-07 19:37:16,775 INFO misc.py line 115 22900] Train: [7/100][57/156] Data 0.001 (0.001) Batch 2.667 (3.477) Remain 14:06:34 loss: 0.8187 Lr: 0.04997
[2023-08-07 19:37:19,613 INFO misc.py line 115 22900] Train: [7/100][58/156] Data 0.001 (0.001) Batch 2.838 (3.466) Remain 14:03:41 loss: 0.8807 Lr: 0.04997
[2023-08-07 19:37:22,981 INFO misc.py line 115 22900] Train: [7/100][59/156] Data 0.001 (0.001) Batch 3.367 (3.464) Remain 14:03:12 loss: 0.9803 Lr: 0.04997
[2023-08-07 19:37:25,720 INFO misc.py line 115 22900] Train: [7/100][60/156] Data 0.001 (0.001) Batch 2.740 (3.451) Remain 14:00:03 loss: 0.8019 Lr: 0.04997
[2023-08-07 19:37:29,618 INFO misc.py line 115 22900] Train: [7/100][61/156] Data 0.001 (0.001) Batch 3.897 (3.459) Remain 14:01:52 loss: 1.1868 Lr: 0.04997
[2023-08-07 19:37:33,505 INFO misc.py line 115 22900] Train: [7/100][62/156] Data 0.001 (0.001) Batch 3.888 (3.466) Remain 14:03:34 loss: 1.1757 Lr: 0.04997
[2023-08-07 19:37:37,592 INFO misc.py line 115 22900] Train: [7/100][63/156] Data 0.001 (0.001) Batch 4.087 (3.477) Remain 14:06:02 loss: 0.8092 Lr: 0.04997
[2023-08-07 19:37:41,051 INFO misc.py line 115 22900] Train: [7/100][64/156] Data 0.001 (0.001) Batch 3.459 (3.476) Remain 14:05:54 loss: 0.9454 Lr: 0.04997
[2023-08-07 19:37:44,291 INFO misc.py line 115 22900] Train: [7/100][65/156] Data 0.001 (0.001) Batch 3.239 (3.473) Remain 14:04:55 loss: 0.5795 Lr: 0.04997
[2023-08-07 19:37:46,456 INFO misc.py line 115 22900] Train: [7/100][66/156] Data 0.001 (0.001) Batch 2.166 (3.452) Remain 13:59:49 loss: 0.6278 Lr: 0.04997
[2023-08-07 19:37:49,798 INFO misc.py line 115 22900] Train: [7/100][67/156] Data 0.001 (0.001) Batch 3.342 (3.450) Remain 13:59:20 loss: 0.9288 Lr: 0.04997
[2023-08-07 19:37:53,435 INFO misc.py line 115 22900] Train: [7/100][68/156] Data 0.001 (0.001) Batch 3.637 (3.453) Remain 13:59:59 loss: 0.6447 Lr: 0.04997
[2023-08-07 19:37:58,173 INFO misc.py line 115 22900] Train: [7/100][69/156] Data 0.001 (0.001) Batch 4.737 (3.472) Remain 14:04:39 loss: 1.3043 Lr: 0.04997
[2023-08-07 19:38:01,633 INFO misc.py line 115 22900] Train: [7/100][70/156] Data 0.001 (0.001) Batch 3.460 (3.472) Remain 14:04:33 loss: 1.0518 Lr: 0.04997
[2023-08-07 19:38:05,326 INFO misc.py line 115 22900] Train: [7/100][71/156] Data 0.001 (0.001) Batch 3.693 (3.475) Remain 14:05:17 loss: 0.9726 Lr: 0.04997
[2023-08-07 19:38:08,316 INFO misc.py line 115 22900] Train: [7/100][72/156] Data 0.001 (0.001) Batch 2.990 (3.468) Remain 14:03:31 loss: 0.8408 Lr: 0.04997
[2023-08-07 19:38:12,030 INFO misc.py line 115 22900] Train: [7/100][73/156] Data 0.001 (0.001) Batch 3.714 (3.472) Remain 14:04:19 loss: 1.3772 Lr: 0.04997
[2023-08-07 19:38:14,699 INFO misc.py line 115 22900] Train: [7/100][74/156] Data 0.001 (0.001) Batch 2.669 (3.461) Remain 14:01:30 loss: 1.0778 Lr: 0.04997
[2023-08-07 19:38:18,433 INFO misc.py line 115 22900] Train: [7/100][75/156] Data 0.001 (0.001) Batch 3.733 (3.464) Remain 14:02:22 loss: 1.1977 Lr: 0.04997
[2023-08-07 19:38:22,399 INFO misc.py line 115 22900] Train: [7/100][76/156] Data 0.001 (0.001) Batch 3.967 (3.471) Remain 14:03:59 loss: 1.0764 Lr: 0.04997
[2023-08-07 19:38:26,321 INFO misc.py line 115 22900] Train: [7/100][77/156] Data 0.001 (0.001) Batch 3.921 (3.477) Remain 14:05:24 loss: 1.2331 Lr: 0.04997
[2023-08-07 19:38:28,999 INFO misc.py line 115 22900] Train: [7/100][78/156] Data 0.001 (0.001) Batch 2.678 (3.467) Remain 14:02:45 loss: 0.8835 Lr: 0.04997
[2023-08-07 19:38:32,566 INFO misc.py line 115 22900] Train: [7/100][79/156] Data 0.001 (0.001) Batch 3.567 (3.468) Remain 14:03:01 loss: 0.8900 Lr: 0.04997
[2023-08-07 19:38:35,923 INFO misc.py line 115 22900] Train: [7/100][80/156] Data 0.002 (0.001) Batch 3.357 (3.467) Remain 14:02:37 loss: 0.9109 Lr: 0.04997
[2023-08-07 19:38:38,000 INFO misc.py line 115 22900] Train: [7/100][81/156] Data 0.001 (0.001) Batch 2.077 (3.449) Remain 13:58:13 loss: 0.3963 Lr: 0.04997
[2023-08-07 19:38:41,572 INFO misc.py line 115 22900] Train: [7/100][82/156] Data 0.001 (0.001) Batch 3.572 (3.450) Remain 13:58:33 loss: 1.2817 Lr: 0.04997
[2023-08-07 19:38:45,645 INFO misc.py line 115 22900] Train: [7/100][83/156] Data 0.001 (0.001) Batch 4.074 (3.458) Remain 14:00:23 loss: 1.0845 Lr: 0.04997
[2023-08-07 19:38:49,196 INFO misc.py line 115 22900] Train: [7/100][84/156] Data 0.001 (0.001) Batch 3.551 (3.459) Remain 14:00:36 loss: 1.0307 Lr: 0.04997
[2023-08-07 19:38:52,711 INFO misc.py line 115 22900] Train: [7/100][85/156] Data 0.001 (0.001) Batch 3.515 (3.460) Remain 14:00:42 loss: 1.3033 Lr: 0.04997
[2023-08-07 19:38:56,098 INFO misc.py line 115 22900] Train: [7/100][86/156] Data 0.001 (0.001) Batch 3.387 (3.459) Remain 14:00:26 loss: 1.2279 Lr: 0.04997
[2023-08-07 19:39:00,202 INFO misc.py line 115 22900] Train: [7/100][87/156] Data 0.001 (0.001) Batch 4.105 (3.467) Remain 14:02:15 loss: 1.3377 Lr: 0.04997
[2023-08-07 19:39:03,483 INFO misc.py line 115 22900] Train: [7/100][88/156] Data 0.001 (0.001) Batch 3.280 (3.465) Remain 14:01:39 loss: 0.8645 Lr: 0.04997
[2023-08-07 19:39:07,961 INFO misc.py line 115 22900] Train: [7/100][89/156] Data 0.001 (0.001) Batch 4.478 (3.476) Remain 14:04:28 loss: 1.2658 Lr: 0.04997
[2023-08-07 19:39:10,667 INFO misc.py line 115 22900] Train: [7/100][90/156] Data 0.001 (0.001) Batch 2.706 (3.468) Remain 14:02:15 loss: 1.0824 Lr: 0.04997
[2023-08-07 19:39:13,440 INFO misc.py line 115 22900] Train: [7/100][91/156] Data 0.001 (0.001) Batch 2.773 (3.460) Remain 14:00:17 loss: 0.9792 Lr: 0.04997
[2023-08-07 19:39:17,472 INFO misc.py line 115 22900] Train: [7/100][92/156] Data 0.001 (0.001) Batch 4.032 (3.466) Remain 14:01:47 loss: 1.2226 Lr: 0.04997
[2023-08-07 19:39:21,456 INFO misc.py line 115 22900] Train: [7/100][93/156] Data 0.001 (0.001) Batch 3.983 (3.472) Remain 14:03:07 loss: 1.1884 Lr: 0.04996
[2023-08-07 19:39:24,370 INFO misc.py line 115 22900] Train: [7/100][94/156] Data 0.001 (0.001) Batch 2.914 (3.466) Remain 14:01:34 loss: 0.9325 Lr: 0.04996
[2023-08-07 19:39:27,581 INFO misc.py line 115 22900] Train: [7/100][95/156] Data 0.001 (0.001) Batch 3.211 (3.463) Remain 14:00:51 loss: 0.7902 Lr: 0.04996
[2023-08-07 19:39:30,234 INFO misc.py line 115 22900] Train: [7/100][96/156] Data 0.001 (0.001) Batch 2.654 (3.454) Remain 13:58:40 loss: 1.4374 Lr: 0.04996
[2023-08-07 19:39:33,837 INFO misc.py line 115 22900] Train: [7/100][97/156] Data 0.001 (0.001) Batch 3.603 (3.456) Remain 13:59:00 loss: 0.7426 Lr: 0.04996
[2023-08-07 19:39:37,965 INFO misc.py line 115 22900] Train: [7/100][98/156] Data 0.001 (0.001) Batch 4.128 (3.463) Remain 14:00:40 loss: 1.2472 Lr: 0.04996
[2023-08-07 19:39:41,228 INFO misc.py line 115 22900] Train: [7/100][99/156] Data 0.001 (0.001) Batch 3.263 (3.461) Remain 14:00:06 loss: 0.9547 Lr: 0.04996
[2023-08-07 19:39:44,875 INFO misc.py line 115 22900] Train: [7/100][100/156] Data 0.001 (0.001) Batch 3.647 (3.463) Remain 14:00:30 loss: 1.1886 Lr: 0.04996
[2023-08-07 19:39:48,595 INFO misc.py line 115 22900] Train: [7/100][101/156] Data 0.001 (0.001) Batch 3.720 (3.465) Remain 14:01:05 loss: 1.1086 Lr: 0.04996
[2023-08-07 19:39:52,312 INFO misc.py line 115 22900] Train: [7/100][102/156] Data 0.001 (0.001) Batch 3.717 (3.468) Remain 14:01:38 loss: 0.9501 Lr: 0.04996
[2023-08-07 19:39:56,346 INFO misc.py line 115 22900] Train: [7/100][103/156] Data 0.001 (0.001) Batch 4.035 (3.474) Remain 14:02:58 loss: 0.8813 Lr: 0.04996
[2023-08-07 19:40:00,202 INFO misc.py line 115 22900] Train: [7/100][104/156] Data 0.001 (0.001) Batch 3.856 (3.477) Remain 14:03:49 loss: 1.0725 Lr: 0.04996
[2023-08-07 19:40:04,082 INFO misc.py line 115 22900] Train: [7/100][105/156] Data 0.001 (0.001) Batch 3.881 (3.481) Remain 14:04:43 loss: 1.1272 Lr: 0.04996
[2023-08-07 19:40:06,733 INFO misc.py line 115 22900] Train: [7/100][106/156] Data 0.001 (0.001) Batch 2.651 (3.473) Remain 14:02:42 loss: 0.6707 Lr: 0.04996
[2023-08-07 19:40:10,149 INFO misc.py line 115 22900] Train: [7/100][107/156] Data 0.001 (0.001) Batch 3.416 (3.473) Remain 14:02:31 loss: 1.1008 Lr: 0.04996
[2023-08-07 19:40:13,897 INFO misc.py line 115 22900] Train: [7/100][108/156] Data 0.001 (0.001) Batch 3.748 (3.475) Remain 14:03:06 loss: 0.8347 Lr: 0.04996
[2023-08-07 19:40:17,496 INFO misc.py line 115 22900] Train: [7/100][109/156] Data 0.001 (0.001) Batch 3.598 (3.476) Remain 14:03:19 loss: 0.9605 Lr: 0.04996
[2023-08-07 19:40:20,653 INFO misc.py line 115 22900] Train: [7/100][110/156] Data 0.001 (0.001) Batch 3.158 (3.473) Remain 14:02:32 loss: 0.8814 Lr: 0.04996
[2023-08-07 19:40:24,300 INFO misc.py line 115 22900] Train: [7/100][111/156] Data 0.001 (0.001) Batch 3.646 (3.475) Remain 14:02:52 loss: 0.9885 Lr: 0.04996
[2023-08-07 19:40:28,014 INFO misc.py line 115 22900] Train: [7/100][112/156] Data 0.001 (0.001) Batch 3.714 (3.477) Remain 14:03:20 loss: 0.9520 Lr: 0.04996
[2023-08-07 19:40:31,791 INFO misc.py line 115 22900] Train: [7/100][113/156] Data 0.001 (0.001) Batch 3.777 (3.480) Remain 14:03:57 loss: 0.7552 Lr: 0.04996
[2023-08-07 19:40:35,588 INFO misc.py line 115 22900] Train: [7/100][114/156] Data 0.001 (0.001) Batch 3.797 (3.483) Remain 14:04:35 loss: 1.1808 Lr: 0.04996
[2023-08-07 19:40:38,631 INFO misc.py line 115 22900] Train: [7/100][115/156] Data 0.001 (0.001) Batch 3.043 (3.479) Remain 14:03:34 loss: 1.2675 Lr: 0.04996
[2023-08-07 19:40:42,286 INFO misc.py line 115 22900] Train: [7/100][116/156] Data 0.001 (0.001) Batch 3.655 (3.480) Remain 14:03:53 loss: 1.0924 Lr: 0.04996
[2023-08-07 19:40:45,392 INFO misc.py line 115 22900] Train: [7/100][117/156] Data 0.001 (0.001) Batch 3.106 (3.477) Remain 14:03:02 loss: 0.8705 Lr: 0.04996
[2023-08-07 19:40:48,880 INFO misc.py line 115 22900] Train: [7/100][118/156] Data 0.001 (0.001) Batch 3.488 (3.477) Remain 14:03:00 loss: 0.9191 Lr: 0.04996
[2023-08-07 19:40:52,716 INFO misc.py line 115 22900] Train: [7/100][119/156] Data 0.001 (0.001) Batch 3.836 (3.480) Remain 14:03:41 loss: 0.8449 Lr: 0.04996
[2023-08-07 19:40:56,076 INFO misc.py line 115 22900] Train: [7/100][120/156] Data 0.001 (0.001) Batch 3.360 (3.479) Remain 14:03:23 loss: 0.9373 Lr: 0.04996
[2023-08-07 19:41:00,064 INFO misc.py line 115 22900] Train: [7/100][121/156] Data 0.001 (0.001) Batch 3.988 (3.484) Remain 14:04:22 loss: 0.9589 Lr: 0.04996
[2023-08-07 19:41:02,946 INFO misc.py line 115 22900] Train: [7/100][122/156] Data 0.001 (0.001) Batch 2.881 (3.479) Remain 14:03:05 loss: 0.6293 Lr: 0.04996
[2023-08-07 19:41:05,429 INFO misc.py line 115 22900] Train: [7/100][123/156] Data 0.001 (0.001) Batch 2.483 (3.470) Remain 14:01:01 loss: 0.8738 Lr: 0.04996
[2023-08-07 19:41:09,441 INFO misc.py line 115 22900] Train: [7/100][124/156] Data 0.001 (0.001) Batch 4.012 (3.475) Remain 14:02:03 loss: 1.0438 Lr: 0.04996
[2023-08-07 19:41:13,263 INFO misc.py line 115 22900] Train: [7/100][125/156] Data 0.001 (0.001) Batch 3.823 (3.478) Remain 14:02:41 loss: 1.0210 Lr: 0.04996
[2023-08-07 19:41:17,312 INFO misc.py line 115 22900] Train: [7/100][126/156] Data 0.001 (0.001) Batch 4.049 (3.482) Remain 14:03:45 loss: 1.1677 Lr: 0.04996
[2023-08-07 19:41:20,301 INFO misc.py line 115 22900] Train: [7/100][127/156] Data 0.001 (0.001) Batch 2.989 (3.478) Remain 14:02:43 loss: 0.8144 Lr: 0.04995
[2023-08-07 19:41:23,903 INFO misc.py line 115 22900] Train: [7/100][128/156] Data 0.001 (0.001) Batch 3.602 (3.479) Remain 14:02:54 loss: 1.1390 Lr: 0.04995
[2023-08-07 19:41:27,902 INFO misc.py line 115 22900] Train: [7/100][129/156] Data 0.001 (0.001) Batch 3.999 (3.483) Remain 14:03:51 loss: 1.1864 Lr: 0.04995
[2023-08-07 19:41:30,940 INFO misc.py line 115 22900] Train: [7/100][130/156] Data 0.001 (0.001) Batch 3.038 (3.480) Remain 14:02:56 loss: 1.2088 Lr: 0.04995
[2023-08-07 19:41:33,738 INFO misc.py line 115 22900] Train: [7/100][131/156] Data 0.001 (0.001) Batch 2.798 (3.475) Remain 14:01:35 loss: 0.9343 Lr: 0.04995
[2023-08-07 19:41:37,429 INFO misc.py line 115 22900] Train: [7/100][132/156] Data 0.001 (0.001) Batch 3.691 (3.476) Remain 14:01:56 loss: 1.2187 Lr: 0.04995
[2023-08-07 19:41:41,039 INFO misc.py line 115 22900] Train: [7/100][133/156] Data 0.001 (0.001) Batch 3.610 (3.477) Remain 14:02:08 loss: 1.3190 Lr: 0.04995
[2023-08-07 19:41:44,306 INFO misc.py line 115 22900] Train: [7/100][134/156] Data 0.001 (0.001) Batch 3.266 (3.476) Remain 14:01:41 loss: 1.1082 Lr: 0.04995
[2023-08-07 19:41:47,834 INFO misc.py line 115 22900] Train: [7/100][135/156] Data 0.001 (0.001) Batch 3.528 (3.476) Remain 14:01:43 loss: 1.0917 Lr: 0.04995
[2023-08-07 19:41:50,274 INFO misc.py line 115 22900] Train: [7/100][136/156] Data 0.001 (0.001) Batch 2.440 (3.468) Remain 13:59:47 loss: 0.8318 Lr: 0.04995
[2023-08-07 19:41:53,091 INFO misc.py line 115 22900] Train: [7/100][137/156] Data 0.001 (0.001) Batch 2.817 (3.463) Remain 13:58:32 loss: 0.7210 Lr: 0.04995
[2023-08-07 19:41:57,116 INFO misc.py line 115 22900] Train: [7/100][138/156] Data 0.001 (0.001) Batch 4.025 (3.468) Remain 13:59:29 loss: 1.1054 Lr: 0.04995
[2023-08-07 19:41:59,736 INFO misc.py line 115 22900] Train: [7/100][139/156] Data 0.001 (0.001) Batch 2.621 (3.461) Remain 13:57:56 loss: 1.0561 Lr: 0.04995
[2023-08-07 19:42:03,359 INFO misc.py line 115 22900] Train: [7/100][140/156] Data 0.001 (0.001) Batch 3.623 (3.463) Remain 13:58:09 loss: 0.9983 Lr: 0.04995
[2023-08-07 19:42:06,601 INFO misc.py line 115 22900] Train: [7/100][141/156] Data 0.001 (0.001) Batch 3.242 (3.461) Remain 13:57:42 loss: 1.3340 Lr: 0.04995
[2023-08-07 19:42:10,105 INFO misc.py line 115 22900] Train: [7/100][142/156] Data 0.001 (0.001) Batch 3.504 (3.461) Remain 13:57:44 loss: 1.1481 Lr: 0.04995
[2023-08-07 19:42:13,464 INFO misc.py line 115 22900] Train: [7/100][143/156] Data 0.001 (0.001) Batch 3.359 (3.461) Remain 13:57:29 loss: 0.7692 Lr: 0.04995
[2023-08-07 19:42:17,643 INFO misc.py line 115 22900] Train: [7/100][144/156] Data 0.001 (0.001) Batch 4.179 (3.466) Remain 13:58:40 loss: 1.6390 Lr: 0.04995
[2023-08-07 19:42:21,714 INFO misc.py line 115 22900] Train: [7/100][145/156] Data 0.001 (0.001) Batch 4.071 (3.470) Remain 13:59:38 loss: 0.9460 Lr: 0.04995
[2023-08-07 19:42:25,767 INFO misc.py line 115 22900] Train: [7/100][146/156] Data 0.001 (0.001) Batch 4.053 (3.474) Remain 14:00:34 loss: 1.0771 Lr: 0.04995
[2023-08-07 19:42:29,273 INFO misc.py line 115 22900] Train: [7/100][147/156] Data 0.001 (0.001) Batch 3.506 (3.474) Remain 14:00:34 loss: 0.8821 Lr: 0.04995
[2023-08-07 19:42:32,525 INFO misc.py line 115 22900] Train: [7/100][148/156] Data 0.001 (0.001) Batch 3.252 (3.473) Remain 14:00:08 loss: 1.2718 Lr: 0.04995
[2023-08-07 19:42:35,967 INFO misc.py line 115 22900] Train: [7/100][149/156] Data 0.001 (0.001) Batch 3.442 (3.472) Remain 14:00:02 loss: 1.0984 Lr: 0.04995
[2023-08-07 19:42:39,396 INFO misc.py line 115 22900] Train: [7/100][150/156] Data 0.001 (0.001) Batch 3.428 (3.472) Remain 13:59:54 loss: 0.8574 Lr: 0.04995
[2023-08-07 19:42:42,555 INFO misc.py line 115 22900] Train: [7/100][151/156] Data 0.001 (0.001) Batch 3.159 (3.470) Remain 13:59:20 loss: 0.8393 Lr: 0.04995
[2023-08-07 19:42:46,323 INFO misc.py line 115 22900] Train: [7/100][152/156] Data 0.001 (0.001) Batch 3.768 (3.472) Remain 13:59:45 loss: 1.1941 Lr: 0.04995
[2023-08-07 19:42:50,070 INFO misc.py line 115 22900] Train: [7/100][153/156] Data 0.001 (0.001) Batch 3.747 (3.474) Remain 14:00:08 loss: 0.9085 Lr: 0.04995
[2023-08-07 19:42:52,916 INFO misc.py line 115 22900] Train: [7/100][154/156] Data 0.001 (0.001) Batch 2.846 (3.470) Remain 13:59:05 loss: 1.2325 Lr: 0.04995
[2023-08-07 19:42:56,270 INFO misc.py line 115 22900] Train: [7/100][155/156] Data 0.001 (0.001) Batch 3.354 (3.469) Remain 13:58:50 loss: 1.0268 Lr: 0.04995
[2023-08-07 19:43:00,352 INFO misc.py line 115 22900] Train: [7/100][156/156] Data 0.001 (0.001) Batch 4.081 (3.473) Remain 13:59:45 loss: 1.1570 Lr: 0.04994
[2023-08-07 19:43:00,352 INFO misc.py line 129 22900] Train result: loss: 1.0238 
[2023-08-07 19:43:00,352 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 19:43:02,450 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.7771 
[2023-08-07 19:43:03,321 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.9241 
[2023-08-07 19:43:04,984 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.9002 
[2023-08-07 19:43:06,507 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.1394 
[2023-08-07 19:43:08,350 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.4224 
[2023-08-07 19:43:10,013 INFO evaluator.py line 122 22900] Test: [6/24] Loss 1.0077 
[2023-08-07 19:43:12,153 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.3115 
[2023-08-07 19:43:13,961 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.7643 
[2023-08-07 19:43:15,244 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.3254 
[2023-08-07 19:43:17,373 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3135 
[2023-08-07 19:43:17,898 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.0571 
[2023-08-07 19:43:19,427 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.8290 
[2023-08-07 19:43:22,139 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.1730 
[2023-08-07 19:43:23,819 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.9702 
[2023-08-07 19:43:25,843 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.5645 
[2023-08-07 19:43:28,553 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.3042 
[2023-08-07 19:43:31,259 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.3294 
[2023-08-07 19:43:33,105 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.3936 
[2023-08-07 19:43:33,855 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.3782 
[2023-08-07 19:43:34,740 INFO evaluator.py line 122 22900] Test: [20/24] Loss 1.0974 
[2023-08-07 19:43:37,001 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.2483 
[2023-08-07 19:43:38,967 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.3724 
[2023-08-07 19:43:40,812 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.2552 
[2023-08-07 19:43:42,749 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.6336 
[2023-08-07 19:43:42,796 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.1503/0.2140/0.6355.
[2023-08-07 19:43:42,797 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.5997/0.9398
[2023-08-07 19:43:42,797 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9386/0.9931
[2023-08-07 19:43:42,797 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.0322/0.0738
[2023-08-07 19:43:42,797 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.0554/0.1110
[2023-08-07 19:43:42,797 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.4790/0.6337
[2023-08-07 19:43:42,797 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.2103/0.4676
[2023-08-07 19:43:42,797 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.4130/0.5169
[2023-08-07 19:43:42,797 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0779/0.0946
[2023-08-07 19:43:42,797 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.0729/0.1722
[2023-08-07 19:43:42,797 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0099/0.0101
[2023-08-07 19:43:42,797 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:43:42,797 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:43:42,797 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.0021/0.0022
[2023-08-07 19:43:42,797 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0201/0.0224
[2023-08-07 19:43:42,797 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:43:42,797 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:43:42,797 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:43:42,797 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.0473/0.1643
[2023-08-07 19:43:42,797 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:43:42,797 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0473/0.0774
[2023-08-07 19:43:42,797 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 19:43:42,797 INFO misc.py line 150 22900] Best validation mIoU updated to: 0.1503
[2023-08-07 19:43:42,797 INFO misc.py line 152 22900] Currently Best mIoU: 0.1503
[2023-08-07 19:43:42,798 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 19:43:49,641 INFO misc.py line 115 22900] Train: [8/100][1/156] Data 1.575 (1.575) Batch 5.780 (5.780) Remain 23:17:26 loss: 1.0084 Lr: 0.04994
[2023-08-07 19:43:52,279 INFO misc.py line 115 22900] Train: [8/100][2/156] Data 0.002 (0.002) Batch 2.638 (2.638) Remain 10:37:46 loss: 0.9553 Lr: 0.04994
[2023-08-07 19:43:55,830 INFO misc.py line 115 22900] Train: [8/100][3/156] Data 0.001 (0.001) Batch 3.551 (3.551) Remain 14:18:30 loss: 0.9833 Lr: 0.04994
[2023-08-07 19:43:59,404 INFO misc.py line 115 22900] Train: [8/100][4/156] Data 0.001 (0.001) Batch 3.574 (3.574) Remain 14:24:01 loss: 0.8172 Lr: 0.04994
[2023-08-07 19:44:01,805 INFO misc.py line 115 22900] Train: [8/100][5/156] Data 0.001 (0.001) Batch 2.401 (2.988) Remain 12:02:09 loss: 0.9020 Lr: 0.04994
[2023-08-07 19:44:05,483 INFO misc.py line 115 22900] Train: [8/100][6/156] Data 0.001 (0.001) Batch 3.677 (3.217) Remain 12:57:39 loss: 0.9962 Lr: 0.04994
[2023-08-07 19:44:09,307 INFO misc.py line 115 22900] Train: [8/100][7/156] Data 0.001 (0.001) Batch 3.825 (3.369) Remain 13:34:17 loss: 0.7584 Lr: 0.04994
[2023-08-07 19:44:13,013 INFO misc.py line 115 22900] Train: [8/100][8/156] Data 0.001 (0.001) Batch 3.706 (3.437) Remain 13:50:31 loss: 0.8570 Lr: 0.04994
[2023-08-07 19:44:15,681 INFO misc.py line 115 22900] Train: [8/100][9/156] Data 0.001 (0.001) Batch 2.668 (3.309) Remain 13:19:30 loss: 1.0335 Lr: 0.04994
[2023-08-07 19:44:18,433 INFO misc.py line 115 22900] Train: [8/100][10/156] Data 0.001 (0.001) Batch 2.751 (3.229) Remain 13:00:13 loss: 0.6413 Lr: 0.04994
[2023-08-07 19:44:21,978 INFO misc.py line 115 22900] Train: [8/100][11/156] Data 0.001 (0.001) Batch 3.545 (3.268) Remain 13:09:42 loss: 1.1663 Lr: 0.04994
[2023-08-07 19:44:24,319 INFO misc.py line 115 22900] Train: [8/100][12/156] Data 0.001 (0.001) Batch 2.341 (3.165) Remain 12:44:45 loss: 0.6770 Lr: 0.04994
[2023-08-07 19:44:26,714 INFO misc.py line 115 22900] Train: [8/100][13/156] Data 0.001 (0.001) Batch 2.395 (3.088) Remain 12:26:05 loss: 0.7208 Lr: 0.04994
[2023-08-07 19:44:29,598 INFO misc.py line 115 22900] Train: [8/100][14/156] Data 0.001 (0.001) Batch 2.885 (3.070) Remain 12:21:34 loss: 0.7086 Lr: 0.04994
[2023-08-07 19:44:33,403 INFO misc.py line 115 22900] Train: [8/100][15/156] Data 0.001 (0.001) Batch 3.805 (3.131) Remain 12:36:18 loss: 1.4640 Lr: 0.04994
[2023-08-07 19:44:35,886 INFO misc.py line 115 22900] Train: [8/100][16/156] Data 0.001 (0.001) Batch 2.483 (3.081) Remain 12:24:12 loss: 0.6096 Lr: 0.04994
[2023-08-07 19:44:38,550 INFO misc.py line 115 22900] Train: [8/100][17/156] Data 0.001 (0.001) Batch 2.664 (3.051) Remain 12:16:58 loss: 0.9331 Lr: 0.04994
[2023-08-07 19:44:42,565 INFO misc.py line 115 22900] Train: [8/100][18/156] Data 0.001 (0.001) Batch 4.015 (3.116) Remain 12:32:25 loss: 0.8475 Lr: 0.04994
[2023-08-07 19:44:46,508 INFO misc.py line 115 22900] Train: [8/100][19/156] Data 0.001 (0.001) Batch 3.943 (3.167) Remain 12:44:51 loss: 1.0386 Lr: 0.04994
[2023-08-07 19:44:50,093 INFO misc.py line 115 22900] Train: [8/100][20/156] Data 0.001 (0.001) Batch 3.585 (3.192) Remain 12:50:44 loss: 0.7934 Lr: 0.04994
[2023-08-07 19:44:52,704 INFO misc.py line 115 22900] Train: [8/100][21/156] Data 0.001 (0.001) Batch 2.611 (3.160) Remain 12:42:54 loss: 0.9063 Lr: 0.04994
[2023-08-07 19:44:56,354 INFO misc.py line 115 22900] Train: [8/100][22/156] Data 0.001 (0.001) Batch 3.650 (3.185) Remain 12:49:04 loss: 0.7699 Lr: 0.04994
[2023-08-07 19:44:58,917 INFO misc.py line 115 22900] Train: [8/100][23/156] Data 0.001 (0.001) Batch 2.562 (3.154) Remain 12:41:30 loss: 1.0242 Lr: 0.04994
[2023-08-07 19:45:03,634 INFO misc.py line 115 22900] Train: [8/100][24/156] Data 0.001 (0.001) Batch 4.718 (3.229) Remain 12:59:25 loss: 1.4064 Lr: 0.04994
[2023-08-07 19:45:07,211 INFO misc.py line 115 22900] Train: [8/100][25/156] Data 0.001 (0.001) Batch 3.576 (3.245) Remain 13:03:11 loss: 0.9627 Lr: 0.04994
[2023-08-07 19:45:11,255 INFO misc.py line 115 22900] Train: [8/100][26/156] Data 0.001 (0.001) Batch 4.045 (3.279) Remain 13:11:31 loss: 1.2362 Lr: 0.04994
[2023-08-07 19:45:16,011 INFO misc.py line 115 22900] Train: [8/100][27/156] Data 0.001 (0.001) Batch 4.756 (3.341) Remain 13:26:19 loss: 1.5583 Lr: 0.04994
[2023-08-07 19:45:19,690 INFO misc.py line 115 22900] Train: [8/100][28/156] Data 0.001 (0.001) Batch 3.678 (3.354) Remain 13:29:31 loss: 1.0431 Lr: 0.04993
[2023-08-07 19:45:21,816 INFO misc.py line 115 22900] Train: [8/100][29/156] Data 0.001 (0.001) Batch 2.126 (3.307) Remain 13:18:04 loss: 0.5886 Lr: 0.04993
[2023-08-07 19:45:25,851 INFO misc.py line 115 22900] Train: [8/100][30/156] Data 0.001 (0.001) Batch 4.035 (3.334) Remain 13:24:31 loss: 1.2222 Lr: 0.04993
[2023-08-07 19:45:29,762 INFO misc.py line 115 22900] Train: [8/100][31/156] Data 0.001 (0.001) Batch 3.911 (3.355) Remain 13:29:26 loss: 1.0549 Lr: 0.04993
[2023-08-07 19:45:33,445 INFO misc.py line 115 22900] Train: [8/100][32/156] Data 0.001 (0.001) Batch 3.683 (3.366) Remain 13:32:06 loss: 0.8647 Lr: 0.04993
[2023-08-07 19:45:37,611 INFO misc.py line 115 22900] Train: [8/100][33/156] Data 0.001 (0.001) Batch 4.166 (3.393) Remain 13:38:29 loss: 1.5716 Lr: 0.04993
[2023-08-07 19:45:40,554 INFO misc.py line 115 22900] Train: [8/100][34/156] Data 0.001 (0.001) Batch 2.943 (3.378) Remain 13:34:55 loss: 0.8404 Lr: 0.04993
[2023-08-07 19:45:43,795 INFO misc.py line 115 22900] Train: [8/100][35/156] Data 0.001 (0.001) Batch 3.241 (3.374) Remain 13:33:50 loss: 1.1378 Lr: 0.04993
[2023-08-07 19:45:46,908 INFO misc.py line 115 22900] Train: [8/100][36/156] Data 0.001 (0.001) Batch 3.113 (3.366) Remain 13:31:52 loss: 0.7982 Lr: 0.04993
[2023-08-07 19:45:50,066 INFO misc.py line 115 22900] Train: [8/100][37/156] Data 0.001 (0.001) Batch 3.158 (3.360) Remain 13:30:20 loss: 0.6929 Lr: 0.04993
[2023-08-07 19:45:54,096 INFO misc.py line 115 22900] Train: [8/100][38/156] Data 0.001 (0.001) Batch 4.030 (3.379) Remain 13:34:54 loss: 0.7988 Lr: 0.04993
[2023-08-07 19:45:58,109 INFO misc.py line 115 22900] Train: [8/100][39/156] Data 0.001 (0.001) Batch 4.013 (3.397) Remain 13:39:06 loss: 1.0758 Lr: 0.04993
[2023-08-07 19:46:00,867 INFO misc.py line 115 22900] Train: [8/100][40/156] Data 0.001 (0.001) Batch 2.758 (3.379) Remain 13:34:52 loss: 0.9250 Lr: 0.04993
[2023-08-07 19:46:03,238 INFO misc.py line 115 22900] Train: [8/100][41/156] Data 0.001 (0.001) Batch 2.371 (3.353) Remain 13:28:25 loss: 0.8486 Lr: 0.04993
[2023-08-07 19:46:06,645 INFO misc.py line 115 22900] Train: [8/100][42/156] Data 0.001 (0.001) Batch 3.407 (3.354) Remain 13:28:42 loss: 0.6275 Lr: 0.04993
[2023-08-07 19:46:10,383 INFO misc.py line 115 22900] Train: [8/100][43/156] Data 0.001 (0.001) Batch 3.738 (3.364) Remain 13:30:57 loss: 0.9138 Lr: 0.04993
[2023-08-07 19:46:13,540 INFO misc.py line 115 22900] Train: [8/100][44/156] Data 0.001 (0.001) Batch 3.156 (3.359) Remain 13:29:41 loss: 0.9997 Lr: 0.04993
[2023-08-07 19:46:16,146 INFO misc.py line 115 22900] Train: [8/100][45/156] Data 0.001 (0.001) Batch 2.607 (3.341) Remain 13:25:18 loss: 0.7454 Lr: 0.04993
[2023-08-07 19:46:19,177 INFO misc.py line 115 22900] Train: [8/100][46/156] Data 0.001 (0.001) Batch 3.031 (3.334) Remain 13:23:31 loss: 0.6992 Lr: 0.04993
[2023-08-07 19:46:22,756 INFO misc.py line 115 22900] Train: [8/100][47/156] Data 0.001 (0.001) Batch 3.578 (3.339) Remain 13:24:48 loss: 0.9724 Lr: 0.04993
[2023-08-07 19:46:26,695 INFO misc.py line 115 22900] Train: [8/100][48/156] Data 0.001 (0.001) Batch 3.939 (3.353) Remain 13:27:57 loss: 0.9657 Lr: 0.04993
[2023-08-07 19:46:30,302 INFO misc.py line 115 22900] Train: [8/100][49/156] Data 0.001 (0.001) Batch 3.608 (3.358) Remain 13:29:14 loss: 0.7828 Lr: 0.04993
[2023-08-07 19:46:34,550 INFO misc.py line 115 22900] Train: [8/100][50/156] Data 0.001 (0.001) Batch 4.248 (3.377) Remain 13:33:45 loss: 1.0905 Lr: 0.04993
[2023-08-07 19:46:38,656 INFO misc.py line 115 22900] Train: [8/100][51/156] Data 0.001 (0.001) Batch 4.106 (3.392) Remain 13:37:21 loss: 1.5103 Lr: 0.04993
[2023-08-07 19:46:41,902 INFO misc.py line 115 22900] Train: [8/100][52/156] Data 0.001 (0.001) Batch 3.246 (3.389) Remain 13:36:34 loss: 0.4761 Lr: 0.04993
[2023-08-07 19:46:45,194 INFO misc.py line 115 22900] Train: [8/100][53/156] Data 0.001 (0.001) Batch 3.292 (3.387) Remain 13:36:03 loss: 1.1400 Lr: 0.04992
[2023-08-07 19:46:49,228 INFO misc.py line 115 22900] Train: [8/100][54/156] Data 0.001 (0.001) Batch 4.034 (3.400) Remain 13:39:03 loss: 1.1896 Lr: 0.04992
[2023-08-07 19:46:53,306 INFO misc.py line 115 22900] Train: [8/100][55/156] Data 0.001 (0.001) Batch 4.078 (3.413) Remain 13:42:07 loss: 1.2191 Lr: 0.04992
[2023-08-07 19:46:56,198 INFO misc.py line 115 22900] Train: [8/100][56/156] Data 0.001 (0.001) Batch 2.892 (3.403) Remain 13:39:42 loss: 1.1921 Lr: 0.04992
[2023-08-07 19:46:59,968 INFO misc.py line 115 22900] Train: [8/100][57/156] Data 0.001 (0.001) Batch 3.770 (3.410) Remain 13:41:17 loss: 1.0721 Lr: 0.04992
[2023-08-07 19:47:03,252 INFO misc.py line 115 22900] Train: [8/100][58/156] Data 0.001 (0.001) Batch 3.283 (3.408) Remain 13:40:40 loss: 0.6957 Lr: 0.04992
[2023-08-07 19:47:06,685 INFO misc.py line 115 22900] Train: [8/100][59/156] Data 0.001 (0.001) Batch 3.433 (3.408) Remain 13:40:43 loss: 1.0671 Lr: 0.04992
[2023-08-07 19:47:08,782 INFO misc.py line 115 22900] Train: [8/100][60/156] Data 0.001 (0.001) Batch 2.097 (3.385) Remain 13:35:08 loss: 0.6809 Lr: 0.04992
[2023-08-07 19:47:11,274 INFO misc.py line 115 22900] Train: [8/100][61/156] Data 0.001 (0.001) Batch 2.492 (3.370) Remain 13:31:22 loss: 0.6397 Lr: 0.04992
[2023-08-07 19:47:13,723 INFO misc.py line 115 22900] Train: [8/100][62/156] Data 0.001 (0.001) Batch 2.449 (3.354) Remain 13:27:33 loss: 0.7378 Lr: 0.04992
[2023-08-07 19:47:17,234 INFO misc.py line 115 22900] Train: [8/100][63/156] Data 0.001 (0.001) Batch 3.510 (3.357) Remain 13:28:07 loss: 0.8745 Lr: 0.04992
[2023-08-07 19:47:22,047 INFO misc.py line 115 22900] Train: [8/100][64/156] Data 0.001 (0.001) Batch 4.813 (3.381) Remain 13:33:49 loss: 1.4690 Lr: 0.04992
[2023-08-07 19:47:26,479 INFO misc.py line 115 22900] Train: [8/100][65/156] Data 0.001 (0.001) Batch 4.432 (3.398) Remain 13:37:51 loss: 1.1024 Lr: 0.04992
[2023-08-07 19:47:29,329 INFO misc.py line 115 22900] Train: [8/100][66/156] Data 0.001 (0.001) Batch 2.850 (3.389) Remain 13:35:42 loss: 1.1136 Lr: 0.04992
[2023-08-07 19:47:32,679 INFO misc.py line 115 22900] Train: [8/100][67/156] Data 0.001 (0.001) Batch 3.350 (3.388) Remain 13:35:29 loss: 1.3936 Lr: 0.04992
[2023-08-07 19:47:35,986 INFO misc.py line 115 22900] Train: [8/100][68/156] Data 0.001 (0.001) Batch 3.306 (3.387) Remain 13:35:08 loss: 1.0706 Lr: 0.04992
[2023-08-07 19:47:40,002 INFO misc.py line 115 22900] Train: [8/100][69/156] Data 0.001 (0.001) Batch 4.016 (3.397) Remain 13:37:22 loss: 1.5465 Lr: 0.04992
[2023-08-07 19:47:43,681 INFO misc.py line 115 22900] Train: [8/100][70/156] Data 0.001 (0.001) Batch 3.679 (3.401) Remain 13:38:20 loss: 1.0115 Lr: 0.04992
[2023-08-07 19:47:47,446 INFO misc.py line 115 22900] Train: [8/100][71/156] Data 0.001 (0.001) Batch 3.764 (3.406) Remain 13:39:33 loss: 1.1507 Lr: 0.04992
[2023-08-07 19:47:50,344 INFO misc.py line 115 22900] Train: [8/100][72/156] Data 0.001 (0.001) Batch 2.899 (3.399) Remain 13:37:44 loss: 1.0063 Lr: 0.04992
[2023-08-07 19:47:53,779 INFO misc.py line 115 22900] Train: [8/100][73/156] Data 0.001 (0.001) Batch 3.434 (3.399) Remain 13:37:48 loss: 0.8190 Lr: 0.04992
[2023-08-07 19:47:57,860 INFO misc.py line 115 22900] Train: [8/100][74/156] Data 0.001 (0.001) Batch 4.081 (3.409) Remain 13:40:03 loss: 0.9589 Lr: 0.04992
[2023-08-07 19:48:01,538 INFO misc.py line 115 22900] Train: [8/100][75/156] Data 0.001 (0.001) Batch 3.678 (3.413) Remain 13:40:54 loss: 0.8832 Lr: 0.04992
[2023-08-07 19:48:04,505 INFO misc.py line 115 22900] Train: [8/100][76/156] Data 0.001 (0.001) Batch 2.967 (3.407) Remain 13:39:22 loss: 1.2031 Lr: 0.04992
[2023-08-07 19:48:07,910 INFO misc.py line 115 22900] Train: [8/100][77/156] Data 0.001 (0.001) Batch 3.405 (3.406) Remain 13:39:18 loss: 0.7474 Lr: 0.04991
[2023-08-07 19:48:11,682 INFO misc.py line 115 22900] Train: [8/100][78/156] Data 0.001 (0.001) Batch 3.773 (3.411) Remain 13:40:25 loss: 0.9384 Lr: 0.04991
[2023-08-07 19:48:14,993 INFO misc.py line 115 22900] Train: [8/100][79/156] Data 0.001 (0.001) Batch 3.311 (3.410) Remain 13:40:03 loss: 0.6659 Lr: 0.04991
[2023-08-07 19:48:17,374 INFO misc.py line 115 22900] Train: [8/100][80/156] Data 0.001 (0.001) Batch 2.381 (3.397) Remain 13:36:47 loss: 0.9416 Lr: 0.04991
[2023-08-07 19:48:21,516 INFO misc.py line 115 22900] Train: [8/100][81/156] Data 0.001 (0.001) Batch 4.142 (3.406) Remain 13:39:01 loss: 0.9762 Lr: 0.04991
[2023-08-07 19:48:25,080 INFO misc.py line 115 22900] Train: [8/100][82/156] Data 0.001 (0.001) Batch 3.564 (3.408) Remain 13:39:27 loss: 0.7535 Lr: 0.04991
[2023-08-07 19:48:28,983 INFO misc.py line 115 22900] Train: [8/100][83/156] Data 0.001 (0.001) Batch 3.903 (3.414) Remain 13:40:52 loss: 1.4316 Lr: 0.04991
[2023-08-07 19:48:32,381 INFO misc.py line 115 22900] Train: [8/100][84/156] Data 0.001 (0.001) Batch 3.398 (3.414) Remain 13:40:46 loss: 1.0081 Lr: 0.04991
[2023-08-07 19:48:36,504 INFO misc.py line 115 22900] Train: [8/100][85/156] Data 0.001 (0.001) Batch 4.123 (3.423) Remain 13:42:47 loss: 1.2675 Lr: 0.04991
[2023-08-07 19:48:40,195 INFO misc.py line 115 22900] Train: [8/100][86/156] Data 0.001 (0.001) Batch 3.691 (3.426) Remain 13:43:30 loss: 1.0067 Lr: 0.04991
[2023-08-07 19:48:43,522 INFO misc.py line 115 22900] Train: [8/100][87/156] Data 0.001 (0.001) Batch 3.327 (3.425) Remain 13:43:10 loss: 0.6308 Lr: 0.04991
[2023-08-07 19:48:46,265 INFO misc.py line 115 22900] Train: [8/100][88/156] Data 0.001 (0.001) Batch 2.742 (3.417) Remain 13:41:11 loss: 0.7263 Lr: 0.04991
[2023-08-07 19:48:49,765 INFO misc.py line 115 22900] Train: [8/100][89/156] Data 0.001 (0.001) Batch 3.501 (3.418) Remain 13:41:21 loss: 0.9555 Lr: 0.04991
[2023-08-07 19:48:53,836 INFO misc.py line 115 22900] Train: [8/100][90/156] Data 0.001 (0.001) Batch 4.071 (3.425) Remain 13:43:06 loss: 0.8742 Lr: 0.04991
[2023-08-07 19:48:56,605 INFO misc.py line 115 22900] Train: [8/100][91/156] Data 0.001 (0.001) Batch 2.768 (3.418) Remain 13:41:15 loss: 0.6137 Lr: 0.04991
[2023-08-07 19:49:00,619 INFO misc.py line 115 22900] Train: [8/100][92/156] Data 0.001 (0.001) Batch 4.015 (3.425) Remain 13:42:48 loss: 1.0296 Lr: 0.04991
[2023-08-07 19:49:03,130 INFO misc.py line 115 22900] Train: [8/100][93/156] Data 0.001 (0.001) Batch 2.511 (3.414) Remain 13:40:19 loss: 0.7727 Lr: 0.04991
[2023-08-07 19:49:07,286 INFO misc.py line 115 22900] Train: [8/100][94/156] Data 0.001 (0.001) Batch 4.157 (3.423) Remain 13:42:13 loss: 1.2320 Lr: 0.04991
[2023-08-07 19:49:10,779 INFO misc.py line 115 22900] Train: [8/100][95/156] Data 0.001 (0.001) Batch 3.493 (3.423) Remain 13:42:20 loss: 0.7123 Lr: 0.04991
[2023-08-07 19:49:15,029 INFO misc.py line 115 22900] Train: [8/100][96/156] Data 0.001 (0.001) Batch 4.250 (3.432) Remain 13:44:25 loss: 1.4411 Lr: 0.04991
[2023-08-07 19:49:19,067 INFO misc.py line 115 22900] Train: [8/100][97/156] Data 0.001 (0.001) Batch 4.038 (3.439) Remain 13:45:54 loss: 1.3592 Lr: 0.04991
[2023-08-07 19:49:23,138 INFO misc.py line 115 22900] Train: [8/100][98/156] Data 0.001 (0.001) Batch 4.071 (3.445) Remain 13:47:27 loss: 0.8316 Lr: 0.04991
[2023-08-07 19:49:26,549 INFO misc.py line 115 22900] Train: [8/100][99/156] Data 0.001 (0.001) Batch 3.411 (3.445) Remain 13:47:18 loss: 0.6764 Lr: 0.04990
[2023-08-07 19:49:29,881 INFO misc.py line 115 22900] Train: [8/100][100/156] Data 0.001 (0.001) Batch 3.332 (3.444) Remain 13:46:58 loss: 0.6485 Lr: 0.04990
[2023-08-07 19:49:33,704 INFO misc.py line 115 22900] Train: [8/100][101/156] Data 0.001 (0.001) Batch 3.823 (3.448) Remain 13:47:50 loss: 1.1229 Lr: 0.04990
[2023-08-07 19:49:36,342 INFO misc.py line 115 22900] Train: [8/100][102/156] Data 0.001 (0.001) Batch 2.638 (3.440) Remain 13:45:49 loss: 0.6989 Lr: 0.04990
[2023-08-07 19:49:39,914 INFO misc.py line 115 22900] Train: [8/100][103/156] Data 0.001 (0.001) Batch 3.572 (3.441) Remain 13:46:05 loss: 0.9421 Lr: 0.04990
[2023-08-07 19:49:44,037 INFO misc.py line 115 22900] Train: [8/100][104/156] Data 0.001 (0.001) Batch 4.124 (3.448) Remain 13:47:39 loss: 1.4762 Lr: 0.04990
[2023-08-07 19:49:47,624 INFO misc.py line 115 22900] Train: [8/100][105/156] Data 0.001 (0.001) Batch 3.587 (3.449) Remain 13:47:55 loss: 0.8364 Lr: 0.04990
[2023-08-07 19:49:50,508 INFO misc.py line 115 22900] Train: [8/100][106/156] Data 0.001 (0.001) Batch 2.884 (3.443) Remain 13:46:32 loss: 0.8802 Lr: 0.04990
[2023-08-07 19:49:53,808 INFO misc.py line 115 22900] Train: [8/100][107/156] Data 0.001 (0.001) Batch 3.299 (3.442) Remain 13:46:09 loss: 1.2483 Lr: 0.04990
[2023-08-07 19:49:57,829 INFO misc.py line 115 22900] Train: [8/100][108/156] Data 0.001 (0.001) Batch 4.021 (3.448) Remain 13:47:25 loss: 1.1188 Lr: 0.04990
[2023-08-07 19:50:00,483 INFO misc.py line 115 22900] Train: [8/100][109/156] Data 0.001 (0.001) Batch 2.654 (3.440) Remain 13:45:34 loss: 0.5746 Lr: 0.04990
[2023-08-07 19:50:03,313 INFO misc.py line 115 22900] Train: [8/100][110/156] Data 0.001 (0.001) Batch 2.830 (3.434) Remain 13:44:08 loss: 0.6300 Lr: 0.04990
[2023-08-07 19:50:07,043 INFO misc.py line 115 22900] Train: [8/100][111/156] Data 0.001 (0.001) Batch 3.730 (3.437) Remain 13:44:44 loss: 1.0540 Lr: 0.04990
[2023-08-07 19:50:10,678 INFO misc.py line 115 22900] Train: [8/100][112/156] Data 0.001 (0.001) Batch 3.635 (3.439) Remain 13:45:07 loss: 0.8586 Lr: 0.04990
[2023-08-07 19:50:14,387 INFO misc.py line 115 22900] Train: [8/100][113/156] Data 0.001 (0.001) Batch 3.709 (3.441) Remain 13:45:39 loss: 1.1134 Lr: 0.04990
[2023-08-07 19:50:17,707 INFO misc.py line 115 22900] Train: [8/100][114/156] Data 0.001 (0.001) Batch 3.320 (3.440) Remain 13:45:20 loss: 0.9038 Lr: 0.04990
[2023-08-07 19:50:21,320 INFO misc.py line 115 22900] Train: [8/100][115/156] Data 0.001 (0.001) Batch 3.614 (3.442) Remain 13:45:38 loss: 0.6686 Lr: 0.04990
[2023-08-07 19:50:25,926 INFO misc.py line 115 22900] Train: [8/100][116/156] Data 0.001 (0.001) Batch 4.606 (3.452) Remain 13:48:03 loss: 1.4779 Lr: 0.04990
[2023-08-07 19:50:29,917 INFO misc.py line 115 22900] Train: [8/100][117/156] Data 0.001 (0.001) Batch 3.991 (3.457) Remain 13:49:08 loss: 1.1828 Lr: 0.04990
[2023-08-07 19:50:33,588 INFO misc.py line 115 22900] Train: [8/100][118/156] Data 0.001 (0.001) Batch 3.671 (3.459) Remain 13:49:31 loss: 0.8288 Lr: 0.04990
[2023-08-07 19:50:36,490 INFO misc.py line 115 22900] Train: [8/100][119/156] Data 0.001 (0.001) Batch 2.902 (3.454) Remain 13:48:19 loss: 0.5722 Lr: 0.04990
[2023-08-07 19:50:38,947 INFO misc.py line 115 22900] Train: [8/100][120/156] Data 0.001 (0.001) Batch 2.457 (3.445) Remain 13:46:13 loss: 1.6752 Lr: 0.04989
[2023-08-07 19:50:42,296 INFO misc.py line 115 22900] Train: [8/100][121/156] Data 0.001 (0.001) Batch 3.349 (3.445) Remain 13:45:57 loss: 0.7031 Lr: 0.04989
[2023-08-07 19:50:46,330 INFO misc.py line 115 22900] Train: [8/100][122/156] Data 0.001 (0.001) Batch 4.034 (3.450) Remain 13:47:05 loss: 0.9668 Lr: 0.04989
[2023-08-07 19:50:49,719 INFO misc.py line 115 22900] Train: [8/100][123/156] Data 0.001 (0.001) Batch 3.389 (3.449) Remain 13:46:54 loss: 1.1617 Lr: 0.04989
[2023-08-07 19:50:53,598 INFO misc.py line 115 22900] Train: [8/100][124/156] Data 0.001 (0.001) Batch 3.879 (3.453) Remain 13:47:42 loss: 0.9135 Lr: 0.04989
[2023-08-07 19:50:58,114 INFO misc.py line 115 22900] Train: [8/100][125/156] Data 0.001 (0.001) Batch 4.516 (3.461) Remain 13:49:44 loss: 1.2344 Lr: 0.04989
[2023-08-07 19:51:01,772 INFO misc.py line 115 22900] Train: [8/100][126/156] Data 0.001 (0.001) Batch 3.658 (3.463) Remain 13:50:03 loss: 0.7057 Lr: 0.04989
[2023-08-07 19:51:04,457 INFO misc.py line 115 22900] Train: [8/100][127/156] Data 0.001 (0.001) Batch 2.685 (3.457) Remain 13:48:30 loss: 0.9497 Lr: 0.04989
[2023-08-07 19:51:08,499 INFO misc.py line 115 22900] Train: [8/100][128/156] Data 0.001 (0.001) Batch 4.042 (3.461) Remain 13:49:34 loss: 0.8908 Lr: 0.04989
[2023-08-07 19:51:12,150 INFO misc.py line 115 22900] Train: [8/100][129/156] Data 0.001 (0.001) Batch 3.651 (3.463) Remain 13:49:52 loss: 1.2219 Lr: 0.04989
[2023-08-07 19:51:15,420 INFO misc.py line 115 22900] Train: [8/100][130/156] Data 0.001 (0.001) Batch 3.269 (3.461) Remain 13:49:27 loss: 0.6818 Lr: 0.04989
[2023-08-07 19:51:17,749 INFO misc.py line 115 22900] Train: [8/100][131/156] Data 0.001 (0.001) Batch 2.329 (3.452) Remain 13:47:16 loss: 0.6792 Lr: 0.04989
[2023-08-07 19:51:21,422 INFO misc.py line 115 22900] Train: [8/100][132/156] Data 0.001 (0.001) Batch 3.673 (3.454) Remain 13:47:37 loss: 1.4209 Lr: 0.04989
[2023-08-07 19:51:24,666 INFO misc.py line 115 22900] Train: [8/100][133/156] Data 0.001 (0.001) Batch 3.244 (3.453) Remain 13:47:10 loss: 0.9052 Lr: 0.04989
[2023-08-07 19:51:28,506 INFO misc.py line 115 22900] Train: [8/100][134/156] Data 0.001 (0.001) Batch 3.841 (3.456) Remain 13:47:49 loss: 1.1124 Lr: 0.04989
[2023-08-07 19:51:32,869 INFO misc.py line 115 22900] Train: [8/100][135/156] Data 0.001 (0.001) Batch 4.362 (3.462) Remain 13:49:25 loss: 1.1176 Lr: 0.04989
[2023-08-07 19:51:36,309 INFO misc.py line 115 22900] Train: [8/100][136/156] Data 0.001 (0.001) Batch 3.440 (3.462) Remain 13:49:19 loss: 1.2686 Lr: 0.04989
[2023-08-07 19:51:40,071 INFO misc.py line 115 22900] Train: [8/100][137/156] Data 0.001 (0.001) Batch 3.762 (3.464) Remain 13:49:48 loss: 0.6603 Lr: 0.04989
[2023-08-07 19:51:43,985 INFO misc.py line 115 22900] Train: [8/100][138/156] Data 0.001 (0.001) Batch 3.914 (3.468) Remain 13:50:32 loss: 1.2987 Lr: 0.04989
[2023-08-07 19:51:47,978 INFO misc.py line 115 22900] Train: [8/100][139/156] Data 0.001 (0.001) Batch 3.993 (3.472) Remain 13:51:24 loss: 0.8980 Lr: 0.04989
[2023-08-07 19:51:51,939 INFO misc.py line 115 22900] Train: [8/100][140/156] Data 0.001 (0.001) Batch 3.961 (3.475) Remain 13:52:12 loss: 0.7369 Lr: 0.04988
[2023-08-07 19:51:54,851 INFO misc.py line 115 22900] Train: [8/100][141/156] Data 0.001 (0.001) Batch 2.912 (3.471) Remain 13:51:10 loss: 0.6355 Lr: 0.04988
[2023-08-07 19:51:56,935 INFO misc.py line 115 22900] Train: [8/100][142/156] Data 0.001 (0.001) Batch 2.084 (3.461) Remain 13:48:43 loss: 0.8528 Lr: 0.04988
[2023-08-07 19:52:00,916 INFO misc.py line 115 22900] Train: [8/100][143/156] Data 0.001 (0.001) Batch 3.981 (3.465) Remain 13:49:33 loss: 1.4780 Lr: 0.04988
[2023-08-07 19:52:03,877 INFO misc.py line 115 22900] Train: [8/100][144/156] Data 0.001 (0.001) Batch 2.962 (3.461) Remain 13:48:38 loss: 1.2594 Lr: 0.04988
[2023-08-07 19:52:07,966 INFO misc.py line 115 22900] Train: [8/100][145/156] Data 0.001 (0.001) Batch 4.088 (3.466) Remain 13:49:38 loss: 1.1420 Lr: 0.04988
[2023-08-07 19:52:11,212 INFO misc.py line 115 22900] Train: [8/100][146/156] Data 0.001 (0.001) Batch 3.246 (3.464) Remain 13:49:12 loss: 0.8437 Lr: 0.04988
[2023-08-07 19:52:14,215 INFO misc.py line 115 22900] Train: [8/100][147/156] Data 0.001 (0.001) Batch 3.004 (3.461) Remain 13:48:23 loss: 0.9351 Lr: 0.04988
[2023-08-07 19:52:17,340 INFO misc.py line 115 22900] Train: [8/100][148/156] Data 0.001 (0.001) Batch 3.125 (3.459) Remain 13:47:46 loss: 1.2004 Lr: 0.04988
[2023-08-07 19:52:21,898 INFO misc.py line 115 22900] Train: [8/100][149/156] Data 0.001 (0.001) Batch 4.557 (3.466) Remain 13:49:31 loss: 1.4250 Lr: 0.04988
[2023-08-07 19:52:25,256 INFO misc.py line 115 22900] Train: [8/100][150/156] Data 0.001 (0.001) Batch 3.359 (3.465) Remain 13:49:17 loss: 1.3229 Lr: 0.04988
[2023-08-07 19:52:28,283 INFO misc.py line 115 22900] Train: [8/100][151/156] Data 0.001 (0.001) Batch 3.026 (3.463) Remain 13:48:31 loss: 1.2612 Lr: 0.04988
[2023-08-07 19:52:31,666 INFO misc.py line 115 22900] Train: [8/100][152/156] Data 0.001 (0.001) Batch 3.383 (3.462) Remain 13:48:20 loss: 0.8282 Lr: 0.04988
[2023-08-07 19:52:36,039 INFO misc.py line 115 22900] Train: [8/100][153/156] Data 0.001 (0.001) Batch 4.373 (3.468) Remain 13:49:43 loss: 1.1782 Lr: 0.04988
[2023-08-07 19:52:39,633 INFO misc.py line 115 22900] Train: [8/100][154/156] Data 0.001 (0.001) Batch 3.594 (3.469) Remain 13:49:52 loss: 0.8672 Lr: 0.04988
[2023-08-07 19:52:41,809 INFO misc.py line 115 22900] Train: [8/100][155/156] Data 0.001 (0.001) Batch 2.175 (3.460) Remain 13:47:46 loss: 0.6934 Lr: 0.04988
[2023-08-07 19:52:45,450 INFO misc.py line 115 22900] Train: [8/100][156/156] Data 0.001 (0.001) Batch 3.641 (3.462) Remain 13:48:00 loss: 0.9763 Lr: 0.04988
[2023-08-07 19:52:45,450 INFO misc.py line 129 22900] Train result: loss: 0.9794 
[2023-08-07 19:52:45,451 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 19:52:47,546 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.7825 
[2023-08-07 19:52:48,416 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.9358 
[2023-08-07 19:52:50,082 INFO evaluator.py line 122 22900] Test: [3/24] Loss 1.3727 
[2023-08-07 19:52:51,602 INFO evaluator.py line 122 22900] Test: [4/24] Loss 0.9980 
[2023-08-07 19:52:53,451 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.6611 
[2023-08-07 19:52:55,113 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.6444 
[2023-08-07 19:52:57,253 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.5076 
[2023-08-07 19:52:59,057 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.7159 
[2023-08-07 19:53:00,341 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.1104 
[2023-08-07 19:53:02,473 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3309 
[2023-08-07 19:53:02,999 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.6006 
[2023-08-07 19:53:04,530 INFO evaluator.py line 122 22900] Test: [12/24] Loss 1.0926 
[2023-08-07 19:53:07,242 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.0675 
[2023-08-07 19:53:08,924 INFO evaluator.py line 122 22900] Test: [14/24] Loss 1.1092 
[2023-08-07 19:53:10,948 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.6674 
[2023-08-07 19:53:13,659 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.0628 
[2023-08-07 19:53:16,364 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.2061 
[2023-08-07 19:53:18,211 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.7527 
[2023-08-07 19:53:18,961 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.2206 
[2023-08-07 19:53:19,845 INFO evaluator.py line 122 22900] Test: [20/24] Loss 1.3114 
[2023-08-07 19:53:22,104 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.2654 
[2023-08-07 19:53:24,069 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.8387 
[2023-08-07 19:53:25,914 INFO evaluator.py line 122 22900] Test: [23/24] Loss 1.9926 
[2023-08-07 19:53:27,849 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.7271 
[2023-08-07 19:53:27,897 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.1418/0.2107/0.6312.
[2023-08-07 19:53:27,897 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6454/0.9018
[2023-08-07 19:53:27,897 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9406/0.9858
[2023-08-07 19:53:27,897 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.0667/0.1826
[2023-08-07 19:53:27,897 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.0664/0.1642
[2023-08-07 19:53:27,897 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.4600/0.9019
[2023-08-07 19:53:27,897 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.0051/0.0053
[2023-08-07 19:53:27,897 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.2914/0.3612
[2023-08-07 19:53:27,897 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0567/0.0619
[2023-08-07 19:53:27,897 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.0688/0.1783
[2023-08-07 19:53:27,897 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0405/0.0426
[2023-08-07 19:53:27,897 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:53:27,897 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:53:27,897 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.0549/0.0870
[2023-08-07 19:53:27,897 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0011/0.0011
[2023-08-07 19:53:27,897 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0010/0.0010
[2023-08-07 19:53:27,898 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:53:27,898 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:53:27,898 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.0759/0.2210
[2023-08-07 19:53:27,898 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 19:53:27,898 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0610/0.1194
[2023-08-07 19:53:27,898 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 19:53:27,898 INFO misc.py line 152 22900] Currently Best mIoU: 0.1503
[2023-08-07 19:53:27,898 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 19:53:32,737 INFO misc.py line 115 22900] Train: [9/100][1/156] Data 1.026 (1.026) Batch 3.990 (3.990) Remain 15:54:15 loss: 0.9945 Lr: 0.04988
[2023-08-07 19:53:36,395 INFO misc.py line 115 22900] Train: [9/100][2/156] Data 0.001 (0.001) Batch 3.657 (3.657) Remain 14:34:44 loss: 1.4246 Lr: 0.04988
[2023-08-07 19:53:39,827 INFO misc.py line 115 22900] Train: [9/100][3/156] Data 0.001 (0.001) Batch 3.433 (3.433) Remain 13:40:57 loss: 0.8829 Lr: 0.04987
[2023-08-07 19:53:42,476 INFO misc.py line 115 22900] Train: [9/100][4/156] Data 0.001 (0.001) Batch 2.648 (2.648) Remain 10:33:18 loss: 0.6990 Lr: 0.04987
[2023-08-07 19:53:44,878 INFO misc.py line 115 22900] Train: [9/100][5/156] Data 0.001 (0.001) Batch 2.402 (2.525) Remain 10:03:51 loss: 0.5219 Lr: 0.04987
[2023-08-07 19:53:48,999 INFO misc.py line 115 22900] Train: [9/100][6/156] Data 0.001 (0.001) Batch 4.121 (3.057) Remain 12:11:00 loss: 0.6544 Lr: 0.04987
[2023-08-07 19:53:53,086 INFO misc.py line 115 22900] Train: [9/100][7/156] Data 0.001 (0.001) Batch 4.087 (3.315) Remain 13:12:28 loss: 1.0276 Lr: 0.04987
[2023-08-07 19:53:57,062 INFO misc.py line 115 22900] Train: [9/100][8/156] Data 0.001 (0.001) Batch 3.976 (3.447) Remain 13:44:02 loss: 0.7376 Lr: 0.04987
[2023-08-07 19:54:00,242 INFO misc.py line 115 22900] Train: [9/100][9/156] Data 0.001 (0.001) Batch 3.180 (3.402) Remain 13:33:21 loss: 0.7952 Lr: 0.04987
[2023-08-07 19:54:03,853 INFO misc.py line 115 22900] Train: [9/100][10/156] Data 0.001 (0.001) Batch 3.611 (3.432) Remain 13:40:24 loss: 0.6811 Lr: 0.04987
[2023-08-07 19:54:06,524 INFO misc.py line 115 22900] Train: [9/100][11/156] Data 0.001 (0.001) Batch 2.671 (3.337) Remain 13:17:36 loss: 0.5680 Lr: 0.04987
[2023-08-07 19:54:09,028 INFO misc.py line 115 22900] Train: [9/100][12/156] Data 0.001 (0.001) Batch 2.504 (3.244) Remain 12:55:26 loss: 0.8684 Lr: 0.04987
[2023-08-07 19:54:12,609 INFO misc.py line 115 22900] Train: [9/100][13/156] Data 0.001 (0.001) Batch 3.581 (3.278) Remain 13:03:25 loss: 1.1116 Lr: 0.04987
[2023-08-07 19:54:17,310 INFO misc.py line 115 22900] Train: [9/100][14/156] Data 0.002 (0.001) Batch 4.701 (3.408) Remain 13:34:17 loss: 1.3034 Lr: 0.04987
[2023-08-07 19:54:20,718 INFO misc.py line 115 22900] Train: [9/100][15/156] Data 0.001 (0.001) Batch 3.408 (3.408) Remain 13:34:13 loss: 0.9315 Lr: 0.04987
[2023-08-07 19:54:24,780 INFO misc.py line 115 22900] Train: [9/100][16/156] Data 0.001 (0.001) Batch 4.062 (3.458) Remain 13:46:12 loss: 1.2079 Lr: 0.04987
[2023-08-07 19:54:28,742 INFO misc.py line 115 22900] Train: [9/100][17/156] Data 0.001 (0.001) Batch 3.962 (3.494) Remain 13:54:45 loss: 0.7957 Lr: 0.04987
[2023-08-07 19:54:30,688 INFO misc.py line 115 22900] Train: [9/100][18/156] Data 0.001 (0.001) Batch 1.946 (3.391) Remain 13:30:02 loss: 0.5083 Lr: 0.04987
[2023-08-07 19:54:34,251 INFO misc.py line 115 22900] Train: [9/100][19/156] Data 0.001 (0.001) Batch 3.563 (3.401) Remain 13:32:33 loss: 1.2033 Lr: 0.04987
[2023-08-07 19:54:37,547 INFO misc.py line 115 22900] Train: [9/100][20/156] Data 0.001 (0.001) Batch 3.296 (3.395) Remain 13:31:01 loss: 0.8376 Lr: 0.04987
[2023-08-07 19:54:39,394 INFO misc.py line 115 22900] Train: [9/100][21/156] Data 0.001 (0.001) Batch 1.847 (3.309) Remain 13:10:25 loss: 0.7387 Lr: 0.04987
[2023-08-07 19:54:43,273 INFO misc.py line 115 22900] Train: [9/100][22/156] Data 0.001 (0.001) Batch 3.879 (3.339) Remain 13:17:31 loss: 1.0229 Lr: 0.04986
[2023-08-07 19:54:47,080 INFO misc.py line 115 22900] Train: [9/100][23/156] Data 0.001 (0.001) Batch 3.807 (3.363) Remain 13:23:02 loss: 1.0904 Lr: 0.04986
[2023-08-07 19:54:50,456 INFO misc.py line 115 22900] Train: [9/100][24/156] Data 0.001 (0.001) Batch 3.376 (3.363) Remain 13:23:08 loss: 0.8212 Lr: 0.04986
[2023-08-07 19:54:54,147 INFO misc.py line 115 22900] Train: [9/100][25/156] Data 0.001 (0.001) Batch 3.691 (3.378) Remain 13:26:38 loss: 0.6880 Lr: 0.04986
[2023-08-07 19:54:57,894 INFO misc.py line 115 22900] Train: [9/100][26/156] Data 0.001 (0.001) Batch 3.748 (3.394) Remain 13:30:25 loss: 1.1802 Lr: 0.04986
[2023-08-07 19:55:01,217 INFO misc.py line 115 22900] Train: [9/100][27/156] Data 0.001 (0.001) Batch 3.323 (3.391) Remain 13:29:39 loss: 0.9410 Lr: 0.04986
[2023-08-07 19:55:04,075 INFO misc.py line 115 22900] Train: [9/100][28/156] Data 0.001 (0.001) Batch 2.858 (3.370) Remain 13:24:30 loss: 0.9250 Lr: 0.04986
[2023-08-07 19:55:07,336 INFO misc.py line 115 22900] Train: [9/100][29/156] Data 0.012 (0.001) Batch 3.261 (3.366) Remain 13:23:27 loss: 1.0384 Lr: 0.04986
[2023-08-07 19:55:11,788 INFO misc.py line 115 22900] Train: [9/100][30/156] Data 0.001 (0.001) Batch 4.452 (3.406) Remain 13:33:00 loss: 1.6044 Lr: 0.04986
[2023-08-07 19:55:14,797 INFO misc.py line 115 22900] Train: [9/100][31/156] Data 0.001 (0.001) Batch 3.009 (3.392) Remain 13:29:33 loss: 1.1374 Lr: 0.04986
[2023-08-07 19:55:17,894 INFO misc.py line 115 22900] Train: [9/100][32/156] Data 0.001 (0.001) Batch 3.097 (3.382) Remain 13:27:04 loss: 0.7707 Lr: 0.04986
[2023-08-07 19:55:21,260 INFO misc.py line 115 22900] Train: [9/100][33/156] Data 0.001 (0.001) Batch 3.366 (3.381) Remain 13:26:53 loss: 0.9693 Lr: 0.04986
[2023-08-07 19:55:25,018 INFO misc.py line 115 22900] Train: [9/100][34/156] Data 0.001 (0.001) Batch 3.758 (3.393) Remain 13:29:44 loss: 0.9673 Lr: 0.04986
[2023-08-07 19:55:29,060 INFO misc.py line 115 22900] Train: [9/100][35/156] Data 0.001 (0.001) Batch 4.042 (3.414) Remain 13:34:31 loss: 0.9595 Lr: 0.04986
[2023-08-07 19:55:32,805 INFO misc.py line 115 22900] Train: [9/100][36/156] Data 0.001 (0.001) Batch 3.745 (3.424) Remain 13:36:51 loss: 1.1051 Lr: 0.04986
[2023-08-07 19:55:35,889 INFO misc.py line 115 22900] Train: [9/100][37/156] Data 0.001 (0.001) Batch 3.084 (3.414) Remain 13:34:25 loss: 0.6687 Lr: 0.04986
[2023-08-07 19:55:39,073 INFO misc.py line 115 22900] Train: [9/100][38/156] Data 0.001 (0.001) Batch 3.184 (3.407) Remain 13:32:48 loss: 1.0680 Lr: 0.04986
[2023-08-07 19:55:42,904 INFO misc.py line 115 22900] Train: [9/100][39/156] Data 0.001 (0.001) Batch 3.830 (3.419) Remain 13:35:32 loss: 0.9506 Lr: 0.04986
[2023-08-07 19:55:45,880 INFO misc.py line 115 22900] Train: [9/100][40/156] Data 0.001 (0.001) Batch 2.976 (3.407) Remain 13:32:38 loss: 0.8147 Lr: 0.04985
[2023-08-07 19:55:49,803 INFO misc.py line 115 22900] Train: [9/100][41/156] Data 0.001 (0.001) Batch 3.923 (3.420) Remain 13:35:49 loss: 0.9776 Lr: 0.04985
[2023-08-07 19:55:53,121 INFO misc.py line 115 22900] Train: [9/100][42/156] Data 0.001 (0.001) Batch 3.319 (3.418) Remain 13:35:08 loss: 0.7711 Lr: 0.04985
[2023-08-07 19:55:57,216 INFO misc.py line 115 22900] Train: [9/100][43/156] Data 0.001 (0.001) Batch 4.094 (3.435) Remain 13:39:07 loss: 1.2064 Lr: 0.04985
[2023-08-07 19:56:00,078 INFO misc.py line 115 22900] Train: [9/100][44/156] Data 0.001 (0.001) Batch 2.862 (3.421) Remain 13:35:44 loss: 0.8685 Lr: 0.04985
[2023-08-07 19:56:02,194 INFO misc.py line 115 22900] Train: [9/100][45/156] Data 0.001 (0.001) Batch 2.116 (3.390) Remain 13:28:16 loss: 0.7424 Lr: 0.04985
[2023-08-07 19:56:05,776 INFO misc.py line 115 22900] Train: [9/100][46/156] Data 0.001 (0.001) Batch 3.582 (3.394) Remain 13:29:16 loss: 0.7639 Lr: 0.04985
[2023-08-07 19:56:09,727 INFO misc.py line 115 22900] Train: [9/100][47/156] Data 0.001 (0.001) Batch 3.950 (3.407) Remain 13:32:14 loss: 1.2822 Lr: 0.04985
[2023-08-07 19:56:13,625 INFO misc.py line 115 22900] Train: [9/100][48/156] Data 0.001 (0.001) Batch 3.898 (3.418) Remain 13:34:47 loss: 0.7129 Lr: 0.04985
[2023-08-07 19:56:18,133 INFO misc.py line 115 22900] Train: [9/100][49/156] Data 0.001 (0.001) Batch 4.508 (3.441) Remain 13:40:22 loss: 1.2207 Lr: 0.04985
[2023-08-07 19:56:22,123 INFO misc.py line 115 22900] Train: [9/100][50/156] Data 0.001 (0.001) Batch 3.990 (3.453) Remain 13:43:06 loss: 0.6704 Lr: 0.04985
[2023-08-07 19:56:26,388 INFO misc.py line 115 22900] Train: [9/100][51/156] Data 0.001 (0.001) Batch 4.265 (3.470) Remain 13:47:04 loss: 1.0833 Lr: 0.04985
[2023-08-07 19:56:29,987 INFO misc.py line 115 22900] Train: [9/100][52/156] Data 0.001 (0.001) Batch 3.600 (3.473) Remain 13:47:38 loss: 1.2541 Lr: 0.04985
[2023-08-07 19:56:33,310 INFO misc.py line 115 22900] Train: [9/100][53/156] Data 0.001 (0.001) Batch 3.323 (3.470) Remain 13:46:52 loss: 1.1080 Lr: 0.04985
[2023-08-07 19:56:36,383 INFO misc.py line 115 22900] Train: [9/100][54/156] Data 0.001 (0.001) Batch 3.073 (3.462) Remain 13:44:57 loss: 1.0175 Lr: 0.04985
[2023-08-07 19:56:39,989 INFO misc.py line 115 22900] Train: [9/100][55/156] Data 0.001 (0.001) Batch 3.606 (3.465) Remain 13:45:34 loss: 0.6955 Lr: 0.04985
[2023-08-07 19:56:43,481 INFO misc.py line 115 22900] Train: [9/100][56/156] Data 0.001 (0.001) Batch 3.492 (3.465) Remain 13:45:38 loss: 1.0619 Lr: 0.04985
[2023-08-07 19:56:45,976 INFO misc.py line 115 22900] Train: [9/100][57/156] Data 0.001 (0.001) Batch 2.494 (3.447) Remain 13:41:17 loss: 0.4506 Lr: 0.04984
[2023-08-07 19:56:48,921 INFO misc.py line 115 22900] Train: [9/100][58/156] Data 0.001 (0.001) Batch 2.945 (3.438) Remain 13:39:03 loss: 0.7323 Lr: 0.04984
[2023-08-07 19:56:51,378 INFO misc.py line 115 22900] Train: [9/100][59/156] Data 0.001 (0.001) Batch 2.457 (3.421) Remain 13:34:49 loss: 0.3316 Lr: 0.04984
[2023-08-07 19:56:55,056 INFO misc.py line 115 22900] Train: [9/100][60/156] Data 0.001 (0.001) Batch 3.678 (3.425) Remain 13:35:50 loss: 1.4596 Lr: 0.04984
[2023-08-07 19:56:58,364 INFO misc.py line 115 22900] Train: [9/100][61/156] Data 0.001 (0.001) Batch 3.308 (3.423) Remain 13:35:18 loss: 1.2034 Lr: 0.04984
[2023-08-07 19:57:02,714 INFO misc.py line 115 22900] Train: [9/100][62/156] Data 0.001 (0.001) Batch 4.350 (3.439) Remain 13:38:59 loss: 1.2306 Lr: 0.04984
[2023-08-07 19:57:06,708 INFO misc.py line 115 22900] Train: [9/100][63/156] Data 0.001 (0.001) Batch 3.994 (3.448) Remain 13:41:08 loss: 0.9786 Lr: 0.04984
[2023-08-07 19:57:10,199 INFO misc.py line 115 22900] Train: [9/100][64/156] Data 0.001 (0.001) Batch 3.491 (3.449) Remain 13:41:15 loss: 0.7197 Lr: 0.04984
[2023-08-07 19:57:14,043 INFO misc.py line 115 22900] Train: [9/100][65/156] Data 0.001 (0.001) Batch 3.844 (3.455) Remain 13:42:42 loss: 0.8356 Lr: 0.04984
[2023-08-07 19:57:17,317 INFO misc.py line 115 22900] Train: [9/100][66/156] Data 0.001 (0.001) Batch 3.274 (3.452) Remain 13:41:58 loss: 0.9399 Lr: 0.04984
[2023-08-07 19:57:19,078 INFO misc.py line 115 22900] Train: [9/100][67/156] Data 0.001 (0.001) Batch 1.762 (3.426) Remain 13:35:37 loss: 1.0623 Lr: 0.04984
[2023-08-07 19:57:23,030 INFO misc.py line 115 22900] Train: [9/100][68/156] Data 0.001 (0.001) Batch 3.952 (3.434) Remain 13:37:29 loss: 0.9146 Lr: 0.04984
[2023-08-07 19:57:26,426 INFO misc.py line 115 22900] Train: [9/100][69/156] Data 0.001 (0.001) Batch 3.396 (3.433) Remain 13:37:18 loss: 0.6105 Lr: 0.04984
[2023-08-07 19:57:30,145 INFO misc.py line 115 22900] Train: [9/100][70/156] Data 0.001 (0.001) Batch 3.718 (3.438) Remain 13:38:15 loss: 1.2235 Lr: 0.04984
[2023-08-07 19:57:32,789 INFO misc.py line 115 22900] Train: [9/100][71/156] Data 0.001 (0.001) Batch 2.644 (3.426) Remain 13:35:25 loss: 0.8457 Lr: 0.04984
[2023-08-07 19:57:36,256 INFO misc.py line 115 22900] Train: [9/100][72/156] Data 0.001 (0.001) Batch 3.467 (3.426) Remain 13:35:30 loss: 0.8476 Lr: 0.04984
[2023-08-07 19:57:39,846 INFO misc.py line 115 22900] Train: [9/100][73/156] Data 0.001 (0.001) Batch 3.590 (3.429) Remain 13:36:00 loss: 0.5666 Lr: 0.04984
[2023-08-07 19:57:43,906 INFO misc.py line 115 22900] Train: [9/100][74/156] Data 0.001 (0.001) Batch 4.061 (3.438) Remain 13:38:03 loss: 1.2846 Lr: 0.04983
[2023-08-07 19:57:46,910 INFO misc.py line 115 22900] Train: [9/100][75/156] Data 0.001 (0.001) Batch 3.003 (3.432) Remain 13:36:34 loss: 0.7140 Lr: 0.04983
[2023-08-07 19:57:50,622 INFO misc.py line 115 22900] Train: [9/100][76/156] Data 0.001 (0.001) Batch 3.712 (3.436) Remain 13:37:25 loss: 0.8116 Lr: 0.04983
[2023-08-07 19:57:54,035 INFO misc.py line 115 22900] Train: [9/100][77/156] Data 0.001 (0.001) Batch 3.413 (3.435) Remain 13:37:18 loss: 1.4990 Lr: 0.04983
[2023-08-07 19:57:57,784 INFO misc.py line 115 22900] Train: [9/100][78/156] Data 0.001 (0.001) Batch 3.749 (3.439) Remain 13:38:14 loss: 0.6403 Lr: 0.04983
[2023-08-07 19:58:01,952 INFO misc.py line 115 22900] Train: [9/100][79/156] Data 0.001 (0.001) Batch 4.168 (3.449) Remain 13:40:27 loss: 1.1881 Lr: 0.04983
[2023-08-07 19:58:05,807 INFO misc.py line 115 22900] Train: [9/100][80/156] Data 0.001 (0.001) Batch 3.854 (3.454) Remain 13:41:39 loss: 1.0889 Lr: 0.04983
[2023-08-07 19:58:08,445 INFO misc.py line 115 22900] Train: [9/100][81/156] Data 0.001 (0.001) Batch 2.638 (3.444) Remain 13:39:06 loss: 0.8804 Lr: 0.04983
[2023-08-07 19:58:11,057 INFO misc.py line 115 22900] Train: [9/100][82/156] Data 0.001 (0.001) Batch 2.612 (3.433) Remain 13:36:32 loss: 0.7815 Lr: 0.04983
[2023-08-07 19:58:14,478 INFO misc.py line 115 22900] Train: [9/100][83/156] Data 0.001 (0.001) Batch 3.421 (3.433) Remain 13:36:27 loss: 0.8324 Lr: 0.04983
[2023-08-07 19:58:18,166 INFO misc.py line 115 22900] Train: [9/100][84/156] Data 0.001 (0.001) Batch 3.687 (3.436) Remain 13:37:08 loss: 0.9403 Lr: 0.04983
[2023-08-07 19:58:20,187 INFO misc.py line 115 22900] Train: [9/100][85/156] Data 0.001 (0.001) Batch 2.021 (3.419) Remain 13:32:59 loss: 0.7370 Lr: 0.04983
[2023-08-07 19:58:23,038 INFO misc.py line 115 22900] Train: [9/100][86/156] Data 0.001 (0.001) Batch 2.851 (3.412) Remain 13:31:18 loss: 0.7589 Lr: 0.04983
[2023-08-07 19:58:26,490 INFO misc.py line 115 22900] Train: [9/100][87/156] Data 0.001 (0.001) Batch 3.453 (3.413) Remain 13:31:21 loss: 0.9241 Lr: 0.04983
[2023-08-07 19:58:30,448 INFO misc.py line 115 22900] Train: [9/100][88/156] Data 0.001 (0.001) Batch 3.957 (3.419) Remain 13:32:49 loss: 1.0353 Lr: 0.04983
[2023-08-07 19:58:34,512 INFO misc.py line 115 22900] Train: [9/100][89/156] Data 0.001 (0.001) Batch 4.064 (3.427) Remain 13:34:32 loss: 1.2812 Lr: 0.04983
[2023-08-07 19:58:38,139 INFO misc.py line 115 22900] Train: [9/100][90/156] Data 0.001 (0.001) Batch 3.628 (3.429) Remain 13:35:02 loss: 1.2456 Lr: 0.04982
[2023-08-07 19:58:42,279 INFO misc.py line 115 22900] Train: [9/100][91/156] Data 0.001 (0.001) Batch 4.139 (3.437) Remain 13:36:54 loss: 0.7557 Lr: 0.04982
[2023-08-07 19:58:45,466 INFO misc.py line 115 22900] Train: [9/100][92/156] Data 0.001 (0.001) Batch 3.187 (3.434) Remain 13:36:10 loss: 0.8058 Lr: 0.04982
[2023-08-07 19:58:49,497 INFO misc.py line 115 22900] Train: [9/100][93/156] Data 0.001 (0.001) Batch 4.031 (3.441) Remain 13:37:41 loss: 1.1928 Lr: 0.04982
[2023-08-07 19:58:52,546 INFO misc.py line 115 22900] Train: [9/100][94/156] Data 0.001 (0.001) Batch 3.049 (3.436) Remain 13:36:37 loss: 0.7388 Lr: 0.04982
[2023-08-07 19:58:55,579 INFO misc.py line 115 22900] Train: [9/100][95/156] Data 0.001 (0.001) Batch 3.033 (3.432) Remain 13:35:31 loss: 1.0019 Lr: 0.04982
[2023-08-07 19:58:58,991 INFO misc.py line 115 22900] Train: [9/100][96/156] Data 0.001 (0.001) Batch 3.412 (3.432) Remain 13:35:24 loss: 0.8439 Lr: 0.04982
[2023-08-07 19:59:02,187 INFO misc.py line 115 22900] Train: [9/100][97/156] Data 0.001 (0.001) Batch 3.196 (3.429) Remain 13:34:45 loss: 0.8120 Lr: 0.04982
[2023-08-07 19:59:04,963 INFO misc.py line 115 22900] Train: [9/100][98/156] Data 0.001 (0.001) Batch 2.776 (3.422) Remain 13:33:03 loss: 0.7683 Lr: 0.04982
[2023-08-07 19:59:07,981 INFO misc.py line 115 22900] Train: [9/100][99/156] Data 0.001 (0.001) Batch 3.018 (3.418) Remain 13:32:00 loss: 0.6923 Lr: 0.04982
[2023-08-07 19:59:11,635 INFO misc.py line 115 22900] Train: [9/100][100/156] Data 0.001 (0.001) Batch 3.654 (3.421) Remain 13:32:31 loss: 0.9868 Lr: 0.04982
[2023-08-07 19:59:14,780 INFO misc.py line 115 22900] Train: [9/100][101/156] Data 0.001 (0.001) Batch 3.145 (3.418) Remain 13:31:48 loss: 0.5017 Lr: 0.04982
[2023-08-07 19:59:18,932 INFO misc.py line 115 22900] Train: [9/100][102/156] Data 0.001 (0.001) Batch 4.152 (3.425) Remain 13:33:30 loss: 0.9314 Lr: 0.04982
[2023-08-07 19:59:22,414 INFO misc.py line 115 22900] Train: [9/100][103/156] Data 0.001 (0.001) Batch 3.482 (3.426) Remain 13:33:35 loss: 0.7321 Lr: 0.04982
[2023-08-07 19:59:26,436 INFO misc.py line 115 22900] Train: [9/100][104/156] Data 0.001 (0.001) Batch 4.022 (3.432) Remain 13:34:55 loss: 1.1048 Lr: 0.04982
[2023-08-07 19:59:29,409 INFO misc.py line 115 22900] Train: [9/100][105/156] Data 0.001 (0.001) Batch 2.973 (3.427) Remain 13:33:48 loss: 0.6349 Lr: 0.04982
[2023-08-07 19:59:33,208 INFO misc.py line 115 22900] Train: [9/100][106/156] Data 0.001 (0.001) Batch 3.799 (3.431) Remain 13:34:36 loss: 0.8735 Lr: 0.04981
[2023-08-07 19:59:37,257 INFO misc.py line 115 22900] Train: [9/100][107/156] Data 0.001 (0.001) Batch 4.049 (3.437) Remain 13:35:57 loss: 0.7759 Lr: 0.04981
[2023-08-07 19:59:40,457 INFO misc.py line 115 22900] Train: [9/100][108/156] Data 0.001 (0.001) Batch 3.200 (3.435) Remain 13:35:21 loss: 0.9716 Lr: 0.04981
[2023-08-07 19:59:43,091 INFO misc.py line 115 22900] Train: [9/100][109/156] Data 0.001 (0.001) Batch 2.634 (3.427) Remain 13:33:30 loss: 0.7389 Lr: 0.04981
[2023-08-07 19:59:46,424 INFO misc.py line 115 22900] Train: [9/100][110/156] Data 0.001 (0.001) Batch 3.333 (3.426) Remain 13:33:15 loss: 1.0423 Lr: 0.04981
[2023-08-07 19:59:49,486 INFO misc.py line 115 22900] Train: [9/100][111/156] Data 0.001 (0.001) Batch 3.062 (3.423) Remain 13:32:23 loss: 0.8997 Lr: 0.04981
[2023-08-07 19:59:52,458 INFO misc.py line 115 22900] Train: [9/100][112/156] Data 0.001 (0.001) Batch 2.972 (3.419) Remain 13:31:21 loss: 1.5437 Lr: 0.04981
[2023-08-07 19:59:55,583 INFO misc.py line 115 22900] Train: [9/100][113/156] Data 0.001 (0.001) Batch 3.125 (3.416) Remain 13:30:39 loss: 0.9439 Lr: 0.04981
[2023-08-07 19:59:58,715 INFO misc.py line 115 22900] Train: [9/100][114/156] Data 0.001 (0.001) Batch 3.132 (3.413) Remain 13:30:00 loss: 0.8494 Lr: 0.04981
[2023-08-07 20:00:02,234 INFO misc.py line 115 22900] Train: [9/100][115/156] Data 0.001 (0.001) Batch 3.519 (3.414) Remain 13:30:09 loss: 0.8848 Lr: 0.04981
[2023-08-07 20:00:05,102 INFO misc.py line 115 22900] Train: [9/100][116/156] Data 0.001 (0.001) Batch 2.868 (3.410) Remain 13:28:57 loss: 0.7911 Lr: 0.04981
[2023-08-07 20:00:07,724 INFO misc.py line 115 22900] Train: [9/100][117/156] Data 0.001 (0.001) Batch 2.622 (3.403) Remain 13:27:16 loss: 0.6120 Lr: 0.04981
[2023-08-07 20:00:11,713 INFO misc.py line 115 22900] Train: [9/100][118/156] Data 0.001 (0.001) Batch 3.988 (3.408) Remain 13:28:25 loss: 1.2484 Lr: 0.04981
[2023-08-07 20:00:14,417 INFO misc.py line 115 22900] Train: [9/100][119/156] Data 0.001 (0.001) Batch 2.704 (3.402) Remain 13:26:55 loss: 0.6508 Lr: 0.04981
[2023-08-07 20:00:18,207 INFO misc.py line 115 22900] Train: [9/100][120/156] Data 0.001 (0.001) Batch 3.790 (3.405) Remain 13:27:39 loss: 0.7465 Lr: 0.04981
[2023-08-07 20:00:22,224 INFO misc.py line 115 22900] Train: [9/100][121/156] Data 0.001 (0.001) Batch 4.017 (3.410) Remain 13:28:49 loss: 0.8286 Lr: 0.04980
[2023-08-07 20:00:25,975 INFO misc.py line 115 22900] Train: [9/100][122/156] Data 0.001 (0.001) Batch 3.751 (3.413) Remain 13:29:27 loss: 1.7823 Lr: 0.04980
[2023-08-07 20:00:28,990 INFO misc.py line 115 22900] Train: [9/100][123/156] Data 0.001 (0.001) Batch 3.015 (3.410) Remain 13:28:36 loss: 0.6523 Lr: 0.04980
[2023-08-07 20:00:33,005 INFO misc.py line 115 22900] Train: [9/100][124/156] Data 0.001 (0.001) Batch 4.016 (3.415) Remain 13:29:44 loss: 1.1105 Lr: 0.04980
[2023-08-07 20:00:37,062 INFO misc.py line 115 22900] Train: [9/100][125/156] Data 0.001 (0.001) Batch 4.056 (3.420) Remain 13:30:55 loss: 0.9094 Lr: 0.04980
[2023-08-07 20:00:41,194 INFO misc.py line 115 22900] Train: [9/100][126/156] Data 0.001 (0.001) Batch 4.132 (3.426) Remain 13:32:14 loss: 1.0713 Lr: 0.04980
[2023-08-07 20:00:43,439 INFO misc.py line 115 22900] Train: [9/100][127/156] Data 0.001 (0.001) Batch 2.245 (3.416) Remain 13:29:55 loss: 0.7700 Lr: 0.04980
[2023-08-07 20:00:48,014 INFO misc.py line 115 22900] Train: [9/100][128/156] Data 0.001 (0.001) Batch 4.576 (3.425) Remain 13:32:04 loss: 1.2869 Lr: 0.04980
[2023-08-07 20:00:51,471 INFO misc.py line 115 22900] Train: [9/100][129/156] Data 0.001 (0.001) Batch 3.456 (3.426) Remain 13:32:04 loss: 0.6842 Lr: 0.04980
[2023-08-07 20:00:55,199 INFO misc.py line 115 22900] Train: [9/100][130/156] Data 0.001 (0.001) Batch 3.728 (3.428) Remain 13:32:34 loss: 0.7098 Lr: 0.04980
[2023-08-07 20:00:58,915 INFO misc.py line 115 22900] Train: [9/100][131/156] Data 0.001 (0.001) Batch 3.717 (3.430) Remain 13:33:03 loss: 0.9414 Lr: 0.04980
[2023-08-07 20:01:02,242 INFO misc.py line 115 22900] Train: [9/100][132/156] Data 0.001 (0.001) Batch 3.327 (3.430) Remain 13:32:48 loss: 0.9387 Lr: 0.04980
[2023-08-07 20:01:06,162 INFO misc.py line 115 22900] Train: [9/100][133/156] Data 0.001 (0.001) Batch 3.920 (3.433) Remain 13:33:38 loss: 1.0228 Lr: 0.04980
[2023-08-07 20:01:09,165 INFO misc.py line 115 22900] Train: [9/100][134/156] Data 0.001 (0.001) Batch 3.004 (3.430) Remain 13:32:48 loss: 0.8411 Lr: 0.04980
[2023-08-07 20:01:13,282 INFO misc.py line 115 22900] Train: [9/100][135/156] Data 0.001 (0.001) Batch 4.117 (3.435) Remain 13:33:59 loss: 0.8648 Lr: 0.04980
[2023-08-07 20:01:17,227 INFO misc.py line 115 22900] Train: [9/100][136/156] Data 0.001 (0.001) Batch 3.945 (3.439) Remain 13:34:50 loss: 0.9275 Lr: 0.04979
[2023-08-07 20:01:21,229 INFO misc.py line 115 22900] Train: [9/100][137/156] Data 0.001 (0.001) Batch 4.002 (3.443) Remain 13:35:46 loss: 0.9892 Lr: 0.04979
[2023-08-07 20:01:24,656 INFO misc.py line 115 22900] Train: [9/100][138/156] Data 0.001 (0.001) Batch 3.427 (3.443) Remain 13:35:41 loss: 1.0303 Lr: 0.04979
[2023-08-07 20:01:28,239 INFO misc.py line 115 22900] Train: [9/100][139/156] Data 0.001 (0.001) Batch 3.582 (3.444) Remain 13:35:52 loss: 0.8045 Lr: 0.04979
[2023-08-07 20:01:31,858 INFO misc.py line 115 22900] Train: [9/100][140/156] Data 0.001 (0.001) Batch 3.620 (3.445) Remain 13:36:07 loss: 0.9570 Lr: 0.04979
[2023-08-07 20:01:35,363 INFO misc.py line 115 22900] Train: [9/100][141/156] Data 0.001 (0.001) Batch 3.504 (3.446) Remain 13:36:09 loss: 0.9996 Lr: 0.04979
[2023-08-07 20:01:40,130 INFO misc.py line 115 22900] Train: [9/100][142/156] Data 0.001 (0.001) Batch 4.768 (3.455) Remain 13:38:21 loss: 0.9830 Lr: 0.04979
[2023-08-07 20:01:42,263 INFO misc.py line 115 22900] Train: [9/100][143/156] Data 0.001 (0.001) Batch 2.133 (3.446) Remain 13:36:03 loss: 0.7319 Lr: 0.04979
[2023-08-07 20:01:46,288 INFO misc.py line 115 22900] Train: [9/100][144/156] Data 0.001 (0.001) Batch 4.025 (3.450) Remain 13:36:58 loss: 1.3167 Lr: 0.04979
[2023-08-07 20:01:49,269 INFO misc.py line 115 22900] Train: [9/100][145/156] Data 0.001 (0.001) Batch 2.981 (3.447) Remain 13:36:08 loss: 0.5812 Lr: 0.04979
[2023-08-07 20:01:53,143 INFO misc.py line 115 22900] Train: [9/100][146/156] Data 0.001 (0.001) Batch 3.874 (3.450) Remain 13:36:47 loss: 0.8780 Lr: 0.04979
[2023-08-07 20:01:57,128 INFO misc.py line 115 22900] Train: [9/100][147/156] Data 0.001 (0.001) Batch 3.985 (3.453) Remain 13:37:36 loss: 0.8915 Lr: 0.04979
[2023-08-07 20:02:01,212 INFO misc.py line 115 22900] Train: [9/100][148/156] Data 0.001 (0.001) Batch 4.084 (3.458) Remain 13:38:34 loss: 1.5335 Lr: 0.04979
[2023-08-07 20:02:04,634 INFO misc.py line 115 22900] Train: [9/100][149/156] Data 0.001 (0.001) Batch 3.422 (3.458) Remain 13:38:27 loss: 1.0278 Lr: 0.04979
[2023-08-07 20:02:08,047 INFO misc.py line 115 22900] Train: [9/100][150/156] Data 0.001 (0.001) Batch 3.413 (3.457) Remain 13:38:20 loss: 0.7555 Lr: 0.04979
[2023-08-07 20:02:11,357 INFO misc.py line 115 22900] Train: [9/100][151/156] Data 0.001 (0.001) Batch 3.310 (3.456) Remain 13:38:02 loss: 0.8034 Lr: 0.04978
[2023-08-07 20:02:15,239 INFO misc.py line 115 22900] Train: [9/100][152/156] Data 0.001 (0.001) Batch 3.882 (3.459) Remain 13:38:39 loss: 1.2968 Lr: 0.04978
[2023-08-07 20:02:18,477 INFO misc.py line 115 22900] Train: [9/100][153/156] Data 0.001 (0.001) Batch 3.238 (3.458) Remain 13:38:15 loss: 0.8197 Lr: 0.04978
[2023-08-07 20:02:21,949 INFO misc.py line 115 22900] Train: [9/100][154/156] Data 0.001 (0.001) Batch 3.472 (3.458) Remain 13:38:13 loss: 0.9398 Lr: 0.04978
[2023-08-07 20:02:25,584 INFO misc.py line 115 22900] Train: [9/100][155/156] Data 0.001 (0.001) Batch 3.635 (3.459) Remain 13:38:26 loss: 1.3650 Lr: 0.04978
[2023-08-07 20:02:28,460 INFO misc.py line 115 22900] Train: [9/100][156/156] Data 0.001 (0.001) Batch 2.876 (3.455) Remain 13:37:28 loss: 0.7919 Lr: 0.04978
[2023-08-07 20:02:28,461 INFO misc.py line 129 22900] Train result: loss: 0.9305 
[2023-08-07 20:02:28,461 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 20:02:30,550 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.7998 
[2023-08-07 20:02:31,418 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.8355 
[2023-08-07 20:02:33,083 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.8815 
[2023-08-07 20:02:34,602 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.1919 
[2023-08-07 20:02:36,446 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.6956 
[2023-08-07 20:02:38,110 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.9055 
[2023-08-07 20:02:40,245 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.4079 
[2023-08-07 20:02:42,048 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.8123 
[2023-08-07 20:02:43,332 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.3517 
[2023-08-07 20:02:45,461 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3424 
[2023-08-07 20:02:45,987 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.5009 
[2023-08-07 20:02:47,518 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.7531 
[2023-08-07 20:02:50,229 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.1540 
[2023-08-07 20:02:51,909 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.9812 
[2023-08-07 20:02:53,929 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.5800 
[2023-08-07 20:02:56,642 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.2707 
[2023-08-07 20:02:59,348 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.2682 
[2023-08-07 20:03:01,195 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.6325 
[2023-08-07 20:03:01,946 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.1546 
[2023-08-07 20:03:02,830 INFO evaluator.py line 122 22900] Test: [20/24] Loss 1.1866 
[2023-08-07 20:03:05,090 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.1878 
[2023-08-07 20:03:07,054 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.8271 
[2023-08-07 20:03:08,902 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.2290 
[2023-08-07 20:03:10,837 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.7902 
[2023-08-07 20:03:10,886 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.1490/0.2226/0.6245.
[2023-08-07 20:03:10,886 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6398/0.9465
[2023-08-07 20:03:10,886 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9398/0.9850
[2023-08-07 20:03:10,886 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.0741/0.2284
[2023-08-07 20:03:10,886 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.0241/0.0375
[2023-08-07 20:03:10,886 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.4764/0.5853
[2023-08-07 20:03:10,886 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.1504/0.1806
[2023-08-07 20:03:10,886 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.3015/0.4926
[2023-08-07 20:03:10,886 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0126/0.0127
[2023-08-07 20:03:10,886 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.0948/0.2742
[2023-08-07 20:03:10,886 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0147/0.0149
[2023-08-07 20:03:10,886 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:03:10,887 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:03:10,887 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1195/0.3982
[2023-08-07 20:03:10,887 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:03:10,887 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:03:10,887 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:03:10,887 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:03:10,887 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.0762/0.1540
[2023-08-07 20:03:10,887 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:03:10,887 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0565/0.1421
[2023-08-07 20:03:10,887 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 20:03:10,887 INFO misc.py line 152 22900] Currently Best mIoU: 0.1503
[2023-08-07 20:03:10,887 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 20:03:14,675 INFO misc.py line 115 22900] Train: [10/100][1/156] Data 0.685 (0.685) Batch 2.988 (2.988) Remain 11:47:00 loss: 0.6114 Lr: 0.04978
[2023-08-07 20:03:19,323 INFO misc.py line 115 22900] Train: [10/100][2/156] Data 0.001 (0.001) Batch 4.648 (4.648) Remain 18:19:37 loss: 1.3277 Lr: 0.04978
[2023-08-07 20:03:21,971 INFO misc.py line 115 22900] Train: [10/100][3/156] Data 0.001 (0.001) Batch 2.648 (2.648) Remain 10:26:28 loss: 0.5190 Lr: 0.04978
[2023-08-07 20:03:25,119 INFO misc.py line 115 22900] Train: [10/100][4/156] Data 0.001 (0.001) Batch 3.148 (3.148) Remain 12:24:35 loss: 0.7353 Lr: 0.04978
[2023-08-07 20:03:29,070 INFO misc.py line 115 22900] Train: [10/100][5/156] Data 0.001 (0.001) Batch 3.950 (3.549) Remain 13:59:26 loss: 1.2692 Lr: 0.04978
[2023-08-07 20:03:33,613 INFO misc.py line 115 22900] Train: [10/100][6/156] Data 0.001 (0.001) Batch 4.544 (3.881) Remain 15:17:46 loss: 1.1674 Lr: 0.04978
[2023-08-07 20:03:37,180 INFO misc.py line 115 22900] Train: [10/100][7/156] Data 0.001 (0.001) Batch 3.567 (3.802) Remain 14:59:09 loss: 0.8915 Lr: 0.04978
[2023-08-07 20:03:40,163 INFO misc.py line 115 22900] Train: [10/100][8/156] Data 0.001 (0.001) Batch 2.983 (3.638) Remain 14:20:20 loss: 0.8112 Lr: 0.04978
[2023-08-07 20:03:43,484 INFO misc.py line 115 22900] Train: [10/100][9/156] Data 0.001 (0.001) Batch 3.321 (3.585) Remain 14:07:45 loss: 1.1094 Lr: 0.04977
[2023-08-07 20:03:46,715 INFO misc.py line 115 22900] Train: [10/100][10/156] Data 0.001 (0.001) Batch 3.231 (3.535) Remain 13:55:43 loss: 0.8970 Lr: 0.04977
[2023-08-07 20:03:50,695 INFO misc.py line 115 22900] Train: [10/100][11/156] Data 0.001 (0.001) Batch 3.980 (3.590) Remain 14:08:50 loss: 0.9330 Lr: 0.04977
[2023-08-07 20:03:54,406 INFO misc.py line 115 22900] Train: [10/100][12/156] Data 0.001 (0.001) Batch 3.711 (3.604) Remain 14:11:56 loss: 0.8039 Lr: 0.04977
[2023-08-07 20:03:57,079 INFO misc.py line 115 22900] Train: [10/100][13/156] Data 0.001 (0.001) Batch 2.674 (3.511) Remain 13:49:53 loss: 0.7787 Lr: 0.04977
[2023-08-07 20:04:00,624 INFO misc.py line 115 22900] Train: [10/100][14/156] Data 0.001 (0.001) Batch 3.545 (3.514) Remain 13:50:33 loss: 0.9823 Lr: 0.04977
[2023-08-07 20:04:03,395 INFO misc.py line 115 22900] Train: [10/100][15/156] Data 0.001 (0.001) Batch 2.771 (3.452) Remain 13:35:52 loss: 0.6545 Lr: 0.04977
[2023-08-07 20:04:06,434 INFO misc.py line 115 22900] Train: [10/100][16/156] Data 0.001 (0.001) Batch 3.039 (3.420) Remain 13:28:18 loss: 0.6540 Lr: 0.04977
[2023-08-07 20:04:10,524 INFO misc.py line 115 22900] Train: [10/100][17/156] Data 0.001 (0.001) Batch 4.090 (3.468) Remain 13:39:33 loss: 0.8015 Lr: 0.04977
[2023-08-07 20:04:14,535 INFO misc.py line 115 22900] Train: [10/100][18/156] Data 0.001 (0.001) Batch 4.011 (3.504) Remain 13:48:03 loss: 1.4059 Lr: 0.04977
[2023-08-07 20:04:18,168 INFO misc.py line 115 22900] Train: [10/100][19/156] Data 0.001 (0.001) Batch 3.633 (3.512) Remain 13:49:53 loss: 0.7585 Lr: 0.04977
[2023-08-07 20:04:21,327 INFO misc.py line 115 22900] Train: [10/100][20/156] Data 0.001 (0.001) Batch 3.159 (3.491) Remain 13:44:55 loss: 0.6088 Lr: 0.04977
[2023-08-07 20:04:25,004 INFO misc.py line 115 22900] Train: [10/100][21/156] Data 0.001 (0.001) Batch 3.677 (3.502) Remain 13:47:17 loss: 0.6489 Lr: 0.04977
[2023-08-07 20:04:28,375 INFO misc.py line 115 22900] Train: [10/100][22/156] Data 0.001 (0.001) Batch 3.371 (3.495) Remain 13:45:36 loss: 0.9817 Lr: 0.04977
[2023-08-07 20:04:32,294 INFO misc.py line 115 22900] Train: [10/100][23/156] Data 0.001 (0.001) Batch 3.919 (3.516) Remain 13:50:33 loss: 1.0159 Lr: 0.04976
[2023-08-07 20:04:35,919 INFO misc.py line 115 22900] Train: [10/100][24/156] Data 0.001 (0.001) Batch 3.625 (3.521) Remain 13:51:44 loss: 1.4632 Lr: 0.04976
[2023-08-07 20:04:38,522 INFO misc.py line 115 22900] Train: [10/100][25/156] Data 0.001 (0.001) Batch 2.603 (3.480) Remain 13:41:48 loss: 0.6158 Lr: 0.04976
[2023-08-07 20:04:43,099 INFO misc.py line 115 22900] Train: [10/100][26/156] Data 0.001 (0.001) Batch 4.577 (3.527) Remain 13:53:01 loss: 1.3378 Lr: 0.04976
[2023-08-07 20:04:46,824 INFO misc.py line 115 22900] Train: [10/100][27/156] Data 0.001 (0.001) Batch 3.725 (3.536) Remain 13:54:54 loss: 0.7689 Lr: 0.04976
[2023-08-07 20:04:49,897 INFO misc.py line 115 22900] Train: [10/100][28/156] Data 0.001 (0.001) Batch 3.073 (3.517) Remain 13:50:29 loss: 1.0085 Lr: 0.04976
[2023-08-07 20:04:53,060 INFO misc.py line 115 22900] Train: [10/100][29/156] Data 0.001 (0.001) Batch 3.163 (3.503) Remain 13:47:12 loss: 0.5764 Lr: 0.04976
[2023-08-07 20:04:54,819 INFO misc.py line 115 22900] Train: [10/100][30/156] Data 0.001 (0.001) Batch 1.758 (3.439) Remain 13:31:53 loss: 1.0372 Lr: 0.04976
[2023-08-07 20:04:57,262 INFO misc.py line 115 22900] Train: [10/100][31/156] Data 0.001 (0.001) Batch 2.443 (3.403) Remain 13:23:26 loss: 0.4330 Lr: 0.04976
[2023-08-07 20:05:01,233 INFO misc.py line 115 22900] Train: [10/100][32/156] Data 0.001 (0.001) Batch 3.971 (3.423) Remain 13:28:00 loss: 1.1076 Lr: 0.04976
[2023-08-07 20:05:04,472 INFO misc.py line 115 22900] Train: [10/100][33/156] Data 0.001 (0.001) Batch 3.239 (3.417) Remain 13:26:30 loss: 0.9985 Lr: 0.04976
[2023-08-07 20:05:07,943 INFO misc.py line 115 22900] Train: [10/100][34/156] Data 0.001 (0.001) Batch 3.471 (3.418) Remain 13:26:51 loss: 1.0196 Lr: 0.04976
[2023-08-07 20:05:10,511 INFO misc.py line 115 22900] Train: [10/100][35/156] Data 0.001 (0.001) Batch 2.569 (3.392) Remain 13:20:32 loss: 0.8185 Lr: 0.04976
[2023-08-07 20:05:13,908 INFO misc.py line 115 22900] Train: [10/100][36/156] Data 0.001 (0.001) Batch 3.397 (3.392) Remain 13:20:31 loss: 1.2297 Lr: 0.04975
[2023-08-07 20:05:17,944 INFO misc.py line 115 22900] Train: [10/100][37/156] Data 0.001 (0.001) Batch 4.035 (3.411) Remain 13:24:55 loss: 1.0444 Lr: 0.04975
[2023-08-07 20:05:20,986 INFO misc.py line 115 22900] Train: [10/100][38/156] Data 0.001 (0.001) Batch 3.042 (3.400) Remain 13:22:22 loss: 0.8439 Lr: 0.04975
[2023-08-07 20:05:23,167 INFO misc.py line 115 22900] Train: [10/100][39/156] Data 0.001 (0.001) Batch 2.182 (3.367) Remain 13:14:20 loss: 0.5539 Lr: 0.04975
[2023-08-07 20:05:26,502 INFO misc.py line 115 22900] Train: [10/100][40/156] Data 0.001 (0.001) Batch 3.335 (3.366) Remain 13:14:04 loss: 0.6532 Lr: 0.04975
[2023-08-07 20:05:30,046 INFO misc.py line 115 22900] Train: [10/100][41/156] Data 0.001 (0.001) Batch 3.544 (3.370) Remain 13:15:07 loss: 1.1353 Lr: 0.04975
[2023-08-07 20:05:33,363 INFO misc.py line 115 22900] Train: [10/100][42/156] Data 0.001 (0.001) Batch 3.316 (3.369) Remain 13:14:44 loss: 1.1888 Lr: 0.04975
[2023-08-07 20:05:37,290 INFO misc.py line 115 22900] Train: [10/100][43/156] Data 0.001 (0.001) Batch 3.928 (3.383) Remain 13:17:59 loss: 1.0297 Lr: 0.04975
[2023-08-07 20:05:40,577 INFO misc.py line 115 22900] Train: [10/100][44/156] Data 0.001 (0.001) Batch 3.287 (3.381) Remain 13:17:22 loss: 1.1848 Lr: 0.04975
[2023-08-07 20:05:44,533 INFO misc.py line 115 22900] Train: [10/100][45/156] Data 0.001 (0.001) Batch 3.956 (3.394) Remain 13:20:33 loss: 0.7662 Lr: 0.04975
[2023-08-07 20:05:47,846 INFO misc.py line 115 22900] Train: [10/100][46/156] Data 0.001 (0.001) Batch 3.313 (3.392) Remain 13:20:03 loss: 0.9913 Lr: 0.04975
[2023-08-07 20:05:51,752 INFO misc.py line 115 22900] Train: [10/100][47/156] Data 0.001 (0.001) Batch 3.905 (3.404) Remain 13:22:44 loss: 1.2263 Lr: 0.04975
[2023-08-07 20:05:54,990 INFO misc.py line 115 22900] Train: [10/100][48/156] Data 0.001 (0.001) Batch 3.239 (3.400) Remain 13:21:49 loss: 0.8110 Lr: 0.04975
[2023-08-07 20:05:57,881 INFO misc.py line 115 22900] Train: [10/100][49/156] Data 0.001 (0.001) Batch 2.890 (3.389) Remain 13:19:08 loss: 0.9701 Lr: 0.04975
[2023-08-07 20:06:00,499 INFO misc.py line 115 22900] Train: [10/100][50/156] Data 0.001 (0.001) Batch 2.618 (3.373) Remain 13:15:13 loss: 0.7574 Lr: 0.04974
[2023-08-07 20:06:04,430 INFO misc.py line 115 22900] Train: [10/100][51/156] Data 0.001 (0.001) Batch 3.931 (3.385) Remain 13:17:54 loss: 0.9383 Lr: 0.04974
[2023-08-07 20:06:08,493 INFO misc.py line 115 22900] Train: [10/100][52/156] Data 0.001 (0.001) Batch 4.063 (3.398) Remain 13:21:06 loss: 0.7280 Lr: 0.04974
[2023-08-07 20:06:12,572 INFO misc.py line 115 22900] Train: [10/100][53/156] Data 0.001 (0.001) Batch 4.079 (3.412) Remain 13:24:16 loss: 0.9451 Lr: 0.04974
[2023-08-07 20:06:16,117 INFO misc.py line 115 22900] Train: [10/100][54/156] Data 0.001 (0.001) Batch 3.545 (3.415) Remain 13:24:49 loss: 0.7282 Lr: 0.04974
[2023-08-07 20:06:19,716 INFO misc.py line 115 22900] Train: [10/100][55/156] Data 0.001 (0.001) Batch 3.599 (3.418) Remain 13:25:36 loss: 0.8698 Lr: 0.04974
[2023-08-07 20:06:23,190 INFO misc.py line 115 22900] Train: [10/100][56/156] Data 0.001 (0.001) Batch 3.474 (3.419) Remain 13:25:47 loss: 0.6862 Lr: 0.04974
[2023-08-07 20:06:26,131 INFO misc.py line 115 22900] Train: [10/100][57/156] Data 0.001 (0.001) Batch 2.942 (3.410) Remain 13:23:39 loss: 0.8748 Lr: 0.04974
[2023-08-07 20:06:30,163 INFO misc.py line 115 22900] Train: [10/100][58/156] Data 0.001 (0.001) Batch 4.032 (3.422) Remain 13:26:15 loss: 0.9119 Lr: 0.04974
[2023-08-07 20:06:32,879 INFO misc.py line 115 22900] Train: [10/100][59/156] Data 0.001 (0.001) Batch 2.716 (3.409) Remain 13:23:13 loss: 0.8431 Lr: 0.04974
[2023-08-07 20:06:36,119 INFO misc.py line 115 22900] Train: [10/100][60/156] Data 0.001 (0.001) Batch 3.240 (3.406) Remain 13:22:28 loss: 0.4311 Lr: 0.04974
[2023-08-07 20:06:40,469 INFO misc.py line 115 22900] Train: [10/100][61/156] Data 0.001 (0.001) Batch 4.350 (3.422) Remain 13:26:15 loss: 1.2454 Lr: 0.04974
[2023-08-07 20:06:43,849 INFO misc.py line 115 22900] Train: [10/100][62/156] Data 0.001 (0.001) Batch 3.380 (3.422) Remain 13:26:01 loss: 1.0072 Lr: 0.04974
[2023-08-07 20:06:48,002 INFO misc.py line 115 22900] Train: [10/100][63/156] Data 0.001 (0.001) Batch 4.153 (3.434) Remain 13:28:50 loss: 1.1216 Lr: 0.04973
[2023-08-07 20:06:51,610 INFO misc.py line 115 22900] Train: [10/100][64/156] Data 0.001 (0.001) Batch 3.608 (3.437) Remain 13:29:27 loss: 0.8895 Lr: 0.04973
[2023-08-07 20:06:55,771 INFO misc.py line 115 22900] Train: [10/100][65/156] Data 0.001 (0.001) Batch 4.161 (3.448) Remain 13:32:08 loss: 0.9317 Lr: 0.04973
[2023-08-07 20:06:58,958 INFO misc.py line 115 22900] Train: [10/100][66/156] Data 0.001 (0.001) Batch 3.187 (3.444) Remain 13:31:06 loss: 0.9177 Lr: 0.04973
[2023-08-07 20:07:02,873 INFO misc.py line 115 22900] Train: [10/100][67/156] Data 0.001 (0.001) Batch 3.915 (3.452) Remain 13:32:47 loss: 0.8142 Lr: 0.04973
[2023-08-07 20:07:05,539 INFO misc.py line 115 22900] Train: [10/100][68/156] Data 0.001 (0.001) Batch 2.667 (3.439) Remain 13:29:53 loss: 1.0158 Lr: 0.04973
[2023-08-07 20:07:09,637 INFO misc.py line 115 22900] Train: [10/100][69/156] Data 0.001 (0.001) Batch 4.098 (3.449) Remain 13:32:10 loss: 1.1912 Lr: 0.04973
[2023-08-07 20:07:13,050 INFO misc.py line 115 22900] Train: [10/100][70/156] Data 0.001 (0.001) Batch 3.412 (3.449) Remain 13:31:59 loss: 0.8164 Lr: 0.04973
[2023-08-07 20:07:16,637 INFO misc.py line 115 22900] Train: [10/100][71/156] Data 0.001 (0.001) Batch 3.587 (3.451) Remain 13:32:24 loss: 0.8963 Lr: 0.04973
[2023-08-07 20:07:20,740 INFO misc.py line 115 22900] Train: [10/100][72/156] Data 0.001 (0.001) Batch 4.103 (3.460) Remain 13:34:34 loss: 1.3140 Lr: 0.04973
[2023-08-07 20:07:24,551 INFO misc.py line 115 22900] Train: [10/100][73/156] Data 0.001 (0.001) Batch 3.812 (3.465) Remain 13:35:42 loss: 0.7667 Lr: 0.04973
[2023-08-07 20:07:27,387 INFO misc.py line 115 22900] Train: [10/100][74/156] Data 0.001 (0.001) Batch 2.836 (3.457) Remain 13:33:33 loss: 0.9278 Lr: 0.04973
[2023-08-07 20:07:31,072 INFO misc.py line 115 22900] Train: [10/100][75/156] Data 0.001 (0.001) Batch 3.685 (3.460) Remain 13:34:14 loss: 0.9691 Lr: 0.04973
[2023-08-07 20:07:35,201 INFO misc.py line 115 22900] Train: [10/100][76/156] Data 0.001 (0.001) Batch 4.129 (3.469) Remain 13:36:20 loss: 0.9863 Lr: 0.04972
[2023-08-07 20:07:38,553 INFO misc.py line 115 22900] Train: [10/100][77/156] Data 0.001 (0.001) Batch 3.351 (3.467) Remain 13:35:54 loss: 0.9397 Lr: 0.04972
[2023-08-07 20:07:41,822 INFO misc.py line 115 22900] Train: [10/100][78/156] Data 0.001 (0.001) Batch 3.269 (3.465) Remain 13:35:14 loss: 0.7859 Lr: 0.04972
[2023-08-07 20:07:44,298 INFO misc.py line 115 22900] Train: [10/100][79/156] Data 0.001 (0.001) Batch 2.477 (3.452) Remain 13:32:07 loss: 0.5830 Lr: 0.04972
[2023-08-07 20:07:47,881 INFO misc.py line 115 22900] Train: [10/100][80/156] Data 0.001 (0.001) Batch 3.583 (3.453) Remain 13:32:27 loss: 0.9400 Lr: 0.04972
[2023-08-07 20:07:51,449 INFO misc.py line 115 22900] Train: [10/100][81/156] Data 0.001 (0.001) Batch 3.568 (3.455) Remain 13:32:44 loss: 0.5617 Lr: 0.04972
[2023-08-07 20:07:55,273 INFO misc.py line 115 22900] Train: [10/100][82/156] Data 0.001 (0.001) Batch 3.825 (3.460) Remain 13:33:47 loss: 0.7789 Lr: 0.04972
[2023-08-07 20:07:59,478 INFO misc.py line 115 22900] Train: [10/100][83/156] Data 0.001 (0.001) Batch 4.205 (3.469) Remain 13:35:55 loss: 1.1587 Lr: 0.04972
[2023-08-07 20:08:03,983 INFO misc.py line 115 22900] Train: [10/100][84/156] Data 0.001 (0.001) Batch 4.505 (3.482) Remain 13:38:52 loss: 1.0470 Lr: 0.04972
[2023-08-07 20:08:06,984 INFO misc.py line 115 22900] Train: [10/100][85/156] Data 0.001 (0.001) Batch 3.001 (3.476) Remain 13:37:26 loss: 1.0385 Lr: 0.04972
[2023-08-07 20:08:10,803 INFO misc.py line 115 22900] Train: [10/100][86/156] Data 0.001 (0.001) Batch 3.819 (3.480) Remain 13:38:21 loss: 0.6866 Lr: 0.04972
[2023-08-07 20:08:13,808 INFO misc.py line 115 22900] Train: [10/100][87/156] Data 0.001 (0.001) Batch 3.005 (3.474) Remain 13:36:58 loss: 0.8599 Lr: 0.04972
[2023-08-07 20:08:17,221 INFO misc.py line 115 22900] Train: [10/100][88/156] Data 0.001 (0.001) Batch 3.413 (3.474) Remain 13:36:44 loss: 0.7636 Lr: 0.04971
[2023-08-07 20:08:20,595 INFO misc.py line 115 22900] Train: [10/100][89/156] Data 0.001 (0.001) Batch 3.374 (3.472) Remain 13:36:24 loss: 0.6720 Lr: 0.04971
[2023-08-07 20:08:24,558 INFO misc.py line 115 22900] Train: [10/100][90/156] Data 0.001 (0.001) Batch 3.963 (3.478) Remain 13:37:40 loss: 1.2305 Lr: 0.04971
[2023-08-07 20:08:29,144 INFO misc.py line 115 22900] Train: [10/100][91/156] Data 0.001 (0.001) Batch 4.587 (3.491) Remain 13:40:34 loss: 1.0932 Lr: 0.04971
[2023-08-07 20:08:32,700 INFO misc.py line 115 22900] Train: [10/100][92/156] Data 0.001 (0.001) Batch 3.556 (3.491) Remain 13:40:41 loss: 1.0785 Lr: 0.04971
[2023-08-07 20:08:35,297 INFO misc.py line 115 22900] Train: [10/100][93/156] Data 0.001 (0.001) Batch 2.597 (3.481) Remain 13:38:18 loss: 1.4578 Lr: 0.04971
[2023-08-07 20:08:38,792 INFO misc.py line 115 22900] Train: [10/100][94/156] Data 0.001 (0.001) Batch 3.495 (3.482) Remain 13:38:16 loss: 1.1792 Lr: 0.04971
[2023-08-07 20:08:43,071 INFO misc.py line 115 22900] Train: [10/100][95/156] Data 0.002 (0.001) Batch 4.279 (3.490) Remain 13:40:15 loss: 1.2031 Lr: 0.04971
[2023-08-07 20:08:47,104 INFO misc.py line 115 22900] Train: [10/100][96/156] Data 0.001 (0.001) Batch 4.033 (3.496) Remain 13:41:34 loss: 0.8554 Lr: 0.04971
[2023-08-07 20:08:49,927 INFO misc.py line 115 22900] Train: [10/100][97/156] Data 0.001 (0.001) Batch 2.823 (3.489) Remain 13:39:49 loss: 0.5790 Lr: 0.04971
[2023-08-07 20:08:52,795 INFO misc.py line 115 22900] Train: [10/100][98/156] Data 0.001 (0.001) Batch 2.868 (3.482) Remain 13:38:14 loss: 0.6513 Lr: 0.04971
[2023-08-07 20:08:55,623 INFO misc.py line 115 22900] Train: [10/100][99/156] Data 0.001 (0.001) Batch 2.828 (3.476) Remain 13:36:34 loss: 0.7283 Lr: 0.04971
[2023-08-07 20:08:57,799 INFO misc.py line 115 22900] Train: [10/100][100/156] Data 0.001 (0.001) Batch 2.176 (3.462) Remain 13:33:22 loss: 0.7314 Lr: 0.04971
[2023-08-07 20:09:00,944 INFO misc.py line 115 22900] Train: [10/100][101/156] Data 0.002 (0.001) Batch 3.145 (3.459) Remain 13:32:33 loss: 0.5904 Lr: 0.04970
[2023-08-07 20:09:03,947 INFO misc.py line 115 22900] Train: [10/100][102/156] Data 0.001 (0.001) Batch 3.004 (3.454) Remain 13:31:24 loss: 0.6117 Lr: 0.04970
[2023-08-07 20:09:07,044 INFO misc.py line 115 22900] Train: [10/100][103/156] Data 0.001 (0.001) Batch 3.096 (3.451) Remain 13:30:31 loss: 0.7856 Lr: 0.04970
[2023-08-07 20:09:11,055 INFO misc.py line 115 22900] Train: [10/100][104/156] Data 0.001 (0.001) Batch 4.012 (3.456) Remain 13:31:45 loss: 1.4674 Lr: 0.04970
[2023-08-07 20:09:14,322 INFO misc.py line 115 22900] Train: [10/100][105/156] Data 0.001 (0.001) Batch 3.266 (3.454) Remain 13:31:16 loss: 0.9896 Lr: 0.04970
[2023-08-07 20:09:17,158 INFO misc.py line 115 22900] Train: [10/100][106/156] Data 0.001 (0.001) Batch 2.836 (3.448) Remain 13:29:48 loss: 0.8308 Lr: 0.04970
[2023-08-07 20:09:21,100 INFO misc.py line 115 22900] Train: [10/100][107/156] Data 0.001 (0.001) Batch 3.942 (3.453) Remain 13:30:51 loss: 1.1183 Lr: 0.04970
[2023-08-07 20:09:24,769 INFO misc.py line 115 22900] Train: [10/100][108/156] Data 0.001 (0.001) Batch 3.670 (3.455) Remain 13:31:17 loss: 0.7100 Lr: 0.04970
[2023-08-07 20:09:28,137 INFO misc.py line 115 22900] Train: [10/100][109/156] Data 0.001 (0.001) Batch 3.367 (3.454) Remain 13:31:01 loss: 0.8775 Lr: 0.04970
[2023-08-07 20:09:30,994 INFO misc.py line 115 22900] Train: [10/100][110/156] Data 0.001 (0.001) Batch 2.858 (3.449) Remain 13:29:39 loss: 0.5133 Lr: 0.04970
[2023-08-07 20:09:34,998 INFO misc.py line 115 22900] Train: [10/100][111/156] Data 0.001 (0.001) Batch 4.004 (3.454) Remain 13:30:48 loss: 1.3840 Lr: 0.04970
[2023-08-07 20:09:38,492 INFO misc.py line 115 22900] Train: [10/100][112/156] Data 0.001 (0.001) Batch 3.494 (3.454) Remain 13:30:50 loss: 1.1110 Lr: 0.04970
[2023-08-07 20:09:42,443 INFO misc.py line 115 22900] Train: [10/100][113/156] Data 0.001 (0.001) Batch 3.951 (3.459) Remain 13:31:50 loss: 0.7546 Lr: 0.04969
[2023-08-07 20:09:45,733 INFO misc.py line 115 22900] Train: [10/100][114/156] Data 0.001 (0.001) Batch 3.290 (3.457) Remain 13:31:25 loss: 0.7886 Lr: 0.04969
[2023-08-07 20:09:49,321 INFO misc.py line 115 22900] Train: [10/100][115/156] Data 0.001 (0.001) Batch 3.587 (3.458) Remain 13:31:38 loss: 0.7965 Lr: 0.04969
[2023-08-07 20:09:52,781 INFO misc.py line 115 22900] Train: [10/100][116/156] Data 0.002 (0.001) Batch 3.461 (3.458) Remain 13:31:35 loss: 0.7068 Lr: 0.04969
[2023-08-07 20:09:56,834 INFO misc.py line 115 22900] Train: [10/100][117/156] Data 0.001 (0.001) Batch 4.053 (3.464) Remain 13:32:45 loss: 0.7032 Lr: 0.04969
[2023-08-07 20:09:59,998 INFO misc.py line 115 22900] Train: [10/100][118/156] Data 0.001 (0.001) Batch 3.164 (3.461) Remain 13:32:05 loss: 0.7718 Lr: 0.04969
[2023-08-07 20:10:04,036 INFO misc.py line 115 22900] Train: [10/100][119/156] Data 0.001 (0.001) Batch 4.038 (3.466) Remain 13:33:11 loss: 0.6239 Lr: 0.04969
[2023-08-07 20:10:07,711 INFO misc.py line 115 22900] Train: [10/100][120/156] Data 0.001 (0.001) Batch 3.675 (3.468) Remain 13:33:33 loss: 0.7807 Lr: 0.04969
[2023-08-07 20:10:10,129 INFO misc.py line 115 22900] Train: [10/100][121/156] Data 0.001 (0.001) Batch 2.418 (3.459) Remain 13:31:24 loss: 1.1626 Lr: 0.04969
[2023-08-07 20:10:13,588 INFO misc.py line 115 22900] Train: [10/100][122/156] Data 0.001 (0.001) Batch 3.459 (3.459) Remain 13:31:21 loss: 1.0377 Lr: 0.04969
[2023-08-07 20:10:16,444 INFO misc.py line 115 22900] Train: [10/100][123/156] Data 0.001 (0.001) Batch 2.856 (3.454) Remain 13:30:07 loss: 0.8087 Lr: 0.04969
[2023-08-07 20:10:20,469 INFO misc.py line 115 22900] Train: [10/100][124/156] Data 0.001 (0.001) Batch 4.024 (3.459) Remain 13:31:10 loss: 0.8312 Lr: 0.04969
[2023-08-07 20:10:24,563 INFO misc.py line 115 22900] Train: [10/100][125/156] Data 0.001 (0.001) Batch 4.094 (3.464) Remain 13:32:19 loss: 1.0265 Lr: 0.04968
[2023-08-07 20:10:28,475 INFO misc.py line 115 22900] Train: [10/100][126/156] Data 0.001 (0.001) Batch 3.912 (3.468) Remain 13:33:07 loss: 1.2506 Lr: 0.04968
[2023-08-07 20:10:32,262 INFO misc.py line 115 22900] Train: [10/100][127/156] Data 0.001 (0.001) Batch 3.787 (3.470) Remain 13:33:40 loss: 0.6912 Lr: 0.04968
[2023-08-07 20:10:35,599 INFO misc.py line 115 22900] Train: [10/100][128/156] Data 0.001 (0.001) Batch 3.338 (3.469) Remain 13:33:22 loss: 0.8432 Lr: 0.04968
[2023-08-07 20:10:38,769 INFO misc.py line 115 22900] Train: [10/100][129/156] Data 0.001 (0.001) Batch 3.170 (3.467) Remain 13:32:45 loss: 0.8664 Lr: 0.04968
[2023-08-07 20:10:43,169 INFO misc.py line 115 22900] Train: [10/100][130/156] Data 0.001 (0.001) Batch 4.399 (3.474) Remain 13:34:25 loss: 1.0904 Lr: 0.04968
[2023-08-07 20:10:46,248 INFO misc.py line 115 22900] Train: [10/100][131/156] Data 0.001 (0.001) Batch 3.080 (3.471) Remain 13:33:38 loss: 0.8640 Lr: 0.04968
[2023-08-07 20:10:49,586 INFO misc.py line 115 22900] Train: [10/100][132/156] Data 0.001 (0.001) Batch 3.338 (3.470) Remain 13:33:20 loss: 0.7301 Lr: 0.04968
[2023-08-07 20:10:53,383 INFO misc.py line 115 22900] Train: [10/100][133/156] Data 0.001 (0.001) Batch 3.797 (3.472) Remain 13:33:52 loss: 0.7792 Lr: 0.04968
[2023-08-07 20:10:56,552 INFO misc.py line 115 22900] Train: [10/100][134/156] Data 0.001 (0.001) Batch 3.169 (3.470) Remain 13:33:16 loss: 0.7129 Lr: 0.04968
[2023-08-07 20:11:00,421 INFO misc.py line 115 22900] Train: [10/100][135/156] Data 0.001 (0.001) Batch 3.869 (3.473) Remain 13:33:55 loss: 0.7746 Lr: 0.04968
[2023-08-07 20:11:03,426 INFO misc.py line 115 22900] Train: [10/100][136/156] Data 0.001 (0.001) Batch 3.005 (3.470) Remain 13:33:02 loss: 0.5947 Lr: 0.04968
[2023-08-07 20:11:07,139 INFO misc.py line 115 22900] Train: [10/100][137/156] Data 0.001 (0.001) Batch 3.713 (3.471) Remain 13:33:24 loss: 0.8734 Lr: 0.04967
[2023-08-07 20:11:11,175 INFO misc.py line 115 22900] Train: [10/100][138/156] Data 0.001 (0.001) Batch 4.036 (3.476) Remain 13:34:19 loss: 1.3108 Lr: 0.04967
[2023-08-07 20:11:14,606 INFO misc.py line 115 22900] Train: [10/100][139/156] Data 0.001 (0.001) Batch 3.431 (3.475) Remain 13:34:11 loss: 1.0389 Lr: 0.04967
[2023-08-07 20:11:17,975 INFO misc.py line 115 22900] Train: [10/100][140/156] Data 0.001 (0.001) Batch 3.370 (3.474) Remain 13:33:57 loss: 1.0391 Lr: 0.04967
[2023-08-07 20:11:20,606 INFO misc.py line 115 22900] Train: [10/100][141/156] Data 0.001 (0.001) Batch 2.631 (3.468) Remain 13:32:27 loss: 1.3150 Lr: 0.04967
[2023-08-07 20:11:23,205 INFO misc.py line 115 22900] Train: [10/100][142/156] Data 0.001 (0.001) Batch 2.598 (3.462) Remain 13:30:56 loss: 0.7426 Lr: 0.04967
[2023-08-07 20:11:27,188 INFO misc.py line 115 22900] Train: [10/100][143/156] Data 0.001 (0.001) Batch 3.984 (3.466) Remain 13:31:45 loss: 0.9101 Lr: 0.04967
[2023-08-07 20:11:30,435 INFO misc.py line 115 22900] Train: [10/100][144/156] Data 0.001 (0.001) Batch 3.247 (3.464) Remain 13:31:20 loss: 0.7844 Lr: 0.04967
[2023-08-07 20:11:33,688 INFO misc.py line 115 22900] Train: [10/100][145/156] Data 0.001 (0.001) Batch 3.253 (3.463) Remain 13:30:55 loss: 0.7013 Lr: 0.04967
[2023-08-07 20:11:37,605 INFO misc.py line 115 22900] Train: [10/100][146/156] Data 0.002 (0.001) Batch 3.917 (3.466) Remain 13:31:36 loss: 1.0447 Lr: 0.04967
[2023-08-07 20:11:41,715 INFO misc.py line 115 22900] Train: [10/100][147/156] Data 0.001 (0.001) Batch 4.110 (3.470) Remain 13:32:36 loss: 1.4090 Lr: 0.04967
[2023-08-07 20:11:45,134 INFO misc.py line 115 22900] Train: [10/100][148/156] Data 0.001 (0.001) Batch 3.419 (3.470) Remain 13:32:27 loss: 0.6278 Lr: 0.04967
[2023-08-07 20:11:49,260 INFO misc.py line 115 22900] Train: [10/100][149/156] Data 0.001 (0.001) Batch 4.127 (3.475) Remain 13:33:27 loss: 0.8450 Lr: 0.04966
[2023-08-07 20:11:52,862 INFO misc.py line 115 22900] Train: [10/100][150/156] Data 0.001 (0.001) Batch 3.601 (3.475) Remain 13:33:36 loss: 0.6525 Lr: 0.04966
[2023-08-07 20:11:55,721 INFO misc.py line 115 22900] Train: [10/100][151/156] Data 0.001 (0.001) Batch 2.859 (3.471) Remain 13:32:34 loss: 1.0124 Lr: 0.04966
[2023-08-07 20:11:59,739 INFO misc.py line 115 22900] Train: [10/100][152/156] Data 0.001 (0.001) Batch 4.018 (3.475) Remain 13:33:22 loss: 0.8194 Lr: 0.04966
[2023-08-07 20:12:02,310 INFO misc.py line 115 22900] Train: [10/100][153/156] Data 0.001 (0.001) Batch 2.571 (3.469) Remain 13:31:54 loss: 0.7312 Lr: 0.04966
[2023-08-07 20:12:05,713 INFO misc.py line 115 22900] Train: [10/100][154/156] Data 0.001 (0.001) Batch 3.403 (3.468) Remain 13:31:44 loss: 0.9930 Lr: 0.04966
[2023-08-07 20:12:09,276 INFO misc.py line 115 22900] Train: [10/100][155/156] Data 0.001 (0.001) Batch 3.563 (3.469) Remain 13:31:49 loss: 0.9587 Lr: 0.04966
[2023-08-07 20:12:13,530 INFO misc.py line 115 22900] Train: [10/100][156/156] Data 0.001 (0.001) Batch 4.253 (3.474) Remain 13:32:58 loss: 1.0292 Lr: 0.04966
[2023-08-07 20:12:13,530 INFO misc.py line 129 22900] Train result: loss: 0.9047 
[2023-08-07 20:12:13,530 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 20:12:15,656 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.6931 
[2023-08-07 20:12:16,526 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.8796 
[2023-08-07 20:12:18,192 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.8881 
[2023-08-07 20:12:19,713 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.2954 
[2023-08-07 20:12:21,559 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.5509 
[2023-08-07 20:12:23,223 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.7039 
[2023-08-07 20:12:25,362 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.4816 
[2023-08-07 20:12:27,173 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.6534 
[2023-08-07 20:12:28,458 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.2559 
[2023-08-07 20:12:30,590 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3454 
[2023-08-07 20:12:31,116 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.5425 
[2023-08-07 20:12:32,649 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.7979 
[2023-08-07 20:12:35,360 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.0631 
[2023-08-07 20:12:37,039 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.8637 
[2023-08-07 20:12:39,062 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.4952 
[2023-08-07 20:12:41,773 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.1635 
[2023-08-07 20:12:44,485 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.2075 
[2023-08-07 20:12:46,331 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.6539 
[2023-08-07 20:12:47,081 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.2414 
[2023-08-07 20:12:47,967 INFO evaluator.py line 122 22900] Test: [20/24] Loss 1.1075 
[2023-08-07 20:12:50,231 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.2614 
[2023-08-07 20:12:52,197 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.7191 
[2023-08-07 20:12:54,043 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.3225 
[2023-08-07 20:12:55,982 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.5553 
[2023-08-07 20:12:56,029 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.1556/0.2227/0.6385.
[2023-08-07 20:12:56,029 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6162/0.9393
[2023-08-07 20:12:56,029 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9390/0.9823
[2023-08-07 20:12:56,029 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.0840/0.2554
[2023-08-07 20:12:56,029 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.0351/0.0645
[2023-08-07 20:12:56,029 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.5189/0.6424
[2023-08-07 20:12:56,029 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.1935/0.2595
[2023-08-07 20:12:56,029 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.4119/0.7056
[2023-08-07 20:12:56,030 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0053/0.0053
[2023-08-07 20:12:56,030 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.0782/0.2056
[2023-08-07 20:12:56,030 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0129/0.0130
[2023-08-07 20:12:56,030 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:12:56,030 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:12:56,030 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.0170/0.0220
[2023-08-07 20:12:56,030 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0642/0.0853
[2023-08-07 20:12:56,030 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0015/0.0019
[2023-08-07 20:12:56,030 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:12:56,030 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:12:56,030 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.0742/0.1484
[2023-08-07 20:12:56,030 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:12:56,030 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0603/0.1227
[2023-08-07 20:12:56,030 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 20:12:56,030 INFO misc.py line 150 22900] Best validation mIoU updated to: 0.1556
[2023-08-07 20:12:56,030 INFO misc.py line 152 22900] Currently Best mIoU: 0.1556
[2023-08-07 20:12:56,030 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 20:13:01,092 INFO misc.py line 115 22900] Train: [11/100][1/156] Data 0.887 (0.887) Batch 4.001 (4.001) Remain 15:36:10 loss: 0.7904 Lr: 0.04966
[2023-08-07 20:13:04,522 INFO misc.py line 115 22900] Train: [11/100][2/156] Data 0.001 (0.001) Batch 3.431 (3.431) Remain 13:22:40 loss: 0.8416 Lr: 0.04966
[2023-08-07 20:13:07,728 INFO misc.py line 115 22900] Train: [11/100][3/156] Data 0.001 (0.001) Batch 3.206 (3.206) Remain 12:30:04 loss: 0.7274 Lr: 0.04966
[2023-08-07 20:13:10,974 INFO misc.py line 115 22900] Train: [11/100][4/156] Data 0.001 (0.001) Batch 3.246 (3.246) Remain 12:39:16 loss: 0.6885 Lr: 0.04965
[2023-08-07 20:13:15,013 INFO misc.py line 115 22900] Train: [11/100][5/156] Data 0.001 (0.001) Batch 4.039 (3.642) Remain 14:11:59 loss: 0.9622 Lr: 0.04965
[2023-08-07 20:13:17,145 INFO misc.py line 115 22900] Train: [11/100][6/156] Data 0.001 (0.001) Batch 2.132 (3.139) Remain 12:14:09 loss: 0.6408 Lr: 0.04965
[2023-08-07 20:13:20,724 INFO misc.py line 115 22900] Train: [11/100][7/156] Data 0.001 (0.001) Batch 3.579 (3.249) Remain 12:39:49 loss: 0.9139 Lr: 0.04965
[2023-08-07 20:13:24,715 INFO misc.py line 115 22900] Train: [11/100][8/156] Data 0.001 (0.001) Batch 3.991 (3.397) Remain 13:14:30 loss: 1.0260 Lr: 0.04965
[2023-08-07 20:13:28,733 INFO misc.py line 115 22900] Train: [11/100][9/156] Data 0.001 (0.001) Batch 4.018 (3.501) Remain 13:38:39 loss: 1.0937 Lr: 0.04965
[2023-08-07 20:13:32,446 INFO misc.py line 115 22900] Train: [11/100][10/156] Data 0.001 (0.001) Batch 3.713 (3.531) Remain 13:45:41 loss: 0.9771 Lr: 0.04965
[2023-08-07 20:13:35,536 INFO misc.py line 115 22900] Train: [11/100][11/156] Data 0.001 (0.001) Batch 3.089 (3.476) Remain 13:32:42 loss: 0.8554 Lr: 0.04965
[2023-08-07 20:13:37,989 INFO misc.py line 115 22900] Train: [11/100][12/156] Data 0.001 (0.001) Batch 2.453 (3.362) Remain 13:06:05 loss: 0.5614 Lr: 0.04965
[2023-08-07 20:13:40,745 INFO misc.py line 115 22900] Train: [11/100][13/156] Data 0.002 (0.001) Batch 2.756 (3.302) Remain 12:51:52 loss: 0.6819 Lr: 0.04965
[2023-08-07 20:13:43,879 INFO misc.py line 115 22900] Train: [11/100][14/156] Data 0.001 (0.001) Batch 3.134 (3.286) Remain 12:48:14 loss: 0.5337 Lr: 0.04965
[2023-08-07 20:13:46,397 INFO misc.py line 115 22900] Train: [11/100][15/156] Data 0.001 (0.001) Batch 2.518 (3.222) Remain 12:33:13 loss: 0.6796 Lr: 0.04964
[2023-08-07 20:13:50,181 INFO misc.py line 115 22900] Train: [11/100][16/156] Data 0.001 (0.001) Batch 3.784 (3.266) Remain 12:43:15 loss: 0.7409 Lr: 0.04964
[2023-08-07 20:13:54,339 INFO misc.py line 115 22900] Train: [11/100][17/156] Data 0.001 (0.001) Batch 4.159 (3.329) Remain 12:58:07 loss: 1.0638 Lr: 0.04964
[2023-08-07 20:13:59,195 INFO misc.py line 115 22900] Train: [11/100][18/156] Data 0.001 (0.001) Batch 4.856 (3.431) Remain 13:21:51 loss: 1.3708 Lr: 0.04964
[2023-08-07 20:14:02,903 INFO misc.py line 115 22900] Train: [11/100][19/156] Data 0.001 (0.001) Batch 3.707 (3.448) Remain 13:25:49 loss: 0.7720 Lr: 0.04964
[2023-08-07 20:14:06,749 INFO misc.py line 115 22900] Train: [11/100][20/156] Data 0.001 (0.001) Batch 3.847 (3.472) Remain 13:31:14 loss: 1.0887 Lr: 0.04964
[2023-08-07 20:14:10,687 INFO misc.py line 115 22900] Train: [11/100][21/156] Data 0.001 (0.001) Batch 3.938 (3.498) Remain 13:37:14 loss: 1.1787 Lr: 0.04964
[2023-08-07 20:14:14,444 INFO misc.py line 115 22900] Train: [11/100][22/156] Data 0.001 (0.001) Batch 3.757 (3.511) Remain 13:40:22 loss: 0.8989 Lr: 0.04964
[2023-08-07 20:14:17,666 INFO misc.py line 115 22900] Train: [11/100][23/156] Data 0.001 (0.001) Batch 3.222 (3.497) Remain 13:36:56 loss: 1.0067 Lr: 0.04964
[2023-08-07 20:14:22,183 INFO misc.py line 115 22900] Train: [11/100][24/156] Data 0.001 (0.001) Batch 4.516 (3.545) Remain 13:48:12 loss: 1.1147 Lr: 0.04964
[2023-08-07 20:14:25,564 INFO misc.py line 115 22900] Train: [11/100][25/156] Data 0.001 (0.001) Batch 3.381 (3.538) Remain 13:46:24 loss: 0.8660 Lr: 0.04964
[2023-08-07 20:14:28,937 INFO misc.py line 115 22900] Train: [11/100][26/156] Data 0.001 (0.001) Batch 3.373 (3.531) Remain 13:44:40 loss: 0.7686 Lr: 0.04964
[2023-08-07 20:14:31,988 INFO misc.py line 115 22900] Train: [11/100][27/156] Data 0.001 (0.001) Batch 3.051 (3.511) Remain 13:39:57 loss: 0.7376 Lr: 0.04963
[2023-08-07 20:14:35,811 INFO misc.py line 115 22900] Train: [11/100][28/156] Data 0.001 (0.001) Batch 3.823 (3.523) Remain 13:42:48 loss: 1.2822 Lr: 0.04963
[2023-08-07 20:14:39,186 INFO misc.py line 115 22900] Train: [11/100][29/156] Data 0.001 (0.001) Batch 3.375 (3.518) Remain 13:41:25 loss: 1.0276 Lr: 0.04963
[2023-08-07 20:14:42,020 INFO misc.py line 115 22900] Train: [11/100][30/156] Data 0.001 (0.001) Batch 2.833 (3.492) Remain 13:35:26 loss: 0.5554 Lr: 0.04963
[2023-08-07 20:14:45,906 INFO misc.py line 115 22900] Train: [11/100][31/156] Data 0.001 (0.001) Batch 3.887 (3.506) Remain 13:38:40 loss: 0.8825 Lr: 0.04963
[2023-08-07 20:14:48,699 INFO misc.py line 115 22900] Train: [11/100][32/156] Data 0.001 (0.001) Batch 2.793 (3.482) Remain 13:32:52 loss: 0.6268 Lr: 0.04963
[2023-08-07 20:14:51,243 INFO misc.py line 115 22900] Train: [11/100][33/156] Data 0.001 (0.001) Batch 2.544 (3.450) Remain 13:25:31 loss: 0.3947 Lr: 0.04963
[2023-08-07 20:14:54,491 INFO misc.py line 115 22900] Train: [11/100][34/156] Data 0.001 (0.001) Batch 3.248 (3.444) Remain 13:23:56 loss: 1.0184 Lr: 0.04963
[2023-08-07 20:14:58,216 INFO misc.py line 115 22900] Train: [11/100][35/156] Data 0.001 (0.001) Batch 3.725 (3.453) Remain 13:25:55 loss: 0.6208 Lr: 0.04963
[2023-08-07 20:15:02,634 INFO misc.py line 115 22900] Train: [11/100][36/156] Data 0.001 (0.001) Batch 4.418 (3.482) Remain 13:32:41 loss: 1.0780 Lr: 0.04963
[2023-08-07 20:15:06,747 INFO misc.py line 115 22900] Train: [11/100][37/156] Data 0.001 (0.001) Batch 4.113 (3.501) Remain 13:36:58 loss: 1.0545 Lr: 0.04963
[2023-08-07 20:15:11,017 INFO misc.py line 115 22900] Train: [11/100][38/156] Data 0.001 (0.001) Batch 4.269 (3.523) Remain 13:42:02 loss: 1.2670 Lr: 0.04962
[2023-08-07 20:15:14,036 INFO misc.py line 115 22900] Train: [11/100][39/156] Data 0.001 (0.001) Batch 3.019 (3.509) Remain 13:38:43 loss: 0.6385 Lr: 0.04962
[2023-08-07 20:15:17,243 INFO misc.py line 115 22900] Train: [11/100][40/156] Data 0.001 (0.001) Batch 3.207 (3.500) Remain 13:36:45 loss: 0.7295 Lr: 0.04962
[2023-08-07 20:15:19,771 INFO misc.py line 115 22900] Train: [11/100][41/156] Data 0.001 (0.001) Batch 2.528 (3.475) Remain 13:30:43 loss: 0.6513 Lr: 0.04962
[2023-08-07 20:15:23,209 INFO misc.py line 115 22900] Train: [11/100][42/156] Data 0.001 (0.001) Batch 3.438 (3.474) Remain 13:30:26 loss: 0.5913 Lr: 0.04962
[2023-08-07 20:15:25,790 INFO misc.py line 115 22900] Train: [11/100][43/156] Data 0.001 (0.001) Batch 2.581 (3.452) Remain 13:25:11 loss: 0.4617 Lr: 0.04962
[2023-08-07 20:15:29,783 INFO misc.py line 115 22900] Train: [11/100][44/156] Data 0.001 (0.001) Batch 3.993 (3.465) Remain 13:28:12 loss: 0.8363 Lr: 0.04962
[2023-08-07 20:15:33,772 INFO misc.py line 115 22900] Train: [11/100][45/156] Data 0.001 (0.001) Batch 3.989 (3.477) Remain 13:31:03 loss: 1.5038 Lr: 0.04962
[2023-08-07 20:15:37,379 INFO misc.py line 115 22900] Train: [11/100][46/156] Data 0.001 (0.001) Batch 3.606 (3.480) Remain 13:31:42 loss: 1.5609 Lr: 0.04962
[2023-08-07 20:15:40,442 INFO misc.py line 115 22900] Train: [11/100][47/156] Data 0.001 (0.001) Batch 3.063 (3.471) Remain 13:29:26 loss: 1.0027 Lr: 0.04962
[2023-08-07 20:15:44,604 INFO misc.py line 115 22900] Train: [11/100][48/156] Data 0.001 (0.001) Batch 4.162 (3.486) Remain 13:32:57 loss: 0.8051 Lr: 0.04961
[2023-08-07 20:15:47,768 INFO misc.py line 115 22900] Train: [11/100][49/156] Data 0.001 (0.001) Batch 3.164 (3.479) Remain 13:31:16 loss: 0.9098 Lr: 0.04961
[2023-08-07 20:15:52,173 INFO misc.py line 115 22900] Train: [11/100][50/156] Data 0.001 (0.001) Batch 4.405 (3.499) Remain 13:35:48 loss: 1.0901 Lr: 0.04961
[2023-08-07 20:15:56,280 INFO misc.py line 115 22900] Train: [11/100][51/156] Data 0.002 (0.001) Batch 4.107 (3.511) Remain 13:38:42 loss: 0.6590 Lr: 0.04961
[2023-08-07 20:16:00,179 INFO misc.py line 115 22900] Train: [11/100][52/156] Data 0.001 (0.001) Batch 3.899 (3.519) Remain 13:40:29 loss: 1.0216 Lr: 0.04961
[2023-08-07 20:16:03,359 INFO misc.py line 115 22900] Train: [11/100][53/156] Data 0.001 (0.001) Batch 3.180 (3.513) Remain 13:38:51 loss: 0.4985 Lr: 0.04961
[2023-08-07 20:16:05,926 INFO misc.py line 115 22900] Train: [11/100][54/156] Data 0.001 (0.001) Batch 2.566 (3.494) Remain 13:34:27 loss: 0.5896 Lr: 0.04961
[2023-08-07 20:16:09,608 INFO misc.py line 115 22900] Train: [11/100][55/156] Data 0.001 (0.001) Batch 3.682 (3.498) Remain 13:35:14 loss: 0.8290 Lr: 0.04961
[2023-08-07 20:16:12,727 INFO misc.py line 115 22900] Train: [11/100][56/156] Data 0.002 (0.001) Batch 3.119 (3.491) Remain 13:33:31 loss: 0.4928 Lr: 0.04961
[2023-08-07 20:16:16,436 INFO misc.py line 115 22900] Train: [11/100][57/156] Data 0.001 (0.001) Batch 3.710 (3.495) Remain 13:34:24 loss: 1.1133 Lr: 0.04961
[2023-08-07 20:16:20,849 INFO misc.py line 115 22900] Train: [11/100][58/156] Data 0.001 (0.001) Batch 4.413 (3.511) Remain 13:38:14 loss: 1.0671 Lr: 0.04961
[2023-08-07 20:16:24,182 INFO misc.py line 115 22900] Train: [11/100][59/156] Data 0.001 (0.001) Batch 3.332 (3.508) Remain 13:37:26 loss: 0.8631 Lr: 0.04960
[2023-08-07 20:16:27,541 INFO misc.py line 115 22900] Train: [11/100][60/156] Data 0.001 (0.001) Batch 3.359 (3.505) Remain 13:36:46 loss: 0.6521 Lr: 0.04960
[2023-08-07 20:16:31,622 INFO misc.py line 115 22900] Train: [11/100][61/156] Data 0.001 (0.001) Batch 4.082 (3.515) Remain 13:39:01 loss: 0.7931 Lr: 0.04960
[2023-08-07 20:16:34,941 INFO misc.py line 115 22900] Train: [11/100][62/156] Data 0.001 (0.001) Batch 3.318 (3.512) Remain 13:38:11 loss: 0.6729 Lr: 0.04960
[2023-08-07 20:16:37,096 INFO misc.py line 115 22900] Train: [11/100][63/156] Data 0.001 (0.001) Batch 2.155 (3.489) Remain 13:32:52 loss: 0.6838 Lr: 0.04960
[2023-08-07 20:16:40,877 INFO misc.py line 115 22900] Train: [11/100][64/156] Data 0.001 (0.001) Batch 3.781 (3.494) Remain 13:33:55 loss: 0.7001 Lr: 0.04960
[2023-08-07 20:16:44,623 INFO misc.py line 115 22900] Train: [11/100][65/156] Data 0.001 (0.001) Batch 3.746 (3.498) Remain 13:34:48 loss: 0.6962 Lr: 0.04960
[2023-08-07 20:16:49,151 INFO misc.py line 115 22900] Train: [11/100][66/156] Data 0.001 (0.001) Batch 4.528 (3.515) Remain 13:38:33 loss: 1.3450 Lr: 0.04960
[2023-08-07 20:16:52,423 INFO misc.py line 115 22900] Train: [11/100][67/156] Data 0.001 (0.001) Batch 3.272 (3.511) Remain 13:37:37 loss: 0.9882 Lr: 0.04960
[2023-08-07 20:16:54,568 INFO misc.py line 115 22900] Train: [11/100][68/156] Data 0.001 (0.001) Batch 2.146 (3.490) Remain 13:32:40 loss: 0.1829 Lr: 0.04960
[2023-08-07 20:16:58,002 INFO misc.py line 115 22900] Train: [11/100][69/156] Data 0.001 (0.001) Batch 3.434 (3.489) Remain 13:32:24 loss: 1.2068 Lr: 0.04960
[2023-08-07 20:17:01,669 INFO misc.py line 115 22900] Train: [11/100][70/156] Data 0.001 (0.001) Batch 3.666 (3.492) Remain 13:32:58 loss: 0.6844 Lr: 0.04959
[2023-08-07 20:17:06,122 INFO misc.py line 115 22900] Train: [11/100][71/156] Data 0.001 (0.001) Batch 4.453 (3.506) Remain 13:36:12 loss: 1.1995 Lr: 0.04959
[2023-08-07 20:17:09,735 INFO misc.py line 115 22900] Train: [11/100][72/156] Data 0.001 (0.001) Batch 3.613 (3.507) Remain 13:36:30 loss: 1.0626 Lr: 0.04959
[2023-08-07 20:17:12,772 INFO misc.py line 115 22900] Train: [11/100][73/156] Data 0.001 (0.001) Batch 3.038 (3.501) Remain 13:34:53 loss: 1.2236 Lr: 0.04959
[2023-08-07 20:17:15,949 INFO misc.py line 115 22900] Train: [11/100][74/156] Data 0.001 (0.001) Batch 3.177 (3.496) Remain 13:33:45 loss: 1.0946 Lr: 0.04959
[2023-08-07 20:17:19,440 INFO misc.py line 115 22900] Train: [11/100][75/156] Data 0.001 (0.001) Batch 3.491 (3.496) Remain 13:33:41 loss: 0.8628 Lr: 0.04959
[2023-08-07 20:17:22,795 INFO misc.py line 115 22900] Train: [11/100][76/156] Data 0.001 (0.001) Batch 3.356 (3.494) Remain 13:33:11 loss: 0.7374 Lr: 0.04959
[2023-08-07 20:17:25,471 INFO misc.py line 115 22900] Train: [11/100][77/156] Data 0.001 (0.001) Batch 2.676 (3.483) Remain 13:30:33 loss: 0.6169 Lr: 0.04959
[2023-08-07 20:17:28,988 INFO misc.py line 115 22900] Train: [11/100][78/156] Data 0.001 (0.001) Batch 3.517 (3.483) Remain 13:30:36 loss: 0.8028 Lr: 0.04959
[2023-08-07 20:17:32,332 INFO misc.py line 115 22900] Train: [11/100][79/156] Data 0.001 (0.001) Batch 3.344 (3.482) Remain 13:30:06 loss: 0.9244 Lr: 0.04959
[2023-08-07 20:17:36,811 INFO misc.py line 115 22900] Train: [11/100][80/156] Data 0.001 (0.001) Batch 4.479 (3.495) Remain 13:33:04 loss: 1.0400 Lr: 0.04958
[2023-08-07 20:17:40,090 INFO misc.py line 115 22900] Train: [11/100][81/156] Data 0.001 (0.001) Batch 3.279 (3.492) Remain 13:32:22 loss: 1.0424 Lr: 0.04958
[2023-08-07 20:17:42,211 INFO misc.py line 115 22900] Train: [11/100][82/156] Data 0.001 (0.001) Batch 2.121 (3.474) Remain 13:28:16 loss: 0.6990 Lr: 0.04958
[2023-08-07 20:17:45,940 INFO misc.py line 115 22900] Train: [11/100][83/156] Data 0.001 (0.001) Batch 3.730 (3.478) Remain 13:28:57 loss: 0.9141 Lr: 0.04958
[2023-08-07 20:17:50,024 INFO misc.py line 115 22900] Train: [11/100][84/156] Data 0.001 (0.001) Batch 4.084 (3.485) Remain 13:30:38 loss: 0.8417 Lr: 0.04958
[2023-08-07 20:17:52,892 INFO misc.py line 115 22900] Train: [11/100][85/156] Data 0.001 (0.001) Batch 2.868 (3.478) Remain 13:28:49 loss: 0.4984 Lr: 0.04958
[2023-08-07 20:17:56,350 INFO misc.py line 115 22900] Train: [11/100][86/156] Data 0.001 (0.001) Batch 3.458 (3.477) Remain 13:28:43 loss: 0.9314 Lr: 0.04958
[2023-08-07 20:18:00,284 INFO misc.py line 115 22900] Train: [11/100][87/156] Data 0.001 (0.001) Batch 3.934 (3.483) Remain 13:29:55 loss: 0.9379 Lr: 0.04958
[2023-08-07 20:18:04,727 INFO misc.py line 115 22900] Train: [11/100][88/156] Data 0.001 (0.001) Batch 4.443 (3.494) Remain 13:32:29 loss: 1.2202 Lr: 0.04958
[2023-08-07 20:18:07,076 INFO misc.py line 115 22900] Train: [11/100][89/156] Data 0.002 (0.001) Batch 2.349 (3.481) Remain 13:29:20 loss: 0.3553 Lr: 0.04958
[2023-08-07 20:18:11,125 INFO misc.py line 115 22900] Train: [11/100][90/156] Data 0.001 (0.001) Batch 4.048 (3.487) Remain 13:30:47 loss: 1.1605 Lr: 0.04958
[2023-08-07 20:18:14,420 INFO misc.py line 115 22900] Train: [11/100][91/156] Data 0.001 (0.001) Batch 3.296 (3.485) Remain 13:30:14 loss: 0.7327 Lr: 0.04957
[2023-08-07 20:18:18,627 INFO misc.py line 115 22900] Train: [11/100][92/156] Data 0.001 (0.001) Batch 4.206 (3.493) Remain 13:32:03 loss: 1.2310 Lr: 0.04957
[2023-08-07 20:18:22,478 INFO misc.py line 115 22900] Train: [11/100][93/156] Data 0.001 (0.001) Batch 3.851 (3.497) Remain 13:32:55 loss: 0.8697 Lr: 0.04957
[2023-08-07 20:18:26,492 INFO misc.py line 115 22900] Train: [11/100][94/156] Data 0.001 (0.001) Batch 4.015 (3.503) Remain 13:34:11 loss: 1.0348 Lr: 0.04957
[2023-08-07 20:18:29,799 INFO misc.py line 115 22900] Train: [11/100][95/156] Data 0.001 (0.001) Batch 3.307 (3.501) Remain 13:33:38 loss: 1.0821 Lr: 0.04957
[2023-08-07 20:18:32,840 INFO misc.py line 115 22900] Train: [11/100][96/156] Data 0.001 (0.001) Batch 3.040 (3.496) Remain 13:32:25 loss: 0.7864 Lr: 0.04957
[2023-08-07 20:18:36,209 INFO misc.py line 115 22900] Train: [11/100][97/156] Data 0.002 (0.001) Batch 3.370 (3.494) Remain 13:32:03 loss: 0.6284 Lr: 0.04957
[2023-08-07 20:18:40,317 INFO misc.py line 115 22900] Train: [11/100][98/156] Data 0.001 (0.001) Batch 4.107 (3.501) Remain 13:33:29 loss: 0.7950 Lr: 0.04957
[2023-08-07 20:18:45,034 INFO misc.py line 115 22900] Train: [11/100][99/156] Data 0.001 (0.001) Batch 4.716 (3.514) Remain 13:36:22 loss: 1.0503 Lr: 0.04957
[2023-08-07 20:18:49,131 INFO misc.py line 115 22900] Train: [11/100][100/156] Data 0.002 (0.001) Batch 4.098 (3.520) Remain 13:37:43 loss: 0.6610 Lr: 0.04957
[2023-08-07 20:18:51,956 INFO misc.py line 115 22900] Train: [11/100][101/156] Data 0.001 (0.001) Batch 2.825 (3.513) Remain 13:36:01 loss: 1.1475 Lr: 0.04956
[2023-08-07 20:18:54,993 INFO misc.py line 115 22900] Train: [11/100][102/156] Data 0.001 (0.001) Batch 3.037 (3.508) Remain 13:34:50 loss: 0.6116 Lr: 0.04956
[2023-08-07 20:18:57,643 INFO misc.py line 115 22900] Train: [11/100][103/156] Data 0.001 (0.001) Batch 2.650 (3.499) Remain 13:32:47 loss: 1.3005 Lr: 0.04956
[2023-08-07 20:19:00,912 INFO misc.py line 115 22900] Train: [11/100][104/156] Data 0.001 (0.001) Batch 3.269 (3.497) Remain 13:32:12 loss: 1.1025 Lr: 0.04956
[2023-08-07 20:19:03,556 INFO misc.py line 115 22900] Train: [11/100][105/156] Data 0.001 (0.001) Batch 2.643 (3.488) Remain 13:30:12 loss: 0.6535 Lr: 0.04956
[2023-08-07 20:19:06,643 INFO misc.py line 115 22900] Train: [11/100][106/156] Data 0.001 (0.001) Batch 3.088 (3.485) Remain 13:29:14 loss: 0.8590 Lr: 0.04956
[2023-08-07 20:19:10,148 INFO misc.py line 115 22900] Train: [11/100][107/156] Data 0.001 (0.001) Batch 3.504 (3.485) Remain 13:29:13 loss: 0.7792 Lr: 0.04956
[2023-08-07 20:19:14,170 INFO misc.py line 115 22900] Train: [11/100][108/156] Data 0.001 (0.001) Batch 4.022 (3.490) Remain 13:30:21 loss: 1.2643 Lr: 0.04956
[2023-08-07 20:19:17,045 INFO misc.py line 115 22900] Train: [11/100][109/156] Data 0.001 (0.001) Batch 2.876 (3.484) Remain 13:28:57 loss: 0.6251 Lr: 0.04956
[2023-08-07 20:19:21,091 INFO misc.py line 115 22900] Train: [11/100][110/156] Data 0.001 (0.001) Batch 4.046 (3.489) Remain 13:30:06 loss: 0.6480 Lr: 0.04956
[2023-08-07 20:19:25,114 INFO misc.py line 115 22900] Train: [11/100][111/156] Data 0.001 (0.001) Batch 4.022 (3.494) Remain 13:31:12 loss: 0.7792 Lr: 0.04955
[2023-08-07 20:19:28,377 INFO misc.py line 115 22900] Train: [11/100][112/156] Data 0.001 (0.001) Batch 3.263 (3.492) Remain 13:30:39 loss: 0.6680 Lr: 0.04955
[2023-08-07 20:19:32,449 INFO misc.py line 115 22900] Train: [11/100][113/156] Data 0.001 (0.001) Batch 4.073 (3.497) Remain 13:31:49 loss: 1.0673 Lr: 0.04955
[2023-08-07 20:19:35,595 INFO misc.py line 115 22900] Train: [11/100][114/156] Data 0.001 (0.001) Batch 3.146 (3.494) Remain 13:31:01 loss: 1.0063 Lr: 0.04955
[2023-08-07 20:19:39,244 INFO misc.py line 115 22900] Train: [11/100][115/156] Data 0.001 (0.001) Batch 3.649 (3.496) Remain 13:31:17 loss: 0.7827 Lr: 0.04955
[2023-08-07 20:19:43,283 INFO misc.py line 115 22900] Train: [11/100][116/156] Data 0.001 (0.001) Batch 4.038 (3.500) Remain 13:32:20 loss: 1.1912 Lr: 0.04955
[2023-08-07 20:19:47,301 INFO misc.py line 115 22900] Train: [11/100][117/156] Data 0.001 (0.001) Batch 4.019 (3.505) Remain 13:33:20 loss: 0.7987 Lr: 0.04955
[2023-08-07 20:19:50,482 INFO misc.py line 115 22900] Train: [11/100][118/156] Data 0.001 (0.001) Batch 3.181 (3.502) Remain 13:32:37 loss: 1.0463 Lr: 0.04955
[2023-08-07 20:19:53,860 INFO misc.py line 115 22900] Train: [11/100][119/156] Data 0.001 (0.001) Batch 3.377 (3.501) Remain 13:32:19 loss: 0.9464 Lr: 0.04955
[2023-08-07 20:19:57,227 INFO misc.py line 115 22900] Train: [11/100][120/156] Data 0.002 (0.001) Batch 3.367 (3.500) Remain 13:31:59 loss: 0.8348 Lr: 0.04955
[2023-08-07 20:20:00,160 INFO misc.py line 115 22900] Train: [11/100][121/156] Data 0.001 (0.001) Batch 2.933 (3.495) Remain 13:30:49 loss: 0.9560 Lr: 0.04954
[2023-08-07 20:20:03,490 INFO misc.py line 115 22900] Train: [11/100][122/156] Data 0.001 (0.001) Batch 3.330 (3.494) Remain 13:30:26 loss: 0.9312 Lr: 0.04954
[2023-08-07 20:20:06,208 INFO misc.py line 115 22900] Train: [11/100][123/156] Data 0.001 (0.001) Batch 2.718 (3.487) Remain 13:28:53 loss: 0.5193 Lr: 0.04954
[2023-08-07 20:20:09,689 INFO misc.py line 115 22900] Train: [11/100][124/156] Data 0.001 (0.001) Batch 3.481 (3.487) Remain 13:28:48 loss: 0.7643 Lr: 0.04954
[2023-08-07 20:20:14,104 INFO misc.py line 115 22900] Train: [11/100][125/156] Data 0.001 (0.001) Batch 4.415 (3.495) Remain 13:30:31 loss: 1.2132 Lr: 0.04954
[2023-08-07 20:20:17,414 INFO misc.py line 115 22900] Train: [11/100][126/156] Data 0.001 (0.001) Batch 3.310 (3.493) Remain 13:30:06 loss: 0.9071 Lr: 0.04954
[2023-08-07 20:20:20,552 INFO misc.py line 115 22900] Train: [11/100][127/156] Data 0.001 (0.001) Batch 3.138 (3.491) Remain 13:29:23 loss: 0.8447 Lr: 0.04954
[2023-08-07 20:20:23,768 INFO misc.py line 115 22900] Train: [11/100][128/156] Data 0.001 (0.001) Batch 3.217 (3.488) Remain 13:28:49 loss: 0.7648 Lr: 0.04954
[2023-08-07 20:20:26,386 INFO misc.py line 115 22900] Train: [11/100][129/156] Data 0.001 (0.001) Batch 2.618 (3.481) Remain 13:27:09 loss: 0.5272 Lr: 0.04954
[2023-08-07 20:20:30,363 INFO misc.py line 115 22900] Train: [11/100][130/156] Data 0.001 (0.001) Batch 3.976 (3.485) Remain 13:28:00 loss: 0.7671 Lr: 0.04954
[2023-08-07 20:20:32,999 INFO misc.py line 115 22900] Train: [11/100][131/156] Data 0.001 (0.001) Batch 2.636 (3.479) Remain 13:26:24 loss: 0.5548 Lr: 0.04953
[2023-08-07 20:20:37,460 INFO misc.py line 115 22900] Train: [11/100][132/156] Data 0.001 (0.001) Batch 4.462 (3.486) Remain 13:28:07 loss: 1.3155 Lr: 0.04953
[2023-08-07 20:20:40,560 INFO misc.py line 115 22900] Train: [11/100][133/156] Data 0.001 (0.001) Batch 3.100 (3.483) Remain 13:27:22 loss: 0.8855 Lr: 0.04953
[2023-08-07 20:20:44,535 INFO misc.py line 115 22900] Train: [11/100][134/156] Data 0.001 (0.001) Batch 3.975 (3.487) Remain 13:28:11 loss: 0.9462 Lr: 0.04953
[2023-08-07 20:20:47,999 INFO misc.py line 115 22900] Train: [11/100][135/156] Data 0.001 (0.001) Batch 3.464 (3.487) Remain 13:28:05 loss: 1.0012 Lr: 0.04953
[2023-08-07 20:20:50,615 INFO misc.py line 115 22900] Train: [11/100][136/156] Data 0.001 (0.001) Batch 2.616 (3.480) Remain 13:26:30 loss: 0.5076 Lr: 0.04953
[2023-08-07 20:20:54,006 INFO misc.py line 115 22900] Train: [11/100][137/156] Data 0.001 (0.001) Batch 3.391 (3.480) Remain 13:26:18 loss: 1.2296 Lr: 0.04953
[2023-08-07 20:20:58,103 INFO misc.py line 115 22900] Train: [11/100][138/156] Data 0.001 (0.001) Batch 4.097 (3.484) Remain 13:27:18 loss: 1.2333 Lr: 0.04953
[2023-08-07 20:21:01,071 INFO misc.py line 115 22900] Train: [11/100][139/156] Data 0.001 (0.001) Batch 2.968 (3.480) Remain 13:26:21 loss: 0.6371 Lr: 0.04953
[2023-08-07 20:21:04,349 INFO misc.py line 115 22900] Train: [11/100][140/156] Data 0.008 (0.001) Batch 3.278 (3.479) Remain 13:25:57 loss: 0.7626 Lr: 0.04953
[2023-08-07 20:21:08,856 INFO misc.py line 115 22900] Train: [11/100][141/156] Data 0.001 (0.001) Batch 4.507 (3.486) Remain 13:27:37 loss: 1.0017 Lr: 0.04952
[2023-08-07 20:21:12,645 INFO misc.py line 115 22900] Train: [11/100][142/156] Data 0.001 (0.001) Batch 3.789 (3.489) Remain 13:28:04 loss: 0.6783 Lr: 0.04952
[2023-08-07 20:21:15,530 INFO misc.py line 115 22900] Train: [11/100][143/156] Data 0.001 (0.001) Batch 2.886 (3.484) Remain 13:27:01 loss: 0.9844 Lr: 0.04952
[2023-08-07 20:21:19,134 INFO misc.py line 115 22900] Train: [11/100][144/156] Data 0.001 (0.001) Batch 3.604 (3.485) Remain 13:27:09 loss: 0.6389 Lr: 0.04952
[2023-08-07 20:21:22,367 INFO misc.py line 115 22900] Train: [11/100][145/156] Data 0.001 (0.001) Batch 3.233 (3.483) Remain 13:26:41 loss: 0.9941 Lr: 0.04952
[2023-08-07 20:21:25,613 INFO misc.py line 115 22900] Train: [11/100][146/156] Data 0.001 (0.001) Batch 3.246 (3.482) Remain 13:26:14 loss: 0.8957 Lr: 0.04952
[2023-08-07 20:21:28,940 INFO misc.py line 115 22900] Train: [11/100][147/156] Data 0.001 (0.001) Batch 3.327 (3.481) Remain 13:25:56 loss: 0.7495 Lr: 0.04952
[2023-08-07 20:21:32,420 INFO misc.py line 115 22900] Train: [11/100][148/156] Data 0.001 (0.001) Batch 3.480 (3.481) Remain 13:25:52 loss: 0.7314 Lr: 0.04952
[2023-08-07 20:21:35,757 INFO misc.py line 115 22900] Train: [11/100][149/156] Data 0.001 (0.001) Batch 3.336 (3.480) Remain 13:25:35 loss: 1.0621 Lr: 0.04952
[2023-08-07 20:21:38,911 INFO misc.py line 115 22900] Train: [11/100][150/156] Data 0.001 (0.001) Batch 3.155 (3.477) Remain 13:25:01 loss: 0.7537 Lr: 0.04951
[2023-08-07 20:21:42,913 INFO misc.py line 115 22900] Train: [11/100][151/156] Data 0.001 (0.001) Batch 4.002 (3.481) Remain 13:25:47 loss: 0.8466 Lr: 0.04951
[2023-08-07 20:21:46,620 INFO misc.py line 115 22900] Train: [11/100][152/156] Data 0.001 (0.001) Batch 3.707 (3.482) Remain 13:26:04 loss: 0.5492 Lr: 0.04951
[2023-08-07 20:21:49,862 INFO misc.py line 115 22900] Train: [11/100][153/156] Data 0.001 (0.001) Batch 3.242 (3.481) Remain 13:25:39 loss: 0.9068 Lr: 0.04951
[2023-08-07 20:21:53,157 INFO misc.py line 115 22900] Train: [11/100][154/156] Data 0.001 (0.001) Batch 3.295 (3.480) Remain 13:25:18 loss: 0.3145 Lr: 0.04951
[2023-08-07 20:21:56,765 INFO misc.py line 115 22900] Train: [11/100][155/156] Data 0.001 (0.001) Batch 3.608 (3.481) Remain 13:25:26 loss: 0.8126 Lr: 0.04951
[2023-08-07 20:22:00,919 INFO misc.py line 115 22900] Train: [11/100][156/156] Data 0.001 (0.001) Batch 4.154 (3.485) Remain 13:26:24 loss: 0.9193 Lr: 0.04951
[2023-08-07 20:22:00,920 INFO misc.py line 129 22900] Train result: loss: 0.8664 
[2023-08-07 20:22:00,920 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 20:22:03,066 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.8201 
[2023-08-07 20:22:03,938 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.8525 
[2023-08-07 20:22:05,602 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.9681 
[2023-08-07 20:22:07,130 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.2218 
[2023-08-07 20:22:08,974 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.7258 
[2023-08-07 20:22:10,639 INFO evaluator.py line 122 22900] Test: [6/24] Loss 1.0337 
[2023-08-07 20:22:12,780 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.2554 
[2023-08-07 20:22:14,587 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.8440 
[2023-08-07 20:22:15,873 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.3958 
[2023-08-07 20:22:18,006 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3281 
[2023-08-07 20:22:18,531 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.5049 
[2023-08-07 20:22:20,067 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.8989 
[2023-08-07 20:22:22,782 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.0810 
[2023-08-07 20:22:24,463 INFO evaluator.py line 122 22900] Test: [14/24] Loss 1.0364 
[2023-08-07 20:22:26,488 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.5463 
[2023-08-07 20:22:29,199 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.0756 
[2023-08-07 20:22:31,910 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.0823 
[2023-08-07 20:22:33,758 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.6305 
[2023-08-07 20:22:34,509 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.3831 
[2023-08-07 20:22:35,394 INFO evaluator.py line 122 22900] Test: [20/24] Loss 1.0891 
[2023-08-07 20:22:37,656 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3632 
[2023-08-07 20:22:39,622 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.7198 
[2023-08-07 20:22:41,472 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.0340 
[2023-08-07 20:22:43,408 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.7631 
[2023-08-07 20:22:43,464 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.1487/0.2188/0.6269.
[2023-08-07 20:22:43,464 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.5825/0.9415
[2023-08-07 20:22:43,464 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9325/0.9878
[2023-08-07 20:22:43,464 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.0723/0.1454
[2023-08-07 20:22:43,464 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.0331/0.0664
[2023-08-07 20:22:43,464 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.5379/0.7007
[2023-08-07 20:22:43,464 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.0861/0.0898
[2023-08-07 20:22:43,464 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.2386/0.2623
[2023-08-07 20:22:43,464 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0122/0.0124
[2023-08-07 20:22:43,464 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1102/0.4271
[2023-08-07 20:22:43,464 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.1188/0.1510
[2023-08-07 20:22:43,464 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:22:43,464 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:22:43,464 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1300/0.4269
[2023-08-07 20:22:43,464 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0274/0.0313
[2023-08-07 20:22:43,464 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:22:43,464 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:22:43,464 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:22:43,464 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.0670/0.1014
[2023-08-07 20:22:43,464 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:22:43,464 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0248/0.0328
[2023-08-07 20:22:43,464 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 20:22:43,465 INFO misc.py line 152 22900] Currently Best mIoU: 0.1556
[2023-08-07 20:22:43,465 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 20:22:50,380 INFO misc.py line 115 22900] Train: [12/100][1/156] Data 2.060 (2.060) Batch 6.111 (6.111) Remain 23:33:57 loss: 1.2734 Lr: 0.04951
[2023-08-07 20:22:53,798 INFO misc.py line 115 22900] Train: [12/100][2/156] Data 0.001 (0.001) Batch 3.419 (3.419) Remain 13:10:58 loss: 0.9648 Lr: 0.04951
[2023-08-07 20:22:56,917 INFO misc.py line 115 22900] Train: [12/100][3/156] Data 0.001 (0.001) Batch 3.119 (3.119) Remain 12:01:37 loss: 0.8842 Lr: 0.04951
[2023-08-07 20:22:59,036 INFO misc.py line 115 22900] Train: [12/100][4/156] Data 0.001 (0.001) Batch 2.119 (2.119) Remain 08:10:12 loss: 0.5110 Lr: 0.04950
[2023-08-07 20:23:02,270 INFO misc.py line 115 22900] Train: [12/100][5/156] Data 0.001 (0.001) Batch 3.234 (2.676) Remain 10:19:04 loss: 0.7369 Lr: 0.04950
[2023-08-07 20:23:05,659 INFO misc.py line 115 22900] Train: [12/100][6/156] Data 0.001 (0.001) Batch 3.389 (2.914) Remain 11:13:59 loss: 0.7975 Lr: 0.04950
[2023-08-07 20:23:09,745 INFO misc.py line 115 22900] Train: [12/100][7/156] Data 0.001 (0.001) Batch 4.086 (3.207) Remain 12:21:42 loss: 1.0978 Lr: 0.04950
[2023-08-07 20:23:12,816 INFO misc.py line 115 22900] Train: [12/100][8/156] Data 0.001 (0.001) Batch 3.071 (3.180) Remain 12:15:22 loss: 0.6944 Lr: 0.04950
[2023-08-07 20:23:16,225 INFO misc.py line 115 22900] Train: [12/100][9/156] Data 0.001 (0.001) Batch 3.408 (3.218) Remain 12:24:07 loss: 0.6129 Lr: 0.04950
[2023-08-07 20:23:19,243 INFO misc.py line 115 22900] Train: [12/100][10/156] Data 0.002 (0.001) Batch 3.018 (3.189) Remain 12:17:29 loss: 0.8809 Lr: 0.04950
[2023-08-07 20:23:21,492 INFO misc.py line 115 22900] Train: [12/100][11/156] Data 0.001 (0.001) Batch 2.249 (3.072) Remain 11:50:15 loss: 0.3950 Lr: 0.04950
[2023-08-07 20:23:25,162 INFO misc.py line 115 22900] Train: [12/100][12/156] Data 0.001 (0.001) Batch 3.670 (3.138) Remain 12:05:33 loss: 0.7740 Lr: 0.04950
[2023-08-07 20:23:28,396 INFO misc.py line 115 22900] Train: [12/100][13/156] Data 0.001 (0.001) Batch 3.234 (3.148) Remain 12:07:44 loss: 0.7198 Lr: 0.04949
[2023-08-07 20:23:32,502 INFO misc.py line 115 22900] Train: [12/100][14/156] Data 0.001 (0.001) Batch 4.106 (3.235) Remain 12:27:49 loss: 0.6526 Lr: 0.04949
[2023-08-07 20:23:36,418 INFO misc.py line 115 22900] Train: [12/100][15/156] Data 0.001 (0.001) Batch 3.916 (3.292) Remain 12:40:52 loss: 0.9274 Lr: 0.04949
[2023-08-07 20:23:39,027 INFO misc.py line 115 22900] Train: [12/100][16/156] Data 0.001 (0.001) Batch 2.609 (3.239) Remain 12:28:41 loss: 0.7629 Lr: 0.04949
[2023-08-07 20:23:43,063 INFO misc.py line 115 22900] Train: [12/100][17/156] Data 0.001 (0.001) Batch 4.035 (3.296) Remain 12:41:47 loss: 0.8729 Lr: 0.04949
[2023-08-07 20:23:46,864 INFO misc.py line 115 22900] Train: [12/100][18/156] Data 0.001 (0.001) Batch 3.801 (3.330) Remain 12:49:30 loss: 0.7760 Lr: 0.04949
[2023-08-07 20:23:48,886 INFO misc.py line 115 22900] Train: [12/100][19/156] Data 0.001 (0.001) Batch 2.022 (3.248) Remain 12:30:34 loss: 0.7589 Lr: 0.04949
[2023-08-07 20:23:53,604 INFO misc.py line 115 22900] Train: [12/100][20/156] Data 0.001 (0.001) Batch 4.716 (3.334) Remain 12:50:28 loss: 0.9986 Lr: 0.04949
[2023-08-07 20:23:56,652 INFO misc.py line 115 22900] Train: [12/100][21/156] Data 0.002 (0.001) Batch 3.050 (3.319) Remain 12:46:45 loss: 0.5104 Lr: 0.04949
[2023-08-07 20:24:00,513 INFO misc.py line 115 22900] Train: [12/100][22/156] Data 0.001 (0.001) Batch 3.861 (3.347) Remain 12:53:18 loss: 0.7702 Lr: 0.04949
[2023-08-07 20:24:05,273 INFO misc.py line 115 22900] Train: [12/100][23/156] Data 0.005 (0.001) Batch 4.759 (3.418) Remain 13:09:33 loss: 1.1338 Lr: 0.04948
[2023-08-07 20:24:09,209 INFO misc.py line 115 22900] Train: [12/100][24/156] Data 0.002 (0.001) Batch 3.937 (3.442) Remain 13:15:12 loss: 1.0053 Lr: 0.04948
[2023-08-07 20:24:13,173 INFO misc.py line 115 22900] Train: [12/100][25/156] Data 0.001 (0.001) Batch 3.964 (3.466) Remain 13:20:37 loss: 1.1958 Lr: 0.04948
[2023-08-07 20:24:16,450 INFO misc.py line 115 22900] Train: [12/100][26/156] Data 0.001 (0.001) Batch 3.277 (3.458) Remain 13:18:40 loss: 0.9076 Lr: 0.04948
[2023-08-07 20:24:20,772 INFO misc.py line 115 22900] Train: [12/100][27/156] Data 0.001 (0.001) Batch 4.322 (3.494) Remain 13:26:55 loss: 1.2214 Lr: 0.04948
[2023-08-07 20:24:24,310 INFO misc.py line 115 22900] Train: [12/100][28/156] Data 0.001 (0.001) Batch 3.538 (3.496) Remain 13:27:16 loss: 0.7618 Lr: 0.04948
[2023-08-07 20:24:28,701 INFO misc.py line 115 22900] Train: [12/100][29/156] Data 0.001 (0.001) Batch 4.390 (3.530) Remain 13:35:09 loss: 1.0178 Lr: 0.04948
[2023-08-07 20:24:31,895 INFO misc.py line 115 22900] Train: [12/100][30/156] Data 0.002 (0.001) Batch 3.194 (3.518) Remain 13:32:13 loss: 0.6937 Lr: 0.04948
[2023-08-07 20:24:36,120 INFO misc.py line 115 22900] Train: [12/100][31/156] Data 0.001 (0.001) Batch 4.225 (3.543) Remain 13:38:00 loss: 0.9898 Lr: 0.04948
[2023-08-07 20:24:39,125 INFO misc.py line 115 22900] Train: [12/100][32/156] Data 0.001 (0.001) Batch 3.005 (3.524) Remain 13:33:39 loss: 0.5346 Lr: 0.04947
[2023-08-07 20:24:43,828 INFO misc.py line 115 22900] Train: [12/100][33/156] Data 0.001 (0.001) Batch 4.703 (3.564) Remain 13:42:40 loss: 0.9861 Lr: 0.04947
[2023-08-07 20:24:47,760 INFO misc.py line 115 22900] Train: [12/100][34/156] Data 0.001 (0.001) Batch 3.933 (3.576) Remain 13:45:21 loss: 0.7216 Lr: 0.04947
[2023-08-07 20:24:50,959 INFO misc.py line 115 22900] Train: [12/100][35/156] Data 0.001 (0.001) Batch 3.199 (3.564) Remain 13:42:34 loss: 0.8361 Lr: 0.04947
[2023-08-07 20:24:53,846 INFO misc.py line 115 22900] Train: [12/100][36/156] Data 0.001 (0.001) Batch 2.886 (3.543) Remain 13:37:47 loss: 0.9083 Lr: 0.04947
[2023-08-07 20:24:56,739 INFO misc.py line 115 22900] Train: [12/100][37/156] Data 0.001 (0.001) Batch 2.894 (3.524) Remain 13:33:19 loss: 1.0766 Lr: 0.04947
[2023-08-07 20:24:59,548 INFO misc.py line 115 22900] Train: [12/100][38/156] Data 0.002 (0.001) Batch 2.809 (3.504) Remain 13:28:32 loss: 0.3531 Lr: 0.04947
[2023-08-07 20:25:02,588 INFO misc.py line 115 22900] Train: [12/100][39/156] Data 0.001 (0.001) Batch 3.039 (3.491) Remain 13:25:30 loss: 0.7093 Lr: 0.04947
[2023-08-07 20:25:06,640 INFO misc.py line 115 22900] Train: [12/100][40/156] Data 0.001 (0.001) Batch 4.053 (3.506) Remain 13:28:57 loss: 0.6133 Lr: 0.04947
[2023-08-07 20:25:10,676 INFO misc.py line 115 22900] Train: [12/100][41/156] Data 0.001 (0.001) Batch 4.036 (3.520) Remain 13:32:06 loss: 0.9320 Lr: 0.04946
[2023-08-07 20:25:13,666 INFO misc.py line 115 22900] Train: [12/100][42/156] Data 0.001 (0.001) Batch 2.990 (3.506) Remain 13:28:55 loss: 0.9148 Lr: 0.04946
[2023-08-07 20:25:17,206 INFO misc.py line 115 22900] Train: [12/100][43/156] Data 0.001 (0.001) Batch 3.540 (3.507) Remain 13:29:03 loss: 0.8102 Lr: 0.04946
[2023-08-07 20:25:20,666 INFO misc.py line 115 22900] Train: [12/100][44/156] Data 0.001 (0.001) Batch 3.461 (3.506) Remain 13:28:44 loss: 1.0676 Lr: 0.04946
[2023-08-07 20:25:25,137 INFO misc.py line 115 22900] Train: [12/100][45/156] Data 0.001 (0.001) Batch 4.471 (3.529) Remain 13:33:58 loss: 0.9822 Lr: 0.04946
[2023-08-07 20:25:29,245 INFO misc.py line 115 22900] Train: [12/100][46/156] Data 0.001 (0.001) Batch 4.108 (3.542) Remain 13:37:01 loss: 1.0638 Lr: 0.04946
[2023-08-07 20:25:33,234 INFO misc.py line 115 22900] Train: [12/100][47/156] Data 0.001 (0.001) Batch 3.989 (3.553) Remain 13:39:17 loss: 0.8983 Lr: 0.04946
[2023-08-07 20:25:36,707 INFO misc.py line 115 22900] Train: [12/100][48/156] Data 0.001 (0.001) Batch 3.473 (3.551) Remain 13:38:49 loss: 0.7083 Lr: 0.04946
[2023-08-07 20:25:40,297 INFO misc.py line 115 22900] Train: [12/100][49/156] Data 0.001 (0.001) Batch 3.591 (3.552) Remain 13:38:58 loss: 0.9256 Lr: 0.04946
[2023-08-07 20:25:43,893 INFO misc.py line 115 22900] Train: [12/100][50/156] Data 0.001 (0.001) Batch 3.595 (3.553) Remain 13:39:07 loss: 0.8449 Lr: 0.04945
[2023-08-07 20:25:47,625 INFO misc.py line 115 22900] Train: [12/100][51/156] Data 0.001 (0.001) Batch 3.732 (3.556) Remain 13:39:55 loss: 0.7708 Lr: 0.04945
[2023-08-07 20:25:52,025 INFO misc.py line 115 22900] Train: [12/100][52/156] Data 0.001 (0.001) Batch 4.399 (3.574) Remain 13:43:50 loss: 1.0118 Lr: 0.04945
[2023-08-07 20:25:56,092 INFO misc.py line 115 22900] Train: [12/100][53/156] Data 0.001 (0.001) Batch 4.068 (3.583) Remain 13:46:03 loss: 0.6107 Lr: 0.04945
[2023-08-07 20:25:59,259 INFO misc.py line 115 22900] Train: [12/100][54/156] Data 0.001 (0.001) Batch 3.167 (3.575) Remain 13:44:06 loss: 0.8205 Lr: 0.04945
[2023-08-07 20:26:02,095 INFO misc.py line 115 22900] Train: [12/100][55/156] Data 0.001 (0.001) Batch 2.836 (3.561) Remain 13:40:46 loss: 0.7617 Lr: 0.04945
[2023-08-07 20:26:05,683 INFO misc.py line 115 22900] Train: [12/100][56/156] Data 0.001 (0.001) Batch 3.588 (3.562) Remain 13:40:49 loss: 0.9844 Lr: 0.04945
[2023-08-07 20:26:08,384 INFO misc.py line 115 22900] Train: [12/100][57/156] Data 0.001 (0.001) Batch 2.701 (3.546) Remain 13:37:05 loss: 0.9490 Lr: 0.04945
[2023-08-07 20:26:10,167 INFO misc.py line 115 22900] Train: [12/100][58/156] Data 0.001 (0.001) Batch 1.783 (3.514) Remain 13:29:39 loss: 0.3693 Lr: 0.04945
[2023-08-07 20:26:12,621 INFO misc.py line 115 22900] Train: [12/100][59/156] Data 0.001 (0.001) Batch 2.454 (3.495) Remain 13:25:14 loss: 0.8566 Lr: 0.04944
[2023-08-07 20:26:15,720 INFO misc.py line 115 22900] Train: [12/100][60/156] Data 0.002 (0.001) Batch 3.100 (3.488) Remain 13:23:34 loss: 0.9509 Lr: 0.04944
[2023-08-07 20:26:19,428 INFO misc.py line 115 22900] Train: [12/100][61/156] Data 0.001 (0.001) Batch 3.707 (3.492) Remain 13:24:23 loss: 0.6424 Lr: 0.04944
[2023-08-07 20:26:22,413 INFO misc.py line 115 22900] Train: [12/100][62/156] Data 0.001 (0.001) Batch 2.985 (3.483) Remain 13:22:21 loss: 0.4724 Lr: 0.04944
[2023-08-07 20:26:25,373 INFO misc.py line 115 22900] Train: [12/100][63/156] Data 0.001 (0.001) Batch 2.960 (3.474) Remain 13:20:17 loss: 0.7544 Lr: 0.04944
[2023-08-07 20:26:28,696 INFO misc.py line 115 22900] Train: [12/100][64/156] Data 0.001 (0.001) Batch 3.323 (3.472) Remain 13:19:40 loss: 0.7275 Lr: 0.04944
[2023-08-07 20:26:32,782 INFO misc.py line 115 22900] Train: [12/100][65/156] Data 0.001 (0.001) Batch 4.086 (3.482) Remain 13:21:53 loss: 0.8474 Lr: 0.04944
[2023-08-07 20:26:36,815 INFO misc.py line 115 22900] Train: [12/100][66/156] Data 0.001 (0.001) Batch 4.033 (3.490) Remain 13:23:50 loss: 0.8671 Lr: 0.04944
[2023-08-07 20:26:39,191 INFO misc.py line 115 22900] Train: [12/100][67/156] Data 0.001 (0.001) Batch 2.376 (3.473) Remain 13:19:46 loss: 0.5264 Lr: 0.04944
[2023-08-07 20:26:41,601 INFO misc.py line 115 22900] Train: [12/100][68/156] Data 0.001 (0.001) Batch 2.410 (3.457) Remain 13:15:57 loss: 1.0671 Lr: 0.04943
[2023-08-07 20:26:45,151 INFO misc.py line 115 22900] Train: [12/100][69/156] Data 0.001 (0.001) Batch 3.550 (3.458) Remain 13:16:13 loss: 0.9615 Lr: 0.04943
[2023-08-07 20:26:48,151 INFO misc.py line 115 22900] Train: [12/100][70/156] Data 0.001 (0.001) Batch 3.000 (3.451) Remain 13:14:35 loss: 1.1221 Lr: 0.04943
[2023-08-07 20:26:52,568 INFO misc.py line 115 22900] Train: [12/100][71/156] Data 0.001 (0.001) Batch 4.417 (3.465) Remain 13:17:48 loss: 1.1691 Lr: 0.04943
[2023-08-07 20:26:56,571 INFO misc.py line 115 22900] Train: [12/100][72/156] Data 0.001 (0.001) Batch 4.003 (3.473) Remain 13:19:32 loss: 1.2204 Lr: 0.04943
[2023-08-07 20:26:58,585 INFO misc.py line 115 22900] Train: [12/100][73/156] Data 0.001 (0.001) Batch 2.014 (3.452) Remain 13:14:41 loss: 0.6577 Lr: 0.04943
[2023-08-07 20:27:01,857 INFO misc.py line 115 22900] Train: [12/100][74/156] Data 0.001 (0.001) Batch 3.271 (3.450) Remain 13:14:02 loss: 0.8910 Lr: 0.04943
[2023-08-07 20:27:05,208 INFO misc.py line 115 22900] Train: [12/100][75/156] Data 0.001 (0.001) Batch 3.351 (3.448) Remain 13:13:40 loss: 0.8503 Lr: 0.04943
[2023-08-07 20:27:09,275 INFO misc.py line 115 22900] Train: [12/100][76/156] Data 0.001 (0.001) Batch 4.067 (3.457) Remain 13:15:33 loss: 0.6178 Lr: 0.04943
[2023-08-07 20:27:12,987 INFO misc.py line 115 22900] Train: [12/100][77/156] Data 0.001 (0.001) Batch 3.711 (3.460) Remain 13:16:17 loss: 0.5394 Lr: 0.04942
[2023-08-07 20:27:16,092 INFO misc.py line 115 22900] Train: [12/100][78/156] Data 0.001 (0.001) Batch 3.105 (3.456) Remain 13:15:08 loss: 1.0320 Lr: 0.04942
[2023-08-07 20:27:19,342 INFO misc.py line 115 22900] Train: [12/100][79/156] Data 0.001 (0.001) Batch 3.251 (3.453) Remain 13:14:28 loss: 0.6436 Lr: 0.04942
[2023-08-07 20:27:22,673 INFO misc.py line 115 22900] Train: [12/100][80/156] Data 0.001 (0.001) Batch 3.330 (3.451) Remain 13:14:02 loss: 0.6614 Lr: 0.04942
[2023-08-07 20:27:26,211 INFO misc.py line 115 22900] Train: [12/100][81/156] Data 0.001 (0.001) Batch 3.538 (3.452) Remain 13:14:14 loss: 0.4919 Lr: 0.04942
[2023-08-07 20:27:29,565 INFO misc.py line 115 22900] Train: [12/100][82/156] Data 0.001 (0.001) Batch 3.354 (3.451) Remain 13:13:53 loss: 0.8661 Lr: 0.04942
[2023-08-07 20:27:34,039 INFO misc.py line 115 22900] Train: [12/100][83/156] Data 0.001 (0.001) Batch 4.474 (3.464) Remain 13:16:46 loss: 1.3381 Lr: 0.04942
[2023-08-07 20:27:37,057 INFO misc.py line 115 22900] Train: [12/100][84/156] Data 0.001 (0.001) Batch 3.018 (3.459) Remain 13:15:27 loss: 1.1675 Lr: 0.04942
[2023-08-07 20:27:41,076 INFO misc.py line 115 22900] Train: [12/100][85/156] Data 0.001 (0.001) Batch 4.019 (3.465) Remain 13:16:58 loss: 0.9296 Lr: 0.04942
[2023-08-07 20:27:44,385 INFO misc.py line 115 22900] Train: [12/100][86/156] Data 0.001 (0.001) Batch 3.309 (3.463) Remain 13:16:28 loss: 0.6350 Lr: 0.04941
[2023-08-07 20:27:48,538 INFO misc.py line 115 22900] Train: [12/100][87/156] Data 0.001 (0.001) Batch 4.153 (3.472) Remain 13:18:18 loss: 0.9369 Lr: 0.04941
[2023-08-07 20:27:52,548 INFO misc.py line 115 22900] Train: [12/100][88/156] Data 0.001 (0.001) Batch 4.010 (3.478) Remain 13:19:42 loss: 1.0348 Lr: 0.04941
[2023-08-07 20:27:55,689 INFO misc.py line 115 22900] Train: [12/100][89/156] Data 0.001 (0.001) Batch 3.141 (3.474) Remain 13:18:45 loss: 0.5246 Lr: 0.04941
[2023-08-07 20:27:58,927 INFO misc.py line 115 22900] Train: [12/100][90/156] Data 0.002 (0.001) Batch 3.238 (3.471) Remain 13:18:04 loss: 0.5900 Lr: 0.04941
[2023-08-07 20:28:01,986 INFO misc.py line 115 22900] Train: [12/100][91/156] Data 0.001 (0.001) Batch 3.059 (3.467) Remain 13:16:56 loss: 0.7575 Lr: 0.04941
[2023-08-07 20:28:06,126 INFO misc.py line 115 22900] Train: [12/100][92/156] Data 0.001 (0.001) Batch 4.140 (3.474) Remain 13:18:36 loss: 0.8791 Lr: 0.04941
[2023-08-07 20:28:10,242 INFO misc.py line 115 22900] Train: [12/100][93/156] Data 0.001 (0.001) Batch 4.116 (3.481) Remain 13:20:11 loss: 1.0151 Lr: 0.04941
[2023-08-07 20:28:13,206 INFO misc.py line 115 22900] Train: [12/100][94/156] Data 0.001 (0.001) Batch 2.964 (3.476) Remain 13:18:49 loss: 0.6562 Lr: 0.04941
[2023-08-07 20:28:15,374 INFO misc.py line 115 22900] Train: [12/100][95/156] Data 0.001 (0.001) Batch 2.168 (3.461) Remain 13:15:30 loss: 0.7832 Lr: 0.04940
[2023-08-07 20:28:18,818 INFO misc.py line 115 22900] Train: [12/100][96/156] Data 0.001 (0.001) Batch 3.444 (3.461) Remain 13:15:24 loss: 0.9538 Lr: 0.04940
[2023-08-07 20:28:22,272 INFO misc.py line 115 22900] Train: [12/100][97/156] Data 0.001 (0.001) Batch 3.454 (3.461) Remain 13:15:19 loss: 0.6623 Lr: 0.04940
[2023-08-07 20:28:24,845 INFO misc.py line 115 22900] Train: [12/100][98/156] Data 0.001 (0.001) Batch 2.573 (3.452) Remain 13:13:07 loss: 0.8891 Lr: 0.04940
[2023-08-07 20:28:28,067 INFO misc.py line 115 22900] Train: [12/100][99/156] Data 0.001 (0.001) Batch 3.222 (3.449) Remain 13:12:31 loss: 1.0415 Lr: 0.04940
[2023-08-07 20:28:31,843 INFO misc.py line 115 22900] Train: [12/100][100/156] Data 0.001 (0.001) Batch 3.776 (3.453) Remain 13:13:13 loss: 0.7121 Lr: 0.04940
[2023-08-07 20:28:35,491 INFO misc.py line 115 22900] Train: [12/100][101/156] Data 0.001 (0.001) Batch 3.648 (3.455) Remain 13:13:37 loss: 0.6842 Lr: 0.04940
[2023-08-07 20:28:39,367 INFO misc.py line 115 22900] Train: [12/100][102/156] Data 0.001 (0.001) Batch 3.876 (3.459) Remain 13:14:33 loss: 0.9218 Lr: 0.04940
[2023-08-07 20:28:42,961 INFO misc.py line 115 22900] Train: [12/100][103/156] Data 0.001 (0.001) Batch 3.594 (3.460) Remain 13:14:48 loss: 1.0955 Lr: 0.04939
[2023-08-07 20:28:47,232 INFO misc.py line 115 22900] Train: [12/100][104/156] Data 0.001 (0.001) Batch 4.271 (3.468) Remain 13:16:35 loss: 1.2110 Lr: 0.04939
[2023-08-07 20:28:50,859 INFO misc.py line 115 22900] Train: [12/100][105/156] Data 0.001 (0.001) Batch 3.628 (3.470) Remain 13:16:53 loss: 0.9932 Lr: 0.04939
[2023-08-07 20:28:54,941 INFO misc.py line 115 22900] Train: [12/100][106/156] Data 0.001 (0.001) Batch 4.081 (3.476) Remain 13:18:11 loss: 0.9610 Lr: 0.04939
[2023-08-07 20:28:58,284 INFO misc.py line 115 22900] Train: [12/100][107/156] Data 0.001 (0.001) Batch 3.343 (3.475) Remain 13:17:50 loss: 0.7688 Lr: 0.04939
[2023-08-07 20:29:01,957 INFO misc.py line 115 22900] Train: [12/100][108/156] Data 0.001 (0.001) Batch 3.673 (3.477) Remain 13:18:13 loss: 1.0071 Lr: 0.04939
[2023-08-07 20:29:06,023 INFO misc.py line 115 22900] Train: [12/100][109/156] Data 0.001 (0.001) Batch 4.066 (3.482) Remain 13:19:26 loss: 0.7179 Lr: 0.04939
[2023-08-07 20:29:08,154 INFO misc.py line 115 22900] Train: [12/100][110/156] Data 0.001 (0.001) Batch 2.131 (3.469) Remain 13:16:28 loss: 0.7222 Lr: 0.04939
[2023-08-07 20:29:11,620 INFO misc.py line 115 22900] Train: [12/100][111/156] Data 0.001 (0.001) Batch 3.466 (3.469) Remain 13:16:24 loss: 0.6429 Lr: 0.04939
[2023-08-07 20:29:14,509 INFO misc.py line 115 22900] Train: [12/100][112/156] Data 0.001 (0.001) Batch 2.890 (3.464) Remain 13:15:08 loss: 0.6530 Lr: 0.04938
[2023-08-07 20:29:17,468 INFO misc.py line 115 22900] Train: [12/100][113/156] Data 0.001 (0.001) Batch 2.959 (3.460) Remain 13:14:01 loss: 0.7216 Lr: 0.04938
[2023-08-07 20:29:20,353 INFO misc.py line 115 22900] Train: [12/100][114/156] Data 0.001 (0.001) Batch 2.884 (3.454) Remain 13:12:46 loss: 0.4910 Lr: 0.04938
[2023-08-07 20:29:23,143 INFO misc.py line 115 22900] Train: [12/100][115/156] Data 0.001 (0.001) Batch 2.790 (3.448) Remain 13:11:21 loss: 0.7700 Lr: 0.04938
[2023-08-07 20:29:26,914 INFO misc.py line 115 22900] Train: [12/100][116/156] Data 0.001 (0.001) Batch 3.772 (3.451) Remain 13:11:57 loss: 0.7894 Lr: 0.04938
[2023-08-07 20:29:29,312 INFO misc.py line 115 22900] Train: [12/100][117/156] Data 0.001 (0.001) Batch 2.397 (3.442) Remain 13:09:46 loss: 0.5618 Lr: 0.04938
[2023-08-07 20:29:32,633 INFO misc.py line 115 22900] Train: [12/100][118/156] Data 0.001 (0.001) Batch 3.321 (3.441) Remain 13:09:28 loss: 0.8168 Lr: 0.04938
[2023-08-07 20:29:36,183 INFO misc.py line 115 22900] Train: [12/100][119/156] Data 0.001 (0.001) Batch 3.550 (3.442) Remain 13:09:38 loss: 0.6717 Lr: 0.04938
[2023-08-07 20:29:39,785 INFO misc.py line 115 22900] Train: [12/100][120/156] Data 0.001 (0.001) Batch 3.602 (3.443) Remain 13:09:53 loss: 0.6595 Lr: 0.04938
[2023-08-07 20:29:43,794 INFO misc.py line 115 22900] Train: [12/100][121/156] Data 0.001 (0.001) Batch 4.009 (3.448) Remain 13:10:56 loss: 0.7366 Lr: 0.04937
[2023-08-07 20:29:47,696 INFO misc.py line 115 22900] Train: [12/100][122/156] Data 0.001 (0.001) Batch 3.902 (3.452) Remain 13:11:45 loss: 0.7809 Lr: 0.04937
[2023-08-07 20:29:51,732 INFO misc.py line 115 22900] Train: [12/100][123/156] Data 0.001 (0.001) Batch 4.036 (3.457) Remain 13:12:48 loss: 0.8840 Lr: 0.04937
[2023-08-07 20:29:55,794 INFO misc.py line 115 22900] Train: [12/100][124/156] Data 0.001 (0.001) Batch 4.061 (3.462) Remain 13:13:54 loss: 1.3731 Lr: 0.04937
[2023-08-07 20:29:58,804 INFO misc.py line 115 22900] Train: [12/100][125/156] Data 0.001 (0.001) Batch 3.010 (3.458) Remain 13:12:59 loss: 1.0347 Lr: 0.04937
[2023-08-07 20:30:02,168 INFO misc.py line 115 22900] Train: [12/100][126/156] Data 0.001 (0.001) Batch 3.365 (3.457) Remain 13:12:45 loss: 1.0523 Lr: 0.04937
[2023-08-07 20:30:06,592 INFO misc.py line 115 22900] Train: [12/100][127/156] Data 0.001 (0.001) Batch 4.424 (3.465) Remain 13:14:29 loss: 1.2517 Lr: 0.04937
[2023-08-07 20:30:10,613 INFO misc.py line 115 22900] Train: [12/100][128/156] Data 0.001 (0.001) Batch 4.021 (3.470) Remain 13:15:27 loss: 0.8550 Lr: 0.04937
[2023-08-07 20:30:14,597 INFO misc.py line 115 22900] Train: [12/100][129/156] Data 0.001 (0.001) Batch 3.983 (3.474) Remain 13:16:19 loss: 0.8563 Lr: 0.04936
[2023-08-07 20:30:18,240 INFO misc.py line 115 22900] Train: [12/100][130/156] Data 0.001 (0.001) Batch 3.643 (3.475) Remain 13:16:34 loss: 0.8046 Lr: 0.04936
[2023-08-07 20:30:22,247 INFO misc.py line 115 22900] Train: [12/100][131/156] Data 0.002 (0.001) Batch 4.008 (3.479) Remain 13:17:28 loss: 1.0073 Lr: 0.04936
[2023-08-07 20:30:24,915 INFO misc.py line 115 22900] Train: [12/100][132/156] Data 0.001 (0.001) Batch 2.668 (3.473) Remain 13:15:58 loss: 0.7443 Lr: 0.04936
[2023-08-07 20:30:27,509 INFO misc.py line 115 22900] Train: [12/100][133/156] Data 0.001 (0.001) Batch 2.594 (3.466) Remain 13:14:22 loss: 0.5866 Lr: 0.04936
[2023-08-07 20:30:30,948 INFO misc.py line 115 22900] Train: [12/100][134/156] Data 0.001 (0.001) Batch 3.439 (3.466) Remain 13:14:15 loss: 0.9763 Lr: 0.04936
[2023-08-07 20:30:34,366 INFO misc.py line 115 22900] Train: [12/100][135/156] Data 0.002 (0.001) Batch 3.418 (3.466) Remain 13:14:07 loss: 0.6717 Lr: 0.04936
[2023-08-07 20:30:37,684 INFO misc.py line 115 22900] Train: [12/100][136/156] Data 0.001 (0.001) Batch 3.318 (3.464) Remain 13:13:48 loss: 0.9068 Lr: 0.04936
[2023-08-07 20:30:40,681 INFO misc.py line 115 22900] Train: [12/100][137/156] Data 0.001 (0.001) Batch 2.997 (3.461) Remain 13:12:57 loss: 0.3194 Lr: 0.04935
[2023-08-07 20:30:44,089 INFO misc.py line 115 22900] Train: [12/100][138/156] Data 0.001 (0.001) Batch 3.407 (3.461) Remain 13:12:48 loss: 0.6953 Lr: 0.04935
[2023-08-07 20:30:47,297 INFO misc.py line 115 22900] Train: [12/100][139/156] Data 0.001 (0.001) Batch 3.209 (3.459) Remain 13:12:19 loss: 0.8708 Lr: 0.04935
[2023-08-07 20:30:49,957 INFO misc.py line 115 22900] Train: [12/100][140/156] Data 0.001 (0.001) Batch 2.660 (3.453) Remain 13:10:55 loss: 0.7352 Lr: 0.04935
[2023-08-07 20:30:52,587 INFO misc.py line 115 22900] Train: [12/100][141/156] Data 0.001 (0.001) Batch 2.629 (3.447) Remain 13:09:30 loss: 0.7132 Lr: 0.04935
[2023-08-07 20:30:56,641 INFO misc.py line 115 22900] Train: [12/100][142/156] Data 0.001 (0.001) Batch 4.054 (3.451) Remain 13:10:27 loss: 0.9728 Lr: 0.04935
[2023-08-07 20:31:00,732 INFO misc.py line 115 22900] Train: [12/100][143/156] Data 0.001 (0.001) Batch 4.090 (3.456) Remain 13:11:26 loss: 0.7760 Lr: 0.04935
[2023-08-07 20:31:04,905 INFO misc.py line 115 22900] Train: [12/100][144/156] Data 0.001 (0.001) Batch 4.174 (3.461) Remain 13:12:32 loss: 0.9147 Lr: 0.04935
[2023-08-07 20:31:09,020 INFO misc.py line 115 22900] Train: [12/100][145/156] Data 0.001 (0.001) Batch 4.115 (3.466) Remain 13:13:32 loss: 1.0244 Lr: 0.04935
[2023-08-07 20:31:12,853 INFO misc.py line 115 22900] Train: [12/100][146/156] Data 0.001 (0.001) Batch 3.832 (3.468) Remain 13:14:04 loss: 0.9545 Lr: 0.04934
[2023-08-07 20:31:16,157 INFO misc.py line 115 22900] Train: [12/100][147/156] Data 0.002 (0.001) Batch 3.304 (3.467) Remain 13:13:45 loss: 0.6418 Lr: 0.04934
[2023-08-07 20:31:19,438 INFO misc.py line 115 22900] Train: [12/100][148/156] Data 0.001 (0.001) Batch 3.281 (3.466) Remain 13:13:24 loss: 0.6054 Lr: 0.04934
[2023-08-07 20:31:23,590 INFO misc.py line 115 22900] Train: [12/100][149/156] Data 0.001 (0.001) Batch 4.152 (3.470) Remain 13:14:25 loss: 0.6847 Lr: 0.04934
[2023-08-07 20:31:26,859 INFO misc.py line 115 22900] Train: [12/100][150/156] Data 0.001 (0.001) Batch 3.269 (3.469) Remain 13:14:03 loss: 0.5431 Lr: 0.04934
[2023-08-07 20:31:30,701 INFO misc.py line 115 22900] Train: [12/100][151/156] Data 0.001 (0.001) Batch 3.842 (3.472) Remain 13:14:34 loss: 0.9829 Lr: 0.04934
[2023-08-07 20:31:34,735 INFO misc.py line 115 22900] Train: [12/100][152/156] Data 0.001 (0.001) Batch 4.034 (3.475) Remain 13:15:22 loss: 0.8625 Lr: 0.04934
[2023-08-07 20:31:37,965 INFO misc.py line 115 22900] Train: [12/100][153/156] Data 0.001 (0.001) Batch 3.230 (3.474) Remain 13:14:56 loss: 0.8548 Lr: 0.04934
[2023-08-07 20:31:41,918 INFO misc.py line 115 22900] Train: [12/100][154/156] Data 0.001 (0.001) Batch 3.954 (3.477) Remain 13:15:36 loss: 0.8399 Lr: 0.04933
[2023-08-07 20:31:45,196 INFO misc.py line 115 22900] Train: [12/100][155/156] Data 0.001 (0.001) Batch 3.278 (3.476) Remain 13:15:15 loss: 0.6875 Lr: 0.04933
[2023-08-07 20:31:48,409 INFO misc.py line 115 22900] Train: [12/100][156/156] Data 0.001 (0.001) Batch 3.213 (3.474) Remain 13:14:48 loss: 0.6901 Lr: 0.04933
[2023-08-07 20:31:48,410 INFO misc.py line 129 22900] Train result: loss: 0.8279 
[2023-08-07 20:31:48,410 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 20:31:50,565 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.9277 
[2023-08-07 20:31:51,434 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.8289 
[2023-08-07 20:31:53,096 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.8727 
[2023-08-07 20:31:54,620 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.0340 
[2023-08-07 20:31:56,464 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.5755 
[2023-08-07 20:31:58,129 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.5044 
[2023-08-07 20:32:00,267 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.5716 
[2023-08-07 20:32:02,071 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.7758 
[2023-08-07 20:32:03,354 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.2040 
[2023-08-07 20:32:05,484 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3227 
[2023-08-07 20:32:06,010 INFO evaluator.py line 122 22900] Test: [11/24] Loss 0.9971 
[2023-08-07 20:32:07,546 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.7375 
[2023-08-07 20:32:10,257 INFO evaluator.py line 122 22900] Test: [13/24] Loss 0.9394 
[2023-08-07 20:32:11,937 INFO evaluator.py line 122 22900] Test: [14/24] Loss 1.0024 
[2023-08-07 20:32:13,961 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.4493 
[2023-08-07 20:32:16,673 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.0033 
[2023-08-07 20:32:19,378 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.1000 
[2023-08-07 20:32:21,226 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.5045 
[2023-08-07 20:32:21,977 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.1769 
[2023-08-07 20:32:22,863 INFO evaluator.py line 122 22900] Test: [20/24] Loss 1.0746 
[2023-08-07 20:32:25,124 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3210 
[2023-08-07 20:32:27,090 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.5673 
[2023-08-07 20:32:28,940 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.4350 
[2023-08-07 20:32:30,877 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.8566 
[2023-08-07 20:32:30,944 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.1657/0.2465/0.6539.
[2023-08-07 20:32:30,944 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6530/0.9226
[2023-08-07 20:32:30,944 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9516/0.9853
[2023-08-07 20:32:30,944 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.0833/0.3099
[2023-08-07 20:32:30,944 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.0399/0.0682
[2023-08-07 20:32:30,944 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.5572/0.8531
[2023-08-07 20:32:30,945 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.0869/0.1034
[2023-08-07 20:32:30,945 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.4599/0.6504
[2023-08-07 20:32:30,945 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0184/0.0188
[2023-08-07 20:32:30,945 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1474/0.3461
[2023-08-07 20:32:30,945 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0059/0.0059
[2023-08-07 20:32:30,945 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:32:30,945 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:32:30,945 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.0142/0.0152
[2023-08-07 20:32:30,945 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0007/0.0007
[2023-08-07 20:32:30,945 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0176/0.0189
[2023-08-07 20:32:30,945 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0327/0.0327
[2023-08-07 20:32:30,945 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:32:30,945 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.1623/0.3931
[2023-08-07 20:32:30,945 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:32:30,945 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0830/0.2064
[2023-08-07 20:32:30,945 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 20:32:30,945 INFO misc.py line 150 22900] Best validation mIoU updated to: 0.1657
[2023-08-07 20:32:30,946 INFO misc.py line 152 22900] Currently Best mIoU: 0.1657
[2023-08-07 20:32:30,946 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 20:32:37,007 INFO misc.py line 115 22900] Train: [13/100][1/156] Data 1.312 (1.312) Batch 4.984 (4.984) Remain 19:00:16 loss: 0.7895 Lr: 0.04933
[2023-08-07 20:32:40,853 INFO misc.py line 115 22900] Train: [13/100][2/156] Data 0.003 (0.003) Batch 3.846 (3.846) Remain 14:39:45 loss: 0.8334 Lr: 0.04933
[2023-08-07 20:32:44,964 INFO misc.py line 115 22900] Train: [13/100][3/156] Data 0.001 (0.001) Batch 4.112 (4.112) Remain 15:40:34 loss: 0.7282 Lr: 0.04933
[2023-08-07 20:32:48,093 INFO misc.py line 115 22900] Train: [13/100][4/156] Data 0.001 (0.001) Batch 3.129 (3.129) Remain 11:55:37 loss: 0.9243 Lr: 0.04933
[2023-08-07 20:32:52,296 INFO misc.py line 115 22900] Train: [13/100][5/156] Data 0.001 (0.001) Batch 4.203 (3.666) Remain 13:58:27 loss: 1.1634 Lr: 0.04933
[2023-08-07 20:32:56,530 INFO misc.py line 115 22900] Train: [13/100][6/156] Data 0.001 (0.001) Batch 4.234 (3.855) Remain 14:41:43 loss: 1.0904 Lr: 0.04932
[2023-08-07 20:33:00,576 INFO misc.py line 115 22900] Train: [13/100][7/156] Data 0.001 (0.001) Batch 4.045 (3.903) Remain 14:52:31 loss: 1.1074 Lr: 0.04932
[2023-08-07 20:33:03,687 INFO misc.py line 115 22900] Train: [13/100][8/156] Data 0.001 (0.001) Batch 3.111 (3.744) Remain 14:16:14 loss: 0.8980 Lr: 0.04932
[2023-08-07 20:33:06,168 INFO misc.py line 115 22900] Train: [13/100][9/156] Data 0.001 (0.001) Batch 2.482 (3.534) Remain 13:28:02 loss: 0.5282 Lr: 0.04932
[2023-08-07 20:33:10,159 INFO misc.py line 115 22900] Train: [13/100][10/156] Data 0.001 (0.001) Batch 3.990 (3.599) Remain 13:42:53 loss: 0.5558 Lr: 0.04932
[2023-08-07 20:33:12,826 INFO misc.py line 115 22900] Train: [13/100][11/156] Data 0.001 (0.001) Batch 2.668 (3.483) Remain 13:16:12 loss: 0.7304 Lr: 0.04932
[2023-08-07 20:33:15,385 INFO misc.py line 115 22900] Train: [13/100][12/156] Data 0.001 (0.001) Batch 2.558 (3.380) Remain 12:52:40 loss: 0.4079 Lr: 0.04932
[2023-08-07 20:33:18,630 INFO misc.py line 115 22900] Train: [13/100][13/156] Data 0.001 (0.001) Batch 3.246 (3.367) Remain 12:49:32 loss: 0.4312 Lr: 0.04932
[2023-08-07 20:33:21,966 INFO misc.py line 115 22900] Train: [13/100][14/156] Data 0.001 (0.001) Batch 3.336 (3.364) Remain 12:48:50 loss: 1.0833 Lr: 0.04931
[2023-08-07 20:33:25,581 INFO misc.py line 115 22900] Train: [13/100][15/156] Data 0.001 (0.001) Batch 3.615 (3.385) Remain 12:53:34 loss: 0.5256 Lr: 0.04931
[2023-08-07 20:33:28,785 INFO misc.py line 115 22900] Train: [13/100][16/156] Data 0.001 (0.001) Batch 3.204 (3.371) Remain 12:50:20 loss: 0.7914 Lr: 0.04931
[2023-08-07 20:33:32,141 INFO misc.py line 115 22900] Train: [13/100][17/156] Data 0.001 (0.001) Batch 3.356 (3.370) Remain 12:50:02 loss: 1.1712 Lr: 0.04931
[2023-08-07 20:33:35,393 INFO misc.py line 115 22900] Train: [13/100][18/156] Data 0.001 (0.001) Batch 3.252 (3.362) Remain 12:48:11 loss: 0.7152 Lr: 0.04931
[2023-08-07 20:33:38,355 INFO misc.py line 115 22900] Train: [13/100][19/156] Data 0.001 (0.001) Batch 2.962 (3.337) Remain 12:42:25 loss: 0.7185 Lr: 0.04931
[2023-08-07 20:33:40,966 INFO misc.py line 115 22900] Train: [13/100][20/156] Data 0.001 (0.001) Batch 2.611 (3.294) Remain 12:32:37 loss: 0.6209 Lr: 0.04931
[2023-08-07 20:33:43,577 INFO misc.py line 115 22900] Train: [13/100][21/156] Data 0.001 (0.001) Batch 2.611 (3.256) Remain 12:23:53 loss: 1.3361 Lr: 0.04931
[2023-08-07 20:33:47,315 INFO misc.py line 115 22900] Train: [13/100][22/156] Data 0.001 (0.001) Batch 3.738 (3.282) Remain 12:29:37 loss: 0.7674 Lr: 0.04930
[2023-08-07 20:33:51,019 INFO misc.py line 115 22900] Train: [13/100][23/156] Data 0.001 (0.001) Batch 3.704 (3.303) Remain 12:34:24 loss: 0.8886 Lr: 0.04930
[2023-08-07 20:33:54,207 INFO misc.py line 115 22900] Train: [13/100][24/156] Data 0.001 (0.001) Batch 3.188 (3.297) Remain 12:33:06 loss: 0.8224 Lr: 0.04930
[2023-08-07 20:33:57,171 INFO misc.py line 115 22900] Train: [13/100][25/156] Data 0.001 (0.001) Batch 2.964 (3.282) Remain 12:29:34 loss: 0.6684 Lr: 0.04930
[2023-08-07 20:34:00,475 INFO misc.py line 115 22900] Train: [13/100][26/156] Data 0.001 (0.001) Batch 3.304 (3.283) Remain 12:29:44 loss: 0.9842 Lr: 0.04930
[2023-08-07 20:34:02,840 INFO misc.py line 115 22900] Train: [13/100][27/156] Data 0.001 (0.001) Batch 2.365 (3.245) Remain 12:20:57 loss: 0.8344 Lr: 0.04930
[2023-08-07 20:34:05,375 INFO misc.py line 115 22900] Train: [13/100][28/156] Data 0.001 (0.001) Batch 2.535 (3.216) Remain 12:14:25 loss: 0.7324 Lr: 0.04930
[2023-08-07 20:34:08,307 INFO misc.py line 115 22900] Train: [13/100][29/156] Data 0.001 (0.001) Batch 2.932 (3.205) Remain 12:11:51 loss: 0.8888 Lr: 0.04930
[2023-08-07 20:34:12,286 INFO misc.py line 115 22900] Train: [13/100][30/156] Data 0.001 (0.001) Batch 3.979 (3.234) Remain 12:18:21 loss: 1.2514 Lr: 0.04929
[2023-08-07 20:34:14,594 INFO misc.py line 115 22900] Train: [13/100][31/156] Data 0.001 (0.001) Batch 2.309 (3.201) Remain 12:10:45 loss: 0.6255 Lr: 0.04929
[2023-08-07 20:34:17,957 INFO misc.py line 115 22900] Train: [13/100][32/156] Data 0.001 (0.001) Batch 3.362 (3.207) Remain 12:11:58 loss: 0.8618 Lr: 0.04929
[2023-08-07 20:34:21,644 INFO misc.py line 115 22900] Train: [13/100][33/156] Data 0.001 (0.001) Batch 3.688 (3.223) Remain 12:15:34 loss: 0.8879 Lr: 0.04929
[2023-08-07 20:34:24,527 INFO misc.py line 115 22900] Train: [13/100][34/156] Data 0.001 (0.001) Batch 2.883 (3.212) Remain 12:13:01 loss: 0.5365 Lr: 0.04929
[2023-08-07 20:34:28,915 INFO misc.py line 115 22900] Train: [13/100][35/156] Data 0.001 (0.001) Batch 4.388 (3.248) Remain 12:21:21 loss: 1.0977 Lr: 0.04929
[2023-08-07 20:34:32,372 INFO misc.py line 115 22900] Train: [13/100][36/156] Data 0.002 (0.001) Batch 3.456 (3.255) Remain 12:22:44 loss: 0.7308 Lr: 0.04929
[2023-08-07 20:34:35,943 INFO misc.py line 115 22900] Train: [13/100][37/156] Data 0.001 (0.001) Batch 3.571 (3.264) Remain 12:24:48 loss: 0.8076 Lr: 0.04929
[2023-08-07 20:34:38,952 INFO misc.py line 115 22900] Train: [13/100][38/156] Data 0.001 (0.001) Batch 3.009 (3.257) Remain 12:23:05 loss: 0.6089 Lr: 0.04928
[2023-08-07 20:34:42,329 INFO misc.py line 115 22900] Train: [13/100][39/156] Data 0.001 (0.001) Batch 3.377 (3.260) Remain 12:23:47 loss: 0.8634 Lr: 0.04928
[2023-08-07 20:34:46,058 INFO misc.py line 115 22900] Train: [13/100][40/156] Data 0.001 (0.001) Batch 3.730 (3.273) Remain 12:26:38 loss: 0.7335 Lr: 0.04928
[2023-08-07 20:34:48,767 INFO misc.py line 115 22900] Train: [13/100][41/156] Data 0.001 (0.001) Batch 2.709 (3.258) Remain 12:23:11 loss: 0.7885 Lr: 0.04928
[2023-08-07 20:34:52,900 INFO misc.py line 115 22900] Train: [13/100][42/156] Data 0.002 (0.001) Batch 4.133 (3.280) Remain 12:28:15 loss: 1.0811 Lr: 0.04928
[2023-08-07 20:34:55,763 INFO misc.py line 115 22900] Train: [13/100][43/156] Data 0.001 (0.001) Batch 2.863 (3.270) Remain 12:25:49 loss: 0.7563 Lr: 0.04928
[2023-08-07 20:34:59,842 INFO misc.py line 115 22900] Train: [13/100][44/156] Data 0.001 (0.001) Batch 4.079 (3.290) Remain 12:30:16 loss: 0.8944 Lr: 0.04928
[2023-08-07 20:35:03,324 INFO misc.py line 115 22900] Train: [13/100][45/156] Data 0.001 (0.001) Batch 3.482 (3.294) Remain 12:31:15 loss: 1.1738 Lr: 0.04928
[2023-08-07 20:35:07,011 INFO misc.py line 115 22900] Train: [13/100][46/156] Data 0.001 (0.001) Batch 3.687 (3.303) Remain 12:33:17 loss: 1.1371 Lr: 0.04927
[2023-08-07 20:35:09,730 INFO misc.py line 115 22900] Train: [13/100][47/156] Data 0.001 (0.001) Batch 2.719 (3.290) Remain 12:30:12 loss: 0.5028 Lr: 0.04927
[2023-08-07 20:35:13,776 INFO misc.py line 115 22900] Train: [13/100][48/156] Data 0.001 (0.001) Batch 4.045 (3.307) Remain 12:33:58 loss: 0.6415 Lr: 0.04927
[2023-08-07 20:35:17,525 INFO misc.py line 115 22900] Train: [13/100][49/156] Data 0.001 (0.001) Batch 3.749 (3.317) Remain 12:36:06 loss: 0.6670 Lr: 0.04927
[2023-08-07 20:35:21,551 INFO misc.py line 115 22900] Train: [13/100][50/156] Data 0.001 (0.001) Batch 4.026 (3.332) Remain 12:39:29 loss: 1.0196 Lr: 0.04927
[2023-08-07 20:35:25,228 INFO misc.py line 115 22900] Train: [13/100][51/156] Data 0.001 (0.001) Batch 3.677 (3.339) Remain 12:41:05 loss: 0.6345 Lr: 0.04927
[2023-08-07 20:35:28,653 INFO misc.py line 115 22900] Train: [13/100][52/156] Data 0.001 (0.001) Batch 3.425 (3.341) Remain 12:41:25 loss: 0.7367 Lr: 0.04927
[2023-08-07 20:35:31,189 INFO misc.py line 115 22900] Train: [13/100][53/156] Data 0.002 (0.001) Batch 2.536 (3.324) Remain 12:37:42 loss: 0.6543 Lr: 0.04927
[2023-08-07 20:35:34,149 INFO misc.py line 115 22900] Train: [13/100][54/156] Data 0.001 (0.001) Batch 2.960 (3.317) Remain 12:36:01 loss: 0.7202 Lr: 0.04926
[2023-08-07 20:35:37,270 INFO misc.py line 115 22900] Train: [13/100][55/156] Data 0.001 (0.001) Batch 3.121 (3.314) Remain 12:35:06 loss: 0.9410 Lr: 0.04926
[2023-08-07 20:35:39,291 INFO misc.py line 115 22900] Train: [13/100][56/156] Data 0.001 (0.001) Batch 2.021 (3.289) Remain 12:29:29 loss: 0.5874 Lr: 0.04926
[2023-08-07 20:35:42,377 INFO misc.py line 115 22900] Train: [13/100][57/156] Data 0.001 (0.001) Batch 3.086 (3.285) Remain 12:28:34 loss: 0.8528 Lr: 0.04926
[2023-08-07 20:35:45,936 INFO misc.py line 115 22900] Train: [13/100][58/156] Data 0.001 (0.001) Batch 3.559 (3.290) Remain 12:29:39 loss: 0.8049 Lr: 0.04926
[2023-08-07 20:35:49,960 INFO misc.py line 115 22900] Train: [13/100][59/156] Data 0.001 (0.001) Batch 4.024 (3.303) Remain 12:32:35 loss: 0.8989 Lr: 0.04926
[2023-08-07 20:35:53,365 INFO misc.py line 115 22900] Train: [13/100][60/156] Data 0.001 (0.001) Batch 3.405 (3.305) Remain 12:32:56 loss: 1.0145 Lr: 0.04926
[2023-08-07 20:35:56,678 INFO misc.py line 115 22900] Train: [13/100][61/156] Data 0.001 (0.001) Batch 3.313 (3.305) Remain 12:32:55 loss: 0.7331 Lr: 0.04926
[2023-08-07 20:36:00,021 INFO misc.py line 115 22900] Train: [13/100][62/156] Data 0.001 (0.001) Batch 3.342 (3.306) Remain 12:33:00 loss: 0.7176 Lr: 0.04925
[2023-08-07 20:36:02,706 INFO misc.py line 115 22900] Train: [13/100][63/156] Data 0.001 (0.001) Batch 2.686 (3.296) Remain 12:30:35 loss: 0.5933 Lr: 0.04925
[2023-08-07 20:36:07,038 INFO misc.py line 115 22900] Train: [13/100][64/156] Data 0.001 (0.001) Batch 4.332 (3.313) Remain 12:34:24 loss: 0.9415 Lr: 0.04925
[2023-08-07 20:36:11,221 INFO misc.py line 115 22900] Train: [13/100][65/156] Data 0.001 (0.001) Batch 4.183 (3.327) Remain 12:37:32 loss: 1.1415 Lr: 0.04925
[2023-08-07 20:36:13,621 INFO misc.py line 115 22900] Train: [13/100][66/156] Data 0.001 (0.001) Batch 2.401 (3.312) Remain 12:34:08 loss: 0.7275 Lr: 0.04925
[2023-08-07 20:36:18,186 INFO misc.py line 115 22900] Train: [13/100][67/156] Data 0.001 (0.001) Batch 4.565 (3.332) Remain 12:38:32 loss: 1.1217 Lr: 0.04925
[2023-08-07 20:36:21,830 INFO misc.py line 115 22900] Train: [13/100][68/156] Data 0.001 (0.001) Batch 3.644 (3.336) Remain 12:39:35 loss: 0.6070 Lr: 0.04925
[2023-08-07 20:36:24,646 INFO misc.py line 115 22900] Train: [13/100][69/156] Data 0.001 (0.001) Batch 2.816 (3.329) Remain 12:37:44 loss: 0.4329 Lr: 0.04925
[2023-08-07 20:36:27,648 INFO misc.py line 115 22900] Train: [13/100][70/156] Data 0.001 (0.001) Batch 3.002 (3.324) Remain 12:36:34 loss: 0.7529 Lr: 0.04924
[2023-08-07 20:36:31,265 INFO misc.py line 115 22900] Train: [13/100][71/156] Data 0.001 (0.001) Batch 3.617 (3.328) Remain 12:37:29 loss: 0.8386 Lr: 0.04924
[2023-08-07 20:36:35,260 INFO misc.py line 115 22900] Train: [13/100][72/156] Data 0.001 (0.001) Batch 3.995 (3.338) Remain 12:39:38 loss: 0.7429 Lr: 0.04924
[2023-08-07 20:36:39,714 INFO misc.py line 115 22900] Train: [13/100][73/156] Data 0.001 (0.001) Batch 4.454 (3.354) Remain 12:43:12 loss: 1.1106 Lr: 0.04924
[2023-08-07 20:36:42,971 INFO misc.py line 115 22900] Train: [13/100][74/156] Data 0.001 (0.001) Batch 3.257 (3.352) Remain 12:42:50 loss: 0.8865 Lr: 0.04924
[2023-08-07 20:36:46,321 INFO misc.py line 115 22900] Train: [13/100][75/156] Data 0.001 (0.001) Batch 3.350 (3.352) Remain 12:42:47 loss: 0.6804 Lr: 0.04924
[2023-08-07 20:36:50,316 INFO misc.py line 115 22900] Train: [13/100][76/156] Data 0.001 (0.001) Batch 3.995 (3.361) Remain 12:44:44 loss: 0.5824 Lr: 0.04924
[2023-08-07 20:36:53,631 INFO misc.py line 115 22900] Train: [13/100][77/156] Data 0.001 (0.001) Batch 3.315 (3.360) Remain 12:44:32 loss: 0.6279 Lr: 0.04924
[2023-08-07 20:36:56,704 INFO misc.py line 115 22900] Train: [13/100][78/156] Data 0.001 (0.001) Batch 3.072 (3.357) Remain 12:43:36 loss: 0.6116 Lr: 0.04923
[2023-08-07 20:37:00,342 INFO misc.py line 115 22900] Train: [13/100][79/156] Data 0.001 (0.001) Batch 3.639 (3.360) Remain 12:44:23 loss: 0.7624 Lr: 0.04923
[2023-08-07 20:37:04,505 INFO misc.py line 115 22900] Train: [13/100][80/156] Data 0.001 (0.001) Batch 4.162 (3.371) Remain 12:46:42 loss: 1.2264 Lr: 0.04923
[2023-08-07 20:37:08,425 INFO misc.py line 115 22900] Train: [13/100][81/156] Data 0.001 (0.001) Batch 3.920 (3.378) Remain 12:48:15 loss: 0.9571 Lr: 0.04923
[2023-08-07 20:37:12,377 INFO misc.py line 115 22900] Train: [13/100][82/156] Data 0.001 (0.001) Batch 3.952 (3.385) Remain 12:49:51 loss: 0.8831 Lr: 0.04923
[2023-08-07 20:37:15,638 INFO misc.py line 115 22900] Train: [13/100][83/156] Data 0.002 (0.001) Batch 3.261 (3.383) Remain 12:49:26 loss: 0.7459 Lr: 0.04923
[2023-08-07 20:37:19,193 INFO misc.py line 115 22900] Train: [13/100][84/156] Data 0.001 (0.001) Batch 3.555 (3.386) Remain 12:49:52 loss: 0.6026 Lr: 0.04923
[2023-08-07 20:37:22,753 INFO misc.py line 115 22900] Train: [13/100][85/156] Data 0.001 (0.001) Batch 3.560 (3.388) Remain 12:50:17 loss: 0.5298 Lr: 0.04922
[2023-08-07 20:37:26,412 INFO misc.py line 115 22900] Train: [13/100][86/156] Data 0.001 (0.001) Batch 3.659 (3.391) Remain 12:50:59 loss: 0.7449 Lr: 0.04922
[2023-08-07 20:37:30,444 INFO misc.py line 115 22900] Train: [13/100][87/156] Data 0.001 (0.001) Batch 4.031 (3.399) Remain 12:52:39 loss: 0.8141 Lr: 0.04922
[2023-08-07 20:37:33,229 INFO misc.py line 115 22900] Train: [13/100][88/156] Data 0.001 (0.001) Batch 2.785 (3.391) Remain 12:50:57 loss: 0.4274 Lr: 0.04922
[2023-08-07 20:37:37,288 INFO misc.py line 115 22900] Train: [13/100][89/156] Data 0.001 (0.001) Batch 4.060 (3.399) Remain 12:52:40 loss: 0.8022 Lr: 0.04922
[2023-08-07 20:37:40,211 INFO misc.py line 115 22900] Train: [13/100][90/156] Data 0.002 (0.001) Batch 2.923 (3.394) Remain 12:51:22 loss: 0.7184 Lr: 0.04922
[2023-08-07 20:37:44,238 INFO misc.py line 115 22900] Train: [13/100][91/156] Data 0.001 (0.001) Batch 4.027 (3.401) Remain 12:52:57 loss: 1.1379 Lr: 0.04922
[2023-08-07 20:37:47,943 INFO misc.py line 115 22900] Train: [13/100][92/156] Data 0.001 (0.001) Batch 3.705 (3.404) Remain 12:53:40 loss: 1.2155 Lr: 0.04922
[2023-08-07 20:37:50,385 INFO misc.py line 115 22900] Train: [13/100][93/156] Data 0.001 (0.001) Batch 2.442 (3.394) Remain 12:51:11 loss: 0.6372 Lr: 0.04921
[2023-08-07 20:37:54,726 INFO misc.py line 115 22900] Train: [13/100][94/156] Data 0.001 (0.001) Batch 4.340 (3.404) Remain 12:53:29 loss: 0.9572 Lr: 0.04921
[2023-08-07 20:37:58,047 INFO misc.py line 115 22900] Train: [13/100][95/156] Data 0.001 (0.001) Batch 3.321 (3.403) Remain 12:53:14 loss: 0.5836 Lr: 0.04921
[2023-08-07 20:38:01,266 INFO misc.py line 115 22900] Train: [13/100][96/156] Data 0.001 (0.001) Batch 3.219 (3.401) Remain 12:52:43 loss: 0.5578 Lr: 0.04921
[2023-08-07 20:38:03,270 INFO misc.py line 115 22900] Train: [13/100][97/156] Data 0.001 (0.001) Batch 2.004 (3.386) Remain 12:49:17 loss: 0.7787 Lr: 0.04921
[2023-08-07 20:38:06,851 INFO misc.py line 115 22900] Train: [13/100][98/156] Data 0.001 (0.001) Batch 3.581 (3.388) Remain 12:49:42 loss: 0.7242 Lr: 0.04921
[2023-08-07 20:38:11,130 INFO misc.py line 115 22900] Train: [13/100][99/156] Data 0.001 (0.001) Batch 4.279 (3.398) Remain 12:51:45 loss: 1.1624 Lr: 0.04921
[2023-08-07 20:38:13,072 INFO misc.py line 115 22900] Train: [13/100][100/156] Data 0.001 (0.001) Batch 1.942 (3.383) Remain 12:48:17 loss: 0.6816 Lr: 0.04920
[2023-08-07 20:38:17,173 INFO misc.py line 115 22900] Train: [13/100][101/156] Data 0.001 (0.001) Batch 4.101 (3.390) Remain 12:49:53 loss: 1.0038 Lr: 0.04920
[2023-08-07 20:38:20,481 INFO misc.py line 115 22900] Train: [13/100][102/156] Data 0.001 (0.001) Batch 3.309 (3.389) Remain 12:49:39 loss: 0.8721 Lr: 0.04920
[2023-08-07 20:38:24,346 INFO misc.py line 115 22900] Train: [13/100][103/156] Data 0.001 (0.001) Batch 3.865 (3.394) Remain 12:50:40 loss: 1.1091 Lr: 0.04920
[2023-08-07 20:38:27,554 INFO misc.py line 115 22900] Train: [13/100][104/156] Data 0.001 (0.001) Batch 3.208 (3.392) Remain 12:50:12 loss: 0.4704 Lr: 0.04920
[2023-08-07 20:38:31,942 INFO misc.py line 115 22900] Train: [13/100][105/156] Data 0.001 (0.001) Batch 4.388 (3.402) Remain 12:52:21 loss: 0.9524 Lr: 0.04920
[2023-08-07 20:38:35,535 INFO misc.py line 115 22900] Train: [13/100][106/156] Data 0.001 (0.001) Batch 3.593 (3.404) Remain 12:52:43 loss: 0.5570 Lr: 0.04920
[2023-08-07 20:38:38,982 INFO misc.py line 115 22900] Train: [13/100][107/156] Data 0.001 (0.001) Batch 3.447 (3.404) Remain 12:52:46 loss: 0.5861 Lr: 0.04920
[2023-08-07 20:38:42,351 INFO misc.py line 115 22900] Train: [13/100][108/156] Data 0.001 (0.001) Batch 3.369 (3.404) Remain 12:52:38 loss: 0.9880 Lr: 0.04919
[2023-08-07 20:38:46,356 INFO misc.py line 115 22900] Train: [13/100][109/156] Data 0.001 (0.001) Batch 4.005 (3.409) Remain 12:53:52 loss: 0.7778 Lr: 0.04919
[2023-08-07 20:38:50,461 INFO misc.py line 115 22900] Train: [13/100][110/156] Data 0.001 (0.001) Batch 4.105 (3.416) Remain 12:55:17 loss: 1.0583 Lr: 0.04919
[2023-08-07 20:38:54,175 INFO misc.py line 115 22900] Train: [13/100][111/156] Data 0.001 (0.001) Batch 3.714 (3.419) Remain 12:55:51 loss: 1.0681 Lr: 0.04919
[2023-08-07 20:38:57,688 INFO misc.py line 115 22900] Train: [13/100][112/156] Data 0.001 (0.001) Batch 3.513 (3.419) Remain 12:55:59 loss: 0.9807 Lr: 0.04919
[2023-08-07 20:39:00,771 INFO misc.py line 115 22900] Train: [13/100][113/156] Data 0.001 (0.001) Batch 3.083 (3.416) Remain 12:55:14 loss: 0.6291 Lr: 0.04919
[2023-08-07 20:39:03,892 INFO misc.py line 115 22900] Train: [13/100][114/156] Data 0.001 (0.001) Batch 3.121 (3.414) Remain 12:54:34 loss: 0.5024 Lr: 0.04919
[2023-08-07 20:39:07,655 INFO misc.py line 115 22900] Train: [13/100][115/156] Data 0.001 (0.001) Batch 3.763 (3.417) Remain 12:55:13 loss: 0.7136 Lr: 0.04918
[2023-08-07 20:39:11,199 INFO misc.py line 115 22900] Train: [13/100][116/156] Data 0.001 (0.001) Batch 3.544 (3.418) Remain 12:55:25 loss: 0.8642 Lr: 0.04918
[2023-08-07 20:39:14,185 INFO misc.py line 115 22900] Train: [13/100][117/156] Data 0.001 (0.001) Batch 2.986 (3.414) Remain 12:54:30 loss: 0.6594 Lr: 0.04918
[2023-08-07 20:39:17,416 INFO misc.py line 115 22900] Train: [13/100][118/156] Data 0.001 (0.001) Batch 3.230 (3.413) Remain 12:54:05 loss: 0.8310 Lr: 0.04918
[2023-08-07 20:39:21,509 INFO misc.py line 115 22900] Train: [13/100][119/156] Data 0.001 (0.001) Batch 4.093 (3.418) Remain 12:55:22 loss: 0.5412 Lr: 0.04918
[2023-08-07 20:39:25,369 INFO misc.py line 115 22900] Train: [13/100][120/156] Data 0.001 (0.001) Batch 3.860 (3.422) Remain 12:56:10 loss: 0.9816 Lr: 0.04918
[2023-08-07 20:39:28,644 INFO misc.py line 115 22900] Train: [13/100][121/156] Data 0.001 (0.001) Batch 3.275 (3.421) Remain 12:55:49 loss: 0.5697 Lr: 0.04918
[2023-08-07 20:39:32,800 INFO misc.py line 115 22900] Train: [13/100][122/156] Data 0.001 (0.001) Batch 4.156 (3.427) Remain 12:57:10 loss: 0.9711 Lr: 0.04918
[2023-08-07 20:39:35,628 INFO misc.py line 115 22900] Train: [13/100][123/156] Data 0.001 (0.001) Batch 2.828 (3.422) Remain 12:55:58 loss: 0.8604 Lr: 0.04917
[2023-08-07 20:39:38,354 INFO misc.py line 115 22900] Train: [13/100][124/156] Data 0.001 (0.001) Batch 2.726 (3.416) Remain 12:54:37 loss: 0.4388 Lr: 0.04917
[2023-08-07 20:39:42,009 INFO misc.py line 115 22900] Train: [13/100][125/156] Data 0.001 (0.001) Batch 3.655 (3.418) Remain 12:55:00 loss: 0.8859 Lr: 0.04917
[2023-08-07 20:39:46,018 INFO misc.py line 115 22900] Train: [13/100][126/156] Data 0.001 (0.001) Batch 4.009 (3.423) Remain 12:56:02 loss: 0.8174 Lr: 0.04917
[2023-08-07 20:39:49,878 INFO misc.py line 115 22900] Train: [13/100][127/156] Data 0.001 (0.001) Batch 3.860 (3.427) Remain 12:56:46 loss: 0.8640 Lr: 0.04917
[2023-08-07 20:39:52,495 INFO misc.py line 115 22900] Train: [13/100][128/156] Data 0.001 (0.001) Batch 2.617 (3.420) Remain 12:55:15 loss: 0.5787 Lr: 0.04917
[2023-08-07 20:39:56,085 INFO misc.py line 115 22900] Train: [13/100][129/156] Data 0.001 (0.001) Batch 3.590 (3.422) Remain 12:55:30 loss: 0.5691 Lr: 0.04917
[2023-08-07 20:40:00,197 INFO misc.py line 115 22900] Train: [13/100][130/156] Data 0.001 (0.001) Batch 4.111 (3.427) Remain 12:56:40 loss: 1.2175 Lr: 0.04916
[2023-08-07 20:40:04,288 INFO misc.py line 115 22900] Train: [13/100][131/156] Data 0.001 (0.001) Batch 4.091 (3.432) Remain 12:57:47 loss: 1.1844 Lr: 0.04916
[2023-08-07 20:40:07,962 INFO misc.py line 115 22900] Train: [13/100][132/156] Data 0.001 (0.001) Batch 3.674 (3.434) Remain 12:58:09 loss: 0.6737 Lr: 0.04916
[2023-08-07 20:40:12,064 INFO misc.py line 115 22900] Train: [13/100][133/156] Data 0.001 (0.001) Batch 4.102 (3.439) Remain 12:59:16 loss: 1.0042 Lr: 0.04916
[2023-08-07 20:40:15,724 INFO misc.py line 115 22900] Train: [13/100][134/156] Data 0.001 (0.001) Batch 3.660 (3.441) Remain 12:59:35 loss: 0.8528 Lr: 0.04916
[2023-08-07 20:40:18,984 INFO misc.py line 115 22900] Train: [13/100][135/156] Data 0.001 (0.001) Batch 3.260 (3.440) Remain 12:59:13 loss: 0.7883 Lr: 0.04916
[2023-08-07 20:40:22,426 INFO misc.py line 115 22900] Train: [13/100][136/156] Data 0.001 (0.001) Batch 3.442 (3.440) Remain 12:59:10 loss: 0.6552 Lr: 0.04916
[2023-08-07 20:40:26,268 INFO misc.py line 115 22900] Train: [13/100][137/156] Data 0.001 (0.001) Batch 3.841 (3.443) Remain 12:59:47 loss: 0.7180 Lr: 0.04915
[2023-08-07 20:40:30,328 INFO misc.py line 115 22900] Train: [13/100][138/156] Data 0.001 (0.001) Batch 4.060 (3.447) Remain 13:00:46 loss: 0.6509 Lr: 0.04915
[2023-08-07 20:40:34,289 INFO misc.py line 115 22900] Train: [13/100][139/156] Data 0.001 (0.001) Batch 3.961 (3.451) Remain 13:01:34 loss: 0.6449 Lr: 0.04915
[2023-08-07 20:40:37,088 INFO misc.py line 115 22900] Train: [13/100][140/156] Data 0.001 (0.001) Batch 2.800 (3.446) Remain 13:00:26 loss: 0.5031 Lr: 0.04915
[2023-08-07 20:40:41,133 INFO misc.py line 115 22900] Train: [13/100][141/156] Data 0.001 (0.001) Batch 4.045 (3.450) Remain 13:01:21 loss: 0.6758 Lr: 0.04915
[2023-08-07 20:40:44,968 INFO misc.py line 115 22900] Train: [13/100][142/156] Data 0.001 (0.001) Batch 3.835 (3.453) Remain 13:01:55 loss: 0.7632 Lr: 0.04915
[2023-08-07 20:40:48,431 INFO misc.py line 115 22900] Train: [13/100][143/156] Data 0.001 (0.001) Batch 3.463 (3.453) Remain 13:01:53 loss: 1.0247 Lr: 0.04915
[2023-08-07 20:40:51,538 INFO misc.py line 115 22900] Train: [13/100][144/156] Data 0.001 (0.001) Batch 3.107 (3.451) Remain 13:01:16 loss: 0.6183 Lr: 0.04915
[2023-08-07 20:40:54,746 INFO misc.py line 115 22900] Train: [13/100][145/156] Data 0.001 (0.001) Batch 3.208 (3.449) Remain 13:00:49 loss: 1.0779 Lr: 0.04914
[2023-08-07 20:40:57,571 INFO misc.py line 115 22900] Train: [13/100][146/156] Data 0.001 (0.001) Batch 2.825 (3.445) Remain 12:59:47 loss: 0.5713 Lr: 0.04914
[2023-08-07 20:41:01,752 INFO misc.py line 115 22900] Train: [13/100][147/156] Data 0.001 (0.001) Batch 4.180 (3.450) Remain 13:00:53 loss: 0.7996 Lr: 0.04914
[2023-08-07 20:41:04,759 INFO misc.py line 115 22900] Train: [13/100][148/156] Data 0.001 (0.001) Batch 3.007 (3.447) Remain 13:00:08 loss: 0.5151 Lr: 0.04914
[2023-08-07 20:41:08,297 INFO misc.py line 115 22900] Train: [13/100][149/156] Data 0.001 (0.001) Batch 3.538 (3.447) Remain 13:00:13 loss: 0.7196 Lr: 0.04914
[2023-08-07 20:41:11,435 INFO misc.py line 115 22900] Train: [13/100][150/156] Data 0.001 (0.001) Batch 3.138 (3.445) Remain 12:59:41 loss: 0.7045 Lr: 0.04914
[2023-08-07 20:41:14,945 INFO misc.py line 115 22900] Train: [13/100][151/156] Data 0.001 (0.001) Batch 3.510 (3.446) Remain 12:59:43 loss: 0.8233 Lr: 0.04914
[2023-08-07 20:41:18,514 INFO misc.py line 115 22900] Train: [13/100][152/156] Data 0.001 (0.001) Batch 3.569 (3.447) Remain 12:59:51 loss: 0.4620 Lr: 0.04913
[2023-08-07 20:41:21,362 INFO misc.py line 115 22900] Train: [13/100][153/156] Data 0.001 (0.001) Batch 2.849 (3.443) Remain 12:58:54 loss: 0.6075 Lr: 0.04913
[2023-08-07 20:41:24,833 INFO misc.py line 115 22900] Train: [13/100][154/156] Data 0.001 (0.001) Batch 3.470 (3.443) Remain 12:58:53 loss: 0.6421 Lr: 0.04913
[2023-08-07 20:41:27,991 INFO misc.py line 115 22900] Train: [13/100][155/156] Data 0.001 (0.001) Batch 3.158 (3.441) Remain 12:58:24 loss: 0.7490 Lr: 0.04913
[2023-08-07 20:41:32,861 INFO misc.py line 115 22900] Train: [13/100][156/156] Data 0.001 (0.001) Batch 4.870 (3.450) Remain 13:00:27 loss: 0.9153 Lr: 0.04913
[2023-08-07 20:41:32,861 INFO misc.py line 129 22900] Train result: loss: 0.7950 
[2023-08-07 20:41:32,861 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 20:41:34,961 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.8633 
[2023-08-07 20:41:35,830 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.7679 
[2023-08-07 20:41:37,494 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.9389 
[2023-08-07 20:41:39,014 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.0518 
[2023-08-07 20:41:40,857 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.4069 
[2023-08-07 20:41:42,518 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.5836 
[2023-08-07 20:41:44,656 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.7073 
[2023-08-07 20:41:46,459 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.8837 
[2023-08-07 20:41:47,741 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.1298 
[2023-08-07 20:41:49,873 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.2624 
[2023-08-07 20:41:50,399 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.0179 
[2023-08-07 20:41:51,931 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.8216 
[2023-08-07 20:41:54,644 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.0741 
[2023-08-07 20:41:56,325 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.9298 
[2023-08-07 20:41:58,348 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.5619 
[2023-08-07 20:42:01,057 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.1974 
[2023-08-07 20:42:03,765 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.2718 
[2023-08-07 20:42:05,609 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.3219 
[2023-08-07 20:42:06,357 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.2970 
[2023-08-07 20:42:07,242 INFO evaluator.py line 122 22900] Test: [20/24] Loss 1.0746 
[2023-08-07 20:42:09,503 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.2275 
[2023-08-07 20:42:11,469 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.4625 
[2023-08-07 20:42:13,317 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.5458 
[2023-08-07 20:42:15,254 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.7737 
[2023-08-07 20:42:15,304 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.1686/0.2538/0.6498.
[2023-08-07 20:42:15,304 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6377/0.8878
[2023-08-07 20:42:15,304 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9454/0.9871
[2023-08-07 20:42:15,304 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.0890/0.1602
[2023-08-07 20:42:15,304 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.0623/0.1474
[2023-08-07 20:42:15,304 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.5444/0.8128
[2023-08-07 20:42:15,304 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.1330/0.1639
[2023-08-07 20:42:15,304 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.4195/0.5297
[2023-08-07 20:42:15,304 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1692/0.2477
[2023-08-07 20:42:15,304 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.0534/0.0882
[2023-08-07 20:42:15,304 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0012/0.0012
[2023-08-07 20:42:15,304 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:42:15,304 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:42:15,304 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1237/0.2494
[2023-08-07 20:42:15,304 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0272/0.0280
[2023-08-07 20:42:15,304 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0017/0.0017
[2023-08-07 20:42:15,304 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0024/0.0024
[2023-08-07 20:42:15,304 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:42:15,305 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.0848/0.5338
[2023-08-07 20:42:15,305 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:42:15,305 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0763/0.2353
[2023-08-07 20:42:15,305 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 20:42:15,305 INFO misc.py line 150 22900] Best validation mIoU updated to: 0.1686
[2023-08-07 20:42:15,305 INFO misc.py line 152 22900] Currently Best mIoU: 0.1686
[2023-08-07 20:42:15,305 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 20:42:20,757 INFO misc.py line 115 22900] Train: [14/100][1/156] Data 1.086 (1.086) Batch 4.386 (4.386) Remain 16:32:00 loss: 1.1712 Lr: 0.04913
[2023-08-07 20:42:24,344 INFO misc.py line 115 22900] Train: [14/100][2/156] Data 0.001 (0.001) Batch 3.587 (3.587) Remain 13:31:18 loss: 0.8306 Lr: 0.04913
[2023-08-07 20:42:28,459 INFO misc.py line 115 22900] Train: [14/100][3/156] Data 0.001 (0.001) Batch 4.115 (4.115) Remain 15:30:38 loss: 0.9161 Lr: 0.04912
[2023-08-07 20:42:32,014 INFO misc.py line 115 22900] Train: [14/100][4/156] Data 0.001 (0.001) Batch 3.554 (3.554) Remain 13:23:45 loss: 0.7043 Lr: 0.04912
[2023-08-07 20:42:33,819 INFO misc.py line 115 22900] Train: [14/100][5/156] Data 0.001 (0.001) Batch 1.806 (2.680) Remain 10:05:58 loss: 0.7495 Lr: 0.04912
[2023-08-07 20:42:37,333 INFO misc.py line 115 22900] Train: [14/100][6/156] Data 0.001 (0.001) Batch 3.514 (2.958) Remain 11:08:48 loss: 0.8464 Lr: 0.04912
[2023-08-07 20:42:39,992 INFO misc.py line 115 22900] Train: [14/100][7/156] Data 0.001 (0.001) Batch 2.659 (2.883) Remain 10:51:49 loss: 0.5577 Lr: 0.04912
[2023-08-07 20:42:43,671 INFO misc.py line 115 22900] Train: [14/100][8/156] Data 0.001 (0.001) Batch 3.679 (3.042) Remain 11:27:45 loss: 1.2940 Lr: 0.04912
[2023-08-07 20:42:47,697 INFO misc.py line 115 22900] Train: [14/100][9/156] Data 0.001 (0.001) Batch 4.026 (3.206) Remain 12:04:45 loss: 0.8204 Lr: 0.04912
[2023-08-07 20:42:51,383 INFO misc.py line 115 22900] Train: [14/100][10/156] Data 0.001 (0.001) Batch 3.687 (3.275) Remain 12:20:13 loss: 1.0340 Lr: 0.04911
[2023-08-07 20:42:54,042 INFO misc.py line 115 22900] Train: [14/100][11/156] Data 0.001 (0.001) Batch 2.658 (3.198) Remain 12:02:45 loss: 0.5681 Lr: 0.04911
[2023-08-07 20:42:57,957 INFO misc.py line 115 22900] Train: [14/100][12/156] Data 0.001 (0.001) Batch 3.916 (3.278) Remain 12:20:43 loss: 1.0634 Lr: 0.04911
[2023-08-07 20:43:00,346 INFO misc.py line 115 22900] Train: [14/100][13/156] Data 0.001 (0.001) Batch 2.389 (3.189) Remain 12:00:34 loss: 0.4743 Lr: 0.04911
[2023-08-07 20:43:03,632 INFO misc.py line 115 22900] Train: [14/100][14/156] Data 0.001 (0.001) Batch 3.286 (3.198) Remain 12:02:31 loss: 0.8152 Lr: 0.04911
[2023-08-07 20:43:07,912 INFO misc.py line 115 22900] Train: [14/100][15/156] Data 0.001 (0.001) Batch 4.280 (3.288) Remain 12:22:51 loss: 1.0237 Lr: 0.04911
[2023-08-07 20:43:11,742 INFO misc.py line 115 22900] Train: [14/100][16/156] Data 0.001 (0.001) Batch 3.829 (3.329) Remain 12:32:13 loss: 0.9617 Lr: 0.04911
[2023-08-07 20:43:13,178 INFO misc.py line 115 22900] Train: [14/100][17/156] Data 0.001 (0.001) Batch 1.436 (3.194) Remain 12:01:37 loss: 0.4101 Lr: 0.04911
[2023-08-07 20:43:16,812 INFO misc.py line 115 22900] Train: [14/100][18/156] Data 0.001 (0.001) Batch 3.634 (3.224) Remain 12:08:11 loss: 0.8440 Lr: 0.04910
[2023-08-07 20:43:21,223 INFO misc.py line 115 22900] Train: [14/100][19/156] Data 0.001 (0.001) Batch 4.411 (3.298) Remain 12:24:54 loss: 0.7972 Lr: 0.04910
[2023-08-07 20:43:24,430 INFO misc.py line 115 22900] Train: [14/100][20/156] Data 0.001 (0.001) Batch 3.206 (3.292) Remain 12:23:38 loss: 0.9360 Lr: 0.04910
[2023-08-07 20:43:28,552 INFO misc.py line 115 22900] Train: [14/100][21/156] Data 0.001 (0.001) Batch 4.122 (3.338) Remain 12:33:59 loss: 0.7334 Lr: 0.04910
[2023-08-07 20:43:31,566 INFO misc.py line 115 22900] Train: [14/100][22/156] Data 0.001 (0.001) Batch 3.015 (3.321) Remain 12:30:05 loss: 0.6095 Lr: 0.04910
[2023-08-07 20:43:34,957 INFO misc.py line 115 22900] Train: [14/100][23/156] Data 0.001 (0.001) Batch 3.391 (3.325) Remain 12:30:48 loss: 0.7129 Lr: 0.04910
[2023-08-07 20:43:38,331 INFO misc.py line 115 22900] Train: [14/100][24/156] Data 0.001 (0.001) Batch 3.374 (3.327) Remain 12:31:17 loss: 0.9806 Lr: 0.04910
[2023-08-07 20:43:42,441 INFO misc.py line 115 22900] Train: [14/100][25/156] Data 0.001 (0.001) Batch 4.109 (3.363) Remain 12:39:15 loss: 0.9401 Lr: 0.04909
[2023-08-07 20:43:46,099 INFO misc.py line 115 22900] Train: [14/100][26/156] Data 0.001 (0.001) Batch 3.658 (3.376) Remain 12:42:06 loss: 0.8396 Lr: 0.04909
[2023-08-07 20:43:49,643 INFO misc.py line 115 22900] Train: [14/100][27/156] Data 0.001 (0.001) Batch 3.544 (3.383) Remain 12:43:37 loss: 0.8291 Lr: 0.04909
[2023-08-07 20:43:52,817 INFO misc.py line 115 22900] Train: [14/100][28/156] Data 0.001 (0.001) Batch 3.174 (3.374) Remain 12:41:41 loss: 1.0419 Lr: 0.04909
[2023-08-07 20:43:55,973 INFO misc.py line 115 22900] Train: [14/100][29/156] Data 0.001 (0.001) Batch 3.157 (3.366) Remain 12:39:44 loss: 0.7371 Lr: 0.04909
[2023-08-07 20:44:00,067 INFO misc.py line 115 22900] Train: [14/100][30/156] Data 0.001 (0.001) Batch 4.094 (3.393) Remain 12:45:46 loss: 0.6933 Lr: 0.04909
[2023-08-07 20:44:03,200 INFO misc.py line 115 22900] Train: [14/100][31/156] Data 0.001 (0.001) Batch 3.132 (3.384) Remain 12:43:36 loss: 0.6943 Lr: 0.04909
[2023-08-07 20:44:06,070 INFO misc.py line 115 22900] Train: [14/100][32/156] Data 0.001 (0.001) Batch 2.870 (3.366) Remain 12:39:33 loss: 0.6791 Lr: 0.04908
[2023-08-07 20:44:09,762 INFO misc.py line 115 22900] Train: [14/100][33/156] Data 0.001 (0.001) Batch 3.692 (3.377) Remain 12:41:57 loss: 0.8914 Lr: 0.04908
[2023-08-07 20:44:13,962 INFO misc.py line 115 22900] Train: [14/100][34/156] Data 0.001 (0.001) Batch 4.200 (3.403) Remain 12:47:53 loss: 1.2606 Lr: 0.04908
[2023-08-07 20:44:16,732 INFO misc.py line 115 22900] Train: [14/100][35/156] Data 0.001 (0.001) Batch 2.770 (3.384) Remain 12:43:22 loss: 0.9350 Lr: 0.04908
[2023-08-07 20:44:20,608 INFO misc.py line 115 22900] Train: [14/100][36/156] Data 0.001 (0.001) Batch 3.876 (3.398) Remain 12:46:41 loss: 0.6718 Lr: 0.04908
[2023-08-07 20:44:24,195 INFO misc.py line 115 22900] Train: [14/100][37/156] Data 0.001 (0.001) Batch 3.588 (3.404) Remain 12:47:53 loss: 0.5374 Lr: 0.04908
[2023-08-07 20:44:26,754 INFO misc.py line 115 22900] Train: [14/100][38/156] Data 0.001 (0.001) Batch 2.558 (3.380) Remain 12:42:22 loss: 0.7362 Lr: 0.04908
[2023-08-07 20:44:30,046 INFO misc.py line 115 22900] Train: [14/100][39/156] Data 0.001 (0.001) Batch 3.293 (3.377) Remain 12:41:46 loss: 0.7384 Lr: 0.04907
[2023-08-07 20:44:33,626 INFO misc.py line 115 22900] Train: [14/100][40/156] Data 0.001 (0.001) Batch 3.580 (3.383) Remain 12:42:57 loss: 0.6559 Lr: 0.04907
[2023-08-07 20:44:36,301 INFO misc.py line 115 22900] Train: [14/100][41/156] Data 0.001 (0.001) Batch 2.674 (3.364) Remain 12:38:41 loss: 1.0409 Lr: 0.04907
[2023-08-07 20:44:40,413 INFO misc.py line 115 22900] Train: [14/100][42/156] Data 0.001 (0.001) Batch 4.112 (3.383) Remain 12:42:57 loss: 1.0861 Lr: 0.04907
[2023-08-07 20:44:43,106 INFO misc.py line 115 22900] Train: [14/100][43/156] Data 0.001 (0.001) Batch 2.694 (3.366) Remain 12:39:00 loss: 0.5898 Lr: 0.04907
[2023-08-07 20:44:46,399 INFO misc.py line 115 22900] Train: [14/100][44/156] Data 0.001 (0.001) Batch 3.293 (3.364) Remain 12:38:33 loss: 1.1383 Lr: 0.04907
[2023-08-07 20:44:50,501 INFO misc.py line 115 22900] Train: [14/100][45/156] Data 0.001 (0.001) Batch 4.102 (3.382) Remain 12:42:27 loss: 1.0184 Lr: 0.04907
[2023-08-07 20:44:54,277 INFO misc.py line 115 22900] Train: [14/100][46/156] Data 0.001 (0.001) Batch 3.776 (3.391) Remain 12:44:28 loss: 0.6887 Lr: 0.04906
[2023-08-07 20:44:57,218 INFO misc.py line 115 22900] Train: [14/100][47/156] Data 0.002 (0.001) Batch 2.941 (3.381) Remain 12:42:06 loss: 0.7931 Lr: 0.04906
[2023-08-07 20:45:00,181 INFO misc.py line 115 22900] Train: [14/100][48/156] Data 0.002 (0.001) Batch 2.963 (3.372) Remain 12:39:57 loss: 0.3479 Lr: 0.04906
[2023-08-07 20:45:03,369 INFO misc.py line 115 22900] Train: [14/100][49/156] Data 0.001 (0.001) Batch 3.189 (3.368) Remain 12:39:00 loss: 0.8545 Lr: 0.04906
[2023-08-07 20:45:06,158 INFO misc.py line 115 22900] Train: [14/100][50/156] Data 0.001 (0.001) Batch 2.788 (3.355) Remain 12:36:10 loss: 0.5293 Lr: 0.04906
[2023-08-07 20:45:08,910 INFO misc.py line 115 22900] Train: [14/100][51/156] Data 0.001 (0.001) Batch 2.753 (3.343) Remain 12:33:17 loss: 0.2713 Lr: 0.04906
[2023-08-07 20:45:12,503 INFO misc.py line 115 22900] Train: [14/100][52/156] Data 0.001 (0.001) Batch 3.592 (3.348) Remain 12:34:22 loss: 0.7614 Lr: 0.04906
[2023-08-07 20:45:15,911 INFO misc.py line 115 22900] Train: [14/100][53/156] Data 0.001 (0.001) Batch 3.409 (3.349) Remain 12:34:35 loss: 0.6287 Lr: 0.04905
[2023-08-07 20:45:19,062 INFO misc.py line 115 22900] Train: [14/100][54/156] Data 0.001 (0.001) Batch 3.151 (3.345) Remain 12:33:39 loss: 0.8524 Lr: 0.04905
[2023-08-07 20:45:22,521 INFO misc.py line 115 22900] Train: [14/100][55/156] Data 0.002 (0.001) Batch 3.458 (3.347) Remain 12:34:05 loss: 0.9843 Lr: 0.04905
[2023-08-07 20:45:26,241 INFO misc.py line 115 22900] Train: [14/100][56/156] Data 0.002 (0.001) Batch 3.720 (3.354) Remain 12:35:37 loss: 0.6258 Lr: 0.04905
[2023-08-07 20:45:28,805 INFO misc.py line 115 22900] Train: [14/100][57/156] Data 0.001 (0.001) Batch 2.564 (3.340) Remain 12:32:16 loss: 0.6663 Lr: 0.04905
[2023-08-07 20:45:32,806 INFO misc.py line 115 22900] Train: [14/100][58/156] Data 0.001 (0.001) Batch 4.001 (3.352) Remain 12:34:55 loss: 0.8530 Lr: 0.04905
[2023-08-07 20:45:37,382 INFO misc.py line 115 22900] Train: [14/100][59/156] Data 0.001 (0.001) Batch 4.576 (3.374) Remain 12:39:47 loss: 1.0871 Lr: 0.04905
[2023-08-07 20:45:40,589 INFO misc.py line 115 22900] Train: [14/100][60/156] Data 0.001 (0.001) Batch 3.208 (3.371) Remain 12:39:04 loss: 1.1893 Lr: 0.04904
[2023-08-07 20:45:43,640 INFO misc.py line 115 22900] Train: [14/100][61/156] Data 0.001 (0.001) Batch 3.050 (3.365) Remain 12:37:46 loss: 0.3892 Lr: 0.04904
[2023-08-07 20:45:47,584 INFO misc.py line 115 22900] Train: [14/100][62/156] Data 0.001 (0.001) Batch 3.945 (3.375) Remain 12:39:56 loss: 0.8242 Lr: 0.04904
[2023-08-07 20:45:51,572 INFO misc.py line 115 22900] Train: [14/100][63/156] Data 0.001 (0.001) Batch 3.987 (3.385) Remain 12:42:10 loss: 1.0625 Lr: 0.04904
[2023-08-07 20:45:54,308 INFO misc.py line 115 22900] Train: [14/100][64/156] Data 0.001 (0.001) Batch 2.736 (3.375) Remain 12:39:43 loss: 0.6220 Lr: 0.04904
[2023-08-07 20:45:58,378 INFO misc.py line 115 22900] Train: [14/100][65/156] Data 0.001 (0.001) Batch 4.070 (3.386) Remain 12:42:11 loss: 0.7494 Lr: 0.04904
[2023-08-07 20:46:01,348 INFO misc.py line 115 22900] Train: [14/100][66/156] Data 0.001 (0.001) Batch 2.970 (3.379) Remain 12:40:39 loss: 0.6926 Lr: 0.04903
[2023-08-07 20:46:04,755 INFO misc.py line 115 22900] Train: [14/100][67/156] Data 0.001 (0.001) Batch 3.407 (3.380) Remain 12:40:41 loss: 0.6792 Lr: 0.04903
[2023-08-07 20:46:07,164 INFO misc.py line 115 22900] Train: [14/100][68/156] Data 0.001 (0.001) Batch 2.409 (3.365) Remain 12:37:16 loss: 0.5284 Lr: 0.04903
[2023-08-07 20:46:10,275 INFO misc.py line 115 22900] Train: [14/100][69/156] Data 0.001 (0.001) Batch 3.111 (3.361) Remain 12:36:21 loss: 0.5334 Lr: 0.04903
[2023-08-07 20:46:14,325 INFO misc.py line 115 22900] Train: [14/100][70/156] Data 0.001 (0.001) Batch 4.050 (3.371) Remain 12:38:37 loss: 0.7811 Lr: 0.04903
[2023-08-07 20:46:18,135 INFO misc.py line 115 22900] Train: [14/100][71/156] Data 0.001 (0.001) Batch 3.809 (3.378) Remain 12:40:00 loss: 0.6724 Lr: 0.04903
[2023-08-07 20:46:20,874 INFO misc.py line 115 22900] Train: [14/100][72/156] Data 0.001 (0.001) Batch 2.740 (3.368) Remain 12:37:52 loss: 0.6011 Lr: 0.04903
[2023-08-07 20:46:24,720 INFO misc.py line 115 22900] Train: [14/100][73/156] Data 0.001 (0.001) Batch 3.846 (3.375) Remain 12:39:21 loss: 0.9550 Lr: 0.04902
[2023-08-07 20:46:26,739 INFO misc.py line 115 22900] Train: [14/100][74/156] Data 0.001 (0.001) Batch 2.019 (3.356) Remain 12:34:59 loss: 0.5709 Lr: 0.04902
[2023-08-07 20:46:30,704 INFO misc.py line 115 22900] Train: [14/100][75/156] Data 0.001 (0.001) Batch 3.965 (3.364) Remain 12:36:50 loss: 0.8455 Lr: 0.04902
[2023-08-07 20:46:32,766 INFO misc.py line 115 22900] Train: [14/100][76/156] Data 0.001 (0.001) Batch 2.062 (3.347) Remain 12:32:46 loss: 0.4763 Lr: 0.04902
[2023-08-07 20:46:36,776 INFO misc.py line 115 22900] Train: [14/100][77/156] Data 0.001 (0.001) Batch 4.011 (3.356) Remain 12:34:44 loss: 0.8991 Lr: 0.04902
[2023-08-07 20:46:40,846 INFO misc.py line 115 22900] Train: [14/100][78/156] Data 0.001 (0.001) Batch 4.070 (3.365) Remain 12:36:49 loss: 1.2896 Lr: 0.04902
[2023-08-07 20:46:43,584 INFO misc.py line 115 22900] Train: [14/100][79/156] Data 0.001 (0.001) Batch 2.738 (3.357) Remain 12:34:54 loss: 0.5572 Lr: 0.04902
[2023-08-07 20:46:47,073 INFO misc.py line 115 22900] Train: [14/100][80/156] Data 0.001 (0.001) Batch 3.488 (3.359) Remain 12:35:14 loss: 0.7629 Lr: 0.04901
[2023-08-07 20:46:51,141 INFO misc.py line 115 22900] Train: [14/100][81/156] Data 0.001 (0.001) Batch 4.068 (3.368) Remain 12:37:13 loss: 0.6487 Lr: 0.04901
[2023-08-07 20:46:54,124 INFO misc.py line 115 22900] Train: [14/100][82/156] Data 0.001 (0.001) Batch 2.983 (3.363) Remain 12:36:04 loss: 1.1361 Lr: 0.04901
[2023-08-07 20:46:57,702 INFO misc.py line 115 22900] Train: [14/100][83/156] Data 0.001 (0.001) Batch 3.578 (3.366) Remain 12:36:37 loss: 1.0575 Lr: 0.04901
[2023-08-07 20:47:01,766 INFO misc.py line 115 22900] Train: [14/100][84/156] Data 0.001 (0.001) Batch 4.063 (3.374) Remain 12:38:30 loss: 0.7316 Lr: 0.04901
[2023-08-07 20:47:05,222 INFO misc.py line 115 22900] Train: [14/100][85/156] Data 0.001 (0.001) Batch 3.456 (3.375) Remain 12:38:40 loss: 0.7432 Lr: 0.04901
[2023-08-07 20:47:08,468 INFO misc.py line 115 22900] Train: [14/100][86/156] Data 0.001 (0.001) Batch 3.246 (3.374) Remain 12:38:16 loss: 0.7325 Lr: 0.04901
[2023-08-07 20:47:12,038 INFO misc.py line 115 22900] Train: [14/100][87/156] Data 0.001 (0.001) Batch 3.570 (3.376) Remain 12:38:44 loss: 0.8173 Lr: 0.04900
[2023-08-07 20:47:15,335 INFO misc.py line 115 22900] Train: [14/100][88/156] Data 0.001 (0.001) Batch 3.297 (3.375) Remain 12:38:28 loss: 0.8258 Lr: 0.04900
[2023-08-07 20:47:19,464 INFO misc.py line 115 22900] Train: [14/100][89/156] Data 0.001 (0.001) Batch 4.129 (3.384) Remain 12:40:23 loss: 0.9781 Lr: 0.04900
[2023-08-07 20:47:22,678 INFO misc.py line 115 22900] Train: [14/100][90/156] Data 0.001 (0.001) Batch 3.214 (3.382) Remain 12:39:53 loss: 0.9046 Lr: 0.04900
[2023-08-07 20:47:25,676 INFO misc.py line 115 22900] Train: [14/100][91/156] Data 0.001 (0.001) Batch 2.998 (3.377) Remain 12:38:51 loss: 0.6624 Lr: 0.04900
[2023-08-07 20:47:29,766 INFO misc.py line 115 22900] Train: [14/100][92/156] Data 0.002 (0.001) Batch 4.089 (3.385) Remain 12:40:36 loss: 0.6638 Lr: 0.04900
[2023-08-07 20:47:32,907 INFO misc.py line 115 22900] Train: [14/100][93/156] Data 0.002 (0.001) Batch 3.142 (3.383) Remain 12:39:56 loss: 0.7869 Lr: 0.04900
[2023-08-07 20:47:36,564 INFO misc.py line 115 22900] Train: [14/100][94/156] Data 0.001 (0.001) Batch 3.656 (3.386) Remain 12:40:33 loss: 0.6266 Lr: 0.04899
[2023-08-07 20:47:40,339 INFO misc.py line 115 22900] Train: [14/100][95/156] Data 0.001 (0.001) Batch 3.776 (3.390) Remain 12:41:27 loss: 0.9703 Lr: 0.04899
[2023-08-07 20:47:43,683 INFO misc.py line 115 22900] Train: [14/100][96/156] Data 0.001 (0.001) Batch 3.343 (3.389) Remain 12:41:16 loss: 0.8385 Lr: 0.04899
[2023-08-07 20:47:47,052 INFO misc.py line 115 22900] Train: [14/100][97/156] Data 0.001 (0.001) Batch 3.370 (3.389) Remain 12:41:10 loss: 0.5969 Lr: 0.04899
[2023-08-07 20:47:51,394 INFO misc.py line 115 22900] Train: [14/100][98/156] Data 0.001 (0.001) Batch 4.341 (3.399) Remain 12:43:22 loss: 1.0807 Lr: 0.04899
[2023-08-07 20:47:54,573 INFO misc.py line 115 22900] Train: [14/100][99/156] Data 0.001 (0.001) Batch 3.180 (3.397) Remain 12:42:48 loss: 0.7014 Lr: 0.04899
[2023-08-07 20:47:58,214 INFO misc.py line 115 22900] Train: [14/100][100/156] Data 0.001 (0.001) Batch 3.640 (3.400) Remain 12:43:18 loss: 0.7396 Lr: 0.04898
[2023-08-07 20:48:01,610 INFO misc.py line 115 22900] Train: [14/100][101/156] Data 0.001 (0.001) Batch 3.396 (3.399) Remain 12:43:14 loss: 1.1959 Lr: 0.04898
[2023-08-07 20:48:05,039 INFO misc.py line 115 22900] Train: [14/100][102/156] Data 0.001 (0.001) Batch 3.430 (3.400) Remain 12:43:15 loss: 0.5950 Lr: 0.04898
[2023-08-07 20:48:08,696 INFO misc.py line 115 22900] Train: [14/100][103/156] Data 0.001 (0.001) Batch 3.657 (3.402) Remain 12:43:46 loss: 0.6918 Lr: 0.04898
[2023-08-07 20:48:12,340 INFO misc.py line 115 22900] Train: [14/100][104/156] Data 0.001 (0.001) Batch 3.645 (3.405) Remain 12:44:15 loss: 0.7528 Lr: 0.04898
[2023-08-07 20:48:16,321 INFO misc.py line 115 22900] Train: [14/100][105/156] Data 0.001 (0.001) Batch 3.981 (3.410) Remain 12:45:27 loss: 0.8722 Lr: 0.04898
[2023-08-07 20:48:19,193 INFO misc.py line 115 22900] Train: [14/100][106/156] Data 0.001 (0.001) Batch 2.872 (3.405) Remain 12:44:14 loss: 0.5381 Lr: 0.04898
[2023-08-07 20:48:22,631 INFO misc.py line 115 22900] Train: [14/100][107/156] Data 0.001 (0.001) Batch 3.438 (3.405) Remain 12:44:14 loss: 0.5791 Lr: 0.04897
[2023-08-07 20:48:26,618 INFO misc.py line 115 22900] Train: [14/100][108/156] Data 0.001 (0.001) Batch 3.987 (3.411) Remain 12:45:26 loss: 1.0108 Lr: 0.04897
[2023-08-07 20:48:30,776 INFO misc.py line 115 22900] Train: [14/100][109/156] Data 0.002 (0.001) Batch 4.158 (3.418) Remain 12:46:57 loss: 0.7048 Lr: 0.04897
[2023-08-07 20:48:34,471 INFO misc.py line 115 22900] Train: [14/100][110/156] Data 0.001 (0.001) Batch 3.696 (3.421) Remain 12:47:29 loss: 0.6065 Lr: 0.04897
[2023-08-07 20:48:38,508 INFO misc.py line 115 22900] Train: [14/100][111/156] Data 0.001 (0.001) Batch 4.037 (3.426) Remain 12:48:42 loss: 0.7692 Lr: 0.04897
[2023-08-07 20:48:42,194 INFO misc.py line 115 22900] Train: [14/100][112/156] Data 0.001 (0.001) Batch 3.686 (3.429) Remain 12:49:11 loss: 0.7272 Lr: 0.04897
[2023-08-07 20:48:45,749 INFO misc.py line 115 22900] Train: [14/100][113/156] Data 0.002 (0.001) Batch 3.555 (3.430) Remain 12:49:23 loss: 0.5477 Lr: 0.04897
[2023-08-07 20:48:49,210 INFO misc.py line 115 22900] Train: [14/100][114/156] Data 0.001 (0.001) Batch 3.460 (3.430) Remain 12:49:23 loss: 0.6797 Lr: 0.04896
[2023-08-07 20:48:52,639 INFO misc.py line 115 22900] Train: [14/100][115/156] Data 0.002 (0.001) Batch 3.430 (3.430) Remain 12:49:19 loss: 0.6634 Lr: 0.04896
[2023-08-07 20:48:57,550 INFO misc.py line 115 22900] Train: [14/100][116/156] Data 0.001 (0.001) Batch 4.911 (3.443) Remain 12:52:12 loss: 1.5607 Lr: 0.04896
[2023-08-07 20:49:00,133 INFO misc.py line 115 22900] Train: [14/100][117/156] Data 0.001 (0.001) Batch 2.583 (3.436) Remain 12:50:27 loss: 0.2824 Lr: 0.04896
[2023-08-07 20:49:03,585 INFO misc.py line 115 22900] Train: [14/100][118/156] Data 0.001 (0.001) Batch 3.452 (3.436) Remain 12:50:26 loss: 0.6262 Lr: 0.04896
[2023-08-07 20:49:06,593 INFO misc.py line 115 22900] Train: [14/100][119/156] Data 0.001 (0.001) Batch 3.007 (3.432) Remain 12:49:33 loss: 0.5452 Lr: 0.04896
[2023-08-07 20:49:10,383 INFO misc.py line 115 22900] Train: [14/100][120/156] Data 0.002 (0.001) Batch 3.790 (3.435) Remain 12:50:10 loss: 0.8883 Lr: 0.04895
[2023-08-07 20:49:13,980 INFO misc.py line 115 22900] Train: [14/100][121/156] Data 0.001 (0.001) Batch 3.597 (3.437) Remain 12:50:25 loss: 0.8323 Lr: 0.04895
[2023-08-07 20:49:18,257 INFO misc.py line 115 22900] Train: [14/100][122/156] Data 0.001 (0.001) Batch 4.277 (3.444) Remain 12:51:57 loss: 1.0627 Lr: 0.04895
[2023-08-07 20:49:21,493 INFO misc.py line 115 22900] Train: [14/100][123/156] Data 0.001 (0.001) Batch 3.237 (3.442) Remain 12:51:30 loss: 1.1125 Lr: 0.04895
[2023-08-07 20:49:24,668 INFO misc.py line 115 22900] Train: [14/100][124/156] Data 0.001 (0.001) Batch 3.174 (3.440) Remain 12:50:57 loss: 1.1552 Lr: 0.04895
[2023-08-07 20:49:27,729 INFO misc.py line 115 22900] Train: [14/100][125/156] Data 0.002 (0.001) Batch 3.062 (3.437) Remain 12:50:12 loss: 0.6879 Lr: 0.04895
[2023-08-07 20:49:29,970 INFO misc.py line 115 22900] Train: [14/100][126/156] Data 0.001 (0.001) Batch 2.241 (3.427) Remain 12:47:58 loss: 0.5264 Lr: 0.04895
[2023-08-07 20:49:33,564 INFO misc.py line 115 22900] Train: [14/100][127/156] Data 0.001 (0.001) Batch 3.593 (3.428) Remain 12:48:12 loss: 0.5872 Lr: 0.04894
[2023-08-07 20:49:36,722 INFO misc.py line 115 22900] Train: [14/100][128/156] Data 0.001 (0.001) Batch 3.158 (3.426) Remain 12:47:40 loss: 1.2513 Lr: 0.04894
[2023-08-07 20:49:40,634 INFO misc.py line 115 22900] Train: [14/100][129/156] Data 0.001 (0.001) Batch 3.912 (3.430) Remain 12:48:28 loss: 0.5981 Lr: 0.04894
[2023-08-07 20:49:44,744 INFO misc.py line 115 22900] Train: [14/100][130/156] Data 0.001 (0.001) Batch 4.110 (3.435) Remain 12:49:37 loss: 0.8994 Lr: 0.04894
[2023-08-07 20:49:48,253 INFO misc.py line 115 22900] Train: [14/100][131/156] Data 0.001 (0.001) Batch 3.509 (3.436) Remain 12:49:41 loss: 1.0430 Lr: 0.04894
[2023-08-07 20:49:52,361 INFO misc.py line 115 22900] Train: [14/100][132/156] Data 0.001 (0.001) Batch 4.108 (3.441) Remain 12:50:48 loss: 0.6607 Lr: 0.04894
[2023-08-07 20:49:55,520 INFO misc.py line 115 22900] Train: [14/100][133/156] Data 0.001 (0.001) Batch 3.159 (3.439) Remain 12:50:15 loss: 0.9495 Lr: 0.04893
[2023-08-07 20:49:59,691 INFO misc.py line 115 22900] Train: [14/100][134/156] Data 0.001 (0.001) Batch 4.170 (3.445) Remain 12:51:27 loss: 1.0285 Lr: 0.04893
[2023-08-07 20:50:03,008 INFO misc.py line 115 22900] Train: [14/100][135/156] Data 0.001 (0.001) Batch 3.318 (3.444) Remain 12:51:11 loss: 0.4395 Lr: 0.04893
[2023-08-07 20:50:06,143 INFO misc.py line 115 22900] Train: [14/100][136/156] Data 0.001 (0.001) Batch 3.135 (3.441) Remain 12:50:36 loss: 0.5970 Lr: 0.04893
[2023-08-07 20:50:10,285 INFO misc.py line 115 22900] Train: [14/100][137/156] Data 0.001 (0.001) Batch 4.141 (3.446) Remain 12:51:43 loss: 1.0191 Lr: 0.04893
[2023-08-07 20:50:13,361 INFO misc.py line 115 22900] Train: [14/100][138/156] Data 0.001 (0.001) Batch 3.076 (3.444) Remain 12:51:02 loss: 0.9118 Lr: 0.04893
[2023-08-07 20:50:16,749 INFO misc.py line 115 22900] Train: [14/100][139/156] Data 0.001 (0.001) Batch 3.387 (3.443) Remain 12:50:53 loss: 0.5490 Lr: 0.04893
[2023-08-07 20:50:20,268 INFO misc.py line 115 22900] Train: [14/100][140/156] Data 0.001 (0.001) Batch 3.520 (3.444) Remain 12:50:57 loss: 0.9382 Lr: 0.04892
[2023-08-07 20:50:24,256 INFO misc.py line 115 22900] Train: [14/100][141/156] Data 0.001 (0.001) Batch 3.988 (3.448) Remain 12:51:47 loss: 0.9057 Lr: 0.04892
[2023-08-07 20:50:27,144 INFO misc.py line 115 22900] Train: [14/100][142/156] Data 0.001 (0.001) Batch 2.888 (3.444) Remain 12:50:49 loss: 0.5268 Lr: 0.04892
[2023-08-07 20:50:30,365 INFO misc.py line 115 22900] Train: [14/100][143/156] Data 0.001 (0.001) Batch 3.221 (3.442) Remain 12:50:25 loss: 0.3665 Lr: 0.04892
[2023-08-07 20:50:34,825 INFO misc.py line 115 22900] Train: [14/100][144/156] Data 0.001 (0.001) Batch 4.460 (3.449) Remain 12:51:58 loss: 1.2151 Lr: 0.04892
[2023-08-07 20:50:38,774 INFO misc.py line 115 22900] Train: [14/100][145/156] Data 0.001 (0.001) Batch 3.949 (3.453) Remain 12:52:42 loss: 0.8375 Lr: 0.04892
[2023-08-07 20:50:42,579 INFO misc.py line 115 22900] Train: [14/100][146/156] Data 0.001 (0.001) Batch 3.805 (3.455) Remain 12:53:11 loss: 0.5215 Lr: 0.04891
[2023-08-07 20:50:45,782 INFO misc.py line 115 22900] Train: [14/100][147/156] Data 0.001 (0.001) Batch 3.203 (3.454) Remain 12:52:44 loss: 0.4946 Lr: 0.04891
[2023-08-07 20:50:50,334 INFO misc.py line 115 22900] Train: [14/100][148/156] Data 0.001 (0.001) Batch 4.553 (3.461) Remain 12:54:23 loss: 0.9474 Lr: 0.04891
[2023-08-07 20:50:53,052 INFO misc.py line 115 22900] Train: [14/100][149/156] Data 0.001 (0.001) Batch 2.718 (3.456) Remain 12:53:11 loss: 0.6554 Lr: 0.04891
[2023-08-07 20:50:57,011 INFO misc.py line 115 22900] Train: [14/100][150/156] Data 0.001 (0.001) Batch 3.959 (3.460) Remain 12:53:53 loss: 0.5937 Lr: 0.04891
[2023-08-07 20:51:00,315 INFO misc.py line 115 22900] Train: [14/100][151/156] Data 0.001 (0.001) Batch 3.304 (3.458) Remain 12:53:36 loss: 0.6611 Lr: 0.04891
[2023-08-07 20:51:04,001 INFO misc.py line 115 22900] Train: [14/100][152/156] Data 0.001 (0.001) Batch 3.686 (3.460) Remain 12:53:53 loss: 1.0176 Lr: 0.04891
[2023-08-07 20:51:08,129 INFO misc.py line 115 22900] Train: [14/100][153/156] Data 0.001 (0.001) Batch 4.128 (3.464) Remain 12:54:49 loss: 0.6800 Lr: 0.04890
[2023-08-07 20:51:10,921 INFO misc.py line 115 22900] Train: [14/100][154/156] Data 0.001 (0.001) Batch 2.792 (3.460) Remain 12:53:46 loss: 0.6592 Lr: 0.04890
[2023-08-07 20:51:14,248 INFO misc.py line 115 22900] Train: [14/100][155/156] Data 0.001 (0.001) Batch 3.327 (3.459) Remain 12:53:31 loss: 0.7803 Lr: 0.04890
[2023-08-07 20:51:18,564 INFO misc.py line 115 22900] Train: [14/100][156/156] Data 0.001 (0.001) Batch 4.316 (3.465) Remain 12:54:42 loss: 0.9354 Lr: 0.04890
[2023-08-07 20:51:18,564 INFO misc.py line 129 22900] Train result: loss: 0.7882 
[2023-08-07 20:51:18,564 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 20:51:20,720 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.7171 
[2023-08-07 20:51:21,589 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.7716 
[2023-08-07 20:51:23,256 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.9004 
[2023-08-07 20:51:24,779 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.2279 
[2023-08-07 20:51:26,624 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.7811 
[2023-08-07 20:51:28,289 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.9744 
[2023-08-07 20:51:30,434 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.5519 
[2023-08-07 20:51:32,242 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.8690 
[2023-08-07 20:51:33,527 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.3515 
[2023-08-07 20:51:35,657 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.2671 
[2023-08-07 20:51:36,183 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.6134 
[2023-08-07 20:51:37,719 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.8597 
[2023-08-07 20:51:40,430 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.0606 
[2023-08-07 20:51:42,110 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.9643 
[2023-08-07 20:51:44,135 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.6732 
[2023-08-07 20:51:46,845 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.1564 
[2023-08-07 20:51:49,554 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.1967 
[2023-08-07 20:51:51,403 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.6468 
[2023-08-07 20:51:52,155 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.1706 
[2023-08-07 20:51:53,040 INFO evaluator.py line 122 22900] Test: [20/24] Loss 1.1096 
[2023-08-07 20:51:55,302 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.1629 
[2023-08-07 20:51:57,273 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.8359 
[2023-08-07 20:51:59,121 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.1770 
[2023-08-07 20:52:01,054 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.4957 
[2023-08-07 20:52:01,119 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.1622/0.2516/0.6210.
[2023-08-07 20:52:01,120 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6139/0.8512
[2023-08-07 20:52:01,120 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9446/0.9904
[2023-08-07 20:52:01,120 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1092/0.3332
[2023-08-07 20:52:01,120 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.0461/0.0928
[2023-08-07 20:52:01,120 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.5844/0.7384
[2023-08-07 20:52:01,120 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.0589/0.0598
[2023-08-07 20:52:01,120 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.2071/0.2373
[2023-08-07 20:52:01,120 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0984/0.1247
[2023-08-07 20:52:01,120 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1169/0.4035
[2023-08-07 20:52:01,120 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0745/0.0762
[2023-08-07 20:52:01,120 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:52:01,120 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:52:01,120 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1349/0.6123
[2023-08-07 20:52:01,120 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.1011/0.1764
[2023-08-07 20:52:01,120 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0345/0.0459
[2023-08-07 20:52:01,120 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0004/0.0004
[2023-08-07 20:52:01,120 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:52:01,120 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.0634/0.1737
[2023-08-07 20:52:01,120 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 20:52:01,120 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0557/0.1151
[2023-08-07 20:52:01,120 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 20:52:01,121 INFO misc.py line 152 22900] Currently Best mIoU: 0.1686
[2023-08-07 20:52:01,121 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 20:52:06,997 INFO misc.py line 115 22900] Train: [15/100][1/156] Data 1.393 (1.393) Batch 5.047 (5.047) Remain 18:48:21 loss: 0.4703 Lr: 0.04890
[2023-08-07 20:52:09,989 INFO misc.py line 115 22900] Train: [15/100][2/156] Data 0.001 (0.001) Batch 2.992 (2.992) Remain 11:08:54 loss: 0.7555 Lr: 0.04890
[2023-08-07 20:52:13,199 INFO misc.py line 115 22900] Train: [15/100][3/156] Data 0.001 (0.001) Batch 3.210 (3.210) Remain 11:57:39 loss: 0.8439 Lr: 0.04889
[2023-08-07 20:52:16,799 INFO misc.py line 115 22900] Train: [15/100][4/156] Data 0.001 (0.001) Batch 3.600 (3.600) Remain 13:24:37 loss: 0.6307 Lr: 0.04889
[2023-08-07 20:52:20,596 INFO misc.py line 115 22900] Train: [15/100][5/156] Data 0.002 (0.001) Batch 3.797 (3.698) Remain 13:46:40 loss: 0.5709 Lr: 0.04889
[2023-08-07 20:52:23,828 INFO misc.py line 115 22900] Train: [15/100][6/156] Data 0.002 (0.001) Batch 3.232 (3.543) Remain 13:11:52 loss: 1.1591 Lr: 0.04889
[2023-08-07 20:52:26,942 INFO misc.py line 115 22900] Train: [15/100][7/156] Data 0.001 (0.001) Batch 3.113 (3.436) Remain 12:47:48 loss: 0.8059 Lr: 0.04889
[2023-08-07 20:52:30,278 INFO misc.py line 115 22900] Train: [15/100][8/156] Data 0.002 (0.001) Batch 3.337 (3.416) Remain 12:43:19 loss: 0.6539 Lr: 0.04889
[2023-08-07 20:52:33,479 INFO misc.py line 115 22900] Train: [15/100][9/156] Data 0.001 (0.001) Batch 3.201 (3.380) Remain 12:35:14 loss: 0.5676 Lr: 0.04889
[2023-08-07 20:52:36,660 INFO misc.py line 115 22900] Train: [15/100][10/156] Data 0.001 (0.001) Batch 3.180 (3.351) Remain 12:28:49 loss: 0.6821 Lr: 0.04888
[2023-08-07 20:52:40,205 INFO misc.py line 115 22900] Train: [15/100][11/156] Data 0.002 (0.001) Batch 3.546 (3.376) Remain 12:34:12 loss: 0.9690 Lr: 0.04888
[2023-08-07 20:52:44,382 INFO misc.py line 115 22900] Train: [15/100][12/156] Data 0.001 (0.001) Batch 4.177 (3.465) Remain 12:54:01 loss: 0.8113 Lr: 0.04888
[2023-08-07 20:52:47,612 INFO misc.py line 115 22900] Train: [15/100][13/156] Data 0.001 (0.001) Batch 3.230 (3.441) Remain 12:48:43 loss: 0.6104 Lr: 0.04888
[2023-08-07 20:52:49,803 INFO misc.py line 115 22900] Train: [15/100][14/156] Data 0.001 (0.001) Batch 2.191 (3.328) Remain 12:23:17 loss: 0.4626 Lr: 0.04888
[2023-08-07 20:52:53,606 INFO misc.py line 115 22900] Train: [15/100][15/156] Data 0.001 (0.001) Batch 3.804 (3.367) Remain 12:32:05 loss: 0.8365 Lr: 0.04888
[2023-08-07 20:52:57,077 INFO misc.py line 115 22900] Train: [15/100][16/156] Data 0.001 (0.001) Batch 3.471 (3.375) Remain 12:33:48 loss: 1.1927 Lr: 0.04887
[2023-08-07 20:52:59,201 INFO misc.py line 115 22900] Train: [15/100][17/156] Data 0.001 (0.001) Batch 2.124 (3.286) Remain 12:13:46 loss: 0.3849 Lr: 0.04887
[2023-08-07 20:53:02,836 INFO misc.py line 115 22900] Train: [15/100][18/156] Data 0.001 (0.001) Batch 3.636 (3.309) Remain 12:18:56 loss: 0.4753 Lr: 0.04887
[2023-08-07 20:53:05,413 INFO misc.py line 115 22900] Train: [15/100][19/156] Data 0.001 (0.001) Batch 2.577 (3.263) Remain 12:08:39 loss: 0.4971 Lr: 0.04887
[2023-08-07 20:53:09,220 INFO misc.py line 115 22900] Train: [15/100][20/156] Data 0.001 (0.001) Batch 3.807 (3.295) Remain 12:15:44 loss: 0.7236 Lr: 0.04887
[2023-08-07 20:53:12,965 INFO misc.py line 115 22900] Train: [15/100][21/156] Data 0.001 (0.001) Batch 3.745 (3.320) Remain 12:21:15 loss: 0.7789 Lr: 0.04887
[2023-08-07 20:53:16,640 INFO misc.py line 115 22900] Train: [15/100][22/156] Data 0.001 (0.001) Batch 3.675 (3.339) Remain 12:25:22 loss: 0.6177 Lr: 0.04886
[2023-08-07 20:53:19,564 INFO misc.py line 115 22900] Train: [15/100][23/156] Data 0.001 (0.001) Batch 2.924 (3.318) Remain 12:20:41 loss: 0.9345 Lr: 0.04886
[2023-08-07 20:53:23,137 INFO misc.py line 115 22900] Train: [15/100][24/156] Data 0.001 (0.001) Batch 3.573 (3.330) Remain 12:23:20 loss: 0.8639 Lr: 0.04886
[2023-08-07 20:53:27,181 INFO misc.py line 115 22900] Train: [15/100][25/156] Data 0.001 (0.001) Batch 4.045 (3.363) Remain 12:30:31 loss: 0.9942 Lr: 0.04886
[2023-08-07 20:53:30,334 INFO misc.py line 115 22900] Train: [15/100][26/156] Data 0.001 (0.001) Batch 3.153 (3.354) Remain 12:28:26 loss: 0.5974 Lr: 0.04886
[2023-08-07 20:53:34,370 INFO misc.py line 115 22900] Train: [15/100][27/156] Data 0.001 (0.001) Batch 4.036 (3.382) Remain 12:34:43 loss: 0.8503 Lr: 0.04886
[2023-08-07 20:53:38,605 INFO misc.py line 115 22900] Train: [15/100][28/156] Data 0.001 (0.001) Batch 4.235 (3.416) Remain 12:42:16 loss: 1.2929 Lr: 0.04886
[2023-08-07 20:53:40,632 INFO misc.py line 115 22900] Train: [15/100][29/156] Data 0.001 (0.001) Batch 2.026 (3.363) Remain 12:30:17 loss: 0.4447 Lr: 0.04885
[2023-08-07 20:53:43,870 INFO misc.py line 115 22900] Train: [15/100][30/156] Data 0.001 (0.001) Batch 3.239 (3.358) Remain 12:29:12 loss: 0.5766 Lr: 0.04885
[2023-08-07 20:53:48,404 INFO misc.py line 115 22900] Train: [15/100][31/156] Data 0.001 (0.001) Batch 4.534 (3.400) Remain 12:38:31 loss: 1.0696 Lr: 0.04885
[2023-08-07 20:53:52,022 INFO misc.py line 115 22900] Train: [15/100][32/156] Data 0.001 (0.001) Batch 3.618 (3.408) Remain 12:40:08 loss: 0.9194 Lr: 0.04885
[2023-08-07 20:53:56,022 INFO misc.py line 115 22900] Train: [15/100][33/156] Data 0.001 (0.001) Batch 4.000 (3.427) Remain 12:44:29 loss: 1.1089 Lr: 0.04885
[2023-08-07 20:54:00,581 INFO misc.py line 115 22900] Train: [15/100][34/156] Data 0.001 (0.001) Batch 4.560 (3.464) Remain 12:52:34 loss: 1.1421 Lr: 0.04885
[2023-08-07 20:54:04,097 INFO misc.py line 115 22900] Train: [15/100][35/156] Data 0.001 (0.001) Batch 3.515 (3.466) Remain 12:52:52 loss: 0.4772 Lr: 0.04884
[2023-08-07 20:54:07,180 INFO misc.py line 115 22900] Train: [15/100][36/156] Data 0.001 (0.001) Batch 3.083 (3.454) Remain 12:50:14 loss: 0.8311 Lr: 0.04884
[2023-08-07 20:54:11,658 INFO misc.py line 115 22900] Train: [15/100][37/156] Data 0.001 (0.001) Batch 4.478 (3.484) Remain 12:56:53 loss: 1.0767 Lr: 0.04884
[2023-08-07 20:54:15,751 INFO misc.py line 115 22900] Train: [15/100][38/156] Data 0.001 (0.001) Batch 4.094 (3.502) Remain 13:00:43 loss: 0.8526 Lr: 0.04884
[2023-08-07 20:54:20,506 INFO misc.py line 115 22900] Train: [15/100][39/156] Data 0.001 (0.001) Batch 4.754 (3.536) Remain 13:08:25 loss: 1.2070 Lr: 0.04884
[2023-08-07 20:54:24,270 INFO misc.py line 115 22900] Train: [15/100][40/156] Data 0.001 (0.001) Batch 3.764 (3.542) Remain 13:09:43 loss: 0.8303 Lr: 0.04884
[2023-08-07 20:54:28,781 INFO misc.py line 115 22900] Train: [15/100][41/156] Data 0.001 (0.001) Batch 4.512 (3.568) Remain 13:15:21 loss: 0.8834 Lr: 0.04883
[2023-08-07 20:54:32,628 INFO misc.py line 115 22900] Train: [15/100][42/156] Data 0.001 (0.001) Batch 3.847 (3.575) Remain 13:16:53 loss: 0.6309 Lr: 0.04883
[2023-08-07 20:54:34,822 INFO misc.py line 115 22900] Train: [15/100][43/156] Data 0.001 (0.001) Batch 2.194 (3.541) Remain 13:09:08 loss: 0.5938 Lr: 0.04883
[2023-08-07 20:54:37,654 INFO misc.py line 115 22900] Train: [15/100][44/156] Data 0.001 (0.001) Batch 2.832 (3.523) Remain 13:05:13 loss: 0.9535 Lr: 0.04883
[2023-08-07 20:54:41,411 INFO misc.py line 115 22900] Train: [15/100][45/156] Data 0.001 (0.001) Batch 3.757 (3.529) Remain 13:06:24 loss: 0.9999 Lr: 0.04883
[2023-08-07 20:54:44,772 INFO misc.py line 115 22900] Train: [15/100][46/156] Data 0.001 (0.001) Batch 3.361 (3.525) Remain 13:05:28 loss: 0.7636 Lr: 0.04883
[2023-08-07 20:54:48,183 INFO misc.py line 115 22900] Train: [15/100][47/156] Data 0.001 (0.001) Batch 3.411 (3.522) Remain 13:04:50 loss: 0.6511 Lr: 0.04883
[2023-08-07 20:54:52,025 INFO misc.py line 115 22900] Train: [15/100][48/156] Data 0.001 (0.001) Batch 3.842 (3.529) Remain 13:06:21 loss: 0.6028 Lr: 0.04882
[2023-08-07 20:54:56,131 INFO misc.py line 115 22900] Train: [15/100][49/156] Data 0.001 (0.001) Batch 4.106 (3.542) Remain 13:09:05 loss: 0.8204 Lr: 0.04882
[2023-08-07 20:54:59,875 INFO misc.py line 115 22900] Train: [15/100][50/156] Data 0.001 (0.001) Batch 3.744 (3.546) Remain 13:09:59 loss: 0.5821 Lr: 0.04882
[2023-08-07 20:55:03,507 INFO misc.py line 115 22900] Train: [15/100][51/156] Data 0.001 (0.001) Batch 3.632 (3.548) Remain 13:10:20 loss: 0.5448 Lr: 0.04882
[2023-08-07 20:55:06,662 INFO misc.py line 115 22900] Train: [15/100][52/156] Data 0.001 (0.001) Batch 3.155 (3.540) Remain 13:08:29 loss: 0.4329 Lr: 0.04882
[2023-08-07 20:55:10,632 INFO misc.py line 115 22900] Train: [15/100][53/156] Data 0.001 (0.001) Batch 3.970 (3.549) Remain 13:10:20 loss: 0.4675 Lr: 0.04882
[2023-08-07 20:55:14,786 INFO misc.py line 115 22900] Train: [15/100][54/156] Data 0.001 (0.001) Batch 4.154 (3.561) Remain 13:12:55 loss: 0.6421 Lr: 0.04881
[2023-08-07 20:55:17,576 INFO misc.py line 115 22900] Train: [15/100][55/156] Data 0.001 (0.001) Batch 2.790 (3.546) Remain 13:09:34 loss: 0.8267 Lr: 0.04881
[2023-08-07 20:55:21,184 INFO misc.py line 115 22900] Train: [15/100][56/156] Data 0.001 (0.001) Batch 3.608 (3.547) Remain 13:09:46 loss: 0.4676 Lr: 0.04881
[2023-08-07 20:55:24,297 INFO misc.py line 115 22900] Train: [15/100][57/156] Data 0.001 (0.001) Batch 3.113 (3.539) Remain 13:07:55 loss: 0.7151 Lr: 0.04881
[2023-08-07 20:55:26,977 INFO misc.py line 115 22900] Train: [15/100][58/156] Data 0.001 (0.001) Batch 2.680 (3.523) Remain 13:04:23 loss: 0.6571 Lr: 0.04881
[2023-08-07 20:55:30,293 INFO misc.py line 115 22900] Train: [15/100][59/156] Data 0.001 (0.001) Batch 3.316 (3.520) Remain 13:03:30 loss: 0.8396 Lr: 0.04881
[2023-08-07 20:55:33,506 INFO misc.py line 115 22900] Train: [15/100][60/156] Data 0.001 (0.001) Batch 3.213 (3.514) Remain 13:02:15 loss: 0.5675 Lr: 0.04880
[2023-08-07 20:55:36,957 INFO misc.py line 115 22900] Train: [15/100][61/156] Data 0.001 (0.001) Batch 3.451 (3.513) Remain 13:01:57 loss: 0.8156 Lr: 0.04880
[2023-08-07 20:55:41,057 INFO misc.py line 115 22900] Train: [15/100][62/156] Data 0.001 (0.001) Batch 4.100 (3.523) Remain 13:04:06 loss: 0.8437 Lr: 0.04880
[2023-08-07 20:55:44,434 INFO misc.py line 115 22900] Train: [15/100][63/156] Data 0.001 (0.001) Batch 3.377 (3.521) Remain 13:03:30 loss: 0.5533 Lr: 0.04880
[2023-08-07 20:55:48,186 INFO misc.py line 115 22900] Train: [15/100][64/156] Data 0.001 (0.001) Batch 3.752 (3.524) Remain 13:04:17 loss: 0.7177 Lr: 0.04880
[2023-08-07 20:55:51,763 INFO misc.py line 115 22900] Train: [15/100][65/156] Data 0.001 (0.001) Batch 3.577 (3.525) Remain 13:04:25 loss: 0.7178 Lr: 0.04880
[2023-08-07 20:55:55,841 INFO misc.py line 115 22900] Train: [15/100][66/156] Data 0.001 (0.001) Batch 4.078 (3.534) Remain 13:06:18 loss: 0.9853 Lr: 0.04879
[2023-08-07 20:55:58,584 INFO misc.py line 115 22900] Train: [15/100][67/156] Data 0.001 (0.001) Batch 2.743 (3.522) Remain 13:03:30 loss: 1.3698 Lr: 0.04879
[2023-08-07 20:56:02,080 INFO misc.py line 115 22900] Train: [15/100][68/156] Data 0.001 (0.001) Batch 3.496 (3.521) Remain 13:03:21 loss: 0.6763 Lr: 0.04879
[2023-08-07 20:56:05,324 INFO misc.py line 115 22900] Train: [15/100][69/156] Data 0.001 (0.001) Batch 3.243 (3.517) Remain 13:02:21 loss: 0.5413 Lr: 0.04879
[2023-08-07 20:56:08,076 INFO misc.py line 115 22900] Train: [15/100][70/156] Data 0.001 (0.001) Batch 2.752 (3.506) Remain 12:59:46 loss: 0.3664 Lr: 0.04879
[2023-08-07 20:56:12,192 INFO misc.py line 115 22900] Train: [15/100][71/156] Data 0.001 (0.001) Batch 4.116 (3.515) Remain 13:01:42 loss: 0.9852 Lr: 0.04879
[2023-08-07 20:56:15,564 INFO misc.py line 115 22900] Train: [15/100][72/156] Data 0.001 (0.001) Batch 3.371 (3.513) Remain 13:01:11 loss: 0.6557 Lr: 0.04878
[2023-08-07 20:56:18,844 INFO misc.py line 115 22900] Train: [15/100][73/156] Data 0.001 (0.001) Batch 3.280 (3.509) Remain 13:00:23 loss: 0.2989 Lr: 0.04878
[2023-08-07 20:56:22,491 INFO misc.py line 115 22900] Train: [15/100][74/156] Data 0.001 (0.001) Batch 3.647 (3.511) Remain 13:00:45 loss: 0.8689 Lr: 0.04878
[2023-08-07 20:56:26,605 INFO misc.py line 115 22900] Train: [15/100][75/156] Data 0.001 (0.001) Batch 4.114 (3.520) Remain 13:02:33 loss: 0.7690 Lr: 0.04878
[2023-08-07 20:56:30,297 INFO misc.py line 115 22900] Train: [15/100][76/156] Data 0.001 (0.001) Batch 3.693 (3.522) Remain 13:03:02 loss: 0.8574 Lr: 0.04878
[2023-08-07 20:56:34,225 INFO misc.py line 115 22900] Train: [15/100][77/156] Data 0.001 (0.001) Batch 3.928 (3.527) Remain 13:04:11 loss: 0.6300 Lr: 0.04878
[2023-08-07 20:56:38,200 INFO misc.py line 115 22900] Train: [15/100][78/156] Data 0.001 (0.001) Batch 3.975 (3.533) Remain 13:05:27 loss: 1.2563 Lr: 0.04877
[2023-08-07 20:56:42,297 INFO misc.py line 115 22900] Train: [15/100][79/156] Data 0.001 (0.001) Batch 4.096 (3.541) Remain 13:07:03 loss: 0.5225 Lr: 0.04877
[2023-08-07 20:56:45,853 INFO misc.py line 115 22900] Train: [15/100][80/156] Data 0.001 (0.001) Batch 3.557 (3.541) Remain 13:07:02 loss: 0.5581 Lr: 0.04877
[2023-08-07 20:56:49,252 INFO misc.py line 115 22900] Train: [15/100][81/156] Data 0.001 (0.001) Batch 3.398 (3.539) Remain 13:06:34 loss: 0.9580 Lr: 0.04877
[2023-08-07 20:56:52,332 INFO misc.py line 115 22900] Train: [15/100][82/156] Data 0.001 (0.001) Batch 3.080 (3.533) Remain 13:05:13 loss: 0.8600 Lr: 0.04877
[2023-08-07 20:56:54,995 INFO misc.py line 115 22900] Train: [15/100][83/156] Data 0.001 (0.001) Batch 2.663 (3.522) Remain 13:02:44 loss: 0.7519 Lr: 0.04877
[2023-08-07 20:56:58,390 INFO misc.py line 115 22900] Train: [15/100][84/156] Data 0.001 (0.001) Batch 3.395 (3.521) Remain 13:02:20 loss: 0.9276 Lr: 0.04876
[2023-08-07 20:57:02,454 INFO misc.py line 115 22900] Train: [15/100][85/156] Data 0.001 (0.001) Batch 4.064 (3.528) Remain 13:03:45 loss: 0.5667 Lr: 0.04876
[2023-08-07 20:57:05,375 INFO misc.py line 115 22900] Train: [15/100][86/156] Data 0.001 (0.001) Batch 2.920 (3.520) Remain 13:02:04 loss: 0.2726 Lr: 0.04876
[2023-08-07 20:57:08,581 INFO misc.py line 115 22900] Train: [15/100][87/156] Data 0.001 (0.001) Batch 3.206 (3.516) Remain 13:01:10 loss: 0.5355 Lr: 0.04876
[2023-08-07 20:57:11,717 INFO misc.py line 115 22900] Train: [15/100][88/156] Data 0.001 (0.001) Batch 3.137 (3.512) Remain 13:00:07 loss: 0.7684 Lr: 0.04876
[2023-08-07 20:57:15,506 INFO misc.py line 115 22900] Train: [15/100][89/156] Data 0.001 (0.001) Batch 3.789 (3.515) Remain 13:00:47 loss: 0.8828 Lr: 0.04876
[2023-08-07 20:57:18,909 INFO misc.py line 115 22900] Train: [15/100][90/156] Data 0.001 (0.001) Batch 3.402 (3.514) Remain 13:00:26 loss: 1.0444 Lr: 0.04876
[2023-08-07 20:57:21,733 INFO misc.py line 115 22900] Train: [15/100][91/156] Data 0.001 (0.001) Batch 2.824 (3.506) Remain 12:58:38 loss: 0.4552 Lr: 0.04875
[2023-08-07 20:57:26,346 INFO misc.py line 115 22900] Train: [15/100][92/156] Data 0.001 (0.001) Batch 4.613 (3.519) Remain 13:01:20 loss: 1.0325 Lr: 0.04875
[2023-08-07 20:57:29,326 INFO misc.py line 115 22900] Train: [15/100][93/156] Data 0.001 (0.001) Batch 2.980 (3.513) Remain 12:59:57 loss: 0.8610 Lr: 0.04875
[2023-08-07 20:57:33,072 INFO misc.py line 115 22900] Train: [15/100][94/156] Data 0.001 (0.001) Batch 3.746 (3.515) Remain 13:00:28 loss: 0.8016 Lr: 0.04875
[2023-08-07 20:57:35,921 INFO misc.py line 115 22900] Train: [15/100][95/156] Data 0.001 (0.001) Batch 2.849 (3.508) Remain 12:58:48 loss: 0.7844 Lr: 0.04875
[2023-08-07 20:57:38,136 INFO misc.py line 115 22900] Train: [15/100][96/156] Data 0.001 (0.001) Batch 2.214 (3.494) Remain 12:55:39 loss: 0.6000 Lr: 0.04875
[2023-08-07 20:57:42,164 INFO misc.py line 115 22900] Train: [15/100][97/156] Data 0.001 (0.001) Batch 4.028 (3.500) Remain 12:56:51 loss: 0.7741 Lr: 0.04874
[2023-08-07 20:57:46,252 INFO misc.py line 115 22900] Train: [15/100][98/156] Data 0.001 (0.001) Batch 4.088 (3.506) Remain 12:58:10 loss: 0.8008 Lr: 0.04874
[2023-08-07 20:57:50,265 INFO misc.py line 115 22900] Train: [15/100][99/156] Data 0.001 (0.001) Batch 4.013 (3.511) Remain 12:59:17 loss: 0.7074 Lr: 0.04874
[2023-08-07 20:57:53,162 INFO misc.py line 115 22900] Train: [15/100][100/156] Data 0.001 (0.001) Batch 2.898 (3.505) Remain 12:57:49 loss: 0.8286 Lr: 0.04874
[2023-08-07 20:57:55,857 INFO misc.py line 115 22900] Train: [15/100][101/156] Data 0.001 (0.001) Batch 2.695 (3.497) Remain 12:55:56 loss: 0.7726 Lr: 0.04874
[2023-08-07 20:57:57,208 INFO misc.py line 115 22900] Train: [15/100][102/156] Data 0.001 (0.001) Batch 1.351 (3.475) Remain 12:51:03 loss: 0.6816 Lr: 0.04874
[2023-08-07 20:58:00,572 INFO misc.py line 115 22900] Train: [15/100][103/156] Data 0.001 (0.001) Batch 3.365 (3.474) Remain 12:50:45 loss: 0.6302 Lr: 0.04873
[2023-08-07 20:58:05,030 INFO misc.py line 115 22900] Train: [15/100][104/156] Data 0.001 (0.001) Batch 4.457 (3.483) Remain 12:52:51 loss: 0.8572 Lr: 0.04873
[2023-08-07 20:58:09,577 INFO misc.py line 115 22900] Train: [15/100][105/156] Data 0.001 (0.001) Batch 4.548 (3.494) Remain 12:55:07 loss: 0.8989 Lr: 0.04873
[2023-08-07 20:58:12,822 INFO misc.py line 115 22900] Train: [15/100][106/156] Data 0.001 (0.001) Batch 3.245 (3.491) Remain 12:54:31 loss: 0.8116 Lr: 0.04873
[2023-08-07 20:58:17,033 INFO misc.py line 115 22900] Train: [15/100][107/156] Data 0.001 (0.001) Batch 4.211 (3.498) Remain 12:56:00 loss: 0.7386 Lr: 0.04873
[2023-08-07 20:58:19,340 INFO misc.py line 115 22900] Train: [15/100][108/156] Data 0.001 (0.001) Batch 2.308 (3.487) Remain 12:53:25 loss: 0.3903 Lr: 0.04873
[2023-08-07 20:58:22,693 INFO misc.py line 115 22900] Train: [15/100][109/156] Data 0.001 (0.001) Batch 3.353 (3.486) Remain 12:53:05 loss: 0.9673 Lr: 0.04872
[2023-08-07 20:58:25,674 INFO misc.py line 115 22900] Train: [15/100][110/156] Data 0.001 (0.001) Batch 2.981 (3.481) Remain 12:51:59 loss: 0.7009 Lr: 0.04872
[2023-08-07 20:58:28,889 INFO misc.py line 115 22900] Train: [15/100][111/156] Data 0.001 (0.001) Batch 3.215 (3.479) Remain 12:51:22 loss: 0.6979 Lr: 0.04872
[2023-08-07 20:58:32,739 INFO misc.py line 115 22900] Train: [15/100][112/156] Data 0.001 (0.001) Batch 3.851 (3.482) Remain 12:52:04 loss: 0.8451 Lr: 0.04872
[2023-08-07 20:58:36,116 INFO misc.py line 115 22900] Train: [15/100][113/156] Data 0.001 (0.001) Batch 3.376 (3.481) Remain 12:51:48 loss: 0.5888 Lr: 0.04872
[2023-08-07 20:58:38,811 INFO misc.py line 115 22900] Train: [15/100][114/156] Data 0.001 (0.001) Batch 2.695 (3.474) Remain 12:50:10 loss: 0.3420 Lr: 0.04872
[2023-08-07 20:58:42,607 INFO misc.py line 115 22900] Train: [15/100][115/156] Data 0.001 (0.001) Batch 3.796 (3.477) Remain 12:50:45 loss: 0.7620 Lr: 0.04871
[2023-08-07 20:58:45,875 INFO misc.py line 115 22900] Train: [15/100][116/156] Data 0.001 (0.001) Batch 3.268 (3.475) Remain 12:50:17 loss: 0.6838 Lr: 0.04871
[2023-08-07 20:58:49,597 INFO misc.py line 115 22900] Train: [15/100][117/156] Data 0.001 (0.001) Batch 3.722 (3.477) Remain 12:50:42 loss: 0.6156 Lr: 0.04871
[2023-08-07 20:58:53,546 INFO misc.py line 115 22900] Train: [15/100][118/156] Data 0.001 (0.001) Batch 3.948 (3.481) Remain 12:51:33 loss: 0.7445 Lr: 0.04871
[2023-08-07 20:58:56,691 INFO misc.py line 115 22900] Train: [15/100][119/156] Data 0.001 (0.001) Batch 3.145 (3.478) Remain 12:50:52 loss: 0.7512 Lr: 0.04871
[2023-08-07 20:59:00,126 INFO misc.py line 115 22900] Train: [15/100][120/156] Data 0.001 (0.001) Batch 3.435 (3.478) Remain 12:50:43 loss: 0.5524 Lr: 0.04871
[2023-08-07 20:59:03,875 INFO misc.py line 115 22900] Train: [15/100][121/156] Data 0.001 (0.001) Batch 3.749 (3.480) Remain 12:51:10 loss: 0.9543 Lr: 0.04870
[2023-08-07 20:59:07,490 INFO misc.py line 115 22900] Train: [15/100][122/156] Data 0.001 (0.001) Batch 3.615 (3.481) Remain 12:51:22 loss: 0.6561 Lr: 0.04870
[2023-08-07 20:59:10,583 INFO misc.py line 115 22900] Train: [15/100][123/156] Data 0.001 (0.001) Batch 3.093 (3.478) Remain 12:50:35 loss: 0.7174 Lr: 0.04870
[2023-08-07 20:59:13,832 INFO misc.py line 115 22900] Train: [15/100][124/156] Data 0.001 (0.001) Batch 3.248 (3.476) Remain 12:50:06 loss: 1.1402 Lr: 0.04870
[2023-08-07 20:59:17,918 INFO misc.py line 115 22900] Train: [15/100][125/156] Data 0.001 (0.001) Batch 4.086 (3.481) Remain 12:51:09 loss: 1.0938 Lr: 0.04870
[2023-08-07 20:59:20,979 INFO misc.py line 115 22900] Train: [15/100][126/156] Data 0.001 (0.001) Batch 3.061 (3.478) Remain 12:50:21 loss: 0.8626 Lr: 0.04869
[2023-08-07 20:59:24,440 INFO misc.py line 115 22900] Train: [15/100][127/156] Data 0.001 (0.001) Batch 3.461 (3.478) Remain 12:50:15 loss: 0.6191 Lr: 0.04869
[2023-08-07 20:59:26,985 INFO misc.py line 115 22900] Train: [15/100][128/156] Data 0.001 (0.001) Batch 2.545 (3.470) Remain 12:48:33 loss: 0.9656 Lr: 0.04869
[2023-08-07 20:59:30,418 INFO misc.py line 115 22900] Train: [15/100][129/156] Data 0.001 (0.001) Batch 3.433 (3.470) Remain 12:48:25 loss: 0.7609 Lr: 0.04869
[2023-08-07 20:59:32,630 INFO misc.py line 115 22900] Train: [15/100][130/156] Data 0.001 (0.001) Batch 2.212 (3.460) Remain 12:46:10 loss: 0.4542 Lr: 0.04869
[2023-08-07 20:59:35,803 INFO misc.py line 115 22900] Train: [15/100][131/156] Data 0.001 (0.001) Batch 3.173 (3.458) Remain 12:45:37 loss: 0.4391 Lr: 0.04869
[2023-08-07 20:59:39,119 INFO misc.py line 115 22900] Train: [15/100][132/156] Data 0.001 (0.001) Batch 3.316 (3.457) Remain 12:45:19 loss: 0.6574 Lr: 0.04868
[2023-08-07 20:59:43,212 INFO misc.py line 115 22900] Train: [15/100][133/156] Data 0.001 (0.001) Batch 4.093 (3.462) Remain 12:46:20 loss: 1.0066 Lr: 0.04868
[2023-08-07 20:59:47,182 INFO misc.py line 115 22900] Train: [15/100][134/156] Data 0.001 (0.001) Batch 3.970 (3.466) Remain 12:47:09 loss: 1.1647 Lr: 0.04868
[2023-08-07 20:59:50,342 INFO misc.py line 115 22900] Train: [15/100][135/156] Data 0.001 (0.001) Batch 3.160 (3.463) Remain 12:46:34 loss: 0.8255 Lr: 0.04868
[2023-08-07 20:59:54,383 INFO misc.py line 115 22900] Train: [15/100][136/156] Data 0.001 (0.001) Batch 4.040 (3.468) Remain 12:47:28 loss: 0.8090 Lr: 0.04868
[2023-08-07 20:59:57,392 INFO misc.py line 115 22900] Train: [15/100][137/156] Data 0.001 (0.001) Batch 3.009 (3.464) Remain 12:46:40 loss: 0.6862 Lr: 0.04868
[2023-08-07 21:00:00,830 INFO misc.py line 115 22900] Train: [15/100][138/156] Data 0.001 (0.001) Batch 3.438 (3.464) Remain 12:46:34 loss: 0.5888 Lr: 0.04867
[2023-08-07 21:00:04,034 INFO misc.py line 115 22900] Train: [15/100][139/156] Data 0.001 (0.001) Batch 3.203 (3.462) Remain 12:46:05 loss: 0.6923 Lr: 0.04867
[2023-08-07 21:00:07,988 INFO misc.py line 115 22900] Train: [15/100][140/156] Data 0.001 (0.001) Batch 3.954 (3.466) Remain 12:46:49 loss: 0.6546 Lr: 0.04867
[2023-08-07 21:00:10,587 INFO misc.py line 115 22900] Train: [15/100][141/156] Data 0.001 (0.001) Batch 2.599 (3.459) Remain 12:45:22 loss: 0.4718 Lr: 0.04867
[2023-08-07 21:00:13,723 INFO misc.py line 115 22900] Train: [15/100][142/156] Data 0.001 (0.001) Batch 3.135 (3.457) Remain 12:44:48 loss: 0.5005 Lr: 0.04867
[2023-08-07 21:00:17,727 INFO misc.py line 115 22900] Train: [15/100][143/156] Data 0.001 (0.001) Batch 4.005 (3.461) Remain 12:45:36 loss: 0.9922 Lr: 0.04867
[2023-08-07 21:00:21,478 INFO misc.py line 115 22900] Train: [15/100][144/156] Data 0.001 (0.001) Batch 3.750 (3.463) Remain 12:46:00 loss: 0.5029 Lr: 0.04866
[2023-08-07 21:00:24,098 INFO misc.py line 115 22900] Train: [15/100][145/156] Data 0.001 (0.001) Batch 2.620 (3.457) Remain 12:44:38 loss: 0.7913 Lr: 0.04866
[2023-08-07 21:00:27,014 INFO misc.py line 115 22900] Train: [15/100][146/156] Data 0.001 (0.001) Batch 2.917 (3.453) Remain 12:43:44 loss: 0.4084 Lr: 0.04866
[2023-08-07 21:00:30,721 INFO misc.py line 115 22900] Train: [15/100][147/156] Data 0.001 (0.001) Batch 3.707 (3.455) Remain 12:44:04 loss: 0.5567 Lr: 0.04866
[2023-08-07 21:00:34,459 INFO misc.py line 115 22900] Train: [15/100][148/156] Data 0.001 (0.001) Batch 3.738 (3.457) Remain 12:44:27 loss: 0.7960 Lr: 0.04866
[2023-08-07 21:00:38,703 INFO misc.py line 115 22900] Train: [15/100][149/156] Data 0.001 (0.001) Batch 4.243 (3.462) Remain 12:45:35 loss: 0.8974 Lr: 0.04866
[2023-08-07 21:00:42,984 INFO misc.py line 115 22900] Train: [15/100][150/156] Data 0.001 (0.001) Batch 4.282 (3.468) Remain 12:46:45 loss: 1.1513 Lr: 0.04865
[2023-08-07 21:00:45,670 INFO misc.py line 115 22900] Train: [15/100][151/156] Data 0.001 (0.001) Batch 2.685 (3.463) Remain 12:45:31 loss: 0.8798 Lr: 0.04865
[2023-08-07 21:00:49,236 INFO misc.py line 115 22900] Train: [15/100][152/156] Data 0.001 (0.001) Batch 3.566 (3.463) Remain 12:45:37 loss: 0.6148 Lr: 0.04865
[2023-08-07 21:00:53,356 INFO misc.py line 115 22900] Train: [15/100][153/156] Data 0.001 (0.001) Batch 4.120 (3.468) Remain 12:46:32 loss: 1.0724 Lr: 0.04865
[2023-08-07 21:00:56,447 INFO misc.py line 115 22900] Train: [15/100][154/156] Data 0.001 (0.001) Batch 3.092 (3.465) Remain 12:45:55 loss: 0.4436 Lr: 0.04865
[2023-08-07 21:00:59,928 INFO misc.py line 115 22900] Train: [15/100][155/156] Data 0.001 (0.001) Batch 3.480 (3.465) Remain 12:45:53 loss: 0.7718 Lr: 0.04865
[2023-08-07 21:01:03,316 INFO misc.py line 115 22900] Train: [15/100][156/156] Data 0.001 (0.001) Batch 3.388 (3.465) Remain 12:45:43 loss: 0.7459 Lr: 0.04864
[2023-08-07 21:01:03,316 INFO misc.py line 129 22900] Train result: loss: 0.7487 
[2023-08-07 21:01:03,316 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 21:01:05,449 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.6341 
[2023-08-07 21:01:06,318 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.7373 
[2023-08-07 21:01:07,983 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.9239 
[2023-08-07 21:01:09,503 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.0630 
[2023-08-07 21:01:11,346 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.6302 
[2023-08-07 21:01:13,010 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.7817 
[2023-08-07 21:01:15,148 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.6688 
[2023-08-07 21:01:16,952 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.7654 
[2023-08-07 21:01:18,236 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.1559 
[2023-08-07 21:01:20,366 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3888 
[2023-08-07 21:01:20,893 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.6349 
[2023-08-07 21:01:22,424 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.8361 
[2023-08-07 21:01:25,132 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.1610 
[2023-08-07 21:01:26,811 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.8648 
[2023-08-07 21:01:28,832 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.5365 
[2023-08-07 21:01:31,540 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.0980 
[2023-08-07 21:01:34,249 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.3392 
[2023-08-07 21:01:36,095 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.5579 
[2023-08-07 21:01:36,843 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.3279 
[2023-08-07 21:01:37,728 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.9469 
[2023-08-07 21:01:39,991 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.4322 
[2023-08-07 21:01:41,958 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.7619 
[2023-08-07 21:01:43,803 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.2304 
[2023-08-07 21:01:45,737 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.3733 
[2023-08-07 21:01:45,787 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.1719/0.2537/0.6498.
[2023-08-07 21:01:45,787 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6537/0.9585
[2023-08-07 21:01:45,787 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9319/0.9752
[2023-08-07 21:01:45,787 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1166/0.2761
[2023-08-07 21:01:45,787 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.0843/0.3793
[2023-08-07 21:01:45,787 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.5924/0.8413
[2023-08-07 21:01:45,787 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.0749/0.0805
[2023-08-07 21:01:45,787 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.3061/0.3666
[2023-08-07 21:01:45,787 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0380/0.0397
[2023-08-07 21:01:45,787 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.0792/0.1881
[2023-08-07 21:01:45,787 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0702/0.0716
[2023-08-07 21:01:45,787 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 21:01:45,787 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0000/0.0000
[2023-08-07 21:01:45,787 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.0968/0.1894
[2023-08-07 21:01:45,787 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0975/0.1160
[2023-08-07 21:01:45,787 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0007/0.0008
[2023-08-07 21:01:45,787 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-07 21:01:45,787 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0000/0.0000
[2023-08-07 21:01:45,787 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.2311/0.4437
[2023-08-07 21:01:45,787 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 21:01:45,787 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0638/0.1465
[2023-08-07 21:01:45,788 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 21:01:45,788 INFO misc.py line 150 22900] Best validation mIoU updated to: 0.1719
[2023-08-07 21:01:45,788 INFO misc.py line 152 22900] Currently Best mIoU: 0.1719
[2023-08-07 21:01:45,788 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 21:01:51,800 INFO misc.py line 115 22900] Train: [16/100][1/156] Data 0.474 (0.474) Batch 4.987 (4.987) Remain 18:22:03 loss: 1.2589 Lr: 0.04864
[2023-08-07 21:01:55,793 INFO misc.py line 115 22900] Train: [16/100][2/156] Data 0.001 (0.001) Batch 3.993 (3.993) Remain 14:42:25 loss: 0.9255 Lr: 0.04864
[2023-08-07 21:01:59,570 INFO misc.py line 115 22900] Train: [16/100][3/156] Data 0.001 (0.001) Batch 3.777 (3.777) Remain 13:54:28 loss: 0.4953 Lr: 0.04864
[2023-08-07 21:02:03,716 INFO misc.py line 115 22900] Train: [16/100][4/156] Data 0.001 (0.001) Batch 4.146 (4.146) Remain 15:15:56 loss: 0.8880 Lr: 0.04864
[2023-08-07 21:02:07,179 INFO misc.py line 115 22900] Train: [16/100][5/156] Data 0.001 (0.001) Batch 3.463 (3.804) Remain 14:00:26 loss: 0.7645 Lr: 0.04864
[2023-08-07 21:02:10,038 INFO misc.py line 115 22900] Train: [16/100][6/156] Data 0.001 (0.001) Batch 2.859 (3.489) Remain 12:50:45 loss: 0.6757 Lr: 0.04863
[2023-08-07 21:02:12,839 INFO misc.py line 115 22900] Train: [16/100][7/156] Data 0.001 (0.001) Batch 2.801 (3.317) Remain 12:12:41 loss: 0.5009 Lr: 0.04863
[2023-08-07 21:02:16,787 INFO misc.py line 115 22900] Train: [16/100][8/156] Data 0.001 (0.001) Batch 3.948 (3.443) Remain 12:40:31 loss: 0.6265 Lr: 0.04863
[2023-08-07 21:02:20,452 INFO misc.py line 115 22900] Train: [16/100][9/156] Data 0.001 (0.001) Batch 3.665 (3.480) Remain 12:48:37 loss: 0.7399 Lr: 0.04863
[2023-08-07 21:02:23,568 INFO misc.py line 115 22900] Train: [16/100][10/156] Data 0.001 (0.001) Batch 3.115 (3.428) Remain 12:37:03 loss: 0.4555 Lr: 0.04863
[2023-08-07 21:02:26,467 INFO misc.py line 115 22900] Train: [16/100][11/156] Data 0.001 (0.001) Batch 2.900 (3.362) Remain 12:22:25 loss: 0.6860 Lr: 0.04862
[2023-08-07 21:02:30,439 INFO misc.py line 115 22900] Train: [16/100][12/156] Data 0.001 (0.001) Batch 3.972 (3.430) Remain 12:37:18 loss: 0.9088 Lr: 0.04862
[2023-08-07 21:02:33,527 INFO misc.py line 115 22900] Train: [16/100][13/156] Data 0.001 (0.001) Batch 3.088 (3.396) Remain 12:29:42 loss: 0.6478 Lr: 0.04862
[2023-08-07 21:02:36,076 INFO misc.py line 115 22900] Train: [16/100][14/156] Data 0.001 (0.001) Batch 2.549 (3.319) Remain 12:12:39 loss: 0.4113 Lr: 0.04862
[2023-08-07 21:02:39,269 INFO misc.py line 115 22900] Train: [16/100][15/156] Data 0.001 (0.001) Batch 3.192 (3.308) Remain 12:10:17 loss: 0.7397 Lr: 0.04862
[2023-08-07 21:02:42,534 INFO misc.py line 115 22900] Train: [16/100][16/156] Data 0.001 (0.001) Batch 3.266 (3.305) Remain 12:09:30 loss: 0.6322 Lr: 0.04862
[2023-08-07 21:02:45,842 INFO misc.py line 115 22900] Train: [16/100][17/156] Data 0.001 (0.001) Batch 3.308 (3.305) Remain 12:09:30 loss: 1.0879 Lr: 0.04861
[2023-08-07 21:02:48,247 INFO misc.py line 115 22900] Train: [16/100][18/156] Data 0.001 (0.001) Batch 2.405 (3.245) Remain 11:56:12 loss: 0.6182 Lr: 0.04861
[2023-08-07 21:02:52,098 INFO misc.py line 115 22900] Train: [16/100][19/156] Data 0.001 (0.001) Batch 3.850 (3.283) Remain 12:04:29 loss: 0.6137 Lr: 0.04861
[2023-08-07 21:02:55,659 INFO misc.py line 115 22900] Train: [16/100][20/156] Data 0.001 (0.001) Batch 3.561 (3.299) Remain 12:08:03 loss: 0.6751 Lr: 0.04861
[2023-08-07 21:02:58,422 INFO misc.py line 115 22900] Train: [16/100][21/156] Data 0.001 (0.001) Batch 2.763 (3.270) Remain 12:01:25 loss: 0.3366 Lr: 0.04861
[2023-08-07 21:03:00,135 INFO misc.py line 115 22900] Train: [16/100][22/156] Data 0.001 (0.001) Batch 1.713 (3.188) Remain 11:43:17 loss: 0.5439 Lr: 0.04861
[2023-08-07 21:03:04,172 INFO misc.py line 115 22900] Train: [16/100][23/156] Data 0.001 (0.001) Batch 4.037 (3.230) Remain 11:52:36 loss: 0.5268 Lr: 0.04860
[2023-08-07 21:03:08,308 INFO misc.py line 115 22900] Train: [16/100][24/156] Data 0.001 (0.001) Batch 4.135 (3.273) Remain 12:02:04 loss: 0.6533 Lr: 0.04860
[2023-08-07 21:03:11,372 INFO misc.py line 115 22900] Train: [16/100][25/156] Data 0.001 (0.001) Batch 3.065 (3.264) Remain 11:59:55 loss: 0.5720 Lr: 0.04860
[2023-08-07 21:03:14,635 INFO misc.py line 115 22900] Train: [16/100][26/156] Data 0.001 (0.001) Batch 3.263 (3.264) Remain 11:59:51 loss: 0.5510 Lr: 0.04860
[2023-08-07 21:03:18,732 INFO misc.py line 115 22900] Train: [16/100][27/156] Data 0.001 (0.001) Batch 4.097 (3.298) Remain 12:07:28 loss: 0.8163 Lr: 0.04860
[2023-08-07 21:03:23,104 INFO misc.py line 115 22900] Train: [16/100][28/156] Data 0.001 (0.001) Batch 4.372 (3.341) Remain 12:16:52 loss: 0.9304 Lr: 0.04860
[2023-08-07 21:03:27,099 INFO misc.py line 115 22900] Train: [16/100][29/156] Data 0.001 (0.001) Batch 3.995 (3.366) Remain 12:22:21 loss: 0.6009 Lr: 0.04859
[2023-08-07 21:03:30,934 INFO misc.py line 115 22900] Train: [16/100][30/156] Data 0.001 (0.001) Batch 3.835 (3.384) Remain 12:26:08 loss: 0.8828 Lr: 0.04859
[2023-08-07 21:03:35,244 INFO misc.py line 115 22900] Train: [16/100][31/156] Data 0.001 (0.001) Batch 4.310 (3.417) Remain 12:33:22 loss: 0.7956 Lr: 0.04859
[2023-08-07 21:03:38,554 INFO misc.py line 115 22900] Train: [16/100][32/156] Data 0.001 (0.001) Batch 3.310 (3.413) Remain 12:32:30 loss: 0.6582 Lr: 0.04859
[2023-08-07 21:03:42,222 INFO misc.py line 115 22900] Train: [16/100][33/156] Data 0.001 (0.001) Batch 3.669 (3.422) Remain 12:34:19 loss: 0.6223 Lr: 0.04859
[2023-08-07 21:03:45,168 INFO misc.py line 115 22900] Train: [16/100][34/156] Data 0.001 (0.001) Batch 2.945 (3.406) Remain 12:30:52 loss: 0.5810 Lr: 0.04858
[2023-08-07 21:03:49,206 INFO misc.py line 115 22900] Train: [16/100][35/156] Data 0.001 (0.001) Batch 4.038 (3.426) Remain 12:35:10 loss: 1.0364 Lr: 0.04858
[2023-08-07 21:03:51,459 INFO misc.py line 115 22900] Train: [16/100][36/156] Data 0.001 (0.001) Batch 2.254 (3.391) Remain 12:27:17 loss: 0.7560 Lr: 0.04858
[2023-08-07 21:03:54,122 INFO misc.py line 115 22900] Train: [16/100][37/156] Data 0.001 (0.001) Batch 2.663 (3.369) Remain 12:22:30 loss: 0.7079 Lr: 0.04858
[2023-08-07 21:03:57,925 INFO misc.py line 115 22900] Train: [16/100][38/156] Data 0.001 (0.001) Batch 3.803 (3.382) Remain 12:25:11 loss: 0.7452 Lr: 0.04858
[2023-08-07 21:04:01,015 INFO misc.py line 115 22900] Train: [16/100][39/156] Data 0.001 (0.001) Batch 3.090 (3.373) Remain 12:23:20 loss: 0.8939 Lr: 0.04858
[2023-08-07 21:04:04,429 INFO misc.py line 115 22900] Train: [16/100][40/156] Data 0.001 (0.001) Batch 3.414 (3.375) Remain 12:23:31 loss: 0.7155 Lr: 0.04857
[2023-08-07 21:04:07,418 INFO misc.py line 115 22900] Train: [16/100][41/156] Data 0.001 (0.001) Batch 2.988 (3.364) Remain 12:21:14 loss: 0.4043 Lr: 0.04857
[2023-08-07 21:04:10,108 INFO misc.py line 115 22900] Train: [16/100][42/156] Data 0.001 (0.001) Batch 2.690 (3.347) Remain 12:17:22 loss: 0.6902 Lr: 0.04857
[2023-08-07 21:04:13,781 INFO misc.py line 115 22900] Train: [16/100][43/156] Data 0.001 (0.001) Batch 3.673 (3.355) Remain 12:19:06 loss: 0.6504 Lr: 0.04857
[2023-08-07 21:04:16,880 INFO misc.py line 115 22900] Train: [16/100][44/156] Data 0.001 (0.001) Batch 3.099 (3.349) Remain 12:17:40 loss: 0.9222 Lr: 0.04857
[2023-08-07 21:04:20,908 INFO misc.py line 115 22900] Train: [16/100][45/156] Data 0.001 (0.001) Batch 4.028 (3.365) Remain 12:21:10 loss: 0.7376 Lr: 0.04857
[2023-08-07 21:04:24,899 INFO misc.py line 115 22900] Train: [16/100][46/156] Data 0.001 (0.001) Batch 3.991 (3.380) Remain 12:24:19 loss: 1.3159 Lr: 0.04856
[2023-08-07 21:04:28,697 INFO misc.py line 115 22900] Train: [16/100][47/156] Data 0.001 (0.001) Batch 3.798 (3.389) Remain 12:26:21 loss: 1.0696 Lr: 0.04856
[2023-08-07 21:04:31,644 INFO misc.py line 115 22900] Train: [16/100][48/156] Data 0.001 (0.001) Batch 2.947 (3.379) Remain 12:24:08 loss: 0.6738 Lr: 0.04856
[2023-08-07 21:04:34,662 INFO misc.py line 115 22900] Train: [16/100][49/156] Data 0.001 (0.001) Batch 3.018 (3.372) Remain 12:22:21 loss: 0.5183 Lr: 0.04856
[2023-08-07 21:04:38,163 INFO misc.py line 115 22900] Train: [16/100][50/156] Data 0.001 (0.001) Batch 3.501 (3.374) Remain 12:22:54 loss: 0.8375 Lr: 0.04856
[2023-08-07 21:04:41,795 INFO misc.py line 115 22900] Train: [16/100][51/156] Data 0.001 (0.001) Batch 3.632 (3.380) Remain 12:24:02 loss: 0.7870 Lr: 0.04855
[2023-08-07 21:04:45,456 INFO misc.py line 115 22900] Train: [16/100][52/156] Data 0.001 (0.001) Batch 3.661 (3.385) Remain 12:25:14 loss: 0.5881 Lr: 0.04855
[2023-08-07 21:04:49,354 INFO misc.py line 115 22900] Train: [16/100][53/156] Data 0.001 (0.001) Batch 3.898 (3.396) Remain 12:27:26 loss: 0.8406 Lr: 0.04855
[2023-08-07 21:04:52,462 INFO misc.py line 115 22900] Train: [16/100][54/156] Data 0.001 (0.001) Batch 3.108 (3.390) Remain 12:26:08 loss: 0.6003 Lr: 0.04855
[2023-08-07 21:04:56,501 INFO misc.py line 115 22900] Train: [16/100][55/156] Data 0.001 (0.001) Batch 4.038 (3.403) Remain 12:28:50 loss: 0.8502 Lr: 0.04855
[2023-08-07 21:05:00,051 INFO misc.py line 115 22900] Train: [16/100][56/156] Data 0.001 (0.001) Batch 3.551 (3.405) Remain 12:29:23 loss: 0.5349 Lr: 0.04855
[2023-08-07 21:05:03,225 INFO misc.py line 115 22900] Train: [16/100][57/156] Data 0.001 (0.001) Batch 3.174 (3.401) Remain 12:28:23 loss: 0.6348 Lr: 0.04854
[2023-08-07 21:05:07,264 INFO misc.py line 115 22900] Train: [16/100][58/156] Data 0.001 (0.001) Batch 4.039 (3.413) Remain 12:30:53 loss: 1.0804 Lr: 0.04854
[2023-08-07 21:05:10,815 INFO misc.py line 115 22900] Train: [16/100][59/156] Data 0.001 (0.001) Batch 3.550 (3.415) Remain 12:31:22 loss: 0.6605 Lr: 0.04854
[2023-08-07 21:05:14,478 INFO misc.py line 115 22900] Train: [16/100][60/156] Data 0.001 (0.001) Batch 3.663 (3.419) Remain 12:32:16 loss: 0.8077 Lr: 0.04854
[2023-08-07 21:05:18,356 INFO misc.py line 115 22900] Train: [16/100][61/156] Data 0.001 (0.001) Batch 3.878 (3.427) Remain 12:33:57 loss: 0.8646 Lr: 0.04854
[2023-08-07 21:05:21,643 INFO misc.py line 115 22900] Train: [16/100][62/156] Data 0.001 (0.001) Batch 3.287 (3.425) Remain 12:33:22 loss: 0.9873 Lr: 0.04853
[2023-08-07 21:05:25,388 INFO misc.py line 115 22900] Train: [16/100][63/156] Data 0.001 (0.001) Batch 3.745 (3.430) Remain 12:34:29 loss: 0.7690 Lr: 0.04853
[2023-08-07 21:05:28,136 INFO misc.py line 115 22900] Train: [16/100][64/156] Data 0.001 (0.001) Batch 2.748 (3.419) Remain 12:31:58 loss: 0.4986 Lr: 0.04853
[2023-08-07 21:05:31,491 INFO misc.py line 115 22900] Train: [16/100][65/156] Data 0.001 (0.001) Batch 3.355 (3.418) Remain 12:31:41 loss: 0.8243 Lr: 0.04853
[2023-08-07 21:05:35,137 INFO misc.py line 115 22900] Train: [16/100][66/156] Data 0.001 (0.001) Batch 3.646 (3.422) Remain 12:32:25 loss: 0.7436 Lr: 0.04853
[2023-08-07 21:05:39,490 INFO misc.py line 115 22900] Train: [16/100][67/156] Data 0.001 (0.001) Batch 4.352 (3.436) Remain 12:35:34 loss: 0.9347 Lr: 0.04853
[2023-08-07 21:05:43,244 INFO misc.py line 115 22900] Train: [16/100][68/156] Data 0.001 (0.001) Batch 3.754 (3.441) Remain 12:36:35 loss: 0.9765 Lr: 0.04852
[2023-08-07 21:05:47,212 INFO misc.py line 115 22900] Train: [16/100][69/156] Data 0.001 (0.001) Batch 3.968 (3.449) Remain 12:38:17 loss: 0.7731 Lr: 0.04852
[2023-08-07 21:05:49,641 INFO misc.py line 115 22900] Train: [16/100][70/156] Data 0.001 (0.001) Batch 2.428 (3.434) Remain 12:34:52 loss: 0.6125 Lr: 0.04852
[2023-08-07 21:05:53,167 INFO misc.py line 115 22900] Train: [16/100][71/156] Data 0.001 (0.001) Batch 3.526 (3.435) Remain 12:35:07 loss: 0.4846 Lr: 0.04852
[2023-08-07 21:05:57,240 INFO misc.py line 115 22900] Train: [16/100][72/156] Data 0.001 (0.001) Batch 4.073 (3.444) Remain 12:37:05 loss: 0.8864 Lr: 0.04852
[2023-08-07 21:06:00,553 INFO misc.py line 115 22900] Train: [16/100][73/156] Data 0.001 (0.001) Batch 3.313 (3.443) Remain 12:36:37 loss: 0.6356 Lr: 0.04852
[2023-08-07 21:06:03,439 INFO misc.py line 115 22900] Train: [16/100][74/156] Data 0.001 (0.001) Batch 2.886 (3.435) Remain 12:34:50 loss: 0.7439 Lr: 0.04851
[2023-08-07 21:06:06,726 INFO misc.py line 115 22900] Train: [16/100][75/156] Data 0.001 (0.001) Batch 3.287 (3.433) Remain 12:34:20 loss: 0.8129 Lr: 0.04851
[2023-08-07 21:06:09,480 INFO misc.py line 115 22900] Train: [16/100][76/156] Data 0.001 (0.001) Batch 2.754 (3.423) Remain 12:32:14 loss: 0.4255 Lr: 0.04851
[2023-08-07 21:06:14,138 INFO misc.py line 115 22900] Train: [16/100][77/156] Data 0.001 (0.001) Batch 4.658 (3.440) Remain 12:35:50 loss: 0.9225 Lr: 0.04851
[2023-08-07 21:06:18,156 INFO misc.py line 115 22900] Train: [16/100][78/156] Data 0.001 (0.001) Batch 4.019 (3.448) Remain 12:37:29 loss: 0.7674 Lr: 0.04851
[2023-08-07 21:06:21,844 INFO misc.py line 115 22900] Train: [16/100][79/156] Data 0.001 (0.001) Batch 3.687 (3.451) Remain 12:38:07 loss: 0.5988 Lr: 0.04850
[2023-08-07 21:06:25,485 INFO misc.py line 115 22900] Train: [16/100][80/156] Data 0.001 (0.001) Batch 3.641 (3.453) Remain 12:38:36 loss: 0.7150 Lr: 0.04850
[2023-08-07 21:06:28,851 INFO misc.py line 115 22900] Train: [16/100][81/156] Data 0.001 (0.001) Batch 3.366 (3.452) Remain 12:38:18 loss: 0.7577 Lr: 0.04850
[2023-08-07 21:06:32,588 INFO misc.py line 115 22900] Train: [16/100][82/156] Data 0.001 (0.001) Batch 3.737 (3.456) Remain 12:39:02 loss: 0.4628 Lr: 0.04850
[2023-08-07 21:06:36,128 INFO misc.py line 115 22900] Train: [16/100][83/156] Data 0.001 (0.001) Batch 3.539 (3.457) Remain 12:39:12 loss: 0.6224 Lr: 0.04850
[2023-08-07 21:06:39,825 INFO misc.py line 115 22900] Train: [16/100][84/156] Data 0.001 (0.001) Batch 3.698 (3.460) Remain 12:39:48 loss: 0.6472 Lr: 0.04850
[2023-08-07 21:06:43,101 INFO misc.py line 115 22900] Train: [16/100][85/156] Data 0.001 (0.001) Batch 3.275 (3.458) Remain 12:39:15 loss: 0.3686 Lr: 0.04849
[2023-08-07 21:06:46,699 INFO misc.py line 115 22900] Train: [16/100][86/156] Data 0.001 (0.001) Batch 3.598 (3.459) Remain 12:39:33 loss: 0.8739 Lr: 0.04849
[2023-08-07 21:06:49,991 INFO misc.py line 115 22900] Train: [16/100][87/156] Data 0.001 (0.001) Batch 3.292 (3.457) Remain 12:39:04 loss: 0.7974 Lr: 0.04849
[2023-08-07 21:06:54,245 INFO misc.py line 115 22900] Train: [16/100][88/156] Data 0.001 (0.001) Batch 4.254 (3.467) Remain 12:41:04 loss: 0.7098 Lr: 0.04849
[2023-08-07 21:06:58,172 INFO misc.py line 115 22900] Train: [16/100][89/156] Data 0.001 (0.001) Batch 3.927 (3.472) Remain 12:42:11 loss: 0.6578 Lr: 0.04849
[2023-08-07 21:07:01,125 INFO misc.py line 115 22900] Train: [16/100][90/156] Data 0.001 (0.001) Batch 2.952 (3.466) Remain 12:40:49 loss: 0.7328 Lr: 0.04848
[2023-08-07 21:07:04,722 INFO misc.py line 115 22900] Train: [16/100][91/156] Data 0.001 (0.001) Batch 3.597 (3.468) Remain 12:41:05 loss: 0.5972 Lr: 0.04848
[2023-08-07 21:07:08,178 INFO misc.py line 115 22900] Train: [16/100][92/156] Data 0.001 (0.001) Batch 3.456 (3.467) Remain 12:41:00 loss: 0.7212 Lr: 0.04848
[2023-08-07 21:07:12,241 INFO misc.py line 115 22900] Train: [16/100][93/156] Data 0.001 (0.001) Batch 4.064 (3.474) Remain 12:42:23 loss: 0.6492 Lr: 0.04848
[2023-08-07 21:07:14,943 INFO misc.py line 115 22900] Train: [16/100][94/156] Data 0.001 (0.001) Batch 2.702 (3.466) Remain 12:40:28 loss: 0.6718 Lr: 0.04848
[2023-08-07 21:07:18,705 INFO misc.py line 115 22900] Train: [16/100][95/156] Data 0.001 (0.001) Batch 3.762 (3.469) Remain 12:41:07 loss: 0.8430 Lr: 0.04848
[2023-08-07 21:07:22,257 INFO misc.py line 115 22900] Train: [16/100][96/156] Data 0.001 (0.001) Batch 3.552 (3.470) Remain 12:41:15 loss: 0.5916 Lr: 0.04847
[2023-08-07 21:07:25,623 INFO misc.py line 115 22900] Train: [16/100][97/156] Data 0.001 (0.001) Batch 3.366 (3.469) Remain 12:40:57 loss: 0.6209 Lr: 0.04847
[2023-08-07 21:07:28,938 INFO misc.py line 115 22900] Train: [16/100][98/156] Data 0.001 (0.001) Batch 3.315 (3.467) Remain 12:40:32 loss: 0.6141 Lr: 0.04847
[2023-08-07 21:07:31,827 INFO misc.py line 115 22900] Train: [16/100][99/156] Data 0.001 (0.001) Batch 2.889 (3.461) Remain 12:39:10 loss: 0.5510 Lr: 0.04847
[2023-08-07 21:07:35,037 INFO misc.py line 115 22900] Train: [16/100][100/156] Data 0.001 (0.001) Batch 3.210 (3.458) Remain 12:38:32 loss: 1.0046 Lr: 0.04847
[2023-08-07 21:07:36,987 INFO misc.py line 115 22900] Train: [16/100][101/156] Data 0.001 (0.001) Batch 1.950 (3.443) Remain 12:35:06 loss: 0.2293 Lr: 0.04846
[2023-08-07 21:07:41,428 INFO misc.py line 115 22900] Train: [16/100][102/156] Data 0.001 (0.001) Batch 4.441 (3.453) Remain 12:37:16 loss: 1.0143 Lr: 0.04846
[2023-08-07 21:07:44,913 INFO misc.py line 115 22900] Train: [16/100][103/156] Data 0.001 (0.001) Batch 3.485 (3.453) Remain 12:37:16 loss: 0.6335 Lr: 0.04846
[2023-08-07 21:07:48,901 INFO misc.py line 115 22900] Train: [16/100][104/156] Data 0.001 (0.001) Batch 3.988 (3.459) Remain 12:38:22 loss: 0.6136 Lr: 0.04846
[2023-08-07 21:07:52,984 INFO misc.py line 115 22900] Train: [16/100][105/156] Data 0.001 (0.001) Batch 4.083 (3.465) Remain 12:39:39 loss: 0.5752 Lr: 0.04846
[2023-08-07 21:07:57,075 INFO misc.py line 115 22900] Train: [16/100][106/156] Data 0.001 (0.001) Batch 4.091 (3.471) Remain 12:40:56 loss: 0.5911 Lr: 0.04846
[2023-08-07 21:08:00,065 INFO misc.py line 115 22900] Train: [16/100][107/156] Data 0.001 (0.001) Batch 2.990 (3.466) Remain 12:39:52 loss: 0.8203 Lr: 0.04845
[2023-08-07 21:08:04,032 INFO misc.py line 115 22900] Train: [16/100][108/156] Data 0.001 (0.001) Batch 3.967 (3.471) Remain 12:40:51 loss: 0.7234 Lr: 0.04845
[2023-08-07 21:08:07,126 INFO misc.py line 115 22900] Train: [16/100][109/156] Data 0.001 (0.001) Batch 3.094 (3.468) Remain 12:40:01 loss: 0.7872 Lr: 0.04845
[2023-08-07 21:08:10,930 INFO misc.py line 115 22900] Train: [16/100][110/156] Data 0.001 (0.001) Batch 3.804 (3.471) Remain 12:40:39 loss: 0.7837 Lr: 0.04845
[2023-08-07 21:08:12,905 INFO misc.py line 115 22900] Train: [16/100][111/156] Data 0.001 (0.001) Batch 1.976 (3.457) Remain 12:37:33 loss: 0.6048 Lr: 0.04845
[2023-08-07 21:08:16,874 INFO misc.py line 115 22900] Train: [16/100][112/156] Data 0.001 (0.001) Batch 3.969 (3.461) Remain 12:38:31 loss: 0.8702 Lr: 0.04844
[2023-08-07 21:08:19,360 INFO misc.py line 115 22900] Train: [16/100][113/156] Data 0.001 (0.001) Batch 2.486 (3.453) Remain 12:36:31 loss: 0.1522 Lr: 0.04844
[2023-08-07 21:08:22,902 INFO misc.py line 115 22900] Train: [16/100][114/156] Data 0.001 (0.001) Batch 3.542 (3.453) Remain 12:36:38 loss: 0.4917 Lr: 0.04844
[2023-08-07 21:08:25,780 INFO misc.py line 115 22900] Train: [16/100][115/156] Data 0.001 (0.001) Batch 2.878 (3.448) Remain 12:35:27 loss: 0.4638 Lr: 0.04844
[2023-08-07 21:08:29,633 INFO misc.py line 115 22900] Train: [16/100][116/156] Data 0.001 (0.001) Batch 3.853 (3.452) Remain 12:36:11 loss: 0.5404 Lr: 0.04844
[2023-08-07 21:08:32,947 INFO misc.py line 115 22900] Train: [16/100][117/156] Data 0.001 (0.001) Batch 3.314 (3.451) Remain 12:35:52 loss: 0.9243 Lr: 0.04844
[2023-08-07 21:08:36,886 INFO misc.py line 115 22900] Train: [16/100][118/156] Data 0.001 (0.001) Batch 3.939 (3.455) Remain 12:36:44 loss: 0.9008 Lr: 0.04843
[2023-08-07 21:08:39,642 INFO misc.py line 115 22900] Train: [16/100][119/156] Data 0.001 (0.001) Batch 2.756 (3.449) Remain 12:35:21 loss: 0.7556 Lr: 0.04843
[2023-08-07 21:08:43,327 INFO misc.py line 115 22900] Train: [16/100][120/156] Data 0.001 (0.001) Batch 3.684 (3.451) Remain 12:35:44 loss: 0.4781 Lr: 0.04843
[2023-08-07 21:08:47,418 INFO misc.py line 115 22900] Train: [16/100][121/156] Data 0.001 (0.001) Batch 4.091 (3.456) Remain 12:36:52 loss: 0.8938 Lr: 0.04843
[2023-08-07 21:08:51,737 INFO misc.py line 115 22900] Train: [16/100][122/156] Data 0.001 (0.001) Batch 4.320 (3.464) Remain 12:38:24 loss: 1.0640 Lr: 0.04843
[2023-08-07 21:08:54,500 INFO misc.py line 115 22900] Train: [16/100][123/156] Data 0.001 (0.001) Batch 2.762 (3.458) Remain 12:37:04 loss: 0.7012 Lr: 0.04842
[2023-08-07 21:08:57,366 INFO misc.py line 115 22900] Train: [16/100][124/156] Data 0.001 (0.001) Batch 2.866 (3.453) Remain 12:35:56 loss: 0.6230 Lr: 0.04842
[2023-08-07 21:09:00,759 INFO misc.py line 115 22900] Train: [16/100][125/156] Data 0.001 (0.001) Batch 3.393 (3.452) Remain 12:35:46 loss: 0.4767 Lr: 0.04842
[2023-08-07 21:09:03,609 INFO misc.py line 115 22900] Train: [16/100][126/156] Data 0.001 (0.001) Batch 2.850 (3.447) Remain 12:34:39 loss: 0.6044 Lr: 0.04842
[2023-08-07 21:09:07,590 INFO misc.py line 115 22900] Train: [16/100][127/156] Data 0.001 (0.001) Batch 3.982 (3.452) Remain 12:35:32 loss: 0.6377 Lr: 0.04842
[2023-08-07 21:09:10,292 INFO misc.py line 115 22900] Train: [16/100][128/156] Data 0.001 (0.001) Batch 2.702 (3.446) Remain 12:34:09 loss: 0.3454 Lr: 0.04841
[2023-08-07 21:09:13,224 INFO misc.py line 115 22900] Train: [16/100][129/156] Data 0.001 (0.001) Batch 2.932 (3.442) Remain 12:33:12 loss: 0.8973 Lr: 0.04841
[2023-08-07 21:09:16,481 INFO misc.py line 115 22900] Train: [16/100][130/156] Data 0.005 (0.001) Batch 3.256 (3.440) Remain 12:32:50 loss: 0.5363 Lr: 0.04841
[2023-08-07 21:09:19,670 INFO misc.py line 115 22900] Train: [16/100][131/156] Data 0.001 (0.001) Batch 3.190 (3.438) Remain 12:32:21 loss: 0.4492 Lr: 0.04841
[2023-08-07 21:09:23,789 INFO misc.py line 115 22900] Train: [16/100][132/156] Data 0.001 (0.001) Batch 4.119 (3.444) Remain 12:33:27 loss: 0.8138 Lr: 0.04841
[2023-08-07 21:09:27,417 INFO misc.py line 115 22900] Train: [16/100][133/156] Data 0.001 (0.001) Batch 3.628 (3.445) Remain 12:33:42 loss: 0.6380 Lr: 0.04841
[2023-08-07 21:09:30,006 INFO misc.py line 115 22900] Train: [16/100][134/156] Data 0.001 (0.001) Batch 2.589 (3.438) Remain 12:32:12 loss: 0.5038 Lr: 0.04840
[2023-08-07 21:09:34,094 INFO misc.py line 115 22900] Train: [16/100][135/156] Data 0.001 (0.001) Batch 4.088 (3.443) Remain 12:33:14 loss: 0.5843 Lr: 0.04840
[2023-08-07 21:09:38,138 INFO misc.py line 115 22900] Train: [16/100][136/156] Data 0.001 (0.001) Batch 4.044 (3.448) Remain 12:34:09 loss: 0.9797 Lr: 0.04840
[2023-08-07 21:09:41,285 INFO misc.py line 115 22900] Train: [16/100][137/156] Data 0.001 (0.001) Batch 3.147 (3.446) Remain 12:33:37 loss: 0.8007 Lr: 0.04840
[2023-08-07 21:09:45,102 INFO misc.py line 115 22900] Train: [16/100][138/156] Data 0.001 (0.001) Batch 3.817 (3.448) Remain 12:34:09 loss: 0.4274 Lr: 0.04840
[2023-08-07 21:09:48,498 INFO misc.py line 115 22900] Train: [16/100][139/156] Data 0.001 (0.001) Batch 3.397 (3.448) Remain 12:34:01 loss: 0.8507 Lr: 0.04839
[2023-08-07 21:09:51,406 INFO misc.py line 115 22900] Train: [16/100][140/156] Data 0.001 (0.001) Batch 2.908 (3.444) Remain 12:33:06 loss: 0.6396 Lr: 0.04839
[2023-08-07 21:09:55,669 INFO misc.py line 115 22900] Train: [16/100][141/156] Data 0.001 (0.001) Batch 4.263 (3.450) Remain 12:34:20 loss: 0.7558 Lr: 0.04839
[2023-08-07 21:09:58,268 INFO misc.py line 115 22900] Train: [16/100][142/156] Data 0.001 (0.001) Batch 2.599 (3.444) Remain 12:32:56 loss: 0.7232 Lr: 0.04839
[2023-08-07 21:10:02,368 INFO misc.py line 115 22900] Train: [16/100][143/156] Data 0.001 (0.001) Batch 4.100 (3.449) Remain 12:33:54 loss: 0.7124 Lr: 0.04839
[2023-08-07 21:10:05,466 INFO misc.py line 115 22900] Train: [16/100][144/156] Data 0.001 (0.001) Batch 3.099 (3.446) Remain 12:33:18 loss: 0.6457 Lr: 0.04838
[2023-08-07 21:10:08,773 INFO misc.py line 115 22900] Train: [16/100][145/156] Data 0.001 (0.001) Batch 3.306 (3.445) Remain 12:33:02 loss: 0.5479 Lr: 0.04838
[2023-08-07 21:10:12,667 INFO misc.py line 115 22900] Train: [16/100][146/156] Data 0.001 (0.001) Batch 3.894 (3.448) Remain 12:33:40 loss: 0.7216 Lr: 0.04838
[2023-08-07 21:10:15,009 INFO misc.py line 115 22900] Train: [16/100][147/156] Data 0.001 (0.001) Batch 2.342 (3.441) Remain 12:31:55 loss: 0.2387 Lr: 0.04838
[2023-08-07 21:10:19,175 INFO misc.py line 115 22900] Train: [16/100][148/156] Data 0.001 (0.001) Batch 4.165 (3.446) Remain 12:32:57 loss: 0.8883 Lr: 0.04838
[2023-08-07 21:10:23,301 INFO misc.py line 115 22900] Train: [16/100][149/156] Data 0.001 (0.001) Batch 4.127 (3.450) Remain 12:33:55 loss: 0.9655 Lr: 0.04838
[2023-08-07 21:10:26,605 INFO misc.py line 115 22900] Train: [16/100][150/156] Data 0.001 (0.001) Batch 3.303 (3.449) Remain 12:33:39 loss: 0.4820 Lr: 0.04837
[2023-08-07 21:10:30,730 INFO misc.py line 115 22900] Train: [16/100][151/156] Data 0.001 (0.001) Batch 4.126 (3.454) Remain 12:34:35 loss: 0.8555 Lr: 0.04837
[2023-08-07 21:10:34,364 INFO misc.py line 115 22900] Train: [16/100][152/156] Data 0.001 (0.001) Batch 3.633 (3.455) Remain 12:34:47 loss: 1.1372 Lr: 0.04837
[2023-08-07 21:10:37,524 INFO misc.py line 115 22900] Train: [16/100][153/156] Data 0.001 (0.001) Batch 3.160 (3.453) Remain 12:34:18 loss: 0.5086 Lr: 0.04837
[2023-08-07 21:10:41,509 INFO misc.py line 115 22900] Train: [16/100][154/156] Data 0.001 (0.001) Batch 3.986 (3.457) Remain 12:35:01 loss: 0.7020 Lr: 0.04837
[2023-08-07 21:10:43,289 INFO misc.py line 115 22900] Train: [16/100][155/156] Data 0.001 (0.001) Batch 1.780 (3.446) Remain 12:32:33 loss: 0.1747 Lr: 0.04836
[2023-08-07 21:10:46,927 INFO misc.py line 115 22900] Train: [16/100][156/156] Data 0.001 (0.001) Batch 3.638 (3.447) Remain 12:32:46 loss: 0.7909 Lr: 0.04836
[2023-08-07 21:10:46,928 INFO misc.py line 129 22900] Train result: loss: 0.7002 
[2023-08-07 21:10:46,928 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 21:10:49,064 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.8675 
[2023-08-07 21:10:49,933 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.7465 
[2023-08-07 21:10:51,598 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.8860 
[2023-08-07 21:10:53,118 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.1644 
[2023-08-07 21:10:54,963 INFO evaluator.py line 122 22900] Test: [5/24] Loss 2.0543 
[2023-08-07 21:10:56,625 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.8763 
[2023-08-07 21:10:58,764 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.5816 
[2023-08-07 21:11:00,569 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.9225 
[2023-08-07 21:11:01,852 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.2125 
[2023-08-07 21:11:03,983 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.2859 
[2023-08-07 21:11:04,508 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.3447 
[2023-08-07 21:11:06,042 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.8341 
[2023-08-07 21:11:08,750 INFO evaluator.py line 122 22900] Test: [13/24] Loss 0.9473 
[2023-08-07 21:11:10,430 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.8621 
[2023-08-07 21:11:12,453 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.5174 
[2023-08-07 21:11:15,162 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.0556 
[2023-08-07 21:11:17,870 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.0727 
[2023-08-07 21:11:19,719 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.9576 
[2023-08-07 21:11:20,467 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.4535 
[2023-08-07 21:11:21,352 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.9157 
[2023-08-07 21:11:23,613 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.4051 
[2023-08-07 21:11:25,578 INFO evaluator.py line 122 22900] Test: [22/24] Loss 2.1275 
[2023-08-07 21:11:27,424 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.3776 
[2023-08-07 21:11:29,357 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.8164 
[2023-08-07 21:11:29,404 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.1726/0.2691/0.6591.
[2023-08-07 21:11:29,404 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6407/0.9497
[2023-08-07 21:11:29,404 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9425/0.9904
[2023-08-07 21:11:29,404 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1159/0.2252
[2023-08-07 21:11:29,404 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.0805/0.1897
[2023-08-07 21:11:29,404 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.5600/0.8297
[2023-08-07 21:11:29,404 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.0975/0.1030
[2023-08-07 21:11:29,405 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.3291/0.4107
[2023-08-07 21:11:29,405 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0721/0.0781
[2023-08-07 21:11:29,405 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1423/0.4281
[2023-08-07 21:11:29,405 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0832/0.0850
[2023-08-07 21:11:29,405 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 21:11:29,405 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0000/0.0000
[2023-08-07 21:11:29,405 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1642/0.4855
[2023-08-07 21:11:29,405 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0017/0.0017
[2023-08-07 21:11:29,405 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0010/0.0011
[2023-08-07 21:11:29,405 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-07 21:11:29,405 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0000/0.0000
[2023-08-07 21:11:29,405 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.1728/0.5262
[2023-08-07 21:11:29,405 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 21:11:29,405 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0479/0.0774
[2023-08-07 21:11:29,405 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 21:11:29,405 INFO misc.py line 150 22900] Best validation mIoU updated to: 0.1726
[2023-08-07 21:11:29,405 INFO misc.py line 152 22900] Currently Best mIoU: 0.1726
[2023-08-07 21:11:29,405 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 21:11:36,455 INFO misc.py line 115 22900] Train: [17/100][1/156] Data 1.530 (1.530) Batch 6.023 (6.023) Remain 21:55:16 loss: 0.8863 Lr: 0.04836
[2023-08-07 21:11:39,440 INFO misc.py line 115 22900] Train: [17/100][2/156] Data 0.001 (0.001) Batch 2.985 (2.985) Remain 10:51:43 loss: 0.4780 Lr: 0.04836
[2023-08-07 21:11:43,191 INFO misc.py line 115 22900] Train: [17/100][3/156] Data 0.001 (0.001) Batch 3.751 (3.751) Remain 13:39:08 loss: 0.7053 Lr: 0.04836
[2023-08-07 21:11:46,044 INFO misc.py line 115 22900] Train: [17/100][4/156] Data 0.001 (0.001) Batch 2.853 (2.853) Remain 10:22:50 loss: 0.8247 Lr: 0.04835
[2023-08-07 21:11:49,614 INFO misc.py line 115 22900] Train: [17/100][5/156] Data 0.001 (0.001) Batch 3.571 (3.212) Remain 11:41:09 loss: 1.0028 Lr: 0.04835
[2023-08-07 21:11:53,046 INFO misc.py line 115 22900] Train: [17/100][6/156] Data 0.001 (0.001) Batch 3.431 (3.285) Remain 11:57:05 loss: 0.6868 Lr: 0.04835
[2023-08-07 21:11:56,202 INFO misc.py line 115 22900] Train: [17/100][7/156] Data 0.001 (0.001) Batch 3.157 (3.253) Remain 11:50:01 loss: 0.9473 Lr: 0.04835
[2023-08-07 21:12:00,333 INFO misc.py line 115 22900] Train: [17/100][8/156] Data 0.001 (0.001) Batch 4.131 (3.428) Remain 12:28:18 loss: 0.8158 Lr: 0.04835
[2023-08-07 21:12:03,773 INFO misc.py line 115 22900] Train: [17/100][9/156] Data 0.001 (0.001) Batch 3.440 (3.430) Remain 12:28:40 loss: 0.6439 Lr: 0.04835
[2023-08-07 21:12:06,635 INFO misc.py line 115 22900] Train: [17/100][10/156] Data 0.001 (0.001) Batch 2.862 (3.349) Remain 12:10:53 loss: 0.4277 Lr: 0.04834
[2023-08-07 21:12:10,342 INFO misc.py line 115 22900] Train: [17/100][11/156] Data 0.001 (0.001) Batch 3.707 (3.394) Remain 12:20:35 loss: 0.9386 Lr: 0.04834
[2023-08-07 21:12:14,357 INFO misc.py line 115 22900] Train: [17/100][12/156] Data 0.001 (0.001) Batch 4.015 (3.463) Remain 12:35:35 loss: 0.8537 Lr: 0.04834
[2023-08-07 21:12:17,784 INFO misc.py line 115 22900] Train: [17/100][13/156] Data 0.001 (0.001) Batch 3.428 (3.459) Remain 12:34:46 loss: 0.5636 Lr: 0.04834
[2023-08-07 21:12:21,942 INFO misc.py line 115 22900] Train: [17/100][14/156] Data 0.001 (0.001) Batch 4.157 (3.523) Remain 12:48:33 loss: 0.9915 Lr: 0.04834
[2023-08-07 21:12:25,213 INFO misc.py line 115 22900] Train: [17/100][15/156] Data 0.001 (0.001) Batch 3.271 (3.502) Remain 12:43:55 loss: 0.8390 Lr: 0.04833
[2023-08-07 21:12:28,328 INFO misc.py line 115 22900] Train: [17/100][16/156] Data 0.001 (0.001) Batch 3.115 (3.472) Remain 12:37:22 loss: 0.5325 Lr: 0.04833
[2023-08-07 21:12:31,428 INFO misc.py line 115 22900] Train: [17/100][17/156] Data 0.001 (0.001) Batch 3.100 (3.446) Remain 12:31:31 loss: 0.5807 Lr: 0.04833
[2023-08-07 21:12:34,139 INFO misc.py line 115 22900] Train: [17/100][18/156] Data 0.001 (0.001) Batch 2.711 (3.397) Remain 12:20:47 loss: 0.4819 Lr: 0.04833
[2023-08-07 21:12:37,988 INFO misc.py line 115 22900] Train: [17/100][19/156] Data 0.001 (0.001) Batch 3.848 (3.425) Remain 12:26:53 loss: 0.6217 Lr: 0.04833
[2023-08-07 21:12:41,691 INFO misc.py line 115 22900] Train: [17/100][20/156] Data 0.001 (0.001) Batch 3.703 (3.441) Remain 12:30:24 loss: 0.9345 Lr: 0.04832
[2023-08-07 21:12:45,940 INFO misc.py line 115 22900] Train: [17/100][21/156] Data 0.001 (0.001) Batch 4.249 (3.486) Remain 12:40:07 loss: 0.9867 Lr: 0.04832
[2023-08-07 21:12:49,665 INFO misc.py line 115 22900] Train: [17/100][22/156] Data 0.001 (0.001) Batch 3.725 (3.499) Remain 12:42:48 loss: 0.5987 Lr: 0.04832
[2023-08-07 21:12:53,422 INFO misc.py line 115 22900] Train: [17/100][23/156] Data 0.001 (0.001) Batch 3.757 (3.512) Remain 12:45:34 loss: 0.7600 Lr: 0.04832
[2023-08-07 21:12:57,420 INFO misc.py line 115 22900] Train: [17/100][24/156] Data 0.001 (0.001) Batch 3.998 (3.535) Remain 12:50:33 loss: 0.6484 Lr: 0.04832
[2023-08-07 21:13:01,972 INFO misc.py line 115 22900] Train: [17/100][25/156] Data 0.001 (0.001) Batch 4.552 (3.581) Remain 13:00:35 loss: 0.9622 Lr: 0.04831
[2023-08-07 21:13:05,844 INFO misc.py line 115 22900] Train: [17/100][26/156] Data 0.001 (0.001) Batch 3.872 (3.594) Remain 13:03:17 loss: 0.7140 Lr: 0.04831
[2023-08-07 21:13:08,919 INFO misc.py line 115 22900] Train: [17/100][27/156] Data 0.001 (0.001) Batch 3.075 (3.572) Remain 12:58:31 loss: 0.7374 Lr: 0.04831
[2023-08-07 21:13:11,355 INFO misc.py line 115 22900] Train: [17/100][28/156] Data 0.001 (0.001) Batch 2.436 (3.527) Remain 12:48:33 loss: 0.4811 Lr: 0.04831
[2023-08-07 21:13:15,393 INFO misc.py line 115 22900] Train: [17/100][29/156] Data 0.001 (0.001) Batch 4.038 (3.546) Remain 12:52:47 loss: 0.5198 Lr: 0.04831
[2023-08-07 21:13:18,934 INFO misc.py line 115 22900] Train: [17/100][30/156] Data 0.001 (0.001) Batch 3.541 (3.546) Remain 12:52:40 loss: 0.5731 Lr: 0.04831
[2023-08-07 21:13:22,599 INFO misc.py line 115 22900] Train: [17/100][31/156] Data 0.001 (0.001) Batch 3.665 (3.550) Remain 12:53:32 loss: 0.6638 Lr: 0.04830
[2023-08-07 21:13:26,222 INFO misc.py line 115 22900] Train: [17/100][32/156] Data 0.001 (0.001) Batch 3.623 (3.553) Remain 12:54:02 loss: 0.6497 Lr: 0.04830
[2023-08-07 21:13:29,878 INFO misc.py line 115 22900] Train: [17/100][33/156] Data 0.001 (0.001) Batch 3.656 (3.556) Remain 12:54:43 loss: 1.0472 Lr: 0.04830
[2023-08-07 21:13:33,074 INFO misc.py line 115 22900] Train: [17/100][34/156] Data 0.001 (0.001) Batch 3.196 (3.545) Remain 12:52:07 loss: 0.5159 Lr: 0.04830
[2023-08-07 21:13:37,135 INFO misc.py line 115 22900] Train: [17/100][35/156] Data 0.001 (0.001) Batch 4.061 (3.561) Remain 12:55:35 loss: 1.1035 Lr: 0.04830
[2023-08-07 21:13:40,886 INFO misc.py line 115 22900] Train: [17/100][36/156] Data 0.001 (0.001) Batch 3.751 (3.567) Remain 12:56:47 loss: 0.5599 Lr: 0.04829
[2023-08-07 21:13:44,012 INFO misc.py line 115 22900] Train: [17/100][37/156] Data 0.001 (0.001) Batch 3.126 (3.554) Remain 12:53:54 loss: 0.4832 Lr: 0.04829
[2023-08-07 21:13:47,388 INFO misc.py line 115 22900] Train: [17/100][38/156] Data 0.001 (0.001) Batch 3.376 (3.548) Remain 12:52:44 loss: 0.6581 Lr: 0.04829
[2023-08-07 21:13:50,762 INFO misc.py line 115 22900] Train: [17/100][39/156] Data 0.001 (0.001) Batch 3.374 (3.544) Remain 12:51:37 loss: 0.6775 Lr: 0.04829
[2023-08-07 21:13:54,304 INFO misc.py line 115 22900] Train: [17/100][40/156] Data 0.001 (0.001) Batch 3.542 (3.544) Remain 12:51:33 loss: 0.4230 Lr: 0.04829
[2023-08-07 21:13:58,243 INFO misc.py line 115 22900] Train: [17/100][41/156] Data 0.001 (0.001) Batch 3.938 (3.554) Remain 12:53:45 loss: 1.1038 Lr: 0.04828
[2023-08-07 21:14:00,843 INFO misc.py line 115 22900] Train: [17/100][42/156] Data 0.001 (0.001) Batch 2.600 (3.530) Remain 12:48:22 loss: 0.7519 Lr: 0.04828
[2023-08-07 21:14:03,172 INFO misc.py line 115 22900] Train: [17/100][43/156] Data 0.001 (0.001) Batch 2.329 (3.500) Remain 12:41:47 loss: 0.6699 Lr: 0.04828
[2023-08-07 21:14:07,126 INFO misc.py line 115 22900] Train: [17/100][44/156] Data 0.001 (0.001) Batch 3.954 (3.511) Remain 12:44:08 loss: 0.8053 Lr: 0.04828
[2023-08-07 21:14:09,941 INFO misc.py line 115 22900] Train: [17/100][45/156] Data 0.001 (0.001) Batch 2.815 (3.494) Remain 12:40:28 loss: 0.5394 Lr: 0.04828
[2023-08-07 21:14:13,951 INFO misc.py line 115 22900] Train: [17/100][46/156] Data 0.001 (0.001) Batch 4.009 (3.506) Remain 12:43:01 loss: 0.5802 Lr: 0.04827
[2023-08-07 21:14:17,629 INFO misc.py line 115 22900] Train: [17/100][47/156] Data 0.001 (0.001) Batch 3.678 (3.510) Remain 12:43:49 loss: 0.6358 Lr: 0.04827
[2023-08-07 21:14:20,986 INFO misc.py line 115 22900] Train: [17/100][48/156] Data 0.001 (0.001) Batch 3.357 (3.507) Remain 12:43:01 loss: 0.9418 Lr: 0.04827
[2023-08-07 21:14:24,888 INFO misc.py line 115 22900] Train: [17/100][49/156] Data 0.001 (0.001) Batch 3.902 (3.515) Remain 12:44:50 loss: 0.7718 Lr: 0.04827
[2023-08-07 21:14:28,918 INFO misc.py line 115 22900] Train: [17/100][50/156] Data 0.001 (0.001) Batch 4.030 (3.526) Remain 12:47:09 loss: 0.7266 Lr: 0.04827
[2023-08-07 21:14:32,573 INFO misc.py line 115 22900] Train: [17/100][51/156] Data 0.001 (0.001) Batch 3.654 (3.529) Remain 12:47:41 loss: 0.5882 Lr: 0.04826
[2023-08-07 21:14:35,079 INFO misc.py line 115 22900] Train: [17/100][52/156] Data 0.001 (0.001) Batch 2.506 (3.508) Remain 12:43:05 loss: 0.4569 Lr: 0.04826
[2023-08-07 21:14:38,792 INFO misc.py line 115 22900] Train: [17/100][53/156] Data 0.001 (0.001) Batch 3.713 (3.512) Remain 12:43:55 loss: 0.8279 Lr: 0.04826
[2023-08-07 21:14:42,762 INFO misc.py line 115 22900] Train: [17/100][54/156] Data 0.001 (0.001) Batch 3.971 (3.521) Remain 12:45:49 loss: 0.7978 Lr: 0.04826
[2023-08-07 21:14:47,186 INFO misc.py line 115 22900] Train: [17/100][55/156] Data 0.001 (0.001) Batch 4.424 (3.538) Remain 12:49:32 loss: 0.9749 Lr: 0.04826
[2023-08-07 21:14:50,963 INFO misc.py line 115 22900] Train: [17/100][56/156] Data 0.001 (0.001) Batch 3.777 (3.543) Remain 12:50:27 loss: 0.8197 Lr: 0.04825
[2023-08-07 21:14:55,055 INFO misc.py line 115 22900] Train: [17/100][57/156] Data 0.001 (0.001) Batch 4.092 (3.553) Remain 12:52:36 loss: 0.7991 Lr: 0.04825
[2023-08-07 21:14:59,651 INFO misc.py line 115 22900] Train: [17/100][58/156] Data 0.001 (0.001) Batch 4.596 (3.572) Remain 12:56:40 loss: 1.1046 Lr: 0.04825
[2023-08-07 21:15:03,653 INFO misc.py line 115 22900] Train: [17/100][59/156] Data 0.001 (0.001) Batch 4.002 (3.580) Remain 12:58:16 loss: 0.7633 Lr: 0.04825
[2023-08-07 21:15:07,545 INFO misc.py line 115 22900] Train: [17/100][60/156] Data 0.001 (0.001) Batch 3.892 (3.585) Remain 12:59:24 loss: 0.6698 Lr: 0.04825
[2023-08-07 21:15:11,735 INFO misc.py line 115 22900] Train: [17/100][61/156] Data 0.001 (0.001) Batch 4.190 (3.596) Remain 13:01:37 loss: 0.8632 Lr: 0.04825
[2023-08-07 21:15:13,886 INFO misc.py line 115 22900] Train: [17/100][62/156] Data 0.001 (0.001) Batch 2.151 (3.571) Remain 12:56:14 loss: 0.5348 Lr: 0.04824
[2023-08-07 21:15:17,430 INFO misc.py line 115 22900] Train: [17/100][63/156] Data 0.001 (0.001) Batch 3.544 (3.571) Remain 12:56:04 loss: 0.8108 Lr: 0.04824
[2023-08-07 21:15:21,019 INFO misc.py line 115 22900] Train: [17/100][64/156] Data 0.001 (0.001) Batch 3.589 (3.571) Remain 12:56:05 loss: 0.5829 Lr: 0.04824
[2023-08-07 21:15:24,480 INFO misc.py line 115 22900] Train: [17/100][65/156] Data 0.001 (0.001) Batch 3.461 (3.569) Remain 12:55:38 loss: 0.9060 Lr: 0.04824
[2023-08-07 21:15:27,794 INFO misc.py line 115 22900] Train: [17/100][66/156] Data 0.001 (0.001) Batch 3.314 (3.565) Remain 12:54:42 loss: 0.5139 Lr: 0.04824
[2023-08-07 21:15:31,066 INFO misc.py line 115 22900] Train: [17/100][67/156] Data 0.001 (0.001) Batch 3.272 (3.561) Remain 12:53:38 loss: 0.6062 Lr: 0.04823
[2023-08-07 21:15:35,094 INFO misc.py line 115 22900] Train: [17/100][68/156] Data 0.001 (0.001) Batch 4.028 (3.568) Remain 12:55:09 loss: 0.6292 Lr: 0.04823
[2023-08-07 21:15:38,495 INFO misc.py line 115 22900] Train: [17/100][69/156] Data 0.001 (0.001) Batch 3.402 (3.565) Remain 12:54:32 loss: 0.9037 Lr: 0.04823
[2023-08-07 21:15:41,987 INFO misc.py line 115 22900] Train: [17/100][70/156] Data 0.001 (0.001) Batch 3.491 (3.564) Remain 12:54:14 loss: 0.5467 Lr: 0.04823
[2023-08-07 21:15:45,403 INFO misc.py line 115 22900] Train: [17/100][71/156] Data 0.001 (0.001) Batch 3.416 (3.562) Remain 12:53:42 loss: 0.8693 Lr: 0.04823
[2023-08-07 21:15:48,310 INFO misc.py line 115 22900] Train: [17/100][72/156] Data 0.001 (0.001) Batch 2.907 (3.552) Remain 12:51:35 loss: 0.3322 Lr: 0.04822
[2023-08-07 21:15:51,799 INFO misc.py line 115 22900] Train: [17/100][73/156] Data 0.001 (0.001) Batch 3.488 (3.552) Remain 12:51:20 loss: 0.6707 Lr: 0.04822
[2023-08-07 21:15:55,294 INFO misc.py line 115 22900] Train: [17/100][74/156] Data 0.001 (0.001) Batch 3.495 (3.551) Remain 12:51:06 loss: 0.7319 Lr: 0.04822
[2023-08-07 21:15:57,539 INFO misc.py line 115 22900] Train: [17/100][75/156] Data 0.001 (0.001) Batch 2.244 (3.533) Remain 12:47:06 loss: 0.6535 Lr: 0.04822
[2023-08-07 21:16:01,659 INFO misc.py line 115 22900] Train: [17/100][76/156] Data 0.001 (0.001) Batch 4.120 (3.541) Remain 12:48:47 loss: 0.9216 Lr: 0.04822
[2023-08-07 21:16:04,728 INFO misc.py line 115 22900] Train: [17/100][77/156] Data 0.001 (0.001) Batch 3.069 (3.534) Remain 12:47:21 loss: 0.7714 Lr: 0.04821
[2023-08-07 21:16:08,541 INFO misc.py line 115 22900] Train: [17/100][78/156] Data 0.001 (0.001) Batch 3.813 (3.538) Remain 12:48:05 loss: 0.6588 Lr: 0.04821
[2023-08-07 21:16:12,769 INFO misc.py line 115 22900] Train: [17/100][79/156] Data 0.001 (0.001) Batch 4.228 (3.547) Remain 12:50:00 loss: 0.8221 Lr: 0.04821
[2023-08-07 21:16:15,530 INFO misc.py line 115 22900] Train: [17/100][80/156] Data 0.001 (0.001) Batch 2.761 (3.537) Remain 12:47:44 loss: 0.4494 Lr: 0.04821
[2023-08-07 21:16:18,939 INFO misc.py line 115 22900] Train: [17/100][81/156] Data 0.001 (0.001) Batch 3.408 (3.535) Remain 12:47:19 loss: 0.6006 Lr: 0.04821
[2023-08-07 21:16:22,403 INFO misc.py line 115 22900] Train: [17/100][82/156] Data 0.001 (0.001) Batch 3.464 (3.534) Remain 12:47:03 loss: 0.7948 Lr: 0.04820
[2023-08-07 21:16:26,077 INFO misc.py line 115 22900] Train: [17/100][83/156] Data 0.001 (0.001) Batch 3.674 (3.536) Remain 12:47:23 loss: 0.8904 Lr: 0.04820
[2023-08-07 21:16:29,801 INFO misc.py line 115 22900] Train: [17/100][84/156] Data 0.001 (0.001) Batch 3.724 (3.538) Remain 12:47:49 loss: 0.3657 Lr: 0.04820
[2023-08-07 21:16:33,623 INFO misc.py line 115 22900] Train: [17/100][85/156] Data 0.001 (0.001) Batch 3.822 (3.542) Remain 12:48:31 loss: 0.7713 Lr: 0.04820
[2023-08-07 21:16:37,730 INFO misc.py line 115 22900] Train: [17/100][86/156] Data 0.001 (0.001) Batch 4.107 (3.549) Remain 12:49:56 loss: 0.7235 Lr: 0.04820
[2023-08-07 21:16:42,165 INFO misc.py line 115 22900] Train: [17/100][87/156] Data 0.001 (0.001) Batch 4.435 (3.559) Remain 12:52:10 loss: 1.2428 Lr: 0.04819
[2023-08-07 21:16:45,982 INFO misc.py line 115 22900] Train: [17/100][88/156] Data 0.001 (0.001) Batch 3.817 (3.562) Remain 12:52:46 loss: 0.7256 Lr: 0.04819
[2023-08-07 21:16:49,722 INFO misc.py line 115 22900] Train: [17/100][89/156] Data 0.001 (0.001) Batch 3.740 (3.564) Remain 12:53:09 loss: 0.4890 Lr: 0.04819
[2023-08-07 21:16:53,217 INFO misc.py line 115 22900] Train: [17/100][90/156] Data 0.001 (0.001) Batch 3.495 (3.564) Remain 12:52:55 loss: 0.5965 Lr: 0.04819
[2023-08-07 21:16:56,474 INFO misc.py line 115 22900] Train: [17/100][91/156] Data 0.001 (0.001) Batch 3.257 (3.560) Remain 12:52:06 loss: 0.5671 Lr: 0.04819
[2023-08-07 21:16:59,764 INFO misc.py line 115 22900] Train: [17/100][92/156] Data 0.001 (0.001) Batch 3.290 (3.557) Remain 12:51:23 loss: 0.8232 Lr: 0.04818
[2023-08-07 21:17:03,134 INFO misc.py line 115 22900] Train: [17/100][93/156] Data 0.001 (0.001) Batch 3.370 (3.555) Remain 12:50:53 loss: 0.5320 Lr: 0.04818
[2023-08-07 21:17:05,833 INFO misc.py line 115 22900] Train: [17/100][94/156] Data 0.001 (0.001) Batch 2.699 (3.546) Remain 12:48:47 loss: 0.5653 Lr: 0.04818
[2023-08-07 21:17:09,248 INFO misc.py line 115 22900] Train: [17/100][95/156] Data 0.001 (0.001) Batch 3.415 (3.544) Remain 12:48:25 loss: 0.7869 Lr: 0.04818
[2023-08-07 21:17:12,778 INFO misc.py line 115 22900] Train: [17/100][96/156] Data 0.001 (0.001) Batch 3.529 (3.544) Remain 12:48:19 loss: 0.4992 Lr: 0.04818
[2023-08-07 21:17:16,669 INFO misc.py line 115 22900] Train: [17/100][97/156] Data 0.001 (0.001) Batch 3.891 (3.548) Remain 12:49:04 loss: 0.7235 Lr: 0.04817
[2023-08-07 21:17:20,921 INFO misc.py line 115 22900] Train: [17/100][98/156] Data 0.001 (0.001) Batch 4.252 (3.555) Remain 12:50:36 loss: 0.9038 Lr: 0.04817
[2023-08-07 21:17:23,575 INFO misc.py line 115 22900] Train: [17/100][99/156] Data 0.001 (0.001) Batch 2.654 (3.546) Remain 12:48:31 loss: 0.7173 Lr: 0.04817
[2023-08-07 21:17:25,864 INFO misc.py line 115 22900] Train: [17/100][100/156] Data 0.001 (0.001) Batch 2.288 (3.533) Remain 12:45:39 loss: 0.4338 Lr: 0.04817
[2023-08-07 21:17:29,354 INFO misc.py line 115 22900] Train: [17/100][101/156] Data 0.001 (0.001) Batch 3.490 (3.532) Remain 12:45:30 loss: 0.7499 Lr: 0.04817
[2023-08-07 21:17:33,361 INFO misc.py line 115 22900] Train: [17/100][102/156] Data 0.001 (0.001) Batch 4.007 (3.537) Remain 12:46:28 loss: 0.8335 Lr: 0.04816
[2023-08-07 21:17:37,282 INFO misc.py line 115 22900] Train: [17/100][103/156] Data 0.001 (0.001) Batch 3.921 (3.541) Remain 12:47:15 loss: 0.9600 Lr: 0.04816
[2023-08-07 21:17:40,847 INFO misc.py line 115 22900] Train: [17/100][104/156] Data 0.001 (0.001) Batch 3.565 (3.541) Remain 12:47:14 loss: 0.8379 Lr: 0.04816
[2023-08-07 21:17:43,817 INFO misc.py line 115 22900] Train: [17/100][105/156] Data 0.001 (0.001) Batch 2.970 (3.536) Remain 12:45:58 loss: 0.4542 Lr: 0.04816
[2023-08-07 21:17:46,852 INFO misc.py line 115 22900] Train: [17/100][106/156] Data 0.001 (0.001) Batch 3.035 (3.531) Remain 12:44:51 loss: 0.6736 Lr: 0.04816
[2023-08-07 21:17:50,257 INFO misc.py line 115 22900] Train: [17/100][107/156] Data 0.001 (0.001) Batch 3.405 (3.529) Remain 12:44:32 loss: 0.5052 Lr: 0.04815
[2023-08-07 21:17:53,284 INFO misc.py line 115 22900] Train: [17/100][108/156] Data 0.001 (0.001) Batch 3.026 (3.525) Remain 12:43:26 loss: 0.5838 Lr: 0.04815
[2023-08-07 21:17:57,640 INFO misc.py line 115 22900] Train: [17/100][109/156] Data 0.001 (0.001) Batch 4.357 (3.533) Remain 12:45:05 loss: 1.1799 Lr: 0.04815
[2023-08-07 21:18:00,246 INFO misc.py line 115 22900] Train: [17/100][110/156] Data 0.001 (0.001) Batch 2.606 (3.524) Remain 12:43:09 loss: 0.6678 Lr: 0.04815
[2023-08-07 21:18:03,456 INFO misc.py line 115 22900] Train: [17/100][111/156] Data 0.001 (0.001) Batch 3.211 (3.521) Remain 12:42:28 loss: 0.3282 Lr: 0.04815
[2023-08-07 21:18:07,427 INFO misc.py line 115 22900] Train: [17/100][112/156] Data 0.001 (0.001) Batch 3.971 (3.525) Remain 12:43:18 loss: 0.8277 Lr: 0.04814
[2023-08-07 21:18:10,019 INFO misc.py line 115 22900] Train: [17/100][113/156] Data 0.001 (0.001) Batch 2.592 (3.517) Remain 12:41:24 loss: 0.4541 Lr: 0.04814
[2023-08-07 21:18:12,367 INFO misc.py line 115 22900] Train: [17/100][114/156] Data 0.001 (0.001) Batch 2.348 (3.506) Remain 12:39:04 loss: 0.5084 Lr: 0.04814
[2023-08-07 21:18:16,552 INFO misc.py line 115 22900] Train: [17/100][115/156] Data 0.001 (0.001) Batch 4.185 (3.512) Remain 12:40:19 loss: 0.4305 Lr: 0.04814
[2023-08-07 21:18:20,134 INFO misc.py line 115 22900] Train: [17/100][116/156] Data 0.001 (0.001) Batch 3.582 (3.513) Remain 12:40:23 loss: 0.5327 Lr: 0.04814
[2023-08-07 21:18:23,732 INFO misc.py line 115 22900] Train: [17/100][117/156] Data 0.001 (0.001) Batch 3.597 (3.514) Remain 12:40:29 loss: 0.5392 Lr: 0.04813
[2023-08-07 21:18:27,138 INFO misc.py line 115 22900] Train: [17/100][118/156] Data 0.001 (0.001) Batch 3.406 (3.513) Remain 12:40:14 loss: 0.6154 Lr: 0.04813
[2023-08-07 21:18:31,244 INFO misc.py line 115 22900] Train: [17/100][119/156] Data 0.001 (0.001) Batch 4.106 (3.518) Remain 12:41:17 loss: 0.9493 Lr: 0.04813
[2023-08-07 21:18:34,569 INFO misc.py line 115 22900] Train: [17/100][120/156] Data 0.001 (0.001) Batch 3.326 (3.516) Remain 12:40:52 loss: 0.7141 Lr: 0.04813
[2023-08-07 21:18:37,840 INFO misc.py line 115 22900] Train: [17/100][121/156] Data 0.001 (0.001) Batch 3.271 (3.514) Remain 12:40:21 loss: 0.5162 Lr: 0.04813
[2023-08-07 21:18:40,212 INFO misc.py line 115 22900] Train: [17/100][122/156] Data 0.001 (0.001) Batch 2.372 (3.504) Remain 12:38:13 loss: 0.2837 Lr: 0.04812
[2023-08-07 21:18:43,402 INFO misc.py line 115 22900] Train: [17/100][123/156] Data 0.001 (0.001) Batch 3.190 (3.502) Remain 12:37:36 loss: 0.4862 Lr: 0.04812
[2023-08-07 21:18:46,328 INFO misc.py line 115 22900] Train: [17/100][124/156] Data 0.001 (0.001) Batch 2.925 (3.497) Remain 12:36:30 loss: 0.2764 Lr: 0.04812
[2023-08-07 21:18:49,819 INFO misc.py line 115 22900] Train: [17/100][125/156] Data 0.001 (0.001) Batch 3.491 (3.497) Remain 12:36:26 loss: 0.9088 Lr: 0.04812
[2023-08-07 21:18:54,161 INFO misc.py line 115 22900] Train: [17/100][126/156] Data 0.001 (0.001) Batch 4.342 (3.504) Remain 12:37:52 loss: 1.0440 Lr: 0.04812
[2023-08-07 21:18:57,735 INFO misc.py line 115 22900] Train: [17/100][127/156] Data 0.001 (0.001) Batch 3.573 (3.504) Remain 12:37:56 loss: 0.5110 Lr: 0.04811
[2023-08-07 21:19:00,384 INFO misc.py line 115 22900] Train: [17/100][128/156] Data 0.001 (0.001) Batch 2.650 (3.498) Remain 12:36:24 loss: 0.4924 Lr: 0.04811
[2023-08-07 21:19:02,800 INFO misc.py line 115 22900] Train: [17/100][129/156] Data 0.001 (0.001) Batch 2.415 (3.489) Remain 12:34:29 loss: 0.4109 Lr: 0.04811
[2023-08-07 21:19:04,957 INFO misc.py line 115 22900] Train: [17/100][130/156] Data 0.001 (0.001) Batch 2.158 (3.478) Remain 12:32:09 loss: 0.7140 Lr: 0.04811
[2023-08-07 21:19:08,423 INFO misc.py line 115 22900] Train: [17/100][131/156] Data 0.001 (0.001) Batch 3.465 (3.478) Remain 12:32:04 loss: 0.8686 Lr: 0.04811
[2023-08-07 21:19:11,344 INFO misc.py line 115 22900] Train: [17/100][132/156] Data 0.001 (0.001) Batch 2.921 (3.474) Remain 12:31:05 loss: 0.6301 Lr: 0.04810
[2023-08-07 21:19:14,794 INFO misc.py line 115 22900] Train: [17/100][133/156] Data 0.001 (0.001) Batch 3.451 (3.474) Remain 12:30:59 loss: 0.4803 Lr: 0.04810
[2023-08-07 21:19:18,853 INFO misc.py line 115 22900] Train: [17/100][134/156] Data 0.001 (0.001) Batch 4.059 (3.478) Remain 12:31:54 loss: 0.9429 Lr: 0.04810
[2023-08-07 21:19:21,006 INFO misc.py line 115 22900] Train: [17/100][135/156] Data 0.001 (0.001) Batch 2.153 (3.468) Remain 12:29:40 loss: 0.7126 Lr: 0.04810
[2023-08-07 21:19:24,474 INFO misc.py line 115 22900] Train: [17/100][136/156] Data 0.001 (0.001) Batch 3.468 (3.468) Remain 12:29:36 loss: 0.6847 Lr: 0.04810
[2023-08-07 21:19:28,046 INFO misc.py line 115 22900] Train: [17/100][137/156] Data 0.001 (0.001) Batch 3.572 (3.469) Remain 12:29:43 loss: 0.8337 Lr: 0.04809
[2023-08-07 21:19:30,911 INFO misc.py line 115 22900] Train: [17/100][138/156] Data 0.001 (0.001) Batch 2.864 (3.465) Remain 12:28:41 loss: 0.6537 Lr: 0.04809
[2023-08-07 21:19:33,806 INFO misc.py line 115 22900] Train: [17/100][139/156] Data 0.001 (0.001) Batch 2.895 (3.460) Remain 12:27:44 loss: 0.5590 Lr: 0.04809
[2023-08-07 21:19:37,449 INFO misc.py line 115 22900] Train: [17/100][140/156] Data 0.001 (0.001) Batch 3.642 (3.462) Remain 12:27:57 loss: 0.7524 Lr: 0.04809
[2023-08-07 21:19:38,897 INFO misc.py line 115 22900] Train: [17/100][141/156] Data 0.001 (0.001) Batch 1.448 (3.447) Remain 12:24:45 loss: 0.2078 Lr: 0.04809
[2023-08-07 21:19:41,838 INFO misc.py line 115 22900] Train: [17/100][142/156] Data 0.001 (0.001) Batch 2.941 (3.444) Remain 12:23:54 loss: 0.4727 Lr: 0.04808
[2023-08-07 21:19:44,654 INFO misc.py line 115 22900] Train: [17/100][143/156] Data 0.001 (0.001) Batch 2.816 (3.439) Remain 12:22:53 loss: 0.6746 Lr: 0.04808
[2023-08-07 21:19:47,216 INFO misc.py line 115 22900] Train: [17/100][144/156] Data 0.001 (0.001) Batch 2.562 (3.433) Remain 12:21:29 loss: 0.4283 Lr: 0.04808
[2023-08-07 21:19:51,259 INFO misc.py line 115 22900] Train: [17/100][145/156] Data 0.001 (0.001) Batch 4.043 (3.437) Remain 12:22:21 loss: 0.9919 Lr: 0.04808
[2023-08-07 21:19:53,898 INFO misc.py line 115 22900] Train: [17/100][146/156] Data 0.001 (0.001) Batch 2.639 (3.432) Remain 12:21:05 loss: 0.4421 Lr: 0.04808
[2023-08-07 21:19:57,348 INFO misc.py line 115 22900] Train: [17/100][147/156] Data 0.001 (0.001) Batch 3.450 (3.432) Remain 12:21:03 loss: 1.2293 Lr: 0.04807
[2023-08-07 21:20:00,681 INFO misc.py line 115 22900] Train: [17/100][148/156] Data 0.001 (0.001) Batch 3.333 (3.431) Remain 12:20:51 loss: 0.6696 Lr: 0.04807
[2023-08-07 21:20:04,679 INFO misc.py line 115 22900] Train: [17/100][149/156] Data 0.001 (0.001) Batch 3.998 (3.435) Remain 12:21:38 loss: 0.7910 Lr: 0.04807
[2023-08-07 21:20:07,606 INFO misc.py line 115 22900] Train: [17/100][150/156] Data 0.001 (0.001) Batch 2.927 (3.431) Remain 12:20:50 loss: 0.5169 Lr: 0.04807
[2023-08-07 21:20:11,685 INFO misc.py line 115 22900] Train: [17/100][151/156] Data 0.001 (0.001) Batch 4.079 (3.436) Remain 12:21:43 loss: 0.7877 Lr: 0.04807
[2023-08-07 21:20:14,981 INFO misc.py line 115 22900] Train: [17/100][152/156] Data 0.001 (0.001) Batch 3.296 (3.435) Remain 12:21:27 loss: 0.5033 Lr: 0.04806
[2023-08-07 21:20:18,998 INFO misc.py line 115 22900] Train: [17/100][153/156] Data 0.001 (0.001) Batch 4.017 (3.439) Remain 12:22:14 loss: 0.7652 Lr: 0.04806
[2023-08-07 21:20:22,801 INFO misc.py line 115 22900] Train: [17/100][154/156] Data 0.001 (0.001) Batch 3.802 (3.441) Remain 12:22:42 loss: 0.7152 Lr: 0.04806
[2023-08-07 21:20:26,379 INFO misc.py line 115 22900] Train: [17/100][155/156] Data 0.001 (0.001) Batch 3.579 (3.442) Remain 12:22:50 loss: 0.8987 Lr: 0.04806
[2023-08-07 21:20:30,171 INFO misc.py line 115 22900] Train: [17/100][156/156] Data 0.001 (0.001) Batch 3.792 (3.444) Remain 12:23:16 loss: 0.6143 Lr: 0.04806
[2023-08-07 21:20:30,172 INFO misc.py line 129 22900] Train result: loss: 0.6936 
[2023-08-07 21:20:30,172 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 21:20:32,278 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.8747 
[2023-08-07 21:20:33,147 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.6993 
[2023-08-07 21:20:34,810 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.7597 
[2023-08-07 21:20:36,331 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.1979 
[2023-08-07 21:20:38,175 INFO evaluator.py line 122 22900] Test: [5/24] Loss 2.0255 
[2023-08-07 21:20:39,838 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.6112 
[2023-08-07 21:20:41,979 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.9261 
[2023-08-07 21:20:43,783 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.0227 
[2023-08-07 21:20:45,065 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.2982 
[2023-08-07 21:20:47,195 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3208 
[2023-08-07 21:20:47,719 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.2533 
[2023-08-07 21:20:49,254 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.7087 
[2023-08-07 21:20:51,964 INFO evaluator.py line 122 22900] Test: [13/24] Loss 0.9808 
[2023-08-07 21:20:53,644 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.7573 
[2023-08-07 21:20:55,666 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.5525 
[2023-08-07 21:20:58,379 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.1101 
[2023-08-07 21:21:01,084 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.1182 
[2023-08-07 21:21:02,933 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.7955 
[2023-08-07 21:21:03,681 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.0155 
[2023-08-07 21:21:04,566 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.8265 
[2023-08-07 21:21:06,826 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3422 
[2023-08-07 21:21:08,792 INFO evaluator.py line 122 22900] Test: [22/24] Loss 2.0490 
[2023-08-07 21:21:10,638 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.5430 
[2023-08-07 21:21:12,572 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.8121 
[2023-08-07 21:21:12,621 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.1852/0.2771/0.6638.
[2023-08-07 21:21:12,621 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6492/0.9265
[2023-08-07 21:21:12,621 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9532/0.9901
[2023-08-07 21:21:12,621 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1158/0.3259
[2023-08-07 21:21:12,621 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.0837/0.1751
[2023-08-07 21:21:12,621 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6194/0.8367
[2023-08-07 21:21:12,621 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.0801/0.0827
[2023-08-07 21:21:12,621 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.4441/0.5005
[2023-08-07 21:21:12,621 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0692/0.0792
[2023-08-07 21:21:12,621 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1962/0.6652
[2023-08-07 21:21:12,621 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0239/0.0241
[2023-08-07 21:21:12,621 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 21:21:12,621 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0000/0.0000
[2023-08-07 21:21:12,621 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1701/0.5342
[2023-08-07 21:21:12,621 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0088/0.0089
[2023-08-07 21:21:12,621 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0419/0.0749
[2023-08-07 21:21:12,621 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-07 21:21:12,621 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0000/0.0000
[2023-08-07 21:21:12,621 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.1696/0.1869
[2023-08-07 21:21:12,621 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 21:21:12,621 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0784/0.1320
[2023-08-07 21:21:12,622 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 21:21:12,622 INFO misc.py line 150 22900] Best validation mIoU updated to: 0.1852
[2023-08-07 21:21:12,622 INFO misc.py line 152 22900] Currently Best mIoU: 0.1852
[2023-08-07 21:21:12,622 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 21:21:18,310 INFO misc.py line 115 22900] Train: [18/100][1/156] Data 1.119 (1.119) Batch 4.644 (4.644) Remain 16:42:04 loss: 0.7269 Lr: 0.04805
[2023-08-07 21:21:22,319 INFO misc.py line 115 22900] Train: [18/100][2/156] Data 0.001 (0.001) Batch 4.010 (4.010) Remain 14:25:08 loss: 0.6528 Lr: 0.04805
[2023-08-07 21:21:24,904 INFO misc.py line 115 22900] Train: [18/100][3/156] Data 0.001 (0.001) Batch 2.584 (2.584) Remain 09:17:33 loss: 0.4443 Lr: 0.04805
[2023-08-07 21:21:28,767 INFO misc.py line 115 22900] Train: [18/100][4/156] Data 0.001 (0.001) Batch 3.863 (3.863) Remain 13:53:27 loss: 0.9855 Lr: 0.04805
[2023-08-07 21:21:32,829 INFO misc.py line 115 22900] Train: [18/100][5/156] Data 0.001 (0.001) Batch 4.062 (3.963) Remain 14:14:49 loss: 0.3774 Lr: 0.04804
[2023-08-07 21:21:35,919 INFO misc.py line 115 22900] Train: [18/100][6/156] Data 0.001 (0.001) Batch 3.090 (3.672) Remain 13:11:58 loss: 0.4370 Lr: 0.04804
[2023-08-07 21:21:39,341 INFO misc.py line 115 22900] Train: [18/100][7/156] Data 0.001 (0.001) Batch 3.422 (3.609) Remain 12:58:27 loss: 0.6687 Lr: 0.04804
[2023-08-07 21:21:42,848 INFO misc.py line 115 22900] Train: [18/100][8/156] Data 0.001 (0.001) Batch 3.507 (3.589) Remain 12:53:59 loss: 1.1805 Lr: 0.04804
[2023-08-07 21:21:46,523 INFO misc.py line 115 22900] Train: [18/100][9/156] Data 0.001 (0.001) Batch 3.675 (3.603) Remain 12:57:02 loss: 0.6542 Lr: 0.04804
[2023-08-07 21:21:50,645 INFO misc.py line 115 22900] Train: [18/100][10/156] Data 0.001 (0.001) Batch 4.122 (3.677) Remain 13:12:56 loss: 1.1040 Lr: 0.04803
[2023-08-07 21:21:53,850 INFO misc.py line 115 22900] Train: [18/100][11/156] Data 0.001 (0.001) Batch 3.205 (3.618) Remain 13:00:09 loss: 0.7338 Lr: 0.04803
[2023-08-07 21:21:56,863 INFO misc.py line 115 22900] Train: [18/100][12/156] Data 0.001 (0.001) Batch 3.013 (3.551) Remain 12:45:36 loss: 0.2960 Lr: 0.04803
[2023-08-07 21:22:00,828 INFO misc.py line 115 22900] Train: [18/100][13/156] Data 0.001 (0.001) Batch 3.965 (3.592) Remain 12:54:28 loss: 0.9984 Lr: 0.04803
[2023-08-07 21:22:04,771 INFO misc.py line 115 22900] Train: [18/100][14/156] Data 0.001 (0.001) Batch 3.942 (3.624) Remain 13:01:16 loss: 0.6808 Lr: 0.04803
[2023-08-07 21:22:07,909 INFO misc.py line 115 22900] Train: [18/100][15/156] Data 0.001 (0.001) Batch 3.139 (3.584) Remain 12:52:29 loss: 0.5686 Lr: 0.04802
[2023-08-07 21:22:12,336 INFO misc.py line 115 22900] Train: [18/100][16/156] Data 0.001 (0.001) Batch 4.427 (3.649) Remain 13:06:24 loss: 0.8448 Lr: 0.04802
[2023-08-07 21:22:14,519 INFO misc.py line 115 22900] Train: [18/100][17/156] Data 0.001 (0.001) Batch 2.183 (3.544) Remain 12:43:46 loss: 0.0963 Lr: 0.04802
[2023-08-07 21:22:16,849 INFO misc.py line 115 22900] Train: [18/100][18/156] Data 0.001 (0.001) Batch 2.330 (3.463) Remain 12:26:17 loss: 0.6404 Lr: 0.04802
[2023-08-07 21:22:20,187 INFO misc.py line 115 22900] Train: [18/100][19/156] Data 0.001 (0.001) Batch 3.337 (3.455) Remain 12:24:32 loss: 0.7080 Lr: 0.04802
[2023-08-07 21:22:23,120 INFO misc.py line 115 22900] Train: [18/100][20/156] Data 0.001 (0.001) Batch 2.934 (3.424) Remain 12:17:51 loss: 0.6350 Lr: 0.04801
[2023-08-07 21:22:26,472 INFO misc.py line 115 22900] Train: [18/100][21/156] Data 0.001 (0.001) Batch 3.351 (3.420) Remain 12:16:55 loss: 0.3882 Lr: 0.04801
[2023-08-07 21:22:30,025 INFO misc.py line 115 22900] Train: [18/100][22/156] Data 0.001 (0.001) Batch 3.554 (3.427) Remain 12:18:23 loss: 0.5084 Lr: 0.04801
[2023-08-07 21:22:32,872 INFO misc.py line 115 22900] Train: [18/100][23/156] Data 0.001 (0.001) Batch 2.847 (3.398) Remain 12:12:04 loss: 0.7701 Lr: 0.04801
[2023-08-07 21:22:36,996 INFO misc.py line 115 22900] Train: [18/100][24/156] Data 0.001 (0.001) Batch 4.124 (3.433) Remain 12:19:27 loss: 1.1858 Lr: 0.04801
[2023-08-07 21:22:41,453 INFO misc.py line 115 22900] Train: [18/100][25/156] Data 0.001 (0.001) Batch 4.456 (3.479) Remain 12:29:25 loss: 1.0655 Lr: 0.04800
[2023-08-07 21:22:45,452 INFO misc.py line 115 22900] Train: [18/100][26/156] Data 0.001 (0.001) Batch 4.000 (3.502) Remain 12:34:14 loss: 0.7273 Lr: 0.04800
[2023-08-07 21:22:48,862 INFO misc.py line 115 22900] Train: [18/100][27/156] Data 0.001 (0.001) Batch 3.410 (3.498) Remain 12:33:21 loss: 0.4687 Lr: 0.04800
[2023-08-07 21:22:51,957 INFO misc.py line 115 22900] Train: [18/100][28/156] Data 0.001 (0.001) Batch 3.095 (3.482) Remain 12:29:49 loss: 0.1962 Lr: 0.04800
[2023-08-07 21:22:53,968 INFO misc.py line 115 22900] Train: [18/100][29/156] Data 0.001 (0.001) Batch 2.010 (3.426) Remain 12:17:34 loss: 0.7188 Lr: 0.04800
[2023-08-07 21:22:57,339 INFO misc.py line 115 22900] Train: [18/100][30/156] Data 0.001 (0.001) Batch 3.371 (3.424) Remain 12:17:05 loss: 0.4367 Lr: 0.04799
[2023-08-07 21:23:00,638 INFO misc.py line 115 22900] Train: [18/100][31/156] Data 0.001 (0.001) Batch 3.299 (3.419) Remain 12:16:04 loss: 1.0249 Lr: 0.04799
[2023-08-07 21:23:03,772 INFO misc.py line 115 22900] Train: [18/100][32/156] Data 0.001 (0.001) Batch 3.134 (3.409) Remain 12:13:54 loss: 0.5571 Lr: 0.04799
[2023-08-07 21:23:06,865 INFO misc.py line 115 22900] Train: [18/100][33/156] Data 0.001 (0.001) Batch 3.093 (3.399) Remain 12:11:34 loss: 0.7895 Lr: 0.04799
[2023-08-07 21:23:10,084 INFO misc.py line 115 22900] Train: [18/100][34/156] Data 0.001 (0.001) Batch 3.219 (3.393) Remain 12:10:16 loss: 0.6381 Lr: 0.04798
[2023-08-07 21:23:13,782 INFO misc.py line 115 22900] Train: [18/100][35/156] Data 0.001 (0.001) Batch 3.698 (3.402) Remain 12:12:15 loss: 0.6255 Lr: 0.04798
[2023-08-07 21:23:17,317 INFO misc.py line 115 22900] Train: [18/100][36/156] Data 0.001 (0.001) Batch 3.535 (3.406) Remain 12:13:04 loss: 0.6359 Lr: 0.04798
[2023-08-07 21:23:20,746 INFO misc.py line 115 22900] Train: [18/100][37/156] Data 0.001 (0.001) Batch 3.429 (3.407) Remain 12:13:09 loss: 0.6590 Lr: 0.04798
[2023-08-07 21:23:24,363 INFO misc.py line 115 22900] Train: [18/100][38/156] Data 0.001 (0.001) Batch 3.618 (3.413) Remain 12:14:23 loss: 1.0810 Lr: 0.04798
[2023-08-07 21:23:28,106 INFO misc.py line 115 22900] Train: [18/100][39/156] Data 0.001 (0.001) Batch 3.742 (3.422) Remain 12:16:18 loss: 0.6077 Lr: 0.04797
[2023-08-07 21:23:32,041 INFO misc.py line 115 22900] Train: [18/100][40/156] Data 0.001 (0.001) Batch 3.935 (3.436) Remain 12:19:13 loss: 0.8384 Lr: 0.04797
[2023-08-07 21:23:36,074 INFO misc.py line 115 22900] Train: [18/100][41/156] Data 0.001 (0.001) Batch 4.033 (3.452) Remain 12:22:33 loss: 0.7590 Lr: 0.04797
[2023-08-07 21:23:39,345 INFO misc.py line 115 22900] Train: [18/100][42/156] Data 0.012 (0.001) Batch 3.271 (3.447) Remain 12:21:29 loss: 0.6973 Lr: 0.04797
[2023-08-07 21:23:43,895 INFO misc.py line 115 22900] Train: [18/100][43/156] Data 0.001 (0.001) Batch 4.550 (3.475) Remain 12:27:22 loss: 0.8367 Lr: 0.04797
[2023-08-07 21:23:47,313 INFO misc.py line 115 22900] Train: [18/100][44/156] Data 0.001 (0.001) Batch 3.418 (3.473) Remain 12:27:00 loss: 0.5229 Lr: 0.04796
[2023-08-07 21:23:50,903 INFO misc.py line 115 22900] Train: [18/100][45/156] Data 0.001 (0.001) Batch 3.590 (3.476) Remain 12:27:33 loss: 0.5338 Lr: 0.04796
[2023-08-07 21:23:55,007 INFO misc.py line 115 22900] Train: [18/100][46/156] Data 0.001 (0.001) Batch 4.103 (3.491) Remain 12:30:37 loss: 0.7075 Lr: 0.04796
[2023-08-07 21:23:58,062 INFO misc.py line 115 22900] Train: [18/100][47/156] Data 0.001 (0.001) Batch 3.056 (3.481) Remain 12:28:26 loss: 0.5553 Lr: 0.04796
[2023-08-07 21:23:59,988 INFO misc.py line 115 22900] Train: [18/100][48/156] Data 0.002 (0.001) Batch 1.925 (3.446) Remain 12:20:57 loss: 0.5181 Lr: 0.04796
[2023-08-07 21:24:03,355 INFO misc.py line 115 22900] Train: [18/100][49/156] Data 0.001 (0.001) Batch 3.367 (3.445) Remain 12:20:31 loss: 0.8162 Lr: 0.04795
[2023-08-07 21:24:07,648 INFO misc.py line 115 22900] Train: [18/100][50/156] Data 0.001 (0.001) Batch 4.293 (3.463) Remain 12:24:21 loss: 1.0183 Lr: 0.04795
[2023-08-07 21:24:11,780 INFO misc.py line 115 22900] Train: [18/100][51/156] Data 0.001 (0.001) Batch 4.132 (3.477) Remain 12:27:17 loss: 0.9158 Lr: 0.04795
[2023-08-07 21:24:15,748 INFO misc.py line 115 22900] Train: [18/100][52/156] Data 0.001 (0.001) Batch 3.968 (3.487) Remain 12:29:23 loss: 1.1207 Lr: 0.04795
[2023-08-07 21:24:19,752 INFO misc.py line 115 22900] Train: [18/100][53/156] Data 0.001 (0.001) Batch 4.004 (3.497) Remain 12:31:33 loss: 0.8061 Lr: 0.04795
[2023-08-07 21:24:23,040 INFO misc.py line 115 22900] Train: [18/100][54/156] Data 0.001 (0.001) Batch 3.288 (3.493) Remain 12:30:37 loss: 0.6863 Lr: 0.04794
[2023-08-07 21:24:24,753 INFO misc.py line 115 22900] Train: [18/100][55/156] Data 0.001 (0.001) Batch 1.713 (3.459) Remain 12:23:12 loss: 0.4808 Lr: 0.04794
[2023-08-07 21:24:29,222 INFO misc.py line 115 22900] Train: [18/100][56/156] Data 0.001 (0.001) Batch 4.469 (3.478) Remain 12:27:14 loss: 0.7728 Lr: 0.04794
[2023-08-07 21:24:32,030 INFO misc.py line 115 22900] Train: [18/100][57/156] Data 0.001 (0.001) Batch 2.808 (3.465) Remain 12:24:31 loss: 0.5018 Lr: 0.04794
[2023-08-07 21:24:35,711 INFO misc.py line 115 22900] Train: [18/100][58/156] Data 0.001 (0.001) Batch 3.681 (3.469) Remain 12:25:18 loss: 0.4716 Lr: 0.04793
[2023-08-07 21:24:39,926 INFO misc.py line 115 22900] Train: [18/100][59/156] Data 0.001 (0.001) Batch 4.215 (3.483) Remain 12:28:06 loss: 0.7579 Lr: 0.04793
[2023-08-07 21:24:43,172 INFO misc.py line 115 22900] Train: [18/100][60/156] Data 0.001 (0.001) Batch 3.246 (3.478) Remain 12:27:09 loss: 0.5998 Lr: 0.04793
[2023-08-07 21:24:46,390 INFO misc.py line 115 22900] Train: [18/100][61/156] Data 0.001 (0.001) Batch 3.218 (3.474) Remain 12:26:08 loss: 0.5292 Lr: 0.04793
[2023-08-07 21:24:50,802 INFO misc.py line 115 22900] Train: [18/100][62/156] Data 0.001 (0.001) Batch 4.412 (3.490) Remain 12:29:29 loss: 0.9864 Lr: 0.04793
[2023-08-07 21:24:53,434 INFO misc.py line 115 22900] Train: [18/100][63/156] Data 0.001 (0.001) Batch 2.632 (3.476) Remain 12:26:21 loss: 0.5735 Lr: 0.04792
[2023-08-07 21:24:56,217 INFO misc.py line 115 22900] Train: [18/100][64/156] Data 0.001 (0.001) Batch 2.783 (3.464) Remain 12:23:52 loss: 0.3298 Lr: 0.04792
[2023-08-07 21:24:59,653 INFO misc.py line 115 22900] Train: [18/100][65/156] Data 0.001 (0.001) Batch 3.436 (3.464) Remain 12:23:42 loss: 0.5329 Lr: 0.04792
[2023-08-07 21:25:02,305 INFO misc.py line 115 22900] Train: [18/100][66/156] Data 0.001 (0.001) Batch 2.652 (3.451) Remain 12:20:53 loss: 0.3354 Lr: 0.04792
[2023-08-07 21:25:06,324 INFO misc.py line 115 22900] Train: [18/100][67/156] Data 0.001 (0.001) Batch 4.019 (3.460) Remain 12:22:44 loss: 0.8126 Lr: 0.04792
[2023-08-07 21:25:09,284 INFO misc.py line 115 22900] Train: [18/100][68/156] Data 0.001 (0.001) Batch 2.959 (3.452) Remain 12:21:01 loss: 0.3174 Lr: 0.04791
[2023-08-07 21:25:12,212 INFO misc.py line 115 22900] Train: [18/100][69/156] Data 0.001 (0.001) Batch 2.928 (3.444) Remain 12:19:16 loss: 0.2958 Lr: 0.04791
[2023-08-07 21:25:15,004 INFO misc.py line 115 22900] Train: [18/100][70/156] Data 0.001 (0.001) Batch 2.792 (3.434) Remain 12:17:07 loss: 0.5878 Lr: 0.04791
[2023-08-07 21:25:17,970 INFO misc.py line 115 22900] Train: [18/100][71/156] Data 0.001 (0.001) Batch 2.966 (3.427) Remain 12:15:35 loss: 0.8352 Lr: 0.04791
[2023-08-07 21:25:21,363 INFO misc.py line 115 22900] Train: [18/100][72/156] Data 0.001 (0.001) Batch 3.393 (3.427) Remain 12:15:25 loss: 0.4870 Lr: 0.04790
[2023-08-07 21:25:24,596 INFO misc.py line 115 22900] Train: [18/100][73/156] Data 0.001 (0.001) Batch 3.233 (3.424) Remain 12:14:46 loss: 0.2939 Lr: 0.04790
[2023-08-07 21:25:27,379 INFO misc.py line 115 22900] Train: [18/100][74/156] Data 0.001 (0.001) Batch 2.783 (3.415) Remain 12:12:46 loss: 0.6258 Lr: 0.04790
[2023-08-07 21:25:30,099 INFO misc.py line 115 22900] Train: [18/100][75/156] Data 0.001 (0.001) Batch 2.721 (3.405) Remain 12:10:38 loss: 0.5464 Lr: 0.04790
[2023-08-07 21:25:34,494 INFO misc.py line 115 22900] Train: [18/100][76/156] Data 0.001 (0.001) Batch 4.395 (3.419) Remain 12:13:29 loss: 0.8156 Lr: 0.04790
[2023-08-07 21:25:39,135 INFO misc.py line 115 22900] Train: [18/100][77/156] Data 0.001 (0.001) Batch 4.641 (3.436) Remain 12:16:59 loss: 1.0893 Lr: 0.04789
[2023-08-07 21:25:42,757 INFO misc.py line 115 22900] Train: [18/100][78/156] Data 0.001 (0.001) Batch 3.621 (3.438) Remain 12:17:27 loss: 0.6098 Lr: 0.04789
[2023-08-07 21:25:45,909 INFO misc.py line 115 22900] Train: [18/100][79/156] Data 0.001 (0.001) Batch 3.153 (3.434) Remain 12:16:35 loss: 0.8242 Lr: 0.04789
[2023-08-07 21:25:48,649 INFO misc.py line 115 22900] Train: [18/100][80/156] Data 0.001 (0.001) Batch 2.740 (3.425) Remain 12:14:36 loss: 0.5534 Lr: 0.04789
[2023-08-07 21:25:50,428 INFO misc.py line 115 22900] Train: [18/100][81/156] Data 0.001 (0.001) Batch 1.779 (3.404) Remain 12:10:01 loss: 0.1080 Lr: 0.04789
[2023-08-07 21:25:54,491 INFO misc.py line 115 22900] Train: [18/100][82/156] Data 0.001 (0.001) Batch 4.063 (3.412) Remain 12:11:45 loss: 0.5321 Lr: 0.04788
[2023-08-07 21:25:57,766 INFO misc.py line 115 22900] Train: [18/100][83/156] Data 0.001 (0.001) Batch 3.275 (3.411) Remain 12:11:19 loss: 0.4529 Lr: 0.04788
[2023-08-07 21:25:59,805 INFO misc.py line 115 22900] Train: [18/100][84/156] Data 0.001 (0.001) Batch 2.039 (3.394) Remain 12:07:38 loss: 0.3092 Lr: 0.04788
[2023-08-07 21:26:02,908 INFO misc.py line 115 22900] Train: [18/100][85/156] Data 0.001 (0.001) Batch 3.104 (3.390) Remain 12:06:49 loss: 0.4994 Lr: 0.04788
[2023-08-07 21:26:07,075 INFO misc.py line 115 22900] Train: [18/100][86/156] Data 0.001 (0.001) Batch 4.166 (3.400) Remain 12:08:46 loss: 0.9287 Lr: 0.04788
[2023-08-07 21:26:10,551 INFO misc.py line 115 22900] Train: [18/100][87/156] Data 0.001 (0.001) Batch 3.477 (3.401) Remain 12:08:54 loss: 0.5951 Lr: 0.04787
[2023-08-07 21:26:14,716 INFO misc.py line 115 22900] Train: [18/100][88/156] Data 0.001 (0.001) Batch 4.165 (3.410) Remain 12:10:46 loss: 0.7574 Lr: 0.04787
[2023-08-07 21:26:18,749 INFO misc.py line 115 22900] Train: [18/100][89/156] Data 0.001 (0.001) Batch 4.033 (3.417) Remain 12:12:16 loss: 0.4173 Lr: 0.04787
[2023-08-07 21:26:22,768 INFO misc.py line 115 22900] Train: [18/100][90/156] Data 0.001 (0.001) Batch 4.019 (3.424) Remain 12:13:42 loss: 0.5163 Lr: 0.04787
[2023-08-07 21:26:26,406 INFO misc.py line 115 22900] Train: [18/100][91/156] Data 0.001 (0.001) Batch 3.638 (3.426) Remain 12:14:10 loss: 0.8133 Lr: 0.04786
[2023-08-07 21:26:30,421 INFO misc.py line 115 22900] Train: [18/100][92/156] Data 0.001 (0.001) Batch 4.016 (3.433) Remain 12:15:31 loss: 1.1626 Lr: 0.04786
[2023-08-07 21:26:34,191 INFO misc.py line 115 22900] Train: [18/100][93/156] Data 0.001 (0.001) Batch 3.769 (3.437) Remain 12:16:16 loss: 0.7279 Lr: 0.04786
[2023-08-07 21:26:38,800 INFO misc.py line 115 22900] Train: [18/100][94/156] Data 0.001 (0.001) Batch 4.609 (3.449) Remain 12:18:58 loss: 0.9420 Lr: 0.04786
[2023-08-07 21:26:42,605 INFO misc.py line 115 22900] Train: [18/100][95/156] Data 0.001 (0.001) Batch 3.805 (3.453) Remain 12:19:44 loss: 0.8041 Lr: 0.04786
[2023-08-07 21:26:46,218 INFO misc.py line 115 22900] Train: [18/100][96/156] Data 0.001 (0.001) Batch 3.613 (3.455) Remain 12:20:03 loss: 0.9509 Lr: 0.04785
[2023-08-07 21:26:50,297 INFO misc.py line 115 22900] Train: [18/100][97/156] Data 0.001 (0.001) Batch 4.079 (3.462) Remain 12:21:25 loss: 0.5919 Lr: 0.04785
[2023-08-07 21:26:54,385 INFO misc.py line 115 22900] Train: [18/100][98/156] Data 0.001 (0.001) Batch 4.088 (3.468) Remain 12:22:46 loss: 0.7392 Lr: 0.04785
[2023-08-07 21:26:58,504 INFO misc.py line 115 22900] Train: [18/100][99/156] Data 0.001 (0.001) Batch 4.119 (3.475) Remain 12:24:10 loss: 0.8813 Lr: 0.04785
[2023-08-07 21:27:01,131 INFO misc.py line 115 22900] Train: [18/100][100/156] Data 0.001 (0.001) Batch 2.627 (3.466) Remain 12:22:14 loss: 0.5820 Lr: 0.04785
[2023-08-07 21:27:04,981 INFO misc.py line 115 22900] Train: [18/100][101/156] Data 0.001 (0.001) Batch 3.850 (3.470) Remain 12:23:01 loss: 0.8825 Lr: 0.04784
[2023-08-07 21:27:09,013 INFO misc.py line 115 22900] Train: [18/100][102/156] Data 0.001 (0.001) Batch 4.031 (3.476) Remain 12:24:10 loss: 0.5675 Lr: 0.04784
[2023-08-07 21:27:13,163 INFO misc.py line 115 22900] Train: [18/100][103/156] Data 0.001 (0.001) Batch 4.150 (3.483) Remain 12:25:33 loss: 0.6167 Lr: 0.04784
[2023-08-07 21:27:16,840 INFO misc.py line 115 22900] Train: [18/100][104/156] Data 0.001 (0.001) Batch 3.678 (3.485) Remain 12:25:55 loss: 0.7140 Lr: 0.04784
[2023-08-07 21:27:21,101 INFO misc.py line 115 22900] Train: [18/100][105/156] Data 0.001 (0.001) Batch 4.260 (3.492) Remain 12:27:29 loss: 0.8226 Lr: 0.04783
[2023-08-07 21:27:24,982 INFO misc.py line 115 22900] Train: [18/100][106/156] Data 0.001 (0.001) Batch 3.881 (3.496) Remain 12:28:14 loss: 0.8736 Lr: 0.04783
[2023-08-07 21:27:27,720 INFO misc.py line 115 22900] Train: [18/100][107/156] Data 0.001 (0.001) Batch 2.738 (3.489) Remain 12:26:37 loss: 0.5531 Lr: 0.04783
[2023-08-07 21:27:31,229 INFO misc.py line 115 22900] Train: [18/100][108/156] Data 0.001 (0.001) Batch 3.509 (3.489) Remain 12:26:36 loss: 1.1559 Lr: 0.04783
[2023-08-07 21:27:33,658 INFO misc.py line 115 22900] Train: [18/100][109/156] Data 0.001 (0.001) Batch 2.429 (3.479) Remain 12:24:24 loss: 0.5005 Lr: 0.04783
[2023-08-07 21:27:37,089 INFO misc.py line 115 22900] Train: [18/100][110/156] Data 0.001 (0.001) Batch 3.431 (3.478) Remain 12:24:15 loss: 0.6423 Lr: 0.04782
[2023-08-07 21:27:41,047 INFO misc.py line 115 22900] Train: [18/100][111/156] Data 0.001 (0.001) Batch 3.958 (3.483) Remain 12:25:08 loss: 0.6200 Lr: 0.04782
[2023-08-07 21:27:45,491 INFO misc.py line 115 22900] Train: [18/100][112/156] Data 0.001 (0.001) Batch 4.444 (3.492) Remain 12:26:58 loss: 0.8212 Lr: 0.04782
[2023-08-07 21:27:49,256 INFO misc.py line 115 22900] Train: [18/100][113/156] Data 0.001 (0.001) Batch 3.765 (3.494) Remain 12:27:26 loss: 0.5582 Lr: 0.04782
[2023-08-07 21:27:53,076 INFO misc.py line 115 22900] Train: [18/100][114/156] Data 0.001 (0.001) Batch 3.819 (3.497) Remain 12:28:01 loss: 0.6391 Lr: 0.04781
[2023-08-07 21:27:57,082 INFO misc.py line 115 22900] Train: [18/100][115/156] Data 0.001 (0.001) Batch 4.007 (3.502) Remain 12:28:55 loss: 0.6159 Lr: 0.04781
[2023-08-07 21:27:59,586 INFO misc.py line 115 22900] Train: [18/100][116/156] Data 0.001 (0.001) Batch 2.504 (3.493) Remain 12:26:59 loss: 0.5844 Lr: 0.04781
[2023-08-07 21:28:02,903 INFO misc.py line 115 22900] Train: [18/100][117/156] Data 0.001 (0.001) Batch 3.316 (3.491) Remain 12:26:35 loss: 0.6641 Lr: 0.04781
[2023-08-07 21:28:06,479 INFO misc.py line 115 22900] Train: [18/100][118/156] Data 0.001 (0.001) Batch 3.576 (3.492) Remain 12:26:41 loss: 0.7837 Lr: 0.04781
[2023-08-07 21:28:09,936 INFO misc.py line 115 22900] Train: [18/100][119/156] Data 0.001 (0.001) Batch 3.456 (3.492) Remain 12:26:34 loss: 0.6113 Lr: 0.04780
[2023-08-07 21:28:13,098 INFO misc.py line 115 22900] Train: [18/100][120/156] Data 0.001 (0.001) Batch 3.162 (3.489) Remain 12:25:54 loss: 0.5855 Lr: 0.04780
[2023-08-07 21:28:15,646 INFO misc.py line 115 22900] Train: [18/100][121/156] Data 0.001 (0.001) Batch 2.548 (3.481) Remain 12:24:09 loss: 0.5092 Lr: 0.04780
[2023-08-07 21:28:19,281 INFO misc.py line 115 22900] Train: [18/100][122/156] Data 0.001 (0.001) Batch 3.635 (3.482) Remain 12:24:22 loss: 0.6900 Lr: 0.04780
[2023-08-07 21:28:22,012 INFO misc.py line 115 22900] Train: [18/100][123/156] Data 0.001 (0.001) Batch 2.731 (3.476) Remain 12:22:58 loss: 0.4280 Lr: 0.04780
[2023-08-07 21:28:24,178 INFO misc.py line 115 22900] Train: [18/100][124/156] Data 0.001 (0.001) Batch 2.166 (3.465) Remain 12:20:36 loss: 0.4407 Lr: 0.04779
[2023-08-07 21:28:27,234 INFO misc.py line 115 22900] Train: [18/100][125/156] Data 0.001 (0.001) Batch 3.055 (3.462) Remain 12:19:49 loss: 0.4816 Lr: 0.04779
[2023-08-07 21:28:30,849 INFO misc.py line 115 22900] Train: [18/100][126/156] Data 0.001 (0.001) Batch 3.615 (3.463) Remain 12:20:02 loss: 0.5992 Lr: 0.04779
[2023-08-07 21:28:33,847 INFO misc.py line 115 22900] Train: [18/100][127/156] Data 0.001 (0.001) Batch 2.999 (3.459) Remain 12:19:10 loss: 0.4893 Lr: 0.04779
[2023-08-07 21:28:38,070 INFO misc.py line 115 22900] Train: [18/100][128/156] Data 0.001 (0.001) Batch 4.223 (3.465) Remain 12:20:25 loss: 1.0174 Lr: 0.04778
[2023-08-07 21:28:40,328 INFO misc.py line 115 22900] Train: [18/100][129/156] Data 0.001 (0.001) Batch 2.257 (3.456) Remain 12:18:19 loss: 0.4851 Lr: 0.04778
[2023-08-07 21:28:43,502 INFO misc.py line 115 22900] Train: [18/100][130/156] Data 0.001 (0.001) Batch 3.175 (3.454) Remain 12:17:47 loss: 0.7842 Lr: 0.04778
[2023-08-07 21:28:46,865 INFO misc.py line 115 22900] Train: [18/100][131/156] Data 0.001 (0.001) Batch 3.363 (3.453) Remain 12:17:34 loss: 0.5611 Lr: 0.04778
[2023-08-07 21:28:50,152 INFO misc.py line 115 22900] Train: [18/100][132/156] Data 0.001 (0.001) Batch 3.287 (3.452) Remain 12:17:14 loss: 0.5871 Lr: 0.04778
[2023-08-07 21:28:53,652 INFO misc.py line 115 22900] Train: [18/100][133/156] Data 0.001 (0.001) Batch 3.500 (3.452) Remain 12:17:16 loss: 0.5545 Lr: 0.04777
[2023-08-07 21:28:56,777 INFO misc.py line 115 22900] Train: [18/100][134/156] Data 0.001 (0.001) Batch 3.125 (3.449) Remain 12:16:40 loss: 0.4073 Lr: 0.04777
[2023-08-07 21:29:00,786 INFO misc.py line 115 22900] Train: [18/100][135/156] Data 0.001 (0.001) Batch 4.009 (3.454) Remain 12:17:31 loss: 1.0141 Lr: 0.04777
[2023-08-07 21:29:02,826 INFO misc.py line 115 22900] Train: [18/100][136/156] Data 0.001 (0.001) Batch 2.040 (3.443) Remain 12:15:12 loss: 0.3368 Lr: 0.04777
[2023-08-07 21:29:05,059 INFO misc.py line 115 22900] Train: [18/100][137/156] Data 0.001 (0.001) Batch 2.233 (3.434) Remain 12:13:12 loss: 0.3540 Lr: 0.04776
[2023-08-07 21:29:08,339 INFO misc.py line 115 22900] Train: [18/100][138/156] Data 0.001 (0.001) Batch 3.279 (3.433) Remain 12:12:54 loss: 0.9039 Lr: 0.04776
[2023-08-07 21:29:12,474 INFO misc.py line 115 22900] Train: [18/100][139/156] Data 0.001 (0.001) Batch 4.135 (3.438) Remain 12:13:57 loss: 0.5084 Lr: 0.04776
[2023-08-07 21:29:16,087 INFO misc.py line 115 22900] Train: [18/100][140/156] Data 0.001 (0.001) Batch 3.613 (3.439) Remain 12:14:10 loss: 0.8439 Lr: 0.04776
[2023-08-07 21:29:20,237 INFO misc.py line 115 22900] Train: [18/100][141/156] Data 0.001 (0.001) Batch 4.150 (3.444) Remain 12:15:12 loss: 0.8296 Lr: 0.04776
[2023-08-07 21:29:23,027 INFO misc.py line 115 22900] Train: [18/100][142/156] Data 0.001 (0.001) Batch 2.791 (3.440) Remain 12:14:09 loss: 0.5052 Lr: 0.04775
[2023-08-07 21:29:26,706 INFO misc.py line 115 22900] Train: [18/100][143/156] Data 0.001 (0.001) Batch 3.679 (3.441) Remain 12:14:27 loss: 0.5761 Lr: 0.04775
[2023-08-07 21:29:29,866 INFO misc.py line 115 22900] Train: [18/100][144/156] Data 0.001 (0.001) Batch 3.160 (3.439) Remain 12:13:58 loss: 0.4887 Lr: 0.04775
[2023-08-07 21:29:32,859 INFO misc.py line 115 22900] Train: [18/100][145/156] Data 0.001 (0.001) Batch 2.993 (3.436) Remain 12:13:14 loss: 0.3321 Lr: 0.04775
[2023-08-07 21:29:36,743 INFO misc.py line 115 22900] Train: [18/100][146/156] Data 0.001 (0.001) Batch 3.884 (3.439) Remain 12:13:51 loss: 0.8666 Lr: 0.04775
[2023-08-07 21:29:40,823 INFO misc.py line 115 22900] Train: [18/100][147/156] Data 0.001 (0.001) Batch 4.080 (3.444) Remain 12:14:45 loss: 0.7550 Lr: 0.04774
[2023-08-07 21:29:44,233 INFO misc.py line 115 22900] Train: [18/100][148/156] Data 0.001 (0.001) Batch 3.410 (3.444) Remain 12:14:38 loss: 0.5726 Lr: 0.04774
[2023-08-07 21:29:46,864 INFO misc.py line 115 22900] Train: [18/100][149/156] Data 0.001 (0.001) Batch 2.631 (3.438) Remain 12:13:24 loss: 0.5912 Lr: 0.04774
[2023-08-07 21:29:50,014 INFO misc.py line 115 22900] Train: [18/100][150/156] Data 0.001 (0.001) Batch 3.151 (3.436) Remain 12:12:55 loss: 0.4286 Lr: 0.04774
[2023-08-07 21:29:53,700 INFO misc.py line 115 22900] Train: [18/100][151/156] Data 0.001 (0.001) Batch 3.685 (3.438) Remain 12:13:13 loss: 0.5947 Lr: 0.04773
[2023-08-07 21:29:57,174 INFO misc.py line 115 22900] Train: [18/100][152/156] Data 0.001 (0.001) Batch 3.475 (3.438) Remain 12:13:13 loss: 0.9995 Lr: 0.04773
[2023-08-07 21:29:59,614 INFO misc.py line 115 22900] Train: [18/100][153/156] Data 0.001 (0.001) Batch 2.440 (3.431) Remain 12:11:44 loss: 0.5229 Lr: 0.04773
[2023-08-07 21:30:02,960 INFO misc.py line 115 22900] Train: [18/100][154/156] Data 0.001 (0.001) Batch 3.346 (3.431) Remain 12:11:34 loss: 0.7653 Lr: 0.04773
[2023-08-07 21:30:06,960 INFO misc.py line 115 22900] Train: [18/100][155/156] Data 0.001 (0.001) Batch 4.000 (3.435) Remain 12:12:18 loss: 0.9533 Lr: 0.04773
[2023-08-07 21:30:09,874 INFO misc.py line 115 22900] Train: [18/100][156/156] Data 0.001 (0.001) Batch 2.914 (3.431) Remain 12:11:31 loss: 0.3599 Lr: 0.04772
[2023-08-07 21:30:09,875 INFO misc.py line 129 22900] Train result: loss: 0.6582 
[2023-08-07 21:30:09,875 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 21:30:11,971 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.3772 
[2023-08-07 21:30:12,839 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.8434 
[2023-08-07 21:30:14,504 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.8073 
[2023-08-07 21:30:16,025 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.1911 
[2023-08-07 21:30:17,870 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.5192 
[2023-08-07 21:30:19,533 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.8055 
[2023-08-07 21:30:21,672 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.6521 
[2023-08-07 21:30:23,476 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.4981 
[2023-08-07 21:30:24,759 INFO evaluator.py line 122 22900] Test: [9/24] Loss 0.9650 
[2023-08-07 21:30:26,887 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.2690 
[2023-08-07 21:30:27,413 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.3226 
[2023-08-07 21:30:28,945 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.7204 
[2023-08-07 21:30:31,653 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.2438 
[2023-08-07 21:30:33,331 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.7844 
[2023-08-07 21:30:35,351 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.5044 
[2023-08-07 21:30:38,063 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.5019 
[2023-08-07 21:30:40,776 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.3482 
[2023-08-07 21:30:42,624 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.4334 
[2023-08-07 21:30:43,374 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.2159 
[2023-08-07 21:30:44,260 INFO evaluator.py line 122 22900] Test: [20/24] Loss 1.0338 
[2023-08-07 21:30:46,520 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3372 
[2023-08-07 21:30:48,484 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.5468 
[2023-08-07 21:30:50,332 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.2330 
[2023-08-07 21:30:52,268 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.2089 
[2023-08-07 21:30:52,316 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.1884/0.2877/0.6608.
[2023-08-07 21:30:52,316 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6631/0.9344
[2023-08-07 21:30:52,316 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9383/0.9943
[2023-08-07 21:30:52,316 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1043/0.3073
[2023-08-07 21:30:52,316 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.0955/0.2749
[2023-08-07 21:30:52,316 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6165/0.7817
[2023-08-07 21:30:52,316 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.1865/0.2099
[2023-08-07 21:30:52,316 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.3715/0.4855
[2023-08-07 21:30:52,316 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0400/0.0414
[2023-08-07 21:30:52,316 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.0987/0.2917
[2023-08-07 21:30:52,316 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0611/0.0631
[2023-08-07 21:30:52,316 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 21:30:52,316 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0000/0.0000
[2023-08-07 21:30:52,316 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1417/0.3402
[2023-08-07 21:30:52,316 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.2023/0.3209
[2023-08-07 21:30:52,316 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0109/0.0113
[2023-08-07 21:30:52,316 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0001/0.0001
[2023-08-07 21:30:52,316 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0003/0.0003
[2023-08-07 21:30:52,316 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.1679/0.5791
[2023-08-07 21:30:52,316 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 21:30:52,316 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0691/0.1184
[2023-08-07 21:30:52,317 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 21:30:52,317 INFO misc.py line 150 22900] Best validation mIoU updated to: 0.1884
[2023-08-07 21:30:52,317 INFO misc.py line 152 22900] Currently Best mIoU: 0.1884
[2023-08-07 21:30:52,317 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 21:30:58,342 INFO misc.py line 115 22900] Train: [19/100][1/156] Data 1.551 (1.551) Batch 4.918 (4.918) Remain 17:28:24 loss: 0.8633 Lr: 0.04772
[2023-08-07 21:31:03,005 INFO misc.py line 115 22900] Train: [19/100][2/156] Data 0.001 (0.001) Batch 4.663 (4.663) Remain 16:33:57 loss: 1.2148 Lr: 0.04772
[2023-08-07 21:31:05,927 INFO misc.py line 115 22900] Train: [19/100][3/156] Data 0.001 (0.001) Batch 2.922 (2.922) Remain 10:22:53 loss: 0.4915 Lr: 0.04772
[2023-08-07 21:31:09,426 INFO misc.py line 115 22900] Train: [19/100][4/156] Data 0.001 (0.001) Batch 3.499 (3.499) Remain 12:25:41 loss: 0.7473 Lr: 0.04771
[2023-08-07 21:31:13,418 INFO misc.py line 115 22900] Train: [19/100][5/156] Data 0.001 (0.001) Batch 3.992 (3.745) Remain 13:18:10 loss: 0.5902 Lr: 0.04771
[2023-08-07 21:31:17,976 INFO misc.py line 115 22900] Train: [19/100][6/156] Data 0.001 (0.001) Batch 4.557 (4.016) Remain 14:15:48 loss: 1.2400 Lr: 0.04771
[2023-08-07 21:31:21,215 INFO misc.py line 115 22900] Train: [19/100][7/156] Data 0.001 (0.001) Batch 3.239 (3.822) Remain 13:34:22 loss: 0.7041 Lr: 0.04771
[2023-08-07 21:31:23,893 INFO misc.py line 115 22900] Train: [19/100][8/156] Data 0.001 (0.001) Batch 2.678 (3.593) Remain 12:45:34 loss: 0.4085 Lr: 0.04771
[2023-08-07 21:31:26,425 INFO misc.py line 115 22900] Train: [19/100][9/156] Data 0.001 (0.001) Batch 2.532 (3.416) Remain 12:07:49 loss: 0.3861 Lr: 0.04770
[2023-08-07 21:31:30,155 INFO misc.py line 115 22900] Train: [19/100][10/156] Data 0.001 (0.001) Batch 3.730 (3.461) Remain 12:17:18 loss: 0.6867 Lr: 0.04770
[2023-08-07 21:31:32,763 INFO misc.py line 115 22900] Train: [19/100][11/156] Data 0.001 (0.001) Batch 2.608 (3.354) Remain 11:54:32 loss: 0.7056 Lr: 0.04770
[2023-08-07 21:31:35,901 INFO misc.py line 115 22900] Train: [19/100][12/156] Data 0.001 (0.001) Batch 3.138 (3.330) Remain 11:49:22 loss: 0.5689 Lr: 0.04770
[2023-08-07 21:31:39,740 INFO misc.py line 115 22900] Train: [19/100][13/156] Data 0.001 (0.001) Batch 3.838 (3.381) Remain 12:00:08 loss: 0.5481 Lr: 0.04769
[2023-08-07 21:31:43,173 INFO misc.py line 115 22900] Train: [19/100][14/156] Data 0.002 (0.001) Batch 3.434 (3.386) Remain 12:01:05 loss: 0.4617 Lr: 0.04769
[2023-08-07 21:31:45,881 INFO misc.py line 115 22900] Train: [19/100][15/156] Data 0.001 (0.001) Batch 2.708 (3.329) Remain 11:49:00 loss: 0.3315 Lr: 0.04769
[2023-08-07 21:31:49,480 INFO misc.py line 115 22900] Train: [19/100][16/156] Data 0.001 (0.001) Batch 3.599 (3.350) Remain 11:53:21 loss: 0.5719 Lr: 0.04769
[2023-08-07 21:31:52,333 INFO misc.py line 115 22900] Train: [19/100][17/156] Data 0.001 (0.001) Batch 2.853 (3.315) Remain 11:45:44 loss: 0.5826 Lr: 0.04769
[2023-08-07 21:31:56,056 INFO misc.py line 115 22900] Train: [19/100][18/156] Data 0.001 (0.001) Batch 3.723 (3.342) Remain 11:51:29 loss: 0.5442 Lr: 0.04768
[2023-08-07 21:31:59,970 INFO misc.py line 115 22900] Train: [19/100][19/156] Data 0.001 (0.001) Batch 3.915 (3.378) Remain 11:59:03 loss: 0.9349 Lr: 0.04768
[2023-08-07 21:32:02,565 INFO misc.py line 115 22900] Train: [19/100][20/156] Data 0.001 (0.001) Batch 2.595 (3.332) Remain 11:49:11 loss: 0.5146 Lr: 0.04768
[2023-08-07 21:32:06,095 INFO misc.py line 115 22900] Train: [19/100][21/156] Data 0.001 (0.001) Batch 3.529 (3.343) Remain 11:51:28 loss: 0.6249 Lr: 0.04768
[2023-08-07 21:32:09,670 INFO misc.py line 115 22900] Train: [19/100][22/156] Data 0.001 (0.001) Batch 3.576 (3.355) Remain 11:54:01 loss: 0.9272 Lr: 0.04767
[2023-08-07 21:32:13,562 INFO misc.py line 115 22900] Train: [19/100][23/156] Data 0.001 (0.001) Batch 3.892 (3.382) Remain 11:59:41 loss: 0.6299 Lr: 0.04767
[2023-08-07 21:32:17,615 INFO misc.py line 115 22900] Train: [19/100][24/156] Data 0.001 (0.001) Batch 4.053 (3.414) Remain 12:06:26 loss: 0.5745 Lr: 0.04767
[2023-08-07 21:32:20,124 INFO misc.py line 115 22900] Train: [19/100][25/156] Data 0.001 (0.001) Batch 2.509 (3.373) Remain 11:57:37 loss: 0.3498 Lr: 0.04767
[2023-08-07 21:32:23,146 INFO misc.py line 115 22900] Train: [19/100][26/156] Data 0.001 (0.001) Batch 3.021 (3.357) Remain 11:54:19 loss: 0.2935 Lr: 0.04767
[2023-08-07 21:32:26,143 INFO misc.py line 115 22900] Train: [19/100][27/156] Data 0.001 (0.001) Batch 2.998 (3.342) Remain 11:51:04 loss: 0.1627 Lr: 0.04766
[2023-08-07 21:32:30,162 INFO misc.py line 115 22900] Train: [19/100][28/156] Data 0.001 (0.001) Batch 4.018 (3.369) Remain 11:56:46 loss: 0.7390 Lr: 0.04766
[2023-08-07 21:32:34,283 INFO misc.py line 115 22900] Train: [19/100][29/156] Data 0.001 (0.001) Batch 4.122 (3.398) Remain 12:02:52 loss: 0.6856 Lr: 0.04766
[2023-08-07 21:32:38,700 INFO misc.py line 115 22900] Train: [19/100][30/156] Data 0.001 (0.001) Batch 4.417 (3.436) Remain 12:10:50 loss: 0.8323 Lr: 0.04766
[2023-08-07 21:32:42,048 INFO misc.py line 115 22900] Train: [19/100][31/156] Data 0.001 (0.001) Batch 3.348 (3.433) Remain 12:10:06 loss: 0.9011 Lr: 0.04765
[2023-08-07 21:32:45,585 INFO misc.py line 115 22900] Train: [19/100][32/156] Data 0.001 (0.001) Batch 3.537 (3.436) Remain 12:10:49 loss: 1.1090 Lr: 0.04765
[2023-08-07 21:32:49,206 INFO misc.py line 115 22900] Train: [19/100][33/156] Data 0.001 (0.001) Batch 3.620 (3.443) Remain 12:12:04 loss: 0.9008 Lr: 0.04765
[2023-08-07 21:32:51,943 INFO misc.py line 115 22900] Train: [19/100][34/156] Data 0.001 (0.001) Batch 2.738 (3.420) Remain 12:07:10 loss: 0.3118 Lr: 0.04765
[2023-08-07 21:32:55,578 INFO misc.py line 115 22900] Train: [19/100][35/156] Data 0.001 (0.001) Batch 3.635 (3.427) Remain 12:08:32 loss: 0.6345 Lr: 0.04765
[2023-08-07 21:32:59,060 INFO misc.py line 115 22900] Train: [19/100][36/156] Data 0.001 (0.001) Batch 3.482 (3.428) Remain 12:08:50 loss: 0.5468 Lr: 0.04764
[2023-08-07 21:33:02,289 INFO misc.py line 115 22900] Train: [19/100][37/156] Data 0.001 (0.001) Batch 3.229 (3.422) Remain 12:07:32 loss: 0.5741 Lr: 0.04764
[2023-08-07 21:33:05,251 INFO misc.py line 115 22900] Train: [19/100][38/156] Data 0.001 (0.001) Batch 2.962 (3.409) Remain 12:04:41 loss: 0.2726 Lr: 0.04764
[2023-08-07 21:33:09,245 INFO misc.py line 115 22900] Train: [19/100][39/156] Data 0.001 (0.001) Batch 3.995 (3.425) Remain 12:08:05 loss: 0.7160 Lr: 0.04764
[2023-08-07 21:33:13,368 INFO misc.py line 115 22900] Train: [19/100][40/156] Data 0.001 (0.001) Batch 4.122 (3.444) Remain 12:12:02 loss: 0.7354 Lr: 0.04763
[2023-08-07 21:33:15,665 INFO misc.py line 115 22900] Train: [19/100][41/156] Data 0.001 (0.001) Batch 2.297 (3.414) Remain 12:05:33 loss: 0.4297 Lr: 0.04763
[2023-08-07 21:33:19,122 INFO misc.py line 115 22900] Train: [19/100][42/156] Data 0.001 (0.001) Batch 3.457 (3.415) Remain 12:05:44 loss: 0.9125 Lr: 0.04763
[2023-08-07 21:33:23,223 INFO misc.py line 115 22900] Train: [19/100][43/156] Data 0.001 (0.001) Batch 4.101 (3.432) Remain 12:09:19 loss: 0.4631 Lr: 0.04763
[2023-08-07 21:33:27,159 INFO misc.py line 115 22900] Train: [19/100][44/156] Data 0.001 (0.001) Batch 3.937 (3.445) Remain 12:11:52 loss: 0.5725 Lr: 0.04762
[2023-08-07 21:33:30,013 INFO misc.py line 115 22900] Train: [19/100][45/156] Data 0.001 (0.001) Batch 2.854 (3.431) Remain 12:08:49 loss: 0.6111 Lr: 0.04762
[2023-08-07 21:33:33,580 INFO misc.py line 115 22900] Train: [19/100][46/156] Data 0.001 (0.001) Batch 3.568 (3.434) Remain 12:09:27 loss: 0.5425 Lr: 0.04762
[2023-08-07 21:33:36,497 INFO misc.py line 115 22900] Train: [19/100][47/156] Data 0.001 (0.001) Batch 2.917 (3.422) Remain 12:06:53 loss: 0.5728 Lr: 0.04762
[2023-08-07 21:33:39,837 INFO misc.py line 115 22900] Train: [19/100][48/156] Data 0.001 (0.001) Batch 3.339 (3.420) Remain 12:06:27 loss: 0.4729 Lr: 0.04762
[2023-08-07 21:33:42,779 INFO misc.py line 115 22900] Train: [19/100][49/156] Data 0.001 (0.001) Batch 2.942 (3.410) Remain 12:04:11 loss: 0.3736 Lr: 0.04761
[2023-08-07 21:33:46,848 INFO misc.py line 115 22900] Train: [19/100][50/156] Data 0.001 (0.001) Batch 4.069 (3.424) Remain 12:07:06 loss: 0.5101 Lr: 0.04761
[2023-08-07 21:33:50,440 INFO misc.py line 115 22900] Train: [19/100][51/156] Data 0.001 (0.001) Batch 3.591 (3.427) Remain 12:07:47 loss: 0.7670 Lr: 0.04761
[2023-08-07 21:33:53,699 INFO misc.py line 115 22900] Train: [19/100][52/156] Data 0.001 (0.001) Batch 3.259 (3.424) Remain 12:07:00 loss: 0.3724 Lr: 0.04761
[2023-08-07 21:33:57,662 INFO misc.py line 115 22900] Train: [19/100][53/156] Data 0.001 (0.001) Batch 3.963 (3.435) Remain 12:09:14 loss: 0.8241 Lr: 0.04760
[2023-08-07 21:34:00,090 INFO misc.py line 115 22900] Train: [19/100][54/156] Data 0.001 (0.001) Batch 2.429 (3.415) Remain 12:04:59 loss: 0.4911 Lr: 0.04760
[2023-08-07 21:34:03,908 INFO misc.py line 115 22900] Train: [19/100][55/156] Data 0.001 (0.001) Batch 3.818 (3.423) Remain 12:06:34 loss: 0.5003 Lr: 0.04760
[2023-08-07 21:34:06,722 INFO misc.py line 115 22900] Train: [19/100][56/156] Data 0.001 (0.001) Batch 2.814 (3.411) Remain 12:04:05 loss: 0.4991 Lr: 0.04760
[2023-08-07 21:34:10,577 INFO misc.py line 115 22900] Train: [19/100][57/156] Data 0.001 (0.001) Batch 3.855 (3.419) Remain 12:05:46 loss: 0.7523 Lr: 0.04760
[2023-08-07 21:34:14,338 INFO misc.py line 115 22900] Train: [19/100][58/156] Data 0.001 (0.001) Batch 3.761 (3.426) Remain 12:07:02 loss: 0.5164 Lr: 0.04759
[2023-08-07 21:34:17,704 INFO misc.py line 115 22900] Train: [19/100][59/156] Data 0.001 (0.001) Batch 3.365 (3.425) Remain 12:06:45 loss: 0.5958 Lr: 0.04759
[2023-08-07 21:34:21,840 INFO misc.py line 115 22900] Train: [19/100][60/156] Data 0.001 (0.001) Batch 4.136 (3.437) Remain 12:09:20 loss: 0.6672 Lr: 0.04759
[2023-08-07 21:34:24,911 INFO misc.py line 115 22900] Train: [19/100][61/156] Data 0.001 (0.001) Batch 3.071 (3.431) Remain 12:07:56 loss: 0.5587 Lr: 0.04759
[2023-08-07 21:34:28,755 INFO misc.py line 115 22900] Train: [19/100][62/156] Data 0.001 (0.001) Batch 3.845 (3.438) Remain 12:09:22 loss: 0.6727 Lr: 0.04758
[2023-08-07 21:34:32,329 INFO misc.py line 115 22900] Train: [19/100][63/156] Data 0.001 (0.001) Batch 3.574 (3.440) Remain 12:09:48 loss: 0.5607 Lr: 0.04758
[2023-08-07 21:34:35,116 INFO misc.py line 115 22900] Train: [19/100][64/156] Data 0.001 (0.001) Batch 2.787 (3.429) Remain 12:07:28 loss: 0.4142 Lr: 0.04758
[2023-08-07 21:34:39,040 INFO misc.py line 115 22900] Train: [19/100][65/156] Data 0.001 (0.001) Batch 3.924 (3.437) Remain 12:09:06 loss: 0.5355 Lr: 0.04758
[2023-08-07 21:34:42,002 INFO misc.py line 115 22900] Train: [19/100][66/156] Data 0.001 (0.001) Batch 2.962 (3.430) Remain 12:07:27 loss: 0.6700 Lr: 0.04757
[2023-08-07 21:34:45,484 INFO misc.py line 115 22900] Train: [19/100][67/156] Data 0.001 (0.001) Batch 3.482 (3.431) Remain 12:07:34 loss: 0.3814 Lr: 0.04757
[2023-08-07 21:34:49,755 INFO misc.py line 115 22900] Train: [19/100][68/156] Data 0.001 (0.001) Batch 4.270 (3.443) Remain 12:10:15 loss: 1.0429 Lr: 0.04757
[2023-08-07 21:34:53,312 INFO misc.py line 115 22900] Train: [19/100][69/156] Data 0.001 (0.001) Batch 3.557 (3.445) Remain 12:10:33 loss: 0.4872 Lr: 0.04757
[2023-08-07 21:34:56,213 INFO misc.py line 115 22900] Train: [19/100][70/156] Data 0.001 (0.001) Batch 2.901 (3.437) Remain 12:08:46 loss: 0.8132 Lr: 0.04757
[2023-08-07 21:35:00,062 INFO misc.py line 115 22900] Train: [19/100][71/156] Data 0.001 (0.001) Batch 3.849 (3.443) Remain 12:10:00 loss: 0.4368 Lr: 0.04756
[2023-08-07 21:35:03,712 INFO misc.py line 115 22900] Train: [19/100][72/156] Data 0.001 (0.001) Batch 3.650 (3.446) Remain 12:10:35 loss: 0.5741 Lr: 0.04756
[2023-08-07 21:35:07,771 INFO misc.py line 115 22900] Train: [19/100][73/156] Data 0.001 (0.001) Batch 4.059 (3.455) Remain 12:12:22 loss: 0.7037 Lr: 0.04756
[2023-08-07 21:35:11,093 INFO misc.py line 115 22900] Train: [19/100][74/156] Data 0.001 (0.001) Batch 3.322 (3.453) Remain 12:11:55 loss: 0.8813 Lr: 0.04756
[2023-08-07 21:35:15,099 INFO misc.py line 115 22900] Train: [19/100][75/156] Data 0.001 (0.001) Batch 4.006 (3.461) Remain 12:13:29 loss: 0.4858 Lr: 0.04755
[2023-08-07 21:35:18,327 INFO misc.py line 115 22900] Train: [19/100][76/156] Data 0.001 (0.001) Batch 3.229 (3.458) Remain 12:12:45 loss: 0.5793 Lr: 0.04755
[2023-08-07 21:35:21,331 INFO misc.py line 115 22900] Train: [19/100][77/156] Data 0.001 (0.001) Batch 3.004 (3.451) Remain 12:11:24 loss: 0.3914 Lr: 0.04755
[2023-08-07 21:35:23,930 INFO misc.py line 115 22900] Train: [19/100][78/156] Data 0.001 (0.001) Batch 2.598 (3.440) Remain 12:08:56 loss: 0.1920 Lr: 0.04755
[2023-08-07 21:35:27,931 INFO misc.py line 115 22900] Train: [19/100][79/156] Data 0.001 (0.001) Batch 4.001 (3.447) Remain 12:10:26 loss: 0.7932 Lr: 0.04755
[2023-08-07 21:35:31,676 INFO misc.py line 115 22900] Train: [19/100][80/156] Data 0.001 (0.001) Batch 3.745 (3.451) Remain 12:11:12 loss: 0.7240 Lr: 0.04754
[2023-08-07 21:35:34,789 INFO misc.py line 115 22900] Train: [19/100][81/156] Data 0.001 (0.001) Batch 3.113 (3.447) Remain 12:10:14 loss: 0.7537 Lr: 0.04754
[2023-08-07 21:35:38,940 INFO misc.py line 115 22900] Train: [19/100][82/156] Data 0.001 (0.001) Batch 4.151 (3.456) Remain 12:12:03 loss: 0.5585 Lr: 0.04754
[2023-08-07 21:35:42,984 INFO misc.py line 115 22900] Train: [19/100][83/156] Data 0.001 (0.001) Batch 4.043 (3.463) Remain 12:13:33 loss: 0.7194 Lr: 0.04754
[2023-08-07 21:35:46,373 INFO misc.py line 115 22900] Train: [19/100][84/156] Data 0.001 (0.001) Batch 3.390 (3.462) Remain 12:13:18 loss: 0.6792 Lr: 0.04753
[2023-08-07 21:35:50,175 INFO misc.py line 115 22900] Train: [19/100][85/156] Data 0.001 (0.001) Batch 3.802 (3.466) Remain 12:14:07 loss: 0.4576 Lr: 0.04753
[2023-08-07 21:35:52,801 INFO misc.py line 115 22900] Train: [19/100][86/156] Data 0.001 (0.001) Batch 2.626 (3.456) Remain 12:11:55 loss: 0.6485 Lr: 0.04753
[2023-08-07 21:35:56,051 INFO misc.py line 115 22900] Train: [19/100][87/156] Data 0.001 (0.001) Batch 3.250 (3.454) Remain 12:11:21 loss: 0.4556 Lr: 0.04753
[2023-08-07 21:35:59,879 INFO misc.py line 115 22900] Train: [19/100][88/156] Data 0.001 (0.001) Batch 3.828 (3.458) Remain 12:12:13 loss: 0.5356 Lr: 0.04752
[2023-08-07 21:36:02,729 INFO misc.py line 115 22900] Train: [19/100][89/156] Data 0.001 (0.001) Batch 2.850 (3.451) Remain 12:10:40 loss: 0.5379 Lr: 0.04752
[2023-08-07 21:36:04,951 INFO misc.py line 115 22900] Train: [19/100][90/156] Data 0.001 (0.001) Batch 2.222 (3.437) Remain 12:07:37 loss: 0.5846 Lr: 0.04752
[2023-08-07 21:36:07,478 INFO misc.py line 115 22900] Train: [19/100][91/156] Data 0.001 (0.001) Batch 2.527 (3.427) Remain 12:05:22 loss: 0.3965 Lr: 0.04752
[2023-08-07 21:36:11,184 INFO misc.py line 115 22900] Train: [19/100][92/156] Data 0.001 (0.001) Batch 3.706 (3.430) Remain 12:05:59 loss: 0.6793 Lr: 0.04752
[2023-08-07 21:36:14,777 INFO misc.py line 115 22900] Train: [19/100][93/156] Data 0.001 (0.001) Batch 3.594 (3.432) Remain 12:06:18 loss: 0.6884 Lr: 0.04751
[2023-08-07 21:36:18,428 INFO misc.py line 115 22900] Train: [19/100][94/156] Data 0.001 (0.001) Batch 3.651 (3.434) Remain 12:06:45 loss: 0.9246 Lr: 0.04751
[2023-08-07 21:36:22,573 INFO misc.py line 115 22900] Train: [19/100][95/156] Data 0.001 (0.001) Batch 4.144 (3.442) Remain 12:08:20 loss: 0.9010 Lr: 0.04751
[2023-08-07 21:36:26,576 INFO misc.py line 115 22900] Train: [19/100][96/156] Data 0.001 (0.001) Batch 4.003 (3.448) Remain 12:09:33 loss: 1.2874 Lr: 0.04751
[2023-08-07 21:36:30,346 INFO misc.py line 115 22900] Train: [19/100][97/156] Data 0.001 (0.001) Batch 3.770 (3.451) Remain 12:10:13 loss: 0.6348 Lr: 0.04750
[2023-08-07 21:36:33,743 INFO misc.py line 115 22900] Train: [19/100][98/156] Data 0.001 (0.001) Batch 3.397 (3.451) Remain 12:10:03 loss: 0.5785 Lr: 0.04750
[2023-08-07 21:36:36,750 INFO misc.py line 115 22900] Train: [19/100][99/156] Data 0.001 (0.001) Batch 3.007 (3.446) Remain 12:09:00 loss: 0.7572 Lr: 0.04750
[2023-08-07 21:36:40,213 INFO misc.py line 115 22900] Train: [19/100][100/156] Data 0.001 (0.001) Batch 3.463 (3.446) Remain 12:08:59 loss: 0.8963 Lr: 0.04750
[2023-08-07 21:36:43,895 INFO misc.py line 115 22900] Train: [19/100][101/156] Data 0.001 (0.001) Batch 3.682 (3.449) Remain 12:09:26 loss: 0.5835 Lr: 0.04749
[2023-08-07 21:36:48,252 INFO misc.py line 115 22900] Train: [19/100][102/156] Data 0.001 (0.001) Batch 4.357 (3.458) Remain 12:11:19 loss: 1.1092 Lr: 0.04749
[2023-08-07 21:36:51,599 INFO misc.py line 115 22900] Train: [19/100][103/156] Data 0.001 (0.001) Batch 3.346 (3.457) Remain 12:11:02 loss: 0.6856 Lr: 0.04749
[2023-08-07 21:36:53,627 INFO misc.py line 115 22900] Train: [19/100][104/156] Data 0.001 (0.001) Batch 2.027 (3.443) Remain 12:07:59 loss: 0.7290 Lr: 0.04749
[2023-08-07 21:36:57,404 INFO misc.py line 115 22900] Train: [19/100][105/156] Data 0.001 (0.001) Batch 3.778 (3.446) Remain 12:08:37 loss: 0.5998 Lr: 0.04749
[2023-08-07 21:37:01,416 INFO misc.py line 115 22900] Train: [19/100][106/156] Data 0.001 (0.001) Batch 4.012 (3.451) Remain 12:09:43 loss: 1.0853 Lr: 0.04748
[2023-08-07 21:37:05,064 INFO misc.py line 115 22900] Train: [19/100][107/156] Data 0.001 (0.001) Batch 3.647 (3.453) Remain 12:10:04 loss: 0.4763 Lr: 0.04748
[2023-08-07 21:37:08,103 INFO misc.py line 115 22900] Train: [19/100][108/156] Data 0.001 (0.001) Batch 3.039 (3.449) Remain 12:09:10 loss: 0.5336 Lr: 0.04748
[2023-08-07 21:37:11,598 INFO misc.py line 115 22900] Train: [19/100][109/156] Data 0.001 (0.001) Batch 3.495 (3.450) Remain 12:09:12 loss: 0.6558 Lr: 0.04748
[2023-08-07 21:37:15,911 INFO misc.py line 115 22900] Train: [19/100][110/156] Data 0.001 (0.001) Batch 4.313 (3.458) Remain 12:10:51 loss: 0.7561 Lr: 0.04747
[2023-08-07 21:37:19,121 INFO misc.py line 115 22900] Train: [19/100][111/156] Data 0.001 (0.001) Batch 3.210 (3.455) Remain 12:10:19 loss: 0.4193 Lr: 0.04747
[2023-08-07 21:37:21,877 INFO misc.py line 115 22900] Train: [19/100][112/156] Data 0.001 (0.001) Batch 2.756 (3.449) Remain 12:08:54 loss: 0.4257 Lr: 0.04747
[2023-08-07 21:37:25,491 INFO misc.py line 115 22900] Train: [19/100][113/156] Data 0.001 (0.001) Batch 3.613 (3.451) Remain 12:09:09 loss: 0.7661 Lr: 0.04747
[2023-08-07 21:37:29,125 INFO misc.py line 115 22900] Train: [19/100][114/156] Data 0.001 (0.001) Batch 3.634 (3.452) Remain 12:09:27 loss: 0.7830 Lr: 0.04746
[2023-08-07 21:37:33,264 INFO misc.py line 115 22900] Train: [19/100][115/156] Data 0.001 (0.001) Batch 4.138 (3.458) Remain 12:10:41 loss: 0.7189 Lr: 0.04746
[2023-08-07 21:37:36,553 INFO misc.py line 115 22900] Train: [19/100][116/156] Data 0.001 (0.001) Batch 3.289 (3.457) Remain 12:10:19 loss: 0.8105 Lr: 0.04746
[2023-08-07 21:37:39,837 INFO misc.py line 115 22900] Train: [19/100][117/156] Data 0.001 (0.001) Batch 3.284 (3.455) Remain 12:09:56 loss: 0.3492 Lr: 0.04746
[2023-08-07 21:37:42,775 INFO misc.py line 115 22900] Train: [19/100][118/156] Data 0.001 (0.001) Batch 2.938 (3.451) Remain 12:08:56 loss: 0.6702 Lr: 0.04746
[2023-08-07 21:37:46,824 INFO misc.py line 115 22900] Train: [19/100][119/156] Data 0.001 (0.001) Batch 4.049 (3.456) Remain 12:09:57 loss: 0.6331 Lr: 0.04745
[2023-08-07 21:37:49,939 INFO misc.py line 115 22900] Train: [19/100][120/156] Data 0.001 (0.001) Batch 3.115 (3.453) Remain 12:09:17 loss: 0.4406 Lr: 0.04745
[2023-08-07 21:37:53,179 INFO misc.py line 115 22900] Train: [19/100][121/156] Data 0.001 (0.001) Batch 3.240 (3.451) Remain 12:08:51 loss: 0.8525 Lr: 0.04745
[2023-08-07 21:37:57,176 INFO misc.py line 115 22900] Train: [19/100][122/156] Data 0.001 (0.001) Batch 3.997 (3.456) Remain 12:09:45 loss: 0.9263 Lr: 0.04745
[2023-08-07 21:38:00,315 INFO misc.py line 115 22900] Train: [19/100][123/156] Data 0.001 (0.001) Batch 3.139 (3.453) Remain 12:09:08 loss: 0.6003 Lr: 0.04744
[2023-08-07 21:38:04,388 INFO misc.py line 115 22900] Train: [19/100][124/156] Data 0.001 (0.001) Batch 4.073 (3.458) Remain 12:10:10 loss: 0.5709 Lr: 0.04744
[2023-08-07 21:38:07,507 INFO misc.py line 115 22900] Train: [19/100][125/156] Data 0.001 (0.001) Batch 3.119 (3.456) Remain 12:09:31 loss: 0.6019 Lr: 0.04744
[2023-08-07 21:38:10,447 INFO misc.py line 115 22900] Train: [19/100][126/156] Data 0.001 (0.001) Batch 2.940 (3.451) Remain 12:08:35 loss: 0.7021 Lr: 0.04744
[2023-08-07 21:38:14,453 INFO misc.py line 115 22900] Train: [19/100][127/156] Data 0.001 (0.001) Batch 4.006 (3.456) Remain 12:09:28 loss: 0.6088 Lr: 0.04743
[2023-08-07 21:38:18,460 INFO misc.py line 115 22900] Train: [19/100][128/156] Data 0.001 (0.001) Batch 4.007 (3.460) Remain 12:10:20 loss: 0.8176 Lr: 0.04743
[2023-08-07 21:38:21,116 INFO misc.py line 115 22900] Train: [19/100][129/156] Data 0.001 (0.001) Batch 2.656 (3.454) Remain 12:08:56 loss: 0.5607 Lr: 0.04743
[2023-08-07 21:38:25,193 INFO misc.py line 115 22900] Train: [19/100][130/156] Data 0.001 (0.001) Batch 4.077 (3.459) Remain 12:09:55 loss: 0.6799 Lr: 0.04743
[2023-08-07 21:38:28,558 INFO misc.py line 115 22900] Train: [19/100][131/156] Data 0.001 (0.001) Batch 3.365 (3.458) Remain 12:09:42 loss: 0.5339 Lr: 0.04742
[2023-08-07 21:38:32,312 INFO misc.py line 115 22900] Train: [19/100][132/156] Data 0.001 (0.001) Batch 3.755 (3.460) Remain 12:10:07 loss: 0.5469 Lr: 0.04742
[2023-08-07 21:38:34,622 INFO misc.py line 115 22900] Train: [19/100][133/156] Data 0.001 (0.001) Batch 2.309 (3.451) Remain 12:08:12 loss: 0.9985 Lr: 0.04742
[2023-08-07 21:38:37,692 INFO misc.py line 115 22900] Train: [19/100][134/156] Data 0.001 (0.001) Batch 3.070 (3.449) Remain 12:07:32 loss: 0.7789 Lr: 0.04742
[2023-08-07 21:38:40,891 INFO misc.py line 115 22900] Train: [19/100][135/156] Data 0.001 (0.001) Batch 3.199 (3.447) Remain 12:07:04 loss: 0.5670 Lr: 0.04742
[2023-08-07 21:38:44,473 INFO misc.py line 115 22900] Train: [19/100][136/156] Data 0.001 (0.001) Batch 3.582 (3.448) Remain 12:07:14 loss: 0.5131 Lr: 0.04741
[2023-08-07 21:38:47,668 INFO misc.py line 115 22900] Train: [19/100][137/156] Data 0.001 (0.001) Batch 3.194 (3.446) Remain 12:06:46 loss: 0.5715 Lr: 0.04741
[2023-08-07 21:38:50,547 INFO misc.py line 115 22900] Train: [19/100][138/156] Data 0.001 (0.001) Batch 2.879 (3.442) Remain 12:05:50 loss: 0.3258 Lr: 0.04741
[2023-08-07 21:38:54,124 INFO misc.py line 115 22900] Train: [19/100][139/156] Data 0.001 (0.001) Batch 3.577 (3.443) Remain 12:05:59 loss: 0.9095 Lr: 0.04741
[2023-08-07 21:38:57,364 INFO misc.py line 115 22900] Train: [19/100][140/156] Data 0.001 (0.001) Batch 3.240 (3.441) Remain 12:05:37 loss: 0.4046 Lr: 0.04740
[2023-08-07 21:39:01,308 INFO misc.py line 115 22900] Train: [19/100][141/156] Data 0.001 (0.001) Batch 3.945 (3.445) Remain 12:06:19 loss: 0.7989 Lr: 0.04740
[2023-08-07 21:39:05,301 INFO misc.py line 115 22900] Train: [19/100][142/156] Data 0.001 (0.001) Batch 3.993 (3.449) Remain 12:07:06 loss: 0.8208 Lr: 0.04740
[2023-08-07 21:39:09,070 INFO misc.py line 115 22900] Train: [19/100][143/156] Data 0.001 (0.001) Batch 3.769 (3.451) Remain 12:07:31 loss: 0.6910 Lr: 0.04740
[2023-08-07 21:39:12,239 INFO misc.py line 115 22900] Train: [19/100][144/156] Data 0.001 (0.001) Batch 3.169 (3.449) Remain 12:07:03 loss: 0.4593 Lr: 0.04739
[2023-08-07 21:39:14,975 INFO misc.py line 115 22900] Train: [19/100][145/156] Data 0.001 (0.001) Batch 2.736 (3.444) Remain 12:05:56 loss: 0.5707 Lr: 0.04739
[2023-08-07 21:39:18,649 INFO misc.py line 115 22900] Train: [19/100][146/156] Data 0.001 (0.001) Batch 3.675 (3.446) Remain 12:06:13 loss: 0.5977 Lr: 0.04739
[2023-08-07 21:39:21,866 INFO misc.py line 115 22900] Train: [19/100][147/156] Data 0.001 (0.001) Batch 3.217 (3.444) Remain 12:05:49 loss: 0.5495 Lr: 0.04739
[2023-08-07 21:39:25,049 INFO misc.py line 115 22900] Train: [19/100][148/156] Data 0.001 (0.001) Batch 3.182 (3.442) Remain 12:05:23 loss: 0.9108 Lr: 0.04738
[2023-08-07 21:39:29,060 INFO misc.py line 115 22900] Train: [19/100][149/156] Data 0.001 (0.001) Batch 4.012 (3.446) Remain 12:06:09 loss: 0.7439 Lr: 0.04738
[2023-08-07 21:39:33,046 INFO misc.py line 115 22900] Train: [19/100][150/156] Data 0.001 (0.001) Batch 3.986 (3.450) Remain 12:06:52 loss: 1.0389 Lr: 0.04738
[2023-08-07 21:39:37,297 INFO misc.py line 115 22900] Train: [19/100][151/156] Data 0.001 (0.001) Batch 4.251 (3.455) Remain 12:07:57 loss: 1.0018 Lr: 0.04738
[2023-08-07 21:39:40,159 INFO misc.py line 115 22900] Train: [19/100][152/156] Data 0.001 (0.001) Batch 2.862 (3.451) Remain 12:07:03 loss: 0.3135 Lr: 0.04738
[2023-08-07 21:39:44,147 INFO misc.py line 115 22900] Train: [19/100][153/156] Data 0.001 (0.001) Batch 3.988 (3.455) Remain 12:07:45 loss: 0.7557 Lr: 0.04737
[2023-08-07 21:39:48,549 INFO misc.py line 115 22900] Train: [19/100][154/156] Data 0.001 (0.001) Batch 4.402 (3.461) Remain 12:09:00 loss: 0.8530 Lr: 0.04737
[2023-08-07 21:39:52,023 INFO misc.py line 115 22900] Train: [19/100][155/156] Data 0.001 (0.001) Batch 3.474 (3.461) Remain 12:08:58 loss: 0.6798 Lr: 0.04737
[2023-08-07 21:39:55,991 INFO misc.py line 115 22900] Train: [19/100][156/156] Data 0.001 (0.001) Batch 3.968 (3.464) Remain 12:09:36 loss: 0.7025 Lr: 0.04737
[2023-08-07 21:39:55,991 INFO misc.py line 129 22900] Train result: loss: 0.6438 
[2023-08-07 21:39:55,999 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 21:39:58,142 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.9533 
[2023-08-07 21:39:59,011 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.6160 
[2023-08-07 21:40:00,677 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.7516 
[2023-08-07 21:40:02,199 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.0276 
[2023-08-07 21:40:04,044 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.2801 
[2023-08-07 21:40:05,706 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.6678 
[2023-08-07 21:40:07,844 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.6983 
[2023-08-07 21:40:09,648 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.9870 
[2023-08-07 21:40:10,932 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.1352 
[2023-08-07 21:40:13,064 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.2561 
[2023-08-07 21:40:13,589 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.0020 
[2023-08-07 21:40:15,124 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.6766 
[2023-08-07 21:40:17,838 INFO evaluator.py line 122 22900] Test: [13/24] Loss 0.9391 
[2023-08-07 21:40:19,521 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.7292 
[2023-08-07 21:40:21,543 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.5637 
[2023-08-07 21:40:24,251 INFO evaluator.py line 122 22900] Test: [16/24] Loss 0.9377 
[2023-08-07 21:40:26,959 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.0975 
[2023-08-07 21:40:28,807 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.3284 
[2023-08-07 21:40:29,556 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.0075 
[2023-08-07 21:40:30,440 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.9661 
[2023-08-07 21:40:32,700 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.2755 
[2023-08-07 21:40:34,663 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.3745 
[2023-08-07 21:40:36,511 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.2269 
[2023-08-07 21:40:38,449 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.8005 
[2023-08-07 21:40:38,520 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.1987/0.2803/0.6725.
[2023-08-07 21:40:38,520 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6537/0.9547
[2023-08-07 21:40:38,520 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9495/0.9879
[2023-08-07 21:40:38,520 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.0928/0.2373
[2023-08-07 21:40:38,520 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.0686/0.1559
[2023-08-07 21:40:38,520 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6199/0.8211
[2023-08-07 21:40:38,520 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.2886/0.3730
[2023-08-07 21:40:38,520 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.4038/0.4585
[2023-08-07 21:40:38,520 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0986/0.1117
[2023-08-07 21:40:38,520 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1023/0.2184
[2023-08-07 21:40:38,520 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0947/0.0963
[2023-08-07 21:40:38,520 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 21:40:38,520 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0005/0.0005
[2023-08-07 21:40:38,520 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1874/0.3832
[2023-08-07 21:40:38,520 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0024/0.0024
[2023-08-07 21:40:38,521 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0847/0.0934
[2023-08-07 21:40:38,521 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-07 21:40:38,521 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0000/0.0000
[2023-08-07 21:40:38,521 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.2463/0.5017
[2023-08-07 21:40:38,521 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 21:40:38,521 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0799/0.2106
[2023-08-07 21:40:38,521 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 21:40:38,521 INFO misc.py line 150 22900] Best validation mIoU updated to: 0.1987
[2023-08-07 21:40:38,521 INFO misc.py line 152 22900] Currently Best mIoU: 0.1987
[2023-08-07 21:40:38,521 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 21:40:44,538 INFO misc.py line 115 22900] Train: [20/100][1/156] Data 1.499 (1.499) Batch 4.958 (4.958) Remain 17:24:01 loss: 0.5295 Lr: 0.04736
[2023-08-07 21:40:48,128 INFO misc.py line 115 22900] Train: [20/100][2/156] Data 0.001 (0.001) Batch 3.590 (3.590) Remain 12:36:02 loss: 0.7512 Lr: 0.04736
[2023-08-07 21:40:52,819 INFO misc.py line 115 22900] Train: [20/100][3/156] Data 0.001 (0.001) Batch 4.691 (4.691) Remain 16:27:36 loss: 0.8364 Lr: 0.04736
[2023-08-07 21:40:56,406 INFO misc.py line 115 22900] Train: [20/100][4/156] Data 0.002 (0.002) Batch 3.587 (3.587) Remain 12:35:08 loss: 0.7792 Lr: 0.04736
[2023-08-07 21:41:00,016 INFO misc.py line 115 22900] Train: [20/100][5/156] Data 0.001 (0.001) Batch 3.611 (3.599) Remain 12:37:35 loss: 0.6802 Lr: 0.04735
[2023-08-07 21:41:04,123 INFO misc.py line 115 22900] Train: [20/100][6/156] Data 0.001 (0.001) Batch 4.106 (3.768) Remain 13:13:08 loss: 0.6197 Lr: 0.04735
[2023-08-07 21:41:08,434 INFO misc.py line 115 22900] Train: [20/100][7/156] Data 0.001 (0.001) Batch 4.311 (3.904) Remain 13:41:39 loss: 1.1037 Lr: 0.04735
[2023-08-07 21:41:11,717 INFO misc.py line 115 22900] Train: [20/100][8/156] Data 0.001 (0.001) Batch 3.284 (3.780) Remain 13:15:29 loss: 0.6889 Lr: 0.04735
[2023-08-07 21:41:14,840 INFO misc.py line 115 22900] Train: [20/100][9/156] Data 0.001 (0.001) Batch 3.123 (3.670) Remain 12:52:23 loss: 0.4838 Lr: 0.04734
[2023-08-07 21:41:18,328 INFO misc.py line 115 22900] Train: [20/100][10/156] Data 0.001 (0.001) Batch 3.488 (3.644) Remain 12:46:51 loss: 0.5035 Lr: 0.04734
[2023-08-07 21:41:21,903 INFO misc.py line 115 22900] Train: [20/100][11/156] Data 0.001 (0.001) Batch 3.575 (3.636) Remain 12:44:58 loss: 0.5449 Lr: 0.04734
[2023-08-07 21:41:24,806 INFO misc.py line 115 22900] Train: [20/100][12/156] Data 0.001 (0.001) Batch 2.903 (3.554) Remain 12:27:46 loss: 0.5505 Lr: 0.04734
[2023-08-07 21:41:28,181 INFO misc.py line 115 22900] Train: [20/100][13/156] Data 0.001 (0.001) Batch 3.376 (3.536) Remain 12:23:57 loss: 0.5807 Lr: 0.04734
[2023-08-07 21:41:31,844 INFO misc.py line 115 22900] Train: [20/100][14/156] Data 0.001 (0.001) Batch 3.662 (3.548) Remain 12:26:18 loss: 0.5688 Lr: 0.04733
[2023-08-07 21:41:34,803 INFO misc.py line 115 22900] Train: [20/100][15/156] Data 0.001 (0.001) Batch 2.959 (3.499) Remain 12:15:56 loss: 0.3867 Lr: 0.04733
[2023-08-07 21:41:38,763 INFO misc.py line 115 22900] Train: [20/100][16/156] Data 0.001 (0.001) Batch 3.961 (3.534) Remain 12:23:21 loss: 0.6012 Lr: 0.04733
[2023-08-07 21:41:41,563 INFO misc.py line 115 22900] Train: [20/100][17/156] Data 0.001 (0.001) Batch 2.799 (3.482) Remain 12:12:15 loss: 1.0278 Lr: 0.04733
[2023-08-07 21:41:44,154 INFO misc.py line 115 22900] Train: [20/100][18/156] Data 0.001 (0.001) Batch 2.591 (3.422) Remain 11:59:42 loss: 0.5935 Lr: 0.04732
[2023-08-07 21:41:47,372 INFO misc.py line 115 22900] Train: [20/100][19/156] Data 0.001 (0.001) Batch 3.219 (3.410) Remain 11:56:58 loss: 0.7190 Lr: 0.04732
[2023-08-07 21:41:50,454 INFO misc.py line 115 22900] Train: [20/100][20/156] Data 0.001 (0.001) Batch 3.081 (3.390) Remain 11:52:51 loss: 0.7864 Lr: 0.04732
[2023-08-07 21:41:53,510 INFO misc.py line 115 22900] Train: [20/100][21/156] Data 0.001 (0.001) Batch 3.056 (3.372) Remain 11:48:54 loss: 0.2484 Lr: 0.04732
[2023-08-07 21:41:57,807 INFO misc.py line 115 22900] Train: [20/100][22/156] Data 0.001 (0.001) Batch 4.297 (3.420) Remain 11:59:05 loss: 0.7563 Lr: 0.04731
[2023-08-07 21:42:02,687 INFO misc.py line 115 22900] Train: [20/100][23/156] Data 0.001 (0.001) Batch 4.880 (3.493) Remain 12:14:22 loss: 1.1402 Lr: 0.04731
[2023-08-07 21:42:05,502 INFO misc.py line 115 22900] Train: [20/100][24/156] Data 0.001 (0.001) Batch 2.814 (3.461) Remain 12:07:31 loss: 0.3232 Lr: 0.04731
[2023-08-07 21:42:09,453 INFO misc.py line 115 22900] Train: [20/100][25/156] Data 0.001 (0.001) Batch 3.951 (3.483) Remain 12:12:08 loss: 0.4289 Lr: 0.04731
[2023-08-07 21:42:13,003 INFO misc.py line 115 22900] Train: [20/100][26/156] Data 0.001 (0.001) Batch 3.550 (3.486) Remain 12:12:41 loss: 0.5254 Lr: 0.04730
[2023-08-07 21:42:16,527 INFO misc.py line 115 22900] Train: [20/100][27/156] Data 0.001 (0.001) Batch 3.524 (3.488) Remain 12:12:58 loss: 0.6733 Lr: 0.04730
[2023-08-07 21:42:20,060 INFO misc.py line 115 22900] Train: [20/100][28/156] Data 0.001 (0.001) Batch 3.532 (3.490) Remain 12:13:17 loss: 0.5456 Lr: 0.04730
[2023-08-07 21:42:24,064 INFO misc.py line 115 22900] Train: [20/100][29/156] Data 0.001 (0.001) Batch 4.004 (3.509) Remain 12:17:23 loss: 0.6407 Lr: 0.04730
[2023-08-07 21:42:26,471 INFO misc.py line 115 22900] Train: [20/100][30/156] Data 0.001 (0.001) Batch 2.407 (3.469) Remain 12:08:44 loss: 0.4197 Lr: 0.04729
[2023-08-07 21:42:30,908 INFO misc.py line 115 22900] Train: [20/100][31/156] Data 0.001 (0.001) Batch 4.437 (3.503) Remain 12:15:57 loss: 1.0464 Lr: 0.04729
[2023-08-07 21:42:34,610 INFO misc.py line 115 22900] Train: [20/100][32/156] Data 0.001 (0.001) Batch 3.703 (3.510) Remain 12:17:20 loss: 0.4478 Lr: 0.04729
[2023-08-07 21:42:38,342 INFO misc.py line 115 22900] Train: [20/100][33/156] Data 0.001 (0.001) Batch 3.731 (3.517) Remain 12:18:50 loss: 0.4257 Lr: 0.04729
[2023-08-07 21:42:41,872 INFO misc.py line 115 22900] Train: [20/100][34/156] Data 0.001 (0.001) Batch 3.531 (3.518) Remain 12:18:51 loss: 0.8228 Lr: 0.04728
[2023-08-07 21:42:45,900 INFO misc.py line 115 22900] Train: [20/100][35/156] Data 0.001 (0.001) Batch 4.028 (3.534) Remain 12:22:09 loss: 0.4735 Lr: 0.04728
[2023-08-07 21:42:48,650 INFO misc.py line 115 22900] Train: [20/100][36/156] Data 0.001 (0.001) Batch 2.750 (3.510) Remain 12:17:06 loss: 0.2080 Lr: 0.04728
[2023-08-07 21:42:51,249 INFO misc.py line 115 22900] Train: [20/100][37/156] Data 0.001 (0.001) Batch 2.599 (3.483) Remain 12:11:25 loss: 0.5240 Lr: 0.04728
[2023-08-07 21:42:53,972 INFO misc.py line 115 22900] Train: [20/100][38/156] Data 0.001 (0.001) Batch 2.723 (3.461) Remain 12:06:47 loss: 0.5131 Lr: 0.04728
[2023-08-07 21:42:58,065 INFO misc.py line 115 22900] Train: [20/100][39/156] Data 0.001 (0.001) Batch 4.093 (3.479) Remain 12:10:25 loss: 0.3960 Lr: 0.04727
[2023-08-07 21:43:00,183 INFO misc.py line 115 22900] Train: [20/100][40/156] Data 0.001 (0.001) Batch 2.119 (3.442) Remain 12:02:38 loss: 0.3695 Lr: 0.04727
[2023-08-07 21:43:03,594 INFO misc.py line 115 22900] Train: [20/100][41/156] Data 0.001 (0.001) Batch 3.411 (3.441) Remain 12:02:24 loss: 0.7402 Lr: 0.04727
[2023-08-07 21:43:06,797 INFO misc.py line 115 22900] Train: [20/100][42/156] Data 0.001 (0.001) Batch 3.203 (3.435) Remain 12:01:04 loss: 0.7793 Lr: 0.04727
[2023-08-07 21:43:11,269 INFO misc.py line 115 22900] Train: [20/100][43/156] Data 0.003 (0.001) Batch 4.472 (3.461) Remain 12:06:27 loss: 0.8078 Lr: 0.04726
[2023-08-07 21:43:14,803 INFO misc.py line 115 22900] Train: [20/100][44/156] Data 0.001 (0.001) Batch 3.535 (3.463) Remain 12:06:46 loss: 0.5054 Lr: 0.04726
[2023-08-07 21:43:17,464 INFO misc.py line 115 22900] Train: [20/100][45/156] Data 0.001 (0.001) Batch 2.661 (3.444) Remain 12:02:42 loss: 0.4005 Lr: 0.04726
[2023-08-07 21:43:20,740 INFO misc.py line 115 22900] Train: [20/100][46/156] Data 0.001 (0.001) Batch 3.276 (3.440) Remain 12:01:49 loss: 0.9707 Lr: 0.04726
[2023-08-07 21:43:23,761 INFO misc.py line 115 22900] Train: [20/100][47/156] Data 0.001 (0.001) Batch 3.021 (3.430) Remain 11:59:46 loss: 0.4935 Lr: 0.04725
[2023-08-07 21:43:27,506 INFO misc.py line 115 22900] Train: [20/100][48/156] Data 0.001 (0.001) Batch 3.745 (3.437) Remain 12:01:10 loss: 0.4224 Lr: 0.04725
[2023-08-07 21:43:31,754 INFO misc.py line 115 22900] Train: [20/100][49/156] Data 0.001 (0.001) Batch 4.249 (3.455) Remain 12:04:49 loss: 0.8057 Lr: 0.04725
[2023-08-07 21:43:34,974 INFO misc.py line 115 22900] Train: [20/100][50/156] Data 0.002 (0.001) Batch 3.219 (3.450) Remain 12:03:42 loss: 0.4517 Lr: 0.04725
[2023-08-07 21:43:38,475 INFO misc.py line 115 22900] Train: [20/100][51/156] Data 0.001 (0.001) Batch 3.501 (3.451) Remain 12:03:52 loss: 0.5514 Lr: 0.04724
[2023-08-07 21:43:42,141 INFO misc.py line 115 22900] Train: [20/100][52/156] Data 0.001 (0.001) Batch 3.667 (3.456) Remain 12:04:44 loss: 0.3157 Lr: 0.04724
[2023-08-07 21:43:46,174 INFO misc.py line 115 22900] Train: [20/100][53/156] Data 0.001 (0.001) Batch 4.033 (3.467) Remain 12:07:06 loss: 0.7583 Lr: 0.04724
[2023-08-07 21:43:50,899 INFO misc.py line 115 22900] Train: [20/100][54/156] Data 0.001 (0.001) Batch 4.724 (3.492) Remain 12:12:13 loss: 0.7676 Lr: 0.04724
[2023-08-07 21:43:53,160 INFO misc.py line 115 22900] Train: [20/100][55/156] Data 0.002 (0.001) Batch 2.261 (3.468) Remain 12:07:11 loss: 0.4726 Lr: 0.04723
[2023-08-07 21:43:57,105 INFO misc.py line 115 22900] Train: [20/100][56/156] Data 0.001 (0.001) Batch 3.945 (3.477) Remain 12:09:01 loss: 0.8273 Lr: 0.04723
[2023-08-07 21:44:01,158 INFO misc.py line 115 22900] Train: [20/100][57/156] Data 0.001 (0.001) Batch 4.054 (3.488) Remain 12:11:12 loss: 0.8906 Lr: 0.04723
[2023-08-07 21:44:04,933 INFO misc.py line 115 22900] Train: [20/100][58/156] Data 0.001 (0.001) Batch 3.775 (3.493) Remain 12:12:14 loss: 0.7453 Lr: 0.04723
[2023-08-07 21:44:07,161 INFO misc.py line 115 22900] Train: [20/100][59/156] Data 0.001 (0.001) Batch 2.228 (3.470) Remain 12:07:27 loss: 0.1997 Lr: 0.04722
[2023-08-07 21:44:11,210 INFO misc.py line 115 22900] Train: [20/100][60/156] Data 0.001 (0.001) Batch 4.049 (3.481) Remain 12:09:31 loss: 0.9438 Lr: 0.04722
[2023-08-07 21:44:14,123 INFO misc.py line 115 22900] Train: [20/100][61/156] Data 0.001 (0.001) Batch 2.913 (3.471) Remain 12:07:24 loss: 0.6852 Lr: 0.04722
[2023-08-07 21:44:17,583 INFO misc.py line 115 22900] Train: [20/100][62/156] Data 0.001 (0.001) Batch 3.460 (3.471) Remain 12:07:19 loss: 0.4654 Lr: 0.04722
[2023-08-07 21:44:21,329 INFO misc.py line 115 22900] Train: [20/100][63/156] Data 0.001 (0.001) Batch 3.746 (3.475) Remain 12:08:13 loss: 0.4953 Lr: 0.04721
[2023-08-07 21:44:25,876 INFO misc.py line 115 22900] Train: [20/100][64/156] Data 0.001 (0.001) Batch 4.547 (3.493) Remain 12:11:50 loss: 0.8039 Lr: 0.04721
[2023-08-07 21:44:28,716 INFO misc.py line 115 22900] Train: [20/100][65/156] Data 0.001 (0.001) Batch 2.840 (3.482) Remain 12:09:34 loss: 0.4264 Lr: 0.04721
[2023-08-07 21:44:32,737 INFO misc.py line 115 22900] Train: [20/100][66/156] Data 0.001 (0.001) Batch 4.021 (3.491) Remain 12:11:18 loss: 0.7866 Lr: 0.04721
[2023-08-07 21:44:36,288 INFO misc.py line 115 22900] Train: [20/100][67/156] Data 0.001 (0.001) Batch 3.550 (3.492) Remain 12:11:27 loss: 0.7732 Lr: 0.04720
[2023-08-07 21:44:39,987 INFO misc.py line 115 22900] Train: [20/100][68/156] Data 0.001 (0.001) Batch 3.699 (3.495) Remain 12:12:03 loss: 0.9590 Lr: 0.04720
[2023-08-07 21:44:43,974 INFO misc.py line 115 22900] Train: [20/100][69/156] Data 0.001 (0.001) Batch 3.988 (3.502) Remain 12:13:34 loss: 0.8519 Lr: 0.04720
[2023-08-07 21:44:47,838 INFO misc.py line 115 22900] Train: [20/100][70/156] Data 0.002 (0.001) Batch 3.863 (3.508) Remain 12:14:38 loss: 0.5505 Lr: 0.04720
[2023-08-07 21:44:50,654 INFO misc.py line 115 22900] Train: [20/100][71/156] Data 0.001 (0.001) Batch 2.817 (3.498) Remain 12:12:27 loss: 0.3844 Lr: 0.04720
[2023-08-07 21:44:53,568 INFO misc.py line 115 22900] Train: [20/100][72/156] Data 0.001 (0.001) Batch 2.913 (3.489) Remain 12:10:37 loss: 0.2317 Lr: 0.04719
[2023-08-07 21:44:56,898 INFO misc.py line 115 22900] Train: [20/100][73/156] Data 0.001 (0.001) Batch 3.331 (3.487) Remain 12:10:05 loss: 0.2767 Lr: 0.04719
[2023-08-07 21:45:00,371 INFO misc.py line 115 22900] Train: [20/100][74/156] Data 0.001 (0.001) Batch 3.472 (3.487) Remain 12:09:59 loss: 0.4672 Lr: 0.04719
[2023-08-07 21:45:03,132 INFO misc.py line 115 22900] Train: [20/100][75/156] Data 0.001 (0.001) Batch 2.762 (3.477) Remain 12:07:49 loss: 0.4336 Lr: 0.04719
[2023-08-07 21:45:06,720 INFO misc.py line 115 22900] Train: [20/100][76/156] Data 0.001 (0.001) Batch 3.588 (3.478) Remain 12:08:04 loss: 0.6286 Lr: 0.04718
[2023-08-07 21:45:10,322 INFO misc.py line 115 22900] Train: [20/100][77/156] Data 0.001 (0.001) Batch 3.602 (3.480) Remain 12:08:22 loss: 0.6539 Lr: 0.04718
[2023-08-07 21:45:14,295 INFO misc.py line 115 22900] Train: [20/100][78/156] Data 0.001 (0.001) Batch 3.973 (3.486) Remain 12:09:41 loss: 0.6578 Lr: 0.04718
[2023-08-07 21:45:17,522 INFO misc.py line 115 22900] Train: [20/100][79/156] Data 0.001 (0.001) Batch 3.226 (3.483) Remain 12:08:55 loss: 0.3625 Lr: 0.04718
[2023-08-07 21:45:21,270 INFO misc.py line 115 22900] Train: [20/100][80/156] Data 0.001 (0.001) Batch 3.748 (3.486) Remain 12:09:34 loss: 0.4366 Lr: 0.04717
[2023-08-07 21:45:24,697 INFO misc.py line 115 22900] Train: [20/100][81/156] Data 0.001 (0.001) Batch 3.427 (3.486) Remain 12:09:21 loss: 0.3599 Lr: 0.04717
[2023-08-07 21:45:27,376 INFO misc.py line 115 22900] Train: [20/100][82/156] Data 0.001 (0.001) Batch 2.679 (3.475) Remain 12:07:10 loss: 0.1888 Lr: 0.04717
[2023-08-07 21:45:31,832 INFO misc.py line 115 22900] Train: [20/100][83/156] Data 0.001 (0.001) Batch 4.456 (3.488) Remain 12:09:40 loss: 0.8239 Lr: 0.04717
[2023-08-07 21:45:35,201 INFO misc.py line 115 22900] Train: [20/100][84/156] Data 0.001 (0.001) Batch 3.369 (3.486) Remain 12:09:18 loss: 0.4240 Lr: 0.04716
[2023-08-07 21:45:38,623 INFO misc.py line 115 22900] Train: [20/100][85/156] Data 0.001 (0.001) Batch 3.422 (3.485) Remain 12:09:05 loss: 0.6013 Lr: 0.04716
[2023-08-07 21:45:41,166 INFO misc.py line 115 22900] Train: [20/100][86/156] Data 0.001 (0.001) Batch 2.543 (3.474) Remain 12:06:39 loss: 0.7225 Lr: 0.04716
[2023-08-07 21:45:44,296 INFO misc.py line 115 22900] Train: [20/100][87/156] Data 0.001 (0.001) Batch 3.130 (3.470) Remain 12:05:44 loss: 0.6534 Lr: 0.04716
[2023-08-07 21:45:48,023 INFO misc.py line 115 22900] Train: [20/100][88/156] Data 0.001 (0.001) Batch 3.727 (3.473) Remain 12:06:19 loss: 0.6483 Lr: 0.04715
[2023-08-07 21:45:51,215 INFO misc.py line 115 22900] Train: [20/100][89/156] Data 0.001 (0.001) Batch 3.193 (3.470) Remain 12:05:34 loss: 0.6051 Lr: 0.04715
[2023-08-07 21:45:54,903 INFO misc.py line 115 22900] Train: [20/100][90/156] Data 0.001 (0.001) Batch 3.688 (3.472) Remain 12:06:02 loss: 0.5439 Lr: 0.04715
[2023-08-07 21:45:57,573 INFO misc.py line 115 22900] Train: [20/100][91/156] Data 0.001 (0.001) Batch 2.670 (3.463) Remain 12:04:04 loss: 0.3883 Lr: 0.04715
[2023-08-07 21:46:00,293 INFO misc.py line 115 22900] Train: [20/100][92/156] Data 0.001 (0.001) Batch 2.721 (3.455) Remain 12:02:16 loss: 0.3656 Lr: 0.04714
[2023-08-07 21:46:03,842 INFO misc.py line 115 22900] Train: [20/100][93/156] Data 0.001 (0.001) Batch 3.549 (3.456) Remain 12:02:26 loss: 0.5938 Lr: 0.04714
[2023-08-07 21:46:06,823 INFO misc.py line 115 22900] Train: [20/100][94/156] Data 0.001 (0.001) Batch 2.980 (3.451) Remain 12:01:17 loss: 0.6329 Lr: 0.04714
[2023-08-07 21:46:10,252 INFO misc.py line 115 22900] Train: [20/100][95/156] Data 0.001 (0.001) Batch 3.429 (3.450) Remain 12:01:10 loss: 0.7776 Lr: 0.04714
[2023-08-07 21:46:13,658 INFO misc.py line 115 22900] Train: [20/100][96/156] Data 0.001 (0.001) Batch 3.406 (3.450) Remain 12:01:01 loss: 0.4126 Lr: 0.04713
[2023-08-07 21:46:17,395 INFO misc.py line 115 22900] Train: [20/100][97/156] Data 0.001 (0.001) Batch 3.737 (3.453) Remain 12:01:36 loss: 0.6855 Lr: 0.04713
[2023-08-07 21:46:21,683 INFO misc.py line 115 22900] Train: [20/100][98/156] Data 0.001 (0.001) Batch 4.288 (3.462) Remain 12:03:23 loss: 0.7697 Lr: 0.04713
[2023-08-07 21:46:24,676 INFO misc.py line 115 22900] Train: [20/100][99/156] Data 0.001 (0.001) Batch 2.994 (3.457) Remain 12:02:18 loss: 0.3791 Lr: 0.04713
[2023-08-07 21:46:27,942 INFO misc.py line 115 22900] Train: [20/100][100/156] Data 0.001 (0.001) Batch 3.265 (3.455) Remain 12:01:50 loss: 0.7505 Lr: 0.04712
[2023-08-07 21:46:32,077 INFO misc.py line 115 22900] Train: [20/100][101/156] Data 0.001 (0.001) Batch 4.135 (3.462) Remain 12:03:13 loss: 0.7649 Lr: 0.04712
[2023-08-07 21:46:34,808 INFO misc.py line 115 22900] Train: [20/100][102/156] Data 0.001 (0.001) Batch 2.731 (3.454) Remain 12:01:37 loss: 0.4680 Lr: 0.04712
[2023-08-07 21:46:37,596 INFO misc.py line 115 22900] Train: [20/100][103/156] Data 0.001 (0.001) Batch 2.788 (3.448) Remain 12:00:10 loss: 0.5265 Lr: 0.04712
[2023-08-07 21:46:40,031 INFO misc.py line 115 22900] Train: [20/100][104/156] Data 0.001 (0.001) Batch 2.435 (3.438) Remain 11:58:01 loss: 0.4542 Lr: 0.04711
[2023-08-07 21:46:43,713 INFO misc.py line 115 22900] Train: [20/100][105/156] Data 0.001 (0.001) Batch 3.681 (3.440) Remain 11:58:28 loss: 0.4107 Lr: 0.04711
[2023-08-07 21:46:47,798 INFO misc.py line 115 22900] Train: [20/100][106/156] Data 0.001 (0.001) Batch 4.086 (3.446) Remain 11:59:43 loss: 0.3621 Lr: 0.04711
[2023-08-07 21:46:51,335 INFO misc.py line 115 22900] Train: [20/100][107/156] Data 0.001 (0.001) Batch 3.537 (3.447) Remain 11:59:50 loss: 0.5513 Lr: 0.04711
[2023-08-07 21:46:53,931 INFO misc.py line 115 22900] Train: [20/100][108/156] Data 0.001 (0.001) Batch 2.596 (3.439) Remain 11:58:05 loss: 0.3890 Lr: 0.04710
[2023-08-07 21:46:57,188 INFO misc.py line 115 22900] Train: [20/100][109/156] Data 0.001 (0.001) Batch 3.257 (3.437) Remain 11:57:40 loss: 0.7191 Lr: 0.04710
[2023-08-07 21:47:00,846 INFO misc.py line 115 22900] Train: [20/100][110/156] Data 0.001 (0.001) Batch 3.658 (3.440) Remain 11:58:03 loss: 0.6779 Lr: 0.04710
[2023-08-07 21:47:04,826 INFO misc.py line 115 22900] Train: [20/100][111/156] Data 0.001 (0.001) Batch 3.980 (3.445) Remain 11:59:02 loss: 0.9192 Lr: 0.04710
[2023-08-07 21:47:06,577 INFO misc.py line 115 22900] Train: [20/100][112/156] Data 0.001 (0.001) Batch 1.751 (3.429) Remain 11:55:44 loss: 0.4554 Lr: 0.04709
[2023-08-07 21:47:09,496 INFO misc.py line 115 22900] Train: [20/100][113/156] Data 0.001 (0.001) Batch 2.919 (3.424) Remain 11:54:42 loss: 0.6011 Lr: 0.04709
[2023-08-07 21:47:12,084 INFO misc.py line 115 22900] Train: [20/100][114/156] Data 0.001 (0.001) Batch 2.588 (3.417) Remain 11:53:05 loss: 0.4101 Lr: 0.04709
[2023-08-07 21:47:15,700 INFO misc.py line 115 22900] Train: [20/100][115/156] Data 0.001 (0.001) Batch 3.616 (3.419) Remain 11:53:23 loss: 0.6549 Lr: 0.04709
[2023-08-07 21:47:18,977 INFO misc.py line 115 22900] Train: [20/100][116/156] Data 0.001 (0.001) Batch 3.278 (3.417) Remain 11:53:04 loss: 0.6031 Lr: 0.04708
[2023-08-07 21:47:21,936 INFO misc.py line 115 22900] Train: [20/100][117/156] Data 0.001 (0.001) Batch 2.958 (3.413) Remain 11:52:11 loss: 0.4477 Lr: 0.04708
[2023-08-07 21:47:25,683 INFO misc.py line 115 22900] Train: [20/100][118/156] Data 0.001 (0.001) Batch 3.747 (3.416) Remain 11:52:44 loss: 0.6540 Lr: 0.04708
[2023-08-07 21:47:29,078 INFO misc.py line 115 22900] Train: [20/100][119/156] Data 0.001 (0.001) Batch 3.395 (3.416) Remain 11:52:38 loss: 0.4831 Lr: 0.04708
[2023-08-07 21:47:33,417 INFO misc.py line 115 22900] Train: [20/100][120/156] Data 0.001 (0.001) Batch 4.339 (3.424) Remain 11:54:13 loss: 0.7914 Lr: 0.04707
[2023-08-07 21:47:36,163 INFO misc.py line 115 22900] Train: [20/100][121/156] Data 0.001 (0.001) Batch 2.747 (3.418) Remain 11:52:58 loss: 0.3246 Lr: 0.04707
[2023-08-07 21:47:40,104 INFO misc.py line 115 22900] Train: [20/100][122/156] Data 0.001 (0.001) Batch 3.940 (3.423) Remain 11:53:49 loss: 0.7351 Lr: 0.04707
[2023-08-07 21:47:42,711 INFO misc.py line 115 22900] Train: [20/100][123/156] Data 0.001 (0.001) Batch 2.608 (3.416) Remain 11:52:21 loss: 0.3678 Lr: 0.04707
[2023-08-07 21:47:47,050 INFO misc.py line 115 22900] Train: [20/100][124/156] Data 0.001 (0.001) Batch 4.338 (3.423) Remain 11:53:53 loss: 1.1493 Lr: 0.04706
[2023-08-07 21:47:50,119 INFO misc.py line 115 22900] Train: [20/100][125/156] Data 0.001 (0.001) Batch 3.069 (3.420) Remain 11:53:13 loss: 0.5509 Lr: 0.04706
[2023-08-07 21:47:54,601 INFO misc.py line 115 22900] Train: [20/100][126/156] Data 0.001 (0.001) Batch 4.482 (3.429) Remain 11:54:58 loss: 1.0370 Lr: 0.04706
[2023-08-07 21:47:58,644 INFO misc.py line 115 22900] Train: [20/100][127/156] Data 0.001 (0.001) Batch 4.043 (3.434) Remain 11:55:56 loss: 0.9063 Lr: 0.04706
[2023-08-07 21:48:03,340 INFO misc.py line 115 22900] Train: [20/100][128/156] Data 0.001 (0.001) Batch 4.697 (3.444) Remain 11:57:59 loss: 0.8900 Lr: 0.04705
[2023-08-07 21:48:06,550 INFO misc.py line 115 22900] Train: [20/100][129/156] Data 0.001 (0.001) Batch 3.209 (3.442) Remain 11:57:32 loss: 0.5444 Lr: 0.04705
[2023-08-07 21:48:10,187 INFO misc.py line 115 22900] Train: [20/100][130/156] Data 0.001 (0.001) Batch 3.637 (3.444) Remain 11:57:48 loss: 0.6979 Lr: 0.04705
[2023-08-07 21:48:13,696 INFO misc.py line 115 22900] Train: [20/100][131/156] Data 0.001 (0.001) Batch 3.509 (3.444) Remain 11:57:51 loss: 0.3726 Lr: 0.04705
[2023-08-07 21:48:17,237 INFO misc.py line 115 22900] Train: [20/100][132/156] Data 0.001 (0.001) Batch 3.541 (3.445) Remain 11:57:57 loss: 0.6849 Lr: 0.04704
[2023-08-07 21:48:21,242 INFO misc.py line 115 22900] Train: [20/100][133/156] Data 0.001 (0.001) Batch 4.005 (3.449) Remain 11:58:47 loss: 0.7767 Lr: 0.04704
[2023-08-07 21:48:24,220 INFO misc.py line 115 22900] Train: [20/100][134/156] Data 0.001 (0.001) Batch 2.977 (3.446) Remain 11:57:59 loss: 0.6865 Lr: 0.04704
[2023-08-07 21:48:28,170 INFO misc.py line 115 22900] Train: [20/100][135/156] Data 0.001 (0.001) Batch 3.951 (3.450) Remain 11:58:43 loss: 0.7105 Lr: 0.04704
[2023-08-07 21:48:32,121 INFO misc.py line 115 22900] Train: [20/100][136/156] Data 0.001 (0.001) Batch 3.951 (3.453) Remain 11:59:27 loss: 0.5786 Lr: 0.04703
[2023-08-07 21:48:35,455 INFO misc.py line 115 22900] Train: [20/100][137/156] Data 0.001 (0.001) Batch 3.334 (3.453) Remain 11:59:12 loss: 0.4843 Lr: 0.04703
[2023-08-07 21:48:39,021 INFO misc.py line 115 22900] Train: [20/100][138/156] Data 0.001 (0.001) Batch 3.566 (3.453) Remain 11:59:19 loss: 0.9991 Lr: 0.04703
[2023-08-07 21:48:43,088 INFO misc.py line 115 22900] Train: [20/100][139/156] Data 0.001 (0.001) Batch 4.067 (3.458) Remain 12:00:12 loss: 0.6790 Lr: 0.04703
[2023-08-07 21:48:46,426 INFO misc.py line 115 22900] Train: [20/100][140/156] Data 0.001 (0.001) Batch 3.338 (3.457) Remain 11:59:58 loss: 0.6129 Lr: 0.04702
[2023-08-07 21:48:49,651 INFO misc.py line 115 22900] Train: [20/100][141/156] Data 0.001 (0.001) Batch 3.225 (3.455) Remain 11:59:34 loss: 0.5029 Lr: 0.04702
[2023-08-07 21:48:53,125 INFO misc.py line 115 22900] Train: [20/100][142/156] Data 0.001 (0.001) Batch 3.474 (3.455) Remain 11:59:32 loss: 0.8008 Lr: 0.04702
[2023-08-07 21:48:56,255 INFO misc.py line 115 22900] Train: [20/100][143/156] Data 0.001 (0.001) Batch 3.129 (3.453) Remain 11:58:59 loss: 0.6228 Lr: 0.04702
[2023-08-07 21:48:59,930 INFO misc.py line 115 22900] Train: [20/100][144/156] Data 0.001 (0.001) Batch 3.675 (3.455) Remain 11:59:15 loss: 0.9236 Lr: 0.04701
[2023-08-07 21:49:04,015 INFO misc.py line 115 22900] Train: [20/100][145/156] Data 0.001 (0.001) Batch 4.085 (3.459) Remain 12:00:07 loss: 0.8164 Lr: 0.04701
[2023-08-07 21:49:06,794 INFO misc.py line 115 22900] Train: [20/100][146/156] Data 0.001 (0.001) Batch 2.778 (3.454) Remain 11:59:05 loss: 0.4919 Lr: 0.04701
[2023-08-07 21:49:10,023 INFO misc.py line 115 22900] Train: [20/100][147/156] Data 0.001 (0.001) Batch 3.229 (3.453) Remain 11:58:42 loss: 0.4923 Lr: 0.04701
[2023-08-07 21:49:13,144 INFO misc.py line 115 22900] Train: [20/100][148/156] Data 0.001 (0.001) Batch 3.122 (3.451) Remain 11:58:10 loss: 0.5129 Lr: 0.04700
[2023-08-07 21:49:16,743 INFO misc.py line 115 22900] Train: [20/100][149/156] Data 0.001 (0.001) Batch 3.599 (3.452) Remain 11:58:19 loss: 0.6656 Lr: 0.04700
[2023-08-07 21:49:19,432 INFO misc.py line 115 22900] Train: [20/100][150/156] Data 0.001 (0.001) Batch 2.688 (3.446) Remain 11:57:11 loss: 0.6224 Lr: 0.04700
[2023-08-07 21:49:23,511 INFO misc.py line 115 22900] Train: [20/100][151/156] Data 0.001 (0.001) Batch 4.079 (3.451) Remain 11:58:00 loss: 0.5526 Lr: 0.04700
[2023-08-07 21:49:27,535 INFO misc.py line 115 22900] Train: [20/100][152/156] Data 0.001 (0.001) Batch 4.023 (3.454) Remain 11:58:45 loss: 0.6660 Lr: 0.04699
[2023-08-07 21:49:31,608 INFO misc.py line 115 22900] Train: [20/100][153/156] Data 0.002 (0.001) Batch 4.074 (3.459) Remain 11:59:33 loss: 0.4021 Lr: 0.04699
[2023-08-07 21:49:34,860 INFO misc.py line 115 22900] Train: [20/100][154/156] Data 0.001 (0.001) Batch 3.251 (3.457) Remain 11:59:13 loss: 0.8377 Lr: 0.04699
[2023-08-07 21:49:37,458 INFO misc.py line 115 22900] Train: [20/100][155/156] Data 0.001 (0.001) Batch 2.598 (3.452) Remain 11:57:59 loss: 0.3866 Lr: 0.04699
[2023-08-07 21:49:41,015 INFO misc.py line 115 22900] Train: [20/100][156/156] Data 0.001 (0.001) Batch 3.557 (3.452) Remain 11:58:04 loss: 0.7422 Lr: 0.04698
[2023-08-07 21:49:41,016 INFO misc.py line 129 22900] Train result: loss: 0.6086 
[2023-08-07 21:49:41,016 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 21:49:43,144 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.7486 
[2023-08-07 21:49:44,012 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.6706 
[2023-08-07 21:49:45,675 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.9345 
[2023-08-07 21:49:47,196 INFO evaluator.py line 122 22900] Test: [4/24] Loss 0.8318 
[2023-08-07 21:49:49,040 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.5714 
[2023-08-07 21:49:50,702 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.5649 
[2023-08-07 21:49:52,841 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.5620 
[2023-08-07 21:49:54,643 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.8656 
[2023-08-07 21:49:55,926 INFO evaluator.py line 122 22900] Test: [9/24] Loss 0.9489 
[2023-08-07 21:49:58,054 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.1197 
[2023-08-07 21:49:58,580 INFO evaluator.py line 122 22900] Test: [11/24] Loss 0.8045 
[2023-08-07 21:50:00,114 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.8023 
[2023-08-07 21:50:02,823 INFO evaluator.py line 122 22900] Test: [13/24] Loss 0.7502 
[2023-08-07 21:50:04,501 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.8382 
[2023-08-07 21:50:06,523 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.4726 
[2023-08-07 21:50:09,233 INFO evaluator.py line 122 22900] Test: [16/24] Loss 0.8106 
[2023-08-07 21:50:11,940 INFO evaluator.py line 122 22900] Test: [17/24] Loss 0.8755 
[2023-08-07 21:50:13,786 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.5276 
[2023-08-07 21:50:14,535 INFO evaluator.py line 122 22900] Test: [19/24] Loss 0.9271 
[2023-08-07 21:50:15,419 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.8376 
[2023-08-07 21:50:17,682 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.1898 
[2023-08-07 21:50:19,648 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.7587 
[2023-08-07 21:50:21,495 INFO evaluator.py line 122 22900] Test: [23/24] Loss 1.9511 
[2023-08-07 21:50:23,431 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.6316 
[2023-08-07 21:50:23,492 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2117/0.3062/0.6929.
[2023-08-07 21:50:23,492 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6729/0.9470
[2023-08-07 21:50:23,492 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9491/0.9908
[2023-08-07 21:50:23,492 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1501/0.3970
[2023-08-07 21:50:23,492 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.0788/0.0919
[2023-08-07 21:50:23,492 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.5844/0.8599
[2023-08-07 21:50:23,492 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.1428/0.1589
[2023-08-07 21:50:23,492 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5589/0.6703
[2023-08-07 21:50:23,492 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1310/0.1471
[2023-08-07 21:50:23,492 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1322/0.3314
[2023-08-07 21:50:23,492 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.1626/0.1719
[2023-08-07 21:50:23,492 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 21:50:23,492 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0000/0.0000
[2023-08-07 21:50:23,492 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.2236/0.4302
[2023-08-07 21:50:23,492 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0224/0.0231
[2023-08-07 21:50:23,492 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0618/0.0760
[2023-08-07 21:50:23,492 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.1090/0.1109
[2023-08-07 21:50:23,492 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0000/0.0000
[2023-08-07 21:50:23,492 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.1909/0.6017
[2023-08-07 21:50:23,492 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 21:50:23,492 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0633/0.1157
[2023-08-07 21:50:23,492 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 21:50:23,493 INFO misc.py line 150 22900] Best validation mIoU updated to: 0.2117
[2023-08-07 21:50:23,493 INFO misc.py line 152 22900] Currently Best mIoU: 0.2117
[2023-08-07 21:50:23,493 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 21:50:28,247 INFO misc.py line 115 22900] Train: [21/100][1/156] Data 0.814 (0.814) Batch 3.745 (3.745) Remain 12:58:48 loss: 0.3239 Lr: 0.04698
[2023-08-07 21:50:31,610 INFO misc.py line 115 22900] Train: [21/100][2/156] Data 0.001 (0.001) Batch 3.363 (3.363) Remain 11:39:20 loss: 0.4403 Lr: 0.04698
[2023-08-07 21:50:33,702 INFO misc.py line 115 22900] Train: [21/100][3/156] Data 0.001 (0.001) Batch 2.092 (2.092) Remain 07:15:05 loss: 0.5140 Lr: 0.04698
[2023-08-07 21:50:37,127 INFO misc.py line 115 22900] Train: [21/100][4/156] Data 0.002 (0.002) Batch 3.425 (3.425) Remain 11:52:04 loss: 0.3161 Lr: 0.04697
[2023-08-07 21:50:40,823 INFO misc.py line 115 22900] Train: [21/100][5/156] Data 0.002 (0.002) Batch 3.697 (3.561) Remain 12:20:19 loss: 0.7966 Lr: 0.04697
[2023-08-07 21:50:44,418 INFO misc.py line 115 22900] Train: [21/100][6/156] Data 0.001 (0.001) Batch 3.594 (3.572) Remain 12:22:34 loss: 0.7266 Lr: 0.04697
[2023-08-07 21:50:48,088 INFO misc.py line 115 22900] Train: [21/100][7/156] Data 0.001 (0.001) Batch 3.671 (3.597) Remain 12:27:39 loss: 0.6408 Lr: 0.04697
[2023-08-07 21:50:51,246 INFO misc.py line 115 22900] Train: [21/100][8/156] Data 0.001 (0.001) Batch 3.158 (3.509) Remain 12:09:21 loss: 0.4012 Lr: 0.04696
[2023-08-07 21:50:54,414 INFO misc.py line 115 22900] Train: [21/100][9/156] Data 0.001 (0.001) Batch 3.168 (3.452) Remain 11:57:29 loss: 0.6505 Lr: 0.04696
[2023-08-07 21:50:57,343 INFO misc.py line 115 22900] Train: [21/100][10/156] Data 0.001 (0.001) Batch 2.929 (3.377) Remain 11:41:54 loss: 0.5822 Lr: 0.04696
[2023-08-07 21:51:01,025 INFO misc.py line 115 22900] Train: [21/100][11/156] Data 0.001 (0.001) Batch 3.681 (3.415) Remain 11:49:45 loss: 0.5546 Lr: 0.04696
[2023-08-07 21:51:05,153 INFO misc.py line 115 22900] Train: [21/100][12/156] Data 0.001 (0.001) Batch 4.129 (3.495) Remain 12:06:10 loss: 0.7464 Lr: 0.04695
[2023-08-07 21:51:08,495 INFO misc.py line 115 22900] Train: [21/100][13/156] Data 0.001 (0.001) Batch 3.342 (3.479) Remain 12:02:56 loss: 0.7003 Lr: 0.04695
[2023-08-07 21:51:11,985 INFO misc.py line 115 22900] Train: [21/100][14/156] Data 0.001 (0.001) Batch 3.490 (3.480) Remain 12:03:05 loss: 0.4970 Lr: 0.04695
[2023-08-07 21:51:14,851 INFO misc.py line 115 22900] Train: [21/100][15/156] Data 0.001 (0.001) Batch 2.866 (3.429) Remain 11:52:23 loss: 0.7368 Lr: 0.04695
[2023-08-07 21:51:17,811 INFO misc.py line 115 22900] Train: [21/100][16/156] Data 0.001 (0.001) Batch 2.960 (3.393) Remain 11:44:50 loss: 0.6138 Lr: 0.04694
[2023-08-07 21:51:21,836 INFO misc.py line 115 22900] Train: [21/100][17/156] Data 0.001 (0.001) Batch 4.025 (3.438) Remain 11:54:09 loss: 0.7011 Lr: 0.04694
[2023-08-07 21:51:24,190 INFO misc.py line 115 22900] Train: [21/100][18/156] Data 0.001 (0.001) Batch 2.354 (3.366) Remain 11:39:05 loss: 0.4690 Lr: 0.04694
[2023-08-07 21:51:27,537 INFO misc.py line 115 22900] Train: [21/100][19/156] Data 0.001 (0.001) Batch 3.347 (3.365) Remain 11:38:47 loss: 0.6166 Lr: 0.04694
[2023-08-07 21:51:31,645 INFO misc.py line 115 22900] Train: [21/100][20/156] Data 0.001 (0.001) Batch 4.108 (3.408) Remain 11:47:48 loss: 0.5896 Lr: 0.04693
[2023-08-07 21:51:34,630 INFO misc.py line 115 22900] Train: [21/100][21/156] Data 0.001 (0.001) Batch 2.985 (3.385) Remain 11:42:52 loss: 0.6302 Lr: 0.04693
[2023-08-07 21:51:38,054 INFO misc.py line 115 22900] Train: [21/100][22/156] Data 0.001 (0.001) Batch 3.423 (3.387) Remain 11:43:14 loss: 0.9357 Lr: 0.04693
[2023-08-07 21:51:41,632 INFO misc.py line 115 22900] Train: [21/100][23/156] Data 0.001 (0.001) Batch 3.578 (3.396) Remain 11:45:09 loss: 0.5545 Lr: 0.04693
[2023-08-07 21:51:43,836 INFO misc.py line 115 22900] Train: [21/100][24/156] Data 0.001 (0.001) Batch 2.205 (3.340) Remain 11:33:19 loss: 0.3325 Lr: 0.04692
[2023-08-07 21:51:47,950 INFO misc.py line 115 22900] Train: [21/100][25/156] Data 0.001 (0.001) Batch 4.113 (3.375) Remain 11:40:34 loss: 0.7429 Lr: 0.04692
[2023-08-07 21:51:50,596 INFO misc.py line 115 22900] Train: [21/100][26/156] Data 0.001 (0.001) Batch 2.647 (3.343) Remain 11:33:56 loss: 0.3565 Lr: 0.04692
[2023-08-07 21:51:54,366 INFO misc.py line 115 22900] Train: [21/100][27/156] Data 0.001 (0.001) Batch 3.769 (3.361) Remain 11:37:34 loss: 0.7102 Lr: 0.04692
[2023-08-07 21:51:58,213 INFO misc.py line 115 22900] Train: [21/100][28/156] Data 0.001 (0.001) Batch 3.847 (3.380) Remain 11:41:32 loss: 0.5863 Lr: 0.04691
[2023-08-07 21:52:00,146 INFO misc.py line 115 22900] Train: [21/100][29/156] Data 0.001 (0.001) Batch 1.934 (3.325) Remain 11:29:56 loss: 0.1819 Lr: 0.04691
[2023-08-07 21:52:04,173 INFO misc.py line 115 22900] Train: [21/100][30/156] Data 0.001 (0.001) Batch 4.026 (3.351) Remain 11:35:16 loss: 0.7889 Lr: 0.04691
[2023-08-07 21:52:07,370 INFO misc.py line 115 22900] Train: [21/100][31/156] Data 0.001 (0.001) Batch 3.198 (3.345) Remain 11:34:05 loss: 0.6983 Lr: 0.04691
[2023-08-07 21:52:10,682 INFO misc.py line 115 22900] Train: [21/100][32/156] Data 0.001 (0.001) Batch 3.311 (3.344) Remain 11:33:47 loss: 0.3762 Lr: 0.04690
[2023-08-07 21:52:14,219 INFO misc.py line 115 22900] Train: [21/100][33/156] Data 0.001 (0.001) Batch 3.538 (3.351) Remain 11:35:04 loss: 0.4792 Lr: 0.04690
[2023-08-07 21:52:18,098 INFO misc.py line 115 22900] Train: [21/100][34/156] Data 0.001 (0.001) Batch 3.879 (3.368) Remain 11:38:33 loss: 0.8245 Lr: 0.04690
[2023-08-07 21:52:22,449 INFO misc.py line 115 22900] Train: [21/100][35/156] Data 0.001 (0.001) Batch 4.351 (3.398) Remain 11:44:52 loss: 0.6993 Lr: 0.04690
[2023-08-07 21:52:26,498 INFO misc.py line 115 22900] Train: [21/100][36/156] Data 0.001 (0.001) Batch 4.049 (3.418) Remain 11:48:54 loss: 0.5372 Lr: 0.04689
[2023-08-07 21:52:28,775 INFO misc.py line 115 22900] Train: [21/100][37/156] Data 0.001 (0.001) Batch 2.277 (3.384) Remain 11:41:53 loss: 0.4841 Lr: 0.04689
[2023-08-07 21:52:33,146 INFO misc.py line 115 22900] Train: [21/100][38/156] Data 0.001 (0.001) Batch 4.371 (3.413) Remain 11:47:40 loss: 0.8313 Lr: 0.04689
[2023-08-07 21:52:36,881 INFO misc.py line 115 22900] Train: [21/100][39/156] Data 0.001 (0.001) Batch 3.735 (3.422) Remain 11:49:28 loss: 0.6456 Lr: 0.04689
[2023-08-07 21:52:40,572 INFO misc.py line 115 22900] Train: [21/100][40/156] Data 0.001 (0.001) Batch 3.691 (3.429) Remain 11:50:55 loss: 0.5650 Lr: 0.04688
[2023-08-07 21:52:43,254 INFO misc.py line 115 22900] Train: [21/100][41/156] Data 0.001 (0.001) Batch 2.682 (3.409) Remain 11:46:47 loss: 0.5759 Lr: 0.04688
[2023-08-07 21:52:46,784 INFO misc.py line 115 22900] Train: [21/100][42/156] Data 0.001 (0.001) Batch 3.530 (3.412) Remain 11:47:22 loss: 0.4966 Lr: 0.04688
[2023-08-07 21:52:49,676 INFO misc.py line 115 22900] Train: [21/100][43/156] Data 0.001 (0.001) Batch 2.892 (3.399) Remain 11:44:37 loss: 0.4385 Lr: 0.04687
[2023-08-07 21:52:53,785 INFO misc.py line 115 22900] Train: [21/100][44/156] Data 0.001 (0.001) Batch 4.109 (3.417) Remain 11:48:09 loss: 0.7695 Lr: 0.04687
[2023-08-07 21:52:56,514 INFO misc.py line 115 22900] Train: [21/100][45/156] Data 0.001 (0.001) Batch 2.729 (3.400) Remain 11:44:42 loss: 0.3822 Lr: 0.04687
[2023-08-07 21:52:59,863 INFO misc.py line 115 22900] Train: [21/100][46/156] Data 0.001 (0.001) Batch 3.349 (3.399) Remain 11:44:24 loss: 0.4483 Lr: 0.04687
[2023-08-07 21:53:03,116 INFO misc.py line 115 22900] Train: [21/100][47/156] Data 0.001 (0.001) Batch 3.253 (3.396) Remain 11:43:39 loss: 0.4684 Lr: 0.04686
[2023-08-07 21:53:06,823 INFO misc.py line 115 22900] Train: [21/100][48/156] Data 0.001 (0.001) Batch 3.707 (3.403) Remain 11:45:02 loss: 0.6584 Lr: 0.04686
[2023-08-07 21:53:09,376 INFO misc.py line 115 22900] Train: [21/100][49/156] Data 0.001 (0.001) Batch 2.553 (3.384) Remain 11:41:08 loss: 0.2451 Lr: 0.04686
[2023-08-07 21:53:12,583 INFO misc.py line 115 22900] Train: [21/100][50/156] Data 0.001 (0.001) Batch 3.208 (3.380) Remain 11:40:18 loss: 0.6676 Lr: 0.04686
[2023-08-07 21:53:16,612 INFO misc.py line 115 22900] Train: [21/100][51/156] Data 0.001 (0.001) Batch 4.029 (3.394) Remain 11:43:03 loss: 0.7484 Lr: 0.04685
[2023-08-07 21:53:19,484 INFO misc.py line 115 22900] Train: [21/100][52/156] Data 0.001 (0.001) Batch 2.872 (3.383) Remain 11:40:47 loss: 0.5664 Lr: 0.04685
[2023-08-07 21:53:23,581 INFO misc.py line 115 22900] Train: [21/100][53/156] Data 0.001 (0.001) Batch 4.097 (3.398) Remain 11:43:41 loss: 0.6329 Lr: 0.04685
[2023-08-07 21:53:27,667 INFO misc.py line 115 22900] Train: [21/100][54/156] Data 0.001 (0.001) Batch 4.086 (3.411) Remain 11:46:25 loss: 0.5784 Lr: 0.04685
[2023-08-07 21:53:30,654 INFO misc.py line 115 22900] Train: [21/100][55/156] Data 0.001 (0.001) Batch 2.988 (3.403) Remain 11:44:41 loss: 0.5151 Lr: 0.04684
[2023-08-07 21:53:33,829 INFO misc.py line 115 22900] Train: [21/100][56/156] Data 0.001 (0.001) Batch 3.175 (3.399) Remain 11:43:44 loss: 0.5025 Lr: 0.04684
[2023-08-07 21:53:37,874 INFO misc.py line 115 22900] Train: [21/100][57/156] Data 0.001 (0.001) Batch 4.045 (3.411) Remain 11:46:09 loss: 0.9753 Lr: 0.04684
[2023-08-07 21:53:40,934 INFO misc.py line 115 22900] Train: [21/100][58/156] Data 0.001 (0.001) Batch 3.060 (3.404) Remain 11:44:47 loss: 0.6213 Lr: 0.04684
[2023-08-07 21:53:44,309 INFO misc.py line 115 22900] Train: [21/100][59/156] Data 0.001 (0.001) Batch 3.375 (3.404) Remain 11:44:37 loss: 0.8044 Lr: 0.04683
[2023-08-07 21:53:47,208 INFO misc.py line 115 22900] Train: [21/100][60/156] Data 0.001 (0.001) Batch 2.899 (3.395) Remain 11:42:43 loss: 0.3654 Lr: 0.04683
[2023-08-07 21:53:49,496 INFO misc.py line 115 22900] Train: [21/100][61/156] Data 0.001 (0.001) Batch 2.289 (3.376) Remain 11:38:43 loss: 0.2315 Lr: 0.04683
[2023-08-07 21:53:52,740 INFO misc.py line 115 22900] Train: [21/100][62/156] Data 0.001 (0.001) Batch 3.243 (3.374) Remain 11:38:12 loss: 0.5470 Lr: 0.04683
[2023-08-07 21:53:55,480 INFO misc.py line 115 22900] Train: [21/100][63/156] Data 0.001 (0.001) Batch 2.740 (3.363) Remain 11:35:57 loss: 0.6247 Lr: 0.04682
[2023-08-07 21:53:58,930 INFO misc.py line 115 22900] Train: [21/100][64/156] Data 0.001 (0.001) Batch 3.450 (3.364) Remain 11:36:12 loss: 0.4963 Lr: 0.04682
[2023-08-07 21:54:02,738 INFO misc.py line 115 22900] Train: [21/100][65/156] Data 0.001 (0.001) Batch 3.808 (3.372) Remain 11:37:37 loss: 0.6622 Lr: 0.04682
[2023-08-07 21:54:06,735 INFO misc.py line 115 22900] Train: [21/100][66/156] Data 0.001 (0.001) Batch 3.997 (3.381) Remain 11:39:37 loss: 0.8092 Lr: 0.04682
[2023-08-07 21:54:10,749 INFO misc.py line 115 22900] Train: [21/100][67/156] Data 0.001 (0.001) Batch 4.014 (3.391) Remain 11:41:36 loss: 0.8817 Lr: 0.04681
[2023-08-07 21:54:14,872 INFO misc.py line 115 22900] Train: [21/100][68/156] Data 0.001 (0.001) Batch 4.123 (3.403) Remain 11:43:53 loss: 1.0025 Lr: 0.04681
[2023-08-07 21:54:19,004 INFO misc.py line 115 22900] Train: [21/100][69/156] Data 0.001 (0.001) Batch 4.131 (3.414) Remain 11:46:06 loss: 0.7572 Lr: 0.04681
[2023-08-07 21:54:22,988 INFO misc.py line 115 22900] Train: [21/100][70/156] Data 0.002 (0.001) Batch 3.985 (3.422) Remain 11:47:49 loss: 0.8633 Lr: 0.04681
[2023-08-07 21:54:26,112 INFO misc.py line 115 22900] Train: [21/100][71/156] Data 0.001 (0.001) Batch 3.125 (3.418) Remain 11:46:51 loss: 0.3652 Lr: 0.04680
[2023-08-07 21:54:29,950 INFO misc.py line 115 22900] Train: [21/100][72/156] Data 0.001 (0.001) Batch 3.838 (3.424) Remain 11:48:03 loss: 0.7978 Lr: 0.04680
[2023-08-07 21:54:33,320 INFO misc.py line 115 22900] Train: [21/100][73/156] Data 0.001 (0.001) Batch 3.370 (3.423) Remain 11:47:50 loss: 0.3385 Lr: 0.04680
[2023-08-07 21:54:36,679 INFO misc.py line 115 22900] Train: [21/100][74/156] Data 0.001 (0.001) Batch 3.359 (3.422) Remain 11:47:35 loss: 0.5060 Lr: 0.04679
[2023-08-07 21:54:39,870 INFO misc.py line 115 22900] Train: [21/100][75/156] Data 0.001 (0.001) Batch 3.191 (3.419) Remain 11:46:52 loss: 0.5499 Lr: 0.04679
[2023-08-07 21:54:43,888 INFO misc.py line 115 22900] Train: [21/100][76/156] Data 0.001 (0.001) Batch 4.018 (3.427) Remain 11:48:31 loss: 0.7373 Lr: 0.04679
[2023-08-07 21:54:47,423 INFO misc.py line 115 22900] Train: [21/100][77/156] Data 0.001 (0.001) Batch 3.535 (3.429) Remain 11:48:45 loss: 0.4456 Lr: 0.04679
[2023-08-07 21:54:50,608 INFO misc.py line 115 22900] Train: [21/100][78/156] Data 0.001 (0.001) Batch 3.184 (3.425) Remain 11:48:01 loss: 0.5444 Lr: 0.04678
[2023-08-07 21:54:54,129 INFO misc.py line 115 22900] Train: [21/100][79/156] Data 0.001 (0.001) Batch 3.521 (3.427) Remain 11:48:14 loss: 0.6423 Lr: 0.04678
[2023-08-07 21:54:56,061 INFO misc.py line 115 22900] Train: [21/100][80/156] Data 0.001 (0.001) Batch 1.933 (3.407) Remain 11:44:10 loss: 0.1728 Lr: 0.04678
[2023-08-07 21:54:59,548 INFO misc.py line 115 22900] Train: [21/100][81/156] Data 0.001 (0.001) Batch 3.486 (3.408) Remain 11:44:19 loss: 0.3339 Lr: 0.04678
[2023-08-07 21:55:02,789 INFO misc.py line 115 22900] Train: [21/100][82/156] Data 0.001 (0.001) Batch 3.241 (3.406) Remain 11:43:49 loss: 0.6560 Lr: 0.04677
[2023-08-07 21:55:05,580 INFO misc.py line 115 22900] Train: [21/100][83/156] Data 0.001 (0.001) Batch 2.791 (3.398) Remain 11:42:10 loss: 0.6199 Lr: 0.04677
[2023-08-07 21:55:09,357 INFO misc.py line 115 22900] Train: [21/100][84/156] Data 0.001 (0.001) Batch 3.777 (3.403) Remain 11:43:05 loss: 0.5736 Lr: 0.04677
[2023-08-07 21:55:11,911 INFO misc.py line 115 22900] Train: [21/100][85/156] Data 0.001 (0.001) Batch 2.554 (3.393) Remain 11:40:53 loss: 0.3860 Lr: 0.04677
[2023-08-07 21:55:15,596 INFO misc.py line 115 22900] Train: [21/100][86/156] Data 0.001 (0.001) Batch 3.685 (3.396) Remain 11:41:33 loss: 0.9142 Lr: 0.04676
[2023-08-07 21:55:20,338 INFO misc.py line 115 22900] Train: [21/100][87/156] Data 0.001 (0.001) Batch 4.742 (3.412) Remain 11:44:49 loss: 0.8028 Lr: 0.04676
[2023-08-07 21:55:23,567 INFO misc.py line 115 22900] Train: [21/100][88/156] Data 0.001 (0.001) Batch 3.229 (3.410) Remain 11:44:18 loss: 0.6071 Lr: 0.04676
[2023-08-07 21:55:26,632 INFO misc.py line 115 22900] Train: [21/100][89/156] Data 0.001 (0.001) Batch 3.065 (3.406) Remain 11:43:25 loss: 0.6581 Lr: 0.04676
[2023-08-07 21:55:29,522 INFO misc.py line 115 22900] Train: [21/100][90/156] Data 0.001 (0.001) Batch 2.890 (3.400) Remain 11:42:08 loss: 0.3005 Lr: 0.04675
[2023-08-07 21:55:32,949 INFO misc.py line 115 22900] Train: [21/100][91/156] Data 0.001 (0.001) Batch 3.426 (3.401) Remain 11:42:09 loss: 0.6661 Lr: 0.04675
[2023-08-07 21:55:36,660 INFO misc.py line 115 22900] Train: [21/100][92/156] Data 0.001 (0.001) Batch 3.711 (3.404) Remain 11:42:48 loss: 0.4835 Lr: 0.04675
[2023-08-07 21:55:40,942 INFO misc.py line 115 22900] Train: [21/100][93/156] Data 0.001 (0.001) Batch 4.282 (3.414) Remain 11:44:46 loss: 0.6165 Lr: 0.04675
[2023-08-07 21:55:43,820 INFO misc.py line 115 22900] Train: [21/100][94/156] Data 0.001 (0.001) Batch 2.878 (3.408) Remain 11:43:30 loss: 0.2984 Lr: 0.04674
[2023-08-07 21:55:47,138 INFO misc.py line 115 22900] Train: [21/100][95/156] Data 0.001 (0.001) Batch 3.319 (3.407) Remain 11:43:14 loss: 0.3524 Lr: 0.04674
[2023-08-07 21:55:50,032 INFO misc.py line 115 22900] Train: [21/100][96/156] Data 0.001 (0.001) Batch 2.894 (3.401) Remain 11:42:02 loss: 0.6212 Lr: 0.04674
[2023-08-07 21:55:54,051 INFO misc.py line 115 22900] Train: [21/100][97/156] Data 0.001 (0.001) Batch 4.019 (3.408) Remain 11:43:20 loss: 0.4123 Lr: 0.04673
[2023-08-07 21:55:57,803 INFO misc.py line 115 22900] Train: [21/100][98/156] Data 0.001 (0.001) Batch 3.752 (3.412) Remain 11:44:02 loss: 0.5124 Lr: 0.04673
[2023-08-07 21:56:00,666 INFO misc.py line 115 22900] Train: [21/100][99/156] Data 0.001 (0.001) Batch 2.864 (3.406) Remain 11:42:48 loss: 0.4082 Lr: 0.04673
[2023-08-07 21:56:04,681 INFO misc.py line 115 22900] Train: [21/100][100/156] Data 0.001 (0.001) Batch 4.015 (3.412) Remain 11:44:02 loss: 0.6261 Lr: 0.04673
[2023-08-07 21:56:08,727 INFO misc.py line 115 22900] Train: [21/100][101/156] Data 0.001 (0.001) Batch 4.046 (3.419) Remain 11:45:19 loss: 0.4113 Lr: 0.04672
[2023-08-07 21:56:12,783 INFO misc.py line 115 22900] Train: [21/100][102/156] Data 0.001 (0.001) Batch 4.056 (3.425) Remain 11:46:35 loss: 0.7242 Lr: 0.04672
[2023-08-07 21:56:16,704 INFO misc.py line 115 22900] Train: [21/100][103/156] Data 0.001 (0.001) Batch 3.920 (3.430) Remain 11:47:33 loss: 0.6074 Lr: 0.04672
[2023-08-07 21:56:19,964 INFO misc.py line 115 22900] Train: [21/100][104/156] Data 0.001 (0.001) Batch 3.260 (3.428) Remain 11:47:09 loss: 0.5113 Lr: 0.04672
[2023-08-07 21:56:23,278 INFO misc.py line 115 22900] Train: [21/100][105/156] Data 0.001 (0.001) Batch 3.314 (3.427) Remain 11:46:51 loss: 0.5216 Lr: 0.04671
[2023-08-07 21:56:26,453 INFO misc.py line 115 22900] Train: [21/100][106/156] Data 0.001 (0.001) Batch 3.175 (3.425) Remain 11:46:18 loss: 0.6957 Lr: 0.04671
[2023-08-07 21:56:28,267 INFO misc.py line 115 22900] Train: [21/100][107/156] Data 0.001 (0.001) Batch 1.814 (3.409) Remain 11:43:02 loss: 0.4347 Lr: 0.04671
[2023-08-07 21:56:31,859 INFO misc.py line 115 22900] Train: [21/100][108/156] Data 0.001 (0.001) Batch 3.593 (3.411) Remain 11:43:21 loss: 0.8180 Lr: 0.04671
[2023-08-07 21:56:35,908 INFO misc.py line 115 22900] Train: [21/100][109/156] Data 0.001 (0.001) Batch 4.048 (3.417) Remain 11:44:32 loss: 0.6653 Lr: 0.04670
[2023-08-07 21:56:38,082 INFO misc.py line 115 22900] Train: [21/100][110/156] Data 0.001 (0.001) Batch 2.175 (3.405) Remain 11:42:05 loss: 0.3196 Lr: 0.04670
[2023-08-07 21:56:41,673 INFO misc.py line 115 22900] Train: [21/100][111/156] Data 0.001 (0.001) Batch 3.591 (3.407) Remain 11:42:22 loss: 0.5075 Lr: 0.04670
[2023-08-07 21:56:45,346 INFO misc.py line 115 22900] Train: [21/100][112/156] Data 0.001 (0.001) Batch 3.673 (3.410) Remain 11:42:49 loss: 0.7922 Lr: 0.04670
[2023-08-07 21:56:48,419 INFO misc.py line 115 22900] Train: [21/100][113/156] Data 0.001 (0.001) Batch 3.073 (3.407) Remain 11:42:08 loss: 0.5253 Lr: 0.04669
[2023-08-07 21:56:51,766 INFO misc.py line 115 22900] Train: [21/100][114/156] Data 0.001 (0.001) Batch 3.347 (3.406) Remain 11:41:58 loss: 0.6683 Lr: 0.04669
[2023-08-07 21:56:55,284 INFO misc.py line 115 22900] Train: [21/100][115/156] Data 0.001 (0.001) Batch 3.518 (3.407) Remain 11:42:07 loss: 0.6491 Lr: 0.04669
[2023-08-07 21:56:58,370 INFO misc.py line 115 22900] Train: [21/100][116/156] Data 0.001 (0.001) Batch 3.086 (3.404) Remain 11:41:28 loss: 0.6703 Lr: 0.04669
[2023-08-07 21:57:00,962 INFO misc.py line 115 22900] Train: [21/100][117/156] Data 0.001 (0.001) Batch 2.592 (3.397) Remain 11:39:57 loss: 0.4461 Lr: 0.04668
[2023-08-07 21:57:05,103 INFO misc.py line 115 22900] Train: [21/100][118/156] Data 0.001 (0.001) Batch 4.141 (3.403) Remain 11:41:13 loss: 0.8111 Lr: 0.04668
[2023-08-07 21:57:08,286 INFO misc.py line 115 22900] Train: [21/100][119/156] Data 0.001 (0.001) Batch 3.183 (3.402) Remain 11:40:46 loss: 0.6362 Lr: 0.04668
[2023-08-07 21:57:11,261 INFO misc.py line 115 22900] Train: [21/100][120/156] Data 0.001 (0.001) Batch 2.975 (3.398) Remain 11:39:58 loss: 0.5529 Lr: 0.04667
[2023-08-07 21:57:14,726 INFO misc.py line 115 22900] Train: [21/100][121/156] Data 0.001 (0.001) Batch 3.465 (3.399) Remain 11:40:02 loss: 0.5447 Lr: 0.04667
[2023-08-07 21:57:18,642 INFO misc.py line 115 22900] Train: [21/100][122/156] Data 0.001 (0.001) Batch 3.917 (3.403) Remain 11:40:52 loss: 0.6471 Lr: 0.04667
[2023-08-07 21:57:23,082 INFO misc.py line 115 22900] Train: [21/100][123/156] Data 0.001 (0.001) Batch 4.439 (3.411) Remain 11:42:35 loss: 0.7230 Lr: 0.04667
[2023-08-07 21:57:27,225 INFO misc.py line 115 22900] Train: [21/100][124/156] Data 0.001 (0.001) Batch 4.143 (3.418) Remain 11:43:47 loss: 0.6748 Lr: 0.04666
[2023-08-07 21:57:31,370 INFO misc.py line 115 22900] Train: [21/100][125/156] Data 0.001 (0.001) Batch 4.145 (3.424) Remain 11:44:57 loss: 0.6179 Lr: 0.04666
[2023-08-07 21:57:34,544 INFO misc.py line 115 22900] Train: [21/100][126/156] Data 0.001 (0.001) Batch 3.174 (3.421) Remain 11:44:28 loss: 0.6168 Lr: 0.04666
[2023-08-07 21:57:37,135 INFO misc.py line 115 22900] Train: [21/100][127/156] Data 0.001 (0.001) Batch 2.591 (3.415) Remain 11:43:02 loss: 0.3861 Lr: 0.04666
[2023-08-07 21:57:41,114 INFO misc.py line 115 22900] Train: [21/100][128/156] Data 0.001 (0.001) Batch 3.979 (3.419) Remain 11:43:55 loss: 0.7747 Lr: 0.04665
[2023-08-07 21:57:43,696 INFO misc.py line 115 22900] Train: [21/100][129/156] Data 0.001 (0.001) Batch 2.583 (3.413) Remain 11:42:29 loss: 0.2719 Lr: 0.04665
[2023-08-07 21:57:46,857 INFO misc.py line 115 22900] Train: [21/100][130/156] Data 0.001 (0.001) Batch 3.160 (3.411) Remain 11:42:01 loss: 0.5252 Lr: 0.04665
[2023-08-07 21:57:50,338 INFO misc.py line 115 22900] Train: [21/100][131/156] Data 0.001 (0.001) Batch 3.481 (3.411) Remain 11:42:05 loss: 0.4966 Lr: 0.04665
[2023-08-07 21:57:54,114 INFO misc.py line 115 22900] Train: [21/100][132/156] Data 0.001 (0.001) Batch 3.776 (3.414) Remain 11:42:36 loss: 0.7494 Lr: 0.04664
[2023-08-07 21:57:57,302 INFO misc.py line 115 22900] Train: [21/100][133/156] Data 0.001 (0.001) Batch 3.189 (3.412) Remain 11:42:11 loss: 0.7365 Lr: 0.04664
[2023-08-07 21:58:01,792 INFO misc.py line 115 22900] Train: [21/100][134/156] Data 0.001 (0.001) Batch 4.490 (3.421) Remain 11:43:49 loss: 0.7832 Lr: 0.04664
[2023-08-07 21:58:04,885 INFO misc.py line 115 22900] Train: [21/100][135/156] Data 0.001 (0.001) Batch 3.093 (3.418) Remain 11:43:15 loss: 0.5059 Lr: 0.04663
[2023-08-07 21:58:07,990 INFO misc.py line 115 22900] Train: [21/100][136/156] Data 0.001 (0.001) Batch 3.105 (3.416) Remain 11:42:43 loss: 0.4123 Lr: 0.04663
[2023-08-07 21:58:10,696 INFO misc.py line 115 22900] Train: [21/100][137/156] Data 0.001 (0.001) Batch 2.706 (3.410) Remain 11:41:34 loss: 1.1767 Lr: 0.04663
[2023-08-07 21:58:14,911 INFO misc.py line 115 22900] Train: [21/100][138/156] Data 0.001 (0.001) Batch 4.215 (3.416) Remain 11:42:44 loss: 0.7122 Lr: 0.04663
[2023-08-07 21:58:18,985 INFO misc.py line 115 22900] Train: [21/100][139/156] Data 0.002 (0.001) Batch 4.074 (3.421) Remain 11:43:40 loss: 0.6638 Lr: 0.04662
[2023-08-07 21:58:22,929 INFO misc.py line 115 22900] Train: [21/100][140/156] Data 0.001 (0.001) Batch 3.944 (3.425) Remain 11:44:24 loss: 0.7242 Lr: 0.04662
[2023-08-07 21:58:27,033 INFO misc.py line 115 22900] Train: [21/100][141/156] Data 0.001 (0.001) Batch 4.104 (3.430) Remain 11:45:21 loss: 0.8736 Lr: 0.04662
[2023-08-07 21:58:31,004 INFO misc.py line 115 22900] Train: [21/100][142/156] Data 0.001 (0.001) Batch 3.971 (3.434) Remain 11:46:06 loss: 0.5978 Lr: 0.04662
[2023-08-07 21:58:33,833 INFO misc.py line 115 22900] Train: [21/100][143/156] Data 0.001 (0.001) Batch 2.830 (3.430) Remain 11:45:09 loss: 0.5797 Lr: 0.04661
[2023-08-07 21:58:38,084 INFO misc.py line 115 22900] Train: [21/100][144/156] Data 0.001 (0.001) Batch 4.250 (3.435) Remain 11:46:18 loss: 0.9890 Lr: 0.04661
[2023-08-07 21:58:42,156 INFO misc.py line 115 22900] Train: [21/100][145/156] Data 0.001 (0.001) Batch 4.072 (3.440) Remain 11:47:10 loss: 0.7192 Lr: 0.04661
[2023-08-07 21:58:46,175 INFO misc.py line 115 22900] Train: [21/100][146/156] Data 0.001 (0.001) Batch 4.019 (3.444) Remain 11:47:56 loss: 0.5164 Lr: 0.04661
[2023-08-07 21:58:49,724 INFO misc.py line 115 22900] Train: [21/100][147/156] Data 0.001 (0.001) Batch 3.549 (3.445) Remain 11:48:02 loss: 0.4984 Lr: 0.04660
[2023-08-07 21:58:53,237 INFO misc.py line 115 22900] Train: [21/100][148/156] Data 0.001 (0.001) Batch 3.513 (3.445) Remain 11:48:04 loss: 0.5493 Lr: 0.04660
[2023-08-07 21:58:56,585 INFO misc.py line 115 22900] Train: [21/100][149/156] Data 0.001 (0.001) Batch 3.348 (3.444) Remain 11:47:52 loss: 0.7100 Lr: 0.04660
[2023-08-07 21:59:01,002 INFO misc.py line 115 22900] Train: [21/100][150/156] Data 0.001 (0.001) Batch 4.417 (3.451) Remain 11:49:11 loss: 0.8646 Lr: 0.04659
[2023-08-07 21:59:05,336 INFO misc.py line 115 22900] Train: [21/100][151/156] Data 0.001 (0.001) Batch 4.334 (3.457) Remain 11:50:21 loss: 0.6249 Lr: 0.04659
[2023-08-07 21:59:08,720 INFO misc.py line 115 22900] Train: [21/100][152/156] Data 0.001 (0.001) Batch 3.384 (3.456) Remain 11:50:11 loss: 0.5806 Lr: 0.04659
[2023-08-07 21:59:11,675 INFO misc.py line 115 22900] Train: [21/100][153/156] Data 0.001 (0.001) Batch 2.955 (3.453) Remain 11:49:26 loss: 0.3011 Lr: 0.04659
[2023-08-07 21:59:15,325 INFO misc.py line 115 22900] Train: [21/100][154/156] Data 0.001 (0.001) Batch 3.650 (3.454) Remain 11:49:39 loss: 0.5819 Lr: 0.04658
[2023-08-07 21:59:18,956 INFO misc.py line 115 22900] Train: [21/100][155/156] Data 0.001 (0.001) Batch 3.631 (3.456) Remain 11:49:50 loss: 0.7160 Lr: 0.04658
[2023-08-07 21:59:21,750 INFO misc.py line 115 22900] Train: [21/100][156/156] Data 0.001 (0.001) Batch 2.794 (3.451) Remain 11:48:53 loss: 0.5615 Lr: 0.04658
[2023-08-07 21:59:21,751 INFO misc.py line 129 22900] Train result: loss: 0.5951 
[2023-08-07 21:59:21,758 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 21:59:23,860 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.8820 
[2023-08-07 21:59:24,728 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.6303 
[2023-08-07 21:59:26,391 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.7310 
[2023-08-07 21:59:27,911 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.2503 
[2023-08-07 21:59:29,755 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.3724 
[2023-08-07 21:59:31,419 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.5872 
[2023-08-07 21:59:33,556 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.4223 
[2023-08-07 21:59:35,359 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.9613 
[2023-08-07 21:59:36,642 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.2696 
[2023-08-07 21:59:38,772 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.2019 
[2023-08-07 21:59:39,297 INFO evaluator.py line 122 22900] Test: [11/24] Loss 0.8382 
[2023-08-07 21:59:40,827 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.5743 
[2023-08-07 21:59:43,538 INFO evaluator.py line 122 22900] Test: [13/24] Loss 0.7785 
[2023-08-07 21:59:45,217 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.6316 
[2023-08-07 21:59:47,241 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.4851 
[2023-08-07 21:59:49,953 INFO evaluator.py line 122 22900] Test: [16/24] Loss 0.8214 
[2023-08-07 21:59:52,660 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.0546 
[2023-08-07 21:59:54,507 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.3936 
[2023-08-07 21:59:55,256 INFO evaluator.py line 122 22900] Test: [19/24] Loss 0.9335 
[2023-08-07 21:59:56,141 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.7901 
[2023-08-07 21:59:58,401 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.1013 
[2023-08-07 22:00:00,365 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.5115 
[2023-08-07 22:00:02,209 INFO evaluator.py line 122 22900] Test: [23/24] Loss 1.9576 
[2023-08-07 22:00:04,143 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.5987 
[2023-08-07 22:00:04,191 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2147/0.3078/0.6806.
[2023-08-07 22:00:04,192 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6666/0.9155
[2023-08-07 22:00:04,192 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9517/0.9850
[2023-08-07 22:00:04,192 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1575/0.4715
[2023-08-07 22:00:04,192 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.0424/0.0623
[2023-08-07 22:00:04,192 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6013/0.7402
[2023-08-07 22:00:04,192 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.3008/0.4076
[2023-08-07 22:00:04,192 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5423/0.7059
[2023-08-07 22:00:04,192 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1221/0.1459
[2023-08-07 22:00:04,192 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1577/0.4481
[2023-08-07 22:00:04,192 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.1135/0.1159
[2023-08-07 22:00:04,192 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 22:00:04,192 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0000/0.0000
[2023-08-07 22:00:04,192 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1529/0.2493
[2023-08-07 22:00:04,192 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0789/0.0879
[2023-08-07 22:00:04,192 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0588/0.0676
[2023-08-07 22:00:04,192 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0337/0.0344
[2023-08-07 22:00:04,192 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0000/0.0000
[2023-08-07 22:00:04,192 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.2481/0.5550
[2023-08-07 22:00:04,192 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 22:00:04,192 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0649/0.1640
[2023-08-07 22:00:04,192 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 22:00:04,193 INFO misc.py line 150 22900] Best validation mIoU updated to: 0.2147
[2023-08-07 22:00:04,193 INFO misc.py line 152 22900] Currently Best mIoU: 0.2147
[2023-08-07 22:00:04,193 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 22:00:08,404 INFO misc.py line 115 22900] Train: [22/100][1/156] Data 0.457 (0.457) Batch 3.146 (3.146) Remain 10:46:03 loss: 0.4861 Lr: 0.04658
[2023-08-07 22:00:12,451 INFO misc.py line 115 22900] Train: [22/100][2/156] Data 0.001 (0.001) Batch 4.047 (4.047) Remain 13:51:11 loss: 0.8164 Lr: 0.04657
[2023-08-07 22:00:15,711 INFO misc.py line 115 22900] Train: [22/100][3/156] Data 0.001 (0.001) Batch 3.260 (3.260) Remain 11:09:27 loss: 0.4507 Lr: 0.04657
[2023-08-07 22:00:19,304 INFO misc.py line 115 22900] Train: [22/100][4/156] Data 0.001 (0.001) Batch 3.593 (3.593) Remain 12:17:43 loss: 0.4329 Lr: 0.04657
[2023-08-07 22:00:22,317 INFO misc.py line 115 22900] Train: [22/100][5/156] Data 0.001 (0.001) Batch 3.013 (3.303) Remain 11:18:08 loss: 0.5912 Lr: 0.04657
[2023-08-07 22:00:26,382 INFO misc.py line 115 22900] Train: [22/100][6/156] Data 0.001 (0.001) Batch 4.065 (3.557) Remain 12:10:13 loss: 0.5899 Lr: 0.04656
[2023-08-07 22:00:29,257 INFO misc.py line 115 22900] Train: [22/100][7/156] Data 0.001 (0.001) Batch 2.875 (3.386) Remain 11:35:08 loss: 0.3398 Lr: 0.04656
[2023-08-07 22:00:32,927 INFO misc.py line 115 22900] Train: [22/100][8/156] Data 0.001 (0.001) Batch 3.671 (3.443) Remain 11:46:45 loss: 0.4371 Lr: 0.04656
[2023-08-07 22:00:36,749 INFO misc.py line 115 22900] Train: [22/100][9/156] Data 0.001 (0.001) Batch 3.822 (3.506) Remain 11:59:39 loss: 0.5033 Lr: 0.04655
[2023-08-07 22:00:39,572 INFO misc.py line 115 22900] Train: [22/100][10/156] Data 0.001 (0.001) Batch 2.823 (3.409) Remain 11:39:33 loss: 0.5870 Lr: 0.04655
[2023-08-07 22:00:43,255 INFO misc.py line 115 22900] Train: [22/100][11/156] Data 0.001 (0.001) Batch 3.683 (3.443) Remain 11:46:32 loss: 0.6225 Lr: 0.04655
[2023-08-07 22:00:45,974 INFO misc.py line 115 22900] Train: [22/100][12/156] Data 0.001 (0.001) Batch 2.719 (3.362) Remain 11:29:58 loss: 0.5881 Lr: 0.04655
[2023-08-07 22:00:49,192 INFO misc.py line 115 22900] Train: [22/100][13/156] Data 0.001 (0.001) Batch 3.218 (3.348) Remain 11:26:57 loss: 0.5708 Lr: 0.04654
[2023-08-07 22:00:52,298 INFO misc.py line 115 22900] Train: [22/100][14/156] Data 0.001 (0.001) Batch 3.106 (3.326) Remain 11:22:23 loss: 0.3696 Lr: 0.04654
[2023-08-07 22:00:55,893 INFO misc.py line 115 22900] Train: [22/100][15/156] Data 0.001 (0.001) Batch 3.596 (3.348) Remain 11:26:56 loss: 0.5288 Lr: 0.04654
[2023-08-07 22:00:58,728 INFO misc.py line 115 22900] Train: [22/100][16/156] Data 0.001 (0.001) Batch 2.835 (3.309) Remain 11:18:47 loss: 0.3846 Lr: 0.04654
[2023-08-07 22:01:02,742 INFO misc.py line 115 22900] Train: [22/100][17/156] Data 0.001 (0.001) Batch 4.013 (3.359) Remain 11:29:02 loss: 0.9109 Lr: 0.04653
[2023-08-07 22:01:06,943 INFO misc.py line 115 22900] Train: [22/100][18/156] Data 0.001 (0.001) Batch 4.201 (3.415) Remain 11:40:30 loss: 0.7125 Lr: 0.04653
[2023-08-07 22:01:11,027 INFO misc.py line 115 22900] Train: [22/100][19/156] Data 0.001 (0.001) Batch 4.084 (3.457) Remain 11:49:01 loss: 0.6173 Lr: 0.04653
[2023-08-07 22:01:14,075 INFO misc.py line 115 22900] Train: [22/100][20/156] Data 0.001 (0.001) Batch 3.047 (3.433) Remain 11:44:01 loss: 0.3379 Lr: 0.04653
[2023-08-07 22:01:18,072 INFO misc.py line 115 22900] Train: [22/100][21/156] Data 0.001 (0.001) Batch 3.997 (3.464) Remain 11:50:23 loss: 0.7892 Lr: 0.04652
[2023-08-07 22:01:20,572 INFO misc.py line 115 22900] Train: [22/100][22/156] Data 0.001 (0.001) Batch 2.501 (3.414) Remain 11:39:55 loss: 0.2657 Lr: 0.04652
[2023-08-07 22:01:24,656 INFO misc.py line 115 22900] Train: [22/100][23/156] Data 0.001 (0.001) Batch 4.084 (3.447) Remain 11:46:44 loss: 0.6675 Lr: 0.04652
[2023-08-07 22:01:28,115 INFO misc.py line 115 22900] Train: [22/100][24/156] Data 0.001 (0.001) Batch 3.459 (3.448) Remain 11:46:47 loss: 0.6111 Lr: 0.04651
[2023-08-07 22:01:32,161 INFO misc.py line 115 22900] Train: [22/100][25/156] Data 0.001 (0.001) Batch 4.046 (3.475) Remain 11:52:18 loss: 0.8213 Lr: 0.04651
[2023-08-07 22:01:36,278 INFO misc.py line 115 22900] Train: [22/100][26/156] Data 0.001 (0.001) Batch 4.117 (3.503) Remain 11:57:58 loss: 0.4910 Lr: 0.04651
[2023-08-07 22:01:40,315 INFO misc.py line 115 22900] Train: [22/100][27/156] Data 0.001 (0.001) Batch 4.037 (3.525) Remain 12:02:28 loss: 0.9690 Lr: 0.04651
[2023-08-07 22:01:43,879 INFO misc.py line 115 22900] Train: [22/100][28/156] Data 0.001 (0.001) Batch 3.565 (3.527) Remain 12:02:44 loss: 0.4965 Lr: 0.04650
[2023-08-07 22:01:47,987 INFO misc.py line 115 22900] Train: [22/100][29/156] Data 0.001 (0.001) Batch 4.107 (3.549) Remain 12:07:15 loss: 0.5453 Lr: 0.04650
[2023-08-07 22:01:52,046 INFO misc.py line 115 22900] Train: [22/100][30/156] Data 0.001 (0.001) Batch 4.059 (3.568) Remain 12:11:04 loss: 0.9837 Lr: 0.04650
[2023-08-07 22:01:55,482 INFO misc.py line 115 22900] Train: [22/100][31/156] Data 0.001 (0.001) Batch 3.437 (3.563) Remain 12:10:03 loss: 0.4889 Lr: 0.04650
[2023-08-07 22:01:58,834 INFO misc.py line 115 22900] Train: [22/100][32/156] Data 0.001 (0.001) Batch 3.352 (3.556) Remain 12:08:29 loss: 0.5356 Lr: 0.04649
[2023-08-07 22:02:02,254 INFO misc.py line 115 22900] Train: [22/100][33/156] Data 0.001 (0.001) Batch 3.419 (3.551) Remain 12:07:30 loss: 0.8049 Lr: 0.04649
[2023-08-07 22:02:04,884 INFO misc.py line 115 22900] Train: [22/100][34/156] Data 0.001 (0.001) Batch 2.630 (3.522) Remain 12:01:21 loss: 0.7907 Lr: 0.04649
[2023-08-07 22:02:07,651 INFO misc.py line 115 22900] Train: [22/100][35/156] Data 0.001 (0.001) Batch 2.767 (3.498) Remain 11:56:28 loss: 0.5319 Lr: 0.04648
[2023-08-07 22:02:10,422 INFO misc.py line 115 22900] Train: [22/100][36/156] Data 0.001 (0.001) Batch 2.771 (3.476) Remain 11:51:54 loss: 0.4660 Lr: 0.04648
[2023-08-07 22:02:13,206 INFO misc.py line 115 22900] Train: [22/100][37/156] Data 0.001 (0.001) Batch 2.783 (3.456) Remain 11:47:40 loss: 0.4964 Lr: 0.04648
[2023-08-07 22:02:15,851 INFO misc.py line 115 22900] Train: [22/100][38/156] Data 0.001 (0.001) Batch 2.646 (3.433) Remain 11:42:52 loss: 0.5322 Lr: 0.04648
[2023-08-07 22:02:19,056 INFO misc.py line 115 22900] Train: [22/100][39/156] Data 0.001 (0.001) Batch 3.205 (3.426) Remain 11:41:31 loss: 0.7677 Lr: 0.04647
[2023-08-07 22:02:21,822 INFO misc.py line 115 22900] Train: [22/100][40/156] Data 0.001 (0.001) Batch 2.766 (3.408) Remain 11:37:48 loss: 0.2978 Lr: 0.04647
[2023-08-07 22:02:25,131 INFO misc.py line 115 22900] Train: [22/100][41/156] Data 0.001 (0.001) Batch 3.309 (3.406) Remain 11:37:13 loss: 0.8282 Lr: 0.04647
[2023-08-07 22:02:28,914 INFO misc.py line 115 22900] Train: [22/100][42/156] Data 0.001 (0.001) Batch 3.783 (3.415) Remain 11:39:08 loss: 0.6079 Lr: 0.04647
[2023-08-07 22:02:31,341 INFO misc.py line 115 22900] Train: [22/100][43/156] Data 0.001 (0.001) Batch 2.427 (3.391) Remain 11:34:01 loss: 0.6654 Lr: 0.04646
[2023-08-07 22:02:34,898 INFO misc.py line 115 22900] Train: [22/100][44/156] Data 0.001 (0.001) Batch 3.557 (3.395) Remain 11:34:48 loss: 0.5184 Lr: 0.04646
[2023-08-07 22:02:37,699 INFO misc.py line 115 22900] Train: [22/100][45/156] Data 0.001 (0.001) Batch 2.801 (3.381) Remain 11:31:51 loss: 0.4772 Lr: 0.04646
[2023-08-07 22:02:40,142 INFO misc.py line 115 22900] Train: [22/100][46/156] Data 0.001 (0.001) Batch 2.443 (3.359) Remain 11:27:20 loss: 0.4679 Lr: 0.04645
[2023-08-07 22:02:44,525 INFO misc.py line 115 22900] Train: [22/100][47/156] Data 0.001 (0.001) Batch 4.383 (3.382) Remain 11:32:02 loss: 0.7543 Lr: 0.04645
[2023-08-07 22:02:48,615 INFO misc.py line 115 22900] Train: [22/100][48/156] Data 0.001 (0.001) Batch 4.090 (3.398) Remain 11:35:11 loss: 0.5824 Lr: 0.04645
[2023-08-07 22:02:52,805 INFO misc.py line 115 22900] Train: [22/100][49/156] Data 0.001 (0.001) Batch 4.190 (3.415) Remain 11:38:40 loss: 0.6771 Lr: 0.04645
[2023-08-07 22:02:55,613 INFO misc.py line 115 22900] Train: [22/100][50/156] Data 0.001 (0.001) Batch 2.808 (3.402) Remain 11:35:58 loss: 0.5904 Lr: 0.04644
[2023-08-07 22:02:59,800 INFO misc.py line 115 22900] Train: [22/100][51/156] Data 0.001 (0.001) Batch 4.187 (3.419) Remain 11:39:15 loss: 0.5657 Lr: 0.04644
[2023-08-07 22:03:03,522 INFO misc.py line 115 22900] Train: [22/100][52/156] Data 0.001 (0.001) Batch 3.722 (3.425) Remain 11:40:27 loss: 0.4259 Lr: 0.04644
[2023-08-07 22:03:06,324 INFO misc.py line 115 22900] Train: [22/100][53/156] Data 0.001 (0.001) Batch 2.802 (3.412) Remain 11:37:51 loss: 0.2885 Lr: 0.04644
[2023-08-07 22:03:08,583 INFO misc.py line 115 22900] Train: [22/100][54/156] Data 0.001 (0.001) Batch 2.259 (3.390) Remain 11:33:10 loss: 0.3790 Lr: 0.04643
[2023-08-07 22:03:12,605 INFO misc.py line 115 22900] Train: [22/100][55/156] Data 0.001 (0.001) Batch 4.023 (3.402) Remain 11:35:36 loss: 0.6824 Lr: 0.04643
[2023-08-07 22:03:16,545 INFO misc.py line 115 22900] Train: [22/100][56/156] Data 0.001 (0.001) Batch 3.940 (3.412) Remain 11:37:37 loss: 0.8613 Lr: 0.04643
[2023-08-07 22:03:19,170 INFO misc.py line 115 22900] Train: [22/100][57/156] Data 0.001 (0.001) Batch 2.625 (3.397) Remain 11:34:35 loss: 0.4276 Lr: 0.04642
[2023-08-07 22:03:23,016 INFO misc.py line 115 22900] Train: [22/100][58/156] Data 0.001 (0.001) Batch 3.846 (3.406) Remain 11:36:12 loss: 0.6806 Lr: 0.04642
[2023-08-07 22:03:26,050 INFO misc.py line 115 22900] Train: [22/100][59/156] Data 0.001 (0.001) Batch 3.034 (3.399) Remain 11:34:47 loss: 0.5191 Lr: 0.04642
[2023-08-07 22:03:30,004 INFO misc.py line 115 22900] Train: [22/100][60/156] Data 0.001 (0.001) Batch 3.954 (3.409) Remain 11:36:43 loss: 0.4860 Lr: 0.04642
[2023-08-07 22:03:34,164 INFO misc.py line 115 22900] Train: [22/100][61/156] Data 0.001 (0.001) Batch 4.160 (3.422) Remain 11:39:19 loss: 0.8327 Lr: 0.04641
[2023-08-07 22:03:37,361 INFO misc.py line 115 22900] Train: [22/100][62/156] Data 0.001 (0.001) Batch 3.197 (3.418) Remain 11:38:28 loss: 0.3914 Lr: 0.04641
[2023-08-07 22:03:41,625 INFO misc.py line 115 22900] Train: [22/100][63/156] Data 0.001 (0.001) Batch 4.264 (3.432) Remain 11:41:18 loss: 0.6702 Lr: 0.04641
[2023-08-07 22:03:45,621 INFO misc.py line 115 22900] Train: [22/100][64/156] Data 0.001 (0.001) Batch 3.996 (3.441) Remain 11:43:08 loss: 0.5528 Lr: 0.04641
[2023-08-07 22:03:49,405 INFO misc.py line 115 22900] Train: [22/100][65/156] Data 0.001 (0.001) Batch 3.784 (3.447) Remain 11:44:12 loss: 0.5763 Lr: 0.04640
[2023-08-07 22:03:53,943 INFO misc.py line 115 22900] Train: [22/100][66/156] Data 0.001 (0.001) Batch 4.538 (3.464) Remain 11:47:41 loss: 0.6724 Lr: 0.04640
[2023-08-07 22:03:57,788 INFO misc.py line 115 22900] Train: [22/100][67/156] Data 0.001 (0.001) Batch 3.845 (3.470) Remain 11:48:51 loss: 0.7915 Lr: 0.04640
[2023-08-07 22:04:00,831 INFO misc.py line 115 22900] Train: [22/100][68/156] Data 0.001 (0.001) Batch 3.043 (3.463) Remain 11:47:27 loss: 0.5969 Lr: 0.04639
[2023-08-07 22:04:03,818 INFO misc.py line 115 22900] Train: [22/100][69/156] Data 0.001 (0.001) Batch 2.987 (3.456) Remain 11:45:55 loss: 0.3155 Lr: 0.04639
[2023-08-07 22:04:06,979 INFO misc.py line 115 22900] Train: [22/100][70/156] Data 0.001 (0.001) Batch 3.161 (3.452) Remain 11:44:57 loss: 0.4409 Lr: 0.04639
[2023-08-07 22:04:10,944 INFO misc.py line 115 22900] Train: [22/100][71/156] Data 0.001 (0.001) Batch 3.965 (3.459) Remain 11:46:26 loss: 0.6126 Lr: 0.04639
[2023-08-07 22:04:14,092 INFO misc.py line 115 22900] Train: [22/100][72/156] Data 0.001 (0.001) Batch 3.148 (3.455) Remain 11:45:28 loss: 0.4603 Lr: 0.04638
[2023-08-07 22:04:16,656 INFO misc.py line 115 22900] Train: [22/100][73/156] Data 0.001 (0.001) Batch 2.564 (3.442) Remain 11:42:48 loss: 0.5180 Lr: 0.04638
[2023-08-07 22:04:19,904 INFO misc.py line 115 22900] Train: [22/100][74/156] Data 0.001 (0.001) Batch 3.248 (3.439) Remain 11:42:11 loss: 0.5202 Lr: 0.04638
[2023-08-07 22:04:22,790 INFO misc.py line 115 22900] Train: [22/100][75/156] Data 0.001 (0.001) Batch 2.887 (3.432) Remain 11:40:34 loss: 0.6296 Lr: 0.04638
[2023-08-07 22:04:26,166 INFO misc.py line 115 22900] Train: [22/100][76/156] Data 0.001 (0.001) Batch 3.375 (3.431) Remain 11:40:21 loss: 0.6178 Lr: 0.04637
[2023-08-07 22:04:29,124 INFO misc.py line 115 22900] Train: [22/100][77/156] Data 0.001 (0.001) Batch 2.958 (3.424) Remain 11:38:59 loss: 0.2822 Lr: 0.04637
[2023-08-07 22:04:31,968 INFO misc.py line 115 22900] Train: [22/100][78/156] Data 0.001 (0.001) Batch 2.844 (3.417) Remain 11:37:21 loss: 0.3630 Lr: 0.04637
[2023-08-07 22:04:35,443 INFO misc.py line 115 22900] Train: [22/100][79/156] Data 0.001 (0.001) Batch 3.475 (3.418) Remain 11:37:27 loss: 0.4173 Lr: 0.04636
[2023-08-07 22:04:39,184 INFO misc.py line 115 22900] Train: [22/100][80/156] Data 0.001 (0.001) Batch 3.741 (3.422) Remain 11:38:15 loss: 0.6851 Lr: 0.04636
[2023-08-07 22:04:42,166 INFO misc.py line 115 22900] Train: [22/100][81/156] Data 0.001 (0.001) Batch 2.982 (3.416) Remain 11:37:03 loss: 0.1680 Lr: 0.04636
[2023-08-07 22:04:45,805 INFO misc.py line 115 22900] Train: [22/100][82/156] Data 0.001 (0.001) Batch 3.639 (3.419) Remain 11:37:34 loss: 0.5176 Lr: 0.04636
[2023-08-07 22:04:48,822 INFO misc.py line 115 22900] Train: [22/100][83/156] Data 0.001 (0.001) Batch 3.017 (3.414) Remain 11:36:29 loss: 0.3816 Lr: 0.04635
[2023-08-07 22:04:51,952 INFO misc.py line 115 22900] Train: [22/100][84/156] Data 0.001 (0.001) Batch 3.129 (3.410) Remain 11:35:42 loss: 0.4331 Lr: 0.04635
[2023-08-07 22:04:54,962 INFO misc.py line 115 22900] Train: [22/100][85/156] Data 0.001 (0.001) Batch 3.010 (3.405) Remain 11:34:39 loss: 0.3477 Lr: 0.04635
[2023-08-07 22:04:57,651 INFO misc.py line 115 22900] Train: [22/100][86/156] Data 0.001 (0.001) Batch 2.689 (3.397) Remain 11:32:50 loss: 0.2738 Lr: 0.04635
[2023-08-07 22:05:00,105 INFO misc.py line 115 22900] Train: [22/100][87/156] Data 0.001 (0.001) Batch 2.453 (3.386) Remain 11:30:29 loss: 0.3077 Lr: 0.04634
[2023-08-07 22:05:03,005 INFO misc.py line 115 22900] Train: [22/100][88/156] Data 0.001 (0.001) Batch 2.901 (3.380) Remain 11:29:16 loss: 0.4162 Lr: 0.04634
[2023-08-07 22:05:06,548 INFO misc.py line 115 22900] Train: [22/100][89/156] Data 0.001 (0.001) Batch 3.543 (3.382) Remain 11:29:36 loss: 0.7681 Lr: 0.04634
[2023-08-07 22:05:09,789 INFO misc.py line 115 22900] Train: [22/100][90/156] Data 0.001 (0.001) Batch 3.240 (3.380) Remain 11:29:13 loss: 0.5260 Lr: 0.04633
[2023-08-07 22:05:13,240 INFO misc.py line 115 22900] Train: [22/100][91/156] Data 0.001 (0.001) Batch 3.451 (3.381) Remain 11:29:19 loss: 0.8983 Lr: 0.04633
[2023-08-07 22:05:16,983 INFO misc.py line 115 22900] Train: [22/100][92/156] Data 0.001 (0.001) Batch 3.743 (3.385) Remain 11:30:06 loss: 0.4417 Lr: 0.04633
[2023-08-07 22:05:20,991 INFO misc.py line 115 22900] Train: [22/100][93/156] Data 0.001 (0.001) Batch 4.008 (3.392) Remain 11:31:27 loss: 0.6301 Lr: 0.04633
[2023-08-07 22:05:24,419 INFO misc.py line 115 22900] Train: [22/100][94/156] Data 0.001 (0.001) Batch 3.428 (3.392) Remain 11:31:28 loss: 0.6560 Lr: 0.04632
[2023-08-07 22:05:27,611 INFO misc.py line 115 22900] Train: [22/100][95/156] Data 0.001 (0.001) Batch 3.192 (3.390) Remain 11:30:58 loss: 0.4087 Lr: 0.04632
[2023-08-07 22:05:30,353 INFO misc.py line 115 22900] Train: [22/100][96/156] Data 0.001 (0.001) Batch 2.742 (3.383) Remain 11:29:30 loss: 1.3732 Lr: 0.04632
[2023-08-07 22:05:34,049 INFO misc.py line 115 22900] Train: [22/100][97/156] Data 0.001 (0.001) Batch 3.696 (3.387) Remain 11:30:07 loss: 0.7176 Lr: 0.04631
[2023-08-07 22:05:36,978 INFO misc.py line 115 22900] Train: [22/100][98/156] Data 0.001 (0.001) Batch 2.929 (3.382) Remain 11:29:05 loss: 0.5408 Lr: 0.04631
[2023-08-07 22:05:40,300 INFO misc.py line 115 22900] Train: [22/100][99/156] Data 0.001 (0.001) Batch 3.322 (3.381) Remain 11:28:54 loss: 0.6802 Lr: 0.04631
[2023-08-07 22:05:43,328 INFO misc.py line 115 22900] Train: [22/100][100/156] Data 0.001 (0.001) Batch 3.028 (3.377) Remain 11:28:06 loss: 0.4922 Lr: 0.04631
[2023-08-07 22:05:47,036 INFO misc.py line 115 22900] Train: [22/100][101/156] Data 0.001 (0.001) Batch 3.708 (3.381) Remain 11:28:44 loss: 0.3817 Lr: 0.04630
[2023-08-07 22:05:51,131 INFO misc.py line 115 22900] Train: [22/100][102/156] Data 0.001 (0.001) Batch 4.095 (3.388) Remain 11:30:09 loss: 0.5573 Lr: 0.04630
[2023-08-07 22:05:54,274 INFO misc.py line 115 22900] Train: [22/100][103/156] Data 0.001 (0.001) Batch 3.143 (3.386) Remain 11:29:35 loss: 0.3365 Lr: 0.04630
[2023-08-07 22:05:58,245 INFO misc.py line 115 22900] Train: [22/100][104/156] Data 0.001 (0.001) Batch 3.971 (3.391) Remain 11:30:43 loss: 0.8562 Lr: 0.04630
[2023-08-07 22:06:02,733 INFO misc.py line 115 22900] Train: [22/100][105/156] Data 0.001 (0.001) Batch 4.488 (3.402) Remain 11:32:51 loss: 0.6630 Lr: 0.04629
[2023-08-07 22:06:06,727 INFO misc.py line 115 22900] Train: [22/100][106/156] Data 0.001 (0.001) Batch 3.994 (3.408) Remain 11:33:57 loss: 0.6813 Lr: 0.04629
[2023-08-07 22:06:11,839 INFO misc.py line 115 22900] Train: [22/100][107/156] Data 0.001 (0.001) Batch 5.111 (3.424) Remain 11:37:14 loss: 0.9752 Lr: 0.04629
[2023-08-07 22:06:15,153 INFO misc.py line 115 22900] Train: [22/100][108/156] Data 0.001 (0.001) Batch 3.315 (3.423) Remain 11:36:58 loss: 0.5522 Lr: 0.04628
[2023-08-07 22:06:18,110 INFO misc.py line 115 22900] Train: [22/100][109/156] Data 0.001 (0.001) Batch 2.957 (3.419) Remain 11:36:01 loss: 0.4936 Lr: 0.04628
[2023-08-07 22:06:21,210 INFO misc.py line 115 22900] Train: [22/100][110/156] Data 0.001 (0.001) Batch 3.099 (3.416) Remain 11:35:21 loss: 0.3976 Lr: 0.04628
[2023-08-07 22:06:25,280 INFO misc.py line 115 22900] Train: [22/100][111/156] Data 0.001 (0.001) Batch 4.070 (3.422) Remain 11:36:32 loss: 0.5116 Lr: 0.04628
[2023-08-07 22:06:29,339 INFO misc.py line 115 22900] Train: [22/100][112/156] Data 0.001 (0.001) Batch 4.059 (3.428) Remain 11:37:39 loss: 0.5556 Lr: 0.04627
[2023-08-07 22:06:33,206 INFO misc.py line 115 22900] Train: [22/100][113/156] Data 0.001 (0.001) Batch 3.867 (3.432) Remain 11:38:25 loss: 0.4252 Lr: 0.04627
[2023-08-07 22:06:36,835 INFO misc.py line 115 22900] Train: [22/100][114/156] Data 0.001 (0.001) Batch 3.629 (3.434) Remain 11:38:43 loss: 0.8633 Lr: 0.04627
[2023-08-07 22:06:40,274 INFO misc.py line 115 22900] Train: [22/100][115/156] Data 0.001 (0.001) Batch 3.439 (3.434) Remain 11:38:40 loss: 0.4181 Lr: 0.04626
[2023-08-07 22:06:44,488 INFO misc.py line 115 22900] Train: [22/100][116/156] Data 0.001 (0.001) Batch 4.214 (3.441) Remain 11:40:01 loss: 0.9491 Lr: 0.04626
[2023-08-07 22:06:47,931 INFO misc.py line 115 22900] Train: [22/100][117/156] Data 0.001 (0.001) Batch 3.443 (3.441) Remain 11:39:58 loss: 0.8432 Lr: 0.04626
[2023-08-07 22:06:51,991 INFO misc.py line 115 22900] Train: [22/100][118/156] Data 0.001 (0.001) Batch 4.060 (3.446) Remain 11:41:00 loss: 0.5853 Lr: 0.04626
[2023-08-07 22:06:54,867 INFO misc.py line 115 22900] Train: [22/100][119/156] Data 0.001 (0.001) Batch 2.876 (3.441) Remain 11:39:57 loss: 0.5979 Lr: 0.04625
[2023-08-07 22:06:58,160 INFO misc.py line 115 22900] Train: [22/100][120/156] Data 0.001 (0.001) Batch 3.293 (3.440) Remain 11:39:38 loss: 0.7129 Lr: 0.04625
[2023-08-07 22:07:02,041 INFO misc.py line 115 22900] Train: [22/100][121/156] Data 0.001 (0.001) Batch 3.881 (3.443) Remain 11:40:20 loss: 0.5209 Lr: 0.04625
[2023-08-07 22:07:04,734 INFO misc.py line 115 22900] Train: [22/100][122/156] Data 0.001 (0.001) Batch 2.693 (3.437) Remain 11:39:00 loss: 0.5199 Lr: 0.04625
[2023-08-07 22:07:08,246 INFO misc.py line 115 22900] Train: [22/100][123/156] Data 0.001 (0.001) Batch 3.512 (3.438) Remain 11:39:04 loss: 0.5696 Lr: 0.04624
[2023-08-07 22:07:12,056 INFO misc.py line 115 22900] Train: [22/100][124/156] Data 0.001 (0.001) Batch 3.810 (3.441) Remain 11:39:38 loss: 0.4702 Lr: 0.04624
[2023-08-07 22:07:16,247 INFO misc.py line 115 22900] Train: [22/100][125/156] Data 0.001 (0.001) Batch 4.191 (3.447) Remain 11:40:50 loss: 0.6857 Lr: 0.04624
[2023-08-07 22:07:20,306 INFO misc.py line 115 22900] Train: [22/100][126/156] Data 0.001 (0.001) Batch 4.058 (3.452) Remain 11:41:47 loss: 0.7768 Lr: 0.04623
[2023-08-07 22:07:23,130 INFO misc.py line 115 22900] Train: [22/100][127/156] Data 0.001 (0.001) Batch 2.824 (3.447) Remain 11:40:42 loss: 0.5218 Lr: 0.04623
[2023-08-07 22:07:27,237 INFO misc.py line 115 22900] Train: [22/100][128/156] Data 0.001 (0.001) Batch 4.106 (3.452) Remain 11:41:43 loss: 0.7480 Lr: 0.04623
[2023-08-07 22:07:30,133 INFO misc.py line 115 22900] Train: [22/100][129/156] Data 0.001 (0.001) Batch 2.896 (3.448) Remain 11:40:45 loss: 0.4928 Lr: 0.04623
[2023-08-07 22:07:33,730 INFO misc.py line 115 22900] Train: [22/100][130/156] Data 0.001 (0.001) Batch 3.597 (3.449) Remain 11:40:56 loss: 0.5960 Lr: 0.04622
[2023-08-07 22:07:38,180 INFO misc.py line 115 22900] Train: [22/100][131/156] Data 0.001 (0.001) Batch 4.450 (3.457) Remain 11:42:28 loss: 0.6732 Lr: 0.04622
[2023-08-07 22:07:41,553 INFO misc.py line 115 22900] Train: [22/100][132/156] Data 0.001 (0.001) Batch 3.373 (3.456) Remain 11:42:17 loss: 0.7057 Lr: 0.04622
[2023-08-07 22:07:45,262 INFO misc.py line 115 22900] Train: [22/100][133/156] Data 0.001 (0.001) Batch 3.710 (3.458) Remain 11:42:37 loss: 0.6702 Lr: 0.04621
[2023-08-07 22:07:48,671 INFO misc.py line 115 22900] Train: [22/100][134/156] Data 0.001 (0.001) Batch 3.408 (3.458) Remain 11:42:29 loss: 0.5010 Lr: 0.04621
[2023-08-07 22:07:50,779 INFO misc.py line 115 22900] Train: [22/100][135/156] Data 0.001 (0.001) Batch 2.108 (3.447) Remain 11:40:21 loss: 0.2871 Lr: 0.04621
[2023-08-07 22:07:54,926 INFO misc.py line 115 22900] Train: [22/100][136/156] Data 0.001 (0.001) Batch 4.147 (3.453) Remain 11:41:21 loss: 1.0946 Lr: 0.04621
[2023-08-07 22:07:58,400 INFO misc.py line 115 22900] Train: [22/100][137/156] Data 0.001 (0.001) Batch 3.474 (3.453) Remain 11:41:20 loss: 0.4816 Lr: 0.04620
[2023-08-07 22:08:01,780 INFO misc.py line 115 22900] Train: [22/100][138/156] Data 0.001 (0.001) Batch 3.380 (3.452) Remain 11:41:10 loss: 0.6468 Lr: 0.04620
[2023-08-07 22:08:05,514 INFO misc.py line 115 22900] Train: [22/100][139/156] Data 0.001 (0.001) Batch 3.734 (3.454) Remain 11:41:32 loss: 0.5317 Lr: 0.04620
[2023-08-07 22:08:09,354 INFO misc.py line 115 22900] Train: [22/100][140/156] Data 0.001 (0.001) Batch 3.841 (3.457) Remain 11:42:03 loss: 0.5644 Lr: 0.04619
[2023-08-07 22:08:13,321 INFO misc.py line 115 22900] Train: [22/100][141/156] Data 0.001 (0.001) Batch 3.967 (3.461) Remain 11:42:44 loss: 0.8387 Lr: 0.04619
[2023-08-07 22:08:16,231 INFO misc.py line 115 22900] Train: [22/100][142/156] Data 0.001 (0.001) Batch 2.910 (3.457) Remain 11:41:52 loss: 0.3914 Lr: 0.04619
[2023-08-07 22:08:19,447 INFO misc.py line 115 22900] Train: [22/100][143/156] Data 0.001 (0.001) Batch 3.216 (3.455) Remain 11:41:28 loss: 0.3085 Lr: 0.04619
[2023-08-07 22:08:22,758 INFO misc.py line 115 22900] Train: [22/100][144/156] Data 0.001 (0.001) Batch 3.311 (3.454) Remain 11:41:12 loss: 0.3820 Lr: 0.04618
[2023-08-07 22:08:26,080 INFO misc.py line 115 22900] Train: [22/100][145/156] Data 0.001 (0.001) Batch 3.322 (3.453) Remain 11:40:57 loss: 0.6030 Lr: 0.04618
[2023-08-07 22:08:29,457 INFO misc.py line 115 22900] Train: [22/100][146/156] Data 0.001 (0.001) Batch 3.377 (3.453) Remain 11:40:47 loss: 0.4470 Lr: 0.04618
[2023-08-07 22:08:32,692 INFO misc.py line 115 22900] Train: [22/100][147/156] Data 0.001 (0.001) Batch 3.235 (3.451) Remain 11:40:25 loss: 0.3864 Lr: 0.04617
[2023-08-07 22:08:35,297 INFO misc.py line 115 22900] Train: [22/100][148/156] Data 0.001 (0.001) Batch 2.606 (3.445) Remain 11:39:11 loss: 0.4283 Lr: 0.04617
[2023-08-07 22:08:39,175 INFO misc.py line 115 22900] Train: [22/100][149/156] Data 0.001 (0.001) Batch 3.878 (3.448) Remain 11:39:44 loss: 0.6742 Lr: 0.04617
[2023-08-07 22:08:43,226 INFO misc.py line 115 22900] Train: [22/100][150/156] Data 0.001 (0.001) Batch 4.051 (3.452) Remain 11:40:30 loss: 0.6767 Lr: 0.04617
[2023-08-07 22:08:46,307 INFO misc.py line 115 22900] Train: [22/100][151/156] Data 0.001 (0.001) Batch 3.081 (3.450) Remain 11:39:56 loss: 0.6980 Lr: 0.04616
[2023-08-07 22:08:49,558 INFO misc.py line 115 22900] Train: [22/100][152/156] Data 0.001 (0.001) Batch 3.250 (3.449) Remain 11:39:36 loss: 0.7990 Lr: 0.04616
[2023-08-07 22:08:53,604 INFO misc.py line 115 22900] Train: [22/100][153/156] Data 0.001 (0.001) Batch 4.046 (3.453) Remain 11:40:21 loss: 0.5740 Lr: 0.04616
[2023-08-07 22:08:57,692 INFO misc.py line 115 22900] Train: [22/100][154/156] Data 0.001 (0.001) Batch 4.088 (3.457) Remain 11:41:09 loss: 0.5983 Lr: 0.04616
[2023-08-07 22:09:00,959 INFO misc.py line 115 22900] Train: [22/100][155/156] Data 0.001 (0.001) Batch 3.268 (3.456) Remain 11:40:50 loss: 0.3948 Lr: 0.04615
[2023-08-07 22:09:04,021 INFO misc.py line 115 22900] Train: [22/100][156/156] Data 0.001 (0.001) Batch 3.062 (3.453) Remain 11:40:16 loss: 0.4326 Lr: 0.04615
[2023-08-07 22:09:04,021 INFO misc.py line 129 22900] Train result: loss: 0.5749 
[2023-08-07 22:09:04,021 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 22:09:06,150 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.7021 
[2023-08-07 22:09:07,019 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.7580 
[2023-08-07 22:09:08,685 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.6990 
[2023-08-07 22:09:10,207 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.1166 
[2023-08-07 22:09:12,050 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.2518 
[2023-08-07 22:09:13,715 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.9771 
[2023-08-07 22:09:15,853 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.5533 
[2023-08-07 22:09:17,658 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.7899 
[2023-08-07 22:09:18,942 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.1629 
[2023-08-07 22:09:21,071 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.2354 
[2023-08-07 22:09:21,598 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.0322 
[2023-08-07 22:09:23,131 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.7398 
[2023-08-07 22:09:25,843 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.1199 
[2023-08-07 22:09:27,522 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.6705 
[2023-08-07 22:09:29,543 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.5105 
[2023-08-07 22:09:32,252 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.1637 
[2023-08-07 22:09:34,957 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.3156 
[2023-08-07 22:09:36,802 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.1965 
[2023-08-07 22:09:37,552 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.1980 
[2023-08-07 22:09:38,436 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.8482 
[2023-08-07 22:09:40,698 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3288 
[2023-08-07 22:09:42,662 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.3097 
[2023-08-07 22:09:44,512 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.3605 
[2023-08-07 22:09:46,448 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.4959 
[2023-08-07 22:09:46,502 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2010/0.3041/0.6662.
[2023-08-07 22:09:46,503 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6392/0.9346
[2023-08-07 22:09:46,503 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9422/0.9937
[2023-08-07 22:09:46,503 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1224/0.2943
[2023-08-07 22:09:46,503 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1395/0.3320
[2023-08-07 22:09:46,503 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.5428/0.5880
[2023-08-07 22:09:46,503 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.3433/0.6725
[2023-08-07 22:09:46,503 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.4002/0.4854
[2023-08-07 22:09:46,503 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1907/0.2558
[2023-08-07 22:09:46,503 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.0793/0.1581
[2023-08-07 22:09:46,503 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0428/0.0430
[2023-08-07 22:09:46,503 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 22:09:46,503 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0057/0.0058
[2023-08-07 22:09:46,503 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.2457/0.5217
[2023-08-07 22:09:46,503 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0660/0.0761
[2023-08-07 22:09:46,503 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0109/0.0128
[2023-08-07 22:09:46,503 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-07 22:09:46,503 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0015/0.0015
[2023-08-07 22:09:46,503 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.1895/0.5895
[2023-08-07 22:09:46,503 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 22:09:46,503 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0589/0.1179
[2023-08-07 22:09:46,503 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 22:09:46,503 INFO misc.py line 152 22900] Currently Best mIoU: 0.2147
[2023-08-07 22:09:46,503 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 22:09:51,572 INFO misc.py line 115 22900] Train: [23/100][1/156] Data 0.961 (0.961) Batch 4.284 (4.284) Remain 14:28:38 loss: 0.4676 Lr: 0.04615
[2023-08-07 22:09:54,394 INFO misc.py line 115 22900] Train: [23/100][2/156] Data 0.001 (0.001) Batch 2.822 (2.822) Remain 09:32:16 loss: 0.3376 Lr: 0.04614
[2023-08-07 22:09:57,184 INFO misc.py line 115 22900] Train: [23/100][3/156] Data 0.001 (0.001) Batch 2.790 (2.790) Remain 09:25:38 loss: 0.4478 Lr: 0.04614
[2023-08-07 22:10:00,292 INFO misc.py line 115 22900] Train: [23/100][4/156] Data 0.001 (0.001) Batch 3.108 (3.108) Remain 10:30:02 loss: 0.3314 Lr: 0.04614
[2023-08-07 22:10:03,146 INFO misc.py line 115 22900] Train: [23/100][5/156] Data 0.001 (0.001) Batch 2.855 (2.981) Remain 10:04:19 loss: 0.4020 Lr: 0.04614
[2023-08-07 22:10:06,876 INFO misc.py line 115 22900] Train: [23/100][6/156] Data 0.001 (0.001) Batch 3.730 (3.231) Remain 10:54:50 loss: 0.5658 Lr: 0.04613
[2023-08-07 22:10:11,157 INFO misc.py line 115 22900] Train: [23/100][7/156] Data 0.001 (0.001) Batch 4.281 (3.493) Remain 11:48:02 loss: 0.7314 Lr: 0.04613
[2023-08-07 22:10:14,580 INFO misc.py line 115 22900] Train: [23/100][8/156] Data 0.001 (0.001) Batch 3.423 (3.479) Remain 11:45:07 loss: 0.3575 Lr: 0.04613
[2023-08-07 22:10:18,185 INFO misc.py line 115 22900] Train: [23/100][9/156] Data 0.001 (0.001) Batch 3.604 (3.500) Remain 11:49:17 loss: 0.4209 Lr: 0.04612
[2023-08-07 22:10:21,874 INFO misc.py line 115 22900] Train: [23/100][10/156] Data 0.001 (0.001) Batch 3.689 (3.527) Remain 11:54:43 loss: 0.4523 Lr: 0.04612
[2023-08-07 22:10:25,929 INFO misc.py line 115 22900] Train: [23/100][11/156] Data 0.001 (0.001) Batch 4.055 (3.593) Remain 12:08:01 loss: 0.6791 Lr: 0.04612
[2023-08-07 22:10:29,139 INFO misc.py line 115 22900] Train: [23/100][12/156] Data 0.001 (0.001) Batch 3.210 (3.551) Remain 11:59:20 loss: 0.4702 Lr: 0.04612
[2023-08-07 22:10:33,503 INFO misc.py line 115 22900] Train: [23/100][13/156] Data 0.002 (0.001) Batch 4.364 (3.632) Remain 12:15:45 loss: 0.9691 Lr: 0.04611
[2023-08-07 22:10:36,916 INFO misc.py line 115 22900] Train: [23/100][14/156] Data 0.001 (0.001) Batch 3.413 (3.612) Remain 12:11:39 loss: 0.6342 Lr: 0.04611
[2023-08-07 22:10:40,011 INFO misc.py line 115 22900] Train: [23/100][15/156] Data 0.001 (0.001) Batch 3.096 (3.569) Remain 12:02:53 loss: 0.5797 Lr: 0.04611
[2023-08-07 22:10:42,780 INFO misc.py line 115 22900] Train: [23/100][16/156] Data 0.001 (0.001) Batch 2.768 (3.507) Remain 11:50:21 loss: 0.4099 Lr: 0.04610
[2023-08-07 22:10:46,189 INFO misc.py line 115 22900] Train: [23/100][17/156] Data 0.001 (0.001) Batch 3.409 (3.500) Remain 11:48:52 loss: 0.5130 Lr: 0.04610
[2023-08-07 22:10:50,207 INFO misc.py line 115 22900] Train: [23/100][18/156] Data 0.002 (0.001) Batch 4.019 (3.535) Remain 11:55:48 loss: 0.6261 Lr: 0.04610
[2023-08-07 22:10:53,540 INFO misc.py line 115 22900] Train: [23/100][19/156] Data 0.001 (0.001) Batch 3.332 (3.522) Remain 11:53:11 loss: 0.6659 Lr: 0.04610
[2023-08-07 22:10:57,604 INFO misc.py line 115 22900] Train: [23/100][20/156] Data 0.001 (0.001) Batch 4.065 (3.554) Remain 11:59:35 loss: 0.3316 Lr: 0.04609
[2023-08-07 22:11:00,116 INFO misc.py line 115 22900] Train: [23/100][21/156] Data 0.002 (0.001) Batch 2.512 (3.496) Remain 11:47:48 loss: 0.3124 Lr: 0.04609
[2023-08-07 22:11:03,483 INFO misc.py line 115 22900] Train: [23/100][22/156] Data 0.001 (0.001) Batch 3.366 (3.489) Remain 11:46:22 loss: 0.5842 Lr: 0.04609
[2023-08-07 22:11:07,303 INFO misc.py line 115 22900] Train: [23/100][23/156] Data 0.001 (0.001) Batch 3.820 (3.506) Remain 11:49:39 loss: 0.4451 Lr: 0.04608
[2023-08-07 22:11:10,953 INFO misc.py line 115 22900] Train: [23/100][24/156] Data 0.001 (0.001) Batch 3.650 (3.513) Remain 11:50:59 loss: 0.5757 Lr: 0.04608
[2023-08-07 22:11:13,645 INFO misc.py line 115 22900] Train: [23/100][25/156] Data 0.001 (0.001) Batch 2.692 (3.475) Remain 11:43:22 loss: 0.1939 Lr: 0.04608
[2023-08-07 22:11:16,743 INFO misc.py line 115 22900] Train: [23/100][26/156] Data 0.001 (0.001) Batch 3.098 (3.459) Remain 11:40:00 loss: 0.6242 Lr: 0.04608
[2023-08-07 22:11:19,360 INFO misc.py line 115 22900] Train: [23/100][27/156] Data 0.001 (0.001) Batch 2.617 (3.424) Remain 11:32:50 loss: 0.9200 Lr: 0.04607
[2023-08-07 22:11:22,599 INFO misc.py line 115 22900] Train: [23/100][28/156] Data 0.001 (0.001) Batch 3.239 (3.417) Remain 11:31:17 loss: 0.6035 Lr: 0.04607
[2023-08-07 22:11:26,886 INFO misc.py line 115 22900] Train: [23/100][29/156] Data 0.001 (0.001) Batch 4.287 (3.450) Remain 11:38:00 loss: 1.1337 Lr: 0.04607
[2023-08-07 22:11:30,152 INFO misc.py line 115 22900] Train: [23/100][30/156] Data 0.001 (0.001) Batch 3.266 (3.443) Remain 11:36:34 loss: 0.4089 Lr: 0.04606
[2023-08-07 22:11:33,761 INFO misc.py line 115 22900] Train: [23/100][31/156] Data 0.001 (0.001) Batch 3.609 (3.449) Remain 11:37:42 loss: 0.6744 Lr: 0.04606
[2023-08-07 22:11:36,096 INFO misc.py line 115 22900] Train: [23/100][32/156] Data 0.001 (0.001) Batch 2.335 (3.411) Remain 11:29:52 loss: 0.4068 Lr: 0.04606
[2023-08-07 22:11:38,600 INFO misc.py line 115 22900] Train: [23/100][33/156] Data 0.001 (0.001) Batch 2.504 (3.381) Remain 11:23:42 loss: 0.2926 Lr: 0.04606
[2023-08-07 22:11:42,448 INFO misc.py line 115 22900] Train: [23/100][34/156] Data 0.001 (0.001) Batch 3.847 (3.396) Remain 11:26:42 loss: 0.5941 Lr: 0.04605
[2023-08-07 22:11:45,173 INFO misc.py line 115 22900] Train: [23/100][35/156] Data 0.001 (0.001) Batch 2.725 (3.375) Remain 11:22:24 loss: 0.3267 Lr: 0.04605
[2023-08-07 22:11:49,152 INFO misc.py line 115 22900] Train: [23/100][36/156] Data 0.001 (0.001) Batch 3.979 (3.393) Remain 11:26:03 loss: 0.5139 Lr: 0.04605
[2023-08-07 22:11:52,907 INFO misc.py line 115 22900] Train: [23/100][37/156] Data 0.001 (0.001) Batch 3.755 (3.404) Remain 11:28:09 loss: 0.5000 Lr: 0.04604
[2023-08-07 22:11:56,966 INFO misc.py line 115 22900] Train: [23/100][38/156] Data 0.001 (0.001) Batch 4.059 (3.422) Remain 11:31:52 loss: 0.7631 Lr: 0.04604
[2023-08-07 22:11:59,006 INFO misc.py line 115 22900] Train: [23/100][39/156] Data 0.001 (0.001) Batch 2.040 (3.384) Remain 11:24:03 loss: 0.1671 Lr: 0.04604
[2023-08-07 22:12:01,158 INFO misc.py line 115 22900] Train: [23/100][40/156] Data 0.001 (0.001) Batch 2.153 (3.351) Remain 11:17:16 loss: 0.3811 Lr: 0.04604
[2023-08-07 22:12:04,349 INFO misc.py line 115 22900] Train: [23/100][41/156] Data 0.001 (0.001) Batch 3.191 (3.346) Remain 11:16:22 loss: 0.4974 Lr: 0.04603
[2023-08-07 22:12:08,215 INFO misc.py line 115 22900] Train: [23/100][42/156] Data 0.002 (0.001) Batch 3.866 (3.360) Remain 11:19:00 loss: 0.4454 Lr: 0.04603
[2023-08-07 22:12:11,989 INFO misc.py line 115 22900] Train: [23/100][43/156] Data 0.001 (0.001) Batch 3.773 (3.370) Remain 11:21:02 loss: 0.5177 Lr: 0.04603
[2023-08-07 22:12:15,529 INFO misc.py line 115 22900] Train: [23/100][44/156] Data 0.002 (0.001) Batch 3.540 (3.374) Remain 11:21:49 loss: 0.3966 Lr: 0.04602
[2023-08-07 22:12:18,996 INFO misc.py line 115 22900] Train: [23/100][45/156] Data 0.001 (0.001) Batch 3.467 (3.376) Remain 11:22:13 loss: 0.5518 Lr: 0.04602
[2023-08-07 22:12:22,962 INFO misc.py line 115 22900] Train: [23/100][46/156] Data 0.001 (0.001) Batch 3.966 (3.390) Remain 11:24:55 loss: 0.5042 Lr: 0.04602
[2023-08-07 22:12:26,437 INFO misc.py line 115 22900] Train: [23/100][47/156] Data 0.001 (0.001) Batch 3.475 (3.392) Remain 11:25:15 loss: 0.5893 Lr: 0.04602
[2023-08-07 22:12:29,130 INFO misc.py line 115 22900] Train: [23/100][48/156] Data 0.001 (0.001) Batch 2.693 (3.377) Remain 11:22:04 loss: 0.2827 Lr: 0.04601
[2023-08-07 22:12:32,400 INFO misc.py line 115 22900] Train: [23/100][49/156] Data 0.001 (0.001) Batch 3.270 (3.374) Remain 11:21:32 loss: 0.7570 Lr: 0.04601
[2023-08-07 22:12:35,352 INFO misc.py line 115 22900] Train: [23/100][50/156] Data 0.001 (0.001) Batch 2.952 (3.365) Remain 11:19:40 loss: 0.3784 Lr: 0.04601
[2023-08-07 22:12:38,607 INFO misc.py line 115 22900] Train: [23/100][51/156] Data 0.001 (0.001) Batch 3.255 (3.363) Remain 11:19:09 loss: 0.5144 Lr: 0.04600
[2023-08-07 22:12:42,662 INFO misc.py line 115 22900] Train: [23/100][52/156] Data 0.001 (0.001) Batch 4.055 (3.377) Remain 11:21:56 loss: 0.4410 Lr: 0.04600
[2023-08-07 22:12:45,196 INFO misc.py line 115 22900] Train: [23/100][53/156] Data 0.001 (0.001) Batch 2.534 (3.360) Remain 11:18:29 loss: 0.2314 Lr: 0.04600
[2023-08-07 22:12:48,129 INFO misc.py line 115 22900] Train: [23/100][54/156] Data 0.001 (0.001) Batch 2.933 (3.352) Remain 11:16:44 loss: 0.4394 Lr: 0.04600
[2023-08-07 22:12:52,476 INFO misc.py line 115 22900] Train: [23/100][55/156] Data 0.001 (0.001) Batch 4.348 (3.371) Remain 11:20:33 loss: 0.7515 Lr: 0.04599
[2023-08-07 22:12:55,319 INFO misc.py line 115 22900] Train: [23/100][56/156] Data 0.001 (0.001) Batch 2.842 (3.361) Remain 11:18:28 loss: 0.4626 Lr: 0.04599
[2023-08-07 22:12:58,924 INFO misc.py line 115 22900] Train: [23/100][57/156] Data 0.001 (0.001) Batch 3.605 (3.366) Remain 11:19:20 loss: 0.5438 Lr: 0.04599
[2023-08-07 22:13:01,807 INFO misc.py line 115 22900] Train: [23/100][58/156] Data 0.001 (0.001) Batch 2.883 (3.357) Remain 11:17:30 loss: 0.4977 Lr: 0.04598
[2023-08-07 22:13:05,625 INFO misc.py line 115 22900] Train: [23/100][59/156] Data 0.001 (0.001) Batch 3.819 (3.365) Remain 11:19:07 loss: 0.5236 Lr: 0.04598
[2023-08-07 22:13:09,134 INFO misc.py line 115 22900] Train: [23/100][60/156] Data 0.001 (0.001) Batch 3.508 (3.368) Remain 11:19:34 loss: 0.6470 Lr: 0.04598
[2023-08-07 22:13:12,528 INFO misc.py line 115 22900] Train: [23/100][61/156] Data 0.001 (0.001) Batch 3.395 (3.368) Remain 11:19:36 loss: 0.4997 Lr: 0.04598
[2023-08-07 22:13:16,561 INFO misc.py line 115 22900] Train: [23/100][62/156] Data 0.001 (0.001) Batch 4.032 (3.379) Remain 11:21:49 loss: 0.5954 Lr: 0.04597
[2023-08-07 22:13:21,131 INFO misc.py line 115 22900] Train: [23/100][63/156] Data 0.003 (0.001) Batch 4.570 (3.399) Remain 11:25:46 loss: 0.8997 Lr: 0.04597
[2023-08-07 22:13:24,807 INFO misc.py line 115 22900] Train: [23/100][64/156] Data 0.002 (0.001) Batch 3.677 (3.404) Remain 11:26:37 loss: 0.9298 Lr: 0.04597
[2023-08-07 22:13:29,264 INFO misc.py line 115 22900] Train: [23/100][65/156] Data 0.001 (0.001) Batch 4.457 (3.421) Remain 11:30:00 loss: 1.0054 Lr: 0.04596
[2023-08-07 22:13:32,441 INFO misc.py line 115 22900] Train: [23/100][66/156] Data 0.001 (0.001) Batch 3.176 (3.417) Remain 11:29:09 loss: 0.5709 Lr: 0.04596
[2023-08-07 22:13:36,610 INFO misc.py line 115 22900] Train: [23/100][67/156] Data 0.001 (0.001) Batch 4.168 (3.429) Remain 11:31:28 loss: 0.7096 Lr: 0.04596
[2023-08-07 22:13:39,198 INFO misc.py line 115 22900] Train: [23/100][68/156] Data 0.003 (0.001) Batch 2.589 (3.416) Remain 11:28:48 loss: 0.4154 Lr: 0.04596
[2023-08-07 22:13:43,412 INFO misc.py line 115 22900] Train: [23/100][69/156] Data 0.002 (0.001) Batch 4.214 (3.428) Remain 11:31:11 loss: 0.7466 Lr: 0.04595
[2023-08-07 22:13:46,721 INFO misc.py line 115 22900] Train: [23/100][70/156] Data 0.001 (0.001) Batch 3.309 (3.426) Remain 11:30:46 loss: 0.4283 Lr: 0.04595
[2023-08-07 22:13:49,608 INFO misc.py line 115 22900] Train: [23/100][71/156] Data 0.001 (0.001) Batch 2.888 (3.418) Remain 11:29:07 loss: 0.2702 Lr: 0.04595
[2023-08-07 22:13:53,311 INFO misc.py line 115 22900] Train: [23/100][72/156] Data 0.001 (0.001) Batch 3.702 (3.422) Remain 11:29:53 loss: 0.4735 Lr: 0.04594
[2023-08-07 22:13:57,150 INFO misc.py line 115 22900] Train: [23/100][73/156] Data 0.002 (0.001) Batch 3.839 (3.428) Remain 11:31:02 loss: 0.6723 Lr: 0.04594
[2023-08-07 22:14:01,120 INFO misc.py line 115 22900] Train: [23/100][74/156] Data 0.002 (0.001) Batch 3.971 (3.436) Remain 11:32:31 loss: 1.0251 Lr: 0.04594
[2023-08-07 22:14:03,000 INFO misc.py line 115 22900] Train: [23/100][75/156] Data 0.002 (0.001) Batch 1.880 (3.414) Remain 11:28:06 loss: 0.2710 Lr: 0.04593
[2023-08-07 22:14:06,304 INFO misc.py line 115 22900] Train: [23/100][76/156] Data 0.002 (0.001) Batch 3.304 (3.413) Remain 11:27:45 loss: 0.7722 Lr: 0.04593
[2023-08-07 22:14:10,097 INFO misc.py line 115 22900] Train: [23/100][77/156] Data 0.002 (0.001) Batch 3.793 (3.418) Remain 11:28:43 loss: 0.8138 Lr: 0.04593
[2023-08-07 22:14:13,264 INFO misc.py line 115 22900] Train: [23/100][78/156] Data 0.002 (0.001) Batch 3.165 (3.414) Remain 11:27:59 loss: 0.3417 Lr: 0.04593
[2023-08-07 22:14:16,709 INFO misc.py line 115 22900] Train: [23/100][79/156] Data 0.004 (0.001) Batch 3.447 (3.415) Remain 11:28:01 loss: 0.5918 Lr: 0.04592
[2023-08-07 22:14:20,985 INFO misc.py line 115 22900] Train: [23/100][80/156] Data 0.002 (0.001) Batch 4.276 (3.426) Remain 11:30:13 loss: 0.5103 Lr: 0.04592
[2023-08-07 22:14:24,724 INFO misc.py line 115 22900] Train: [23/100][81/156] Data 0.002 (0.001) Batch 3.739 (3.430) Remain 11:30:58 loss: 0.4383 Lr: 0.04592
[2023-08-07 22:14:28,350 INFO misc.py line 115 22900] Train: [23/100][82/156] Data 0.001 (0.001) Batch 3.626 (3.432) Remain 11:31:24 loss: 0.4044 Lr: 0.04591
[2023-08-07 22:14:31,593 INFO misc.py line 115 22900] Train: [23/100][83/156] Data 0.002 (0.001) Batch 3.244 (3.430) Remain 11:30:52 loss: 0.4220 Lr: 0.04591
[2023-08-07 22:14:35,289 INFO misc.py line 115 22900] Train: [23/100][84/156] Data 0.001 (0.001) Batch 3.695 (3.433) Remain 11:31:29 loss: 0.4161 Lr: 0.04591
[2023-08-07 22:14:39,346 INFO misc.py line 115 22900] Train: [23/100][85/156] Data 0.002 (0.001) Batch 4.057 (3.441) Remain 11:32:57 loss: 1.0282 Lr: 0.04591
[2023-08-07 22:14:42,838 INFO misc.py line 115 22900] Train: [23/100][86/156] Data 0.001 (0.001) Batch 3.492 (3.442) Remain 11:33:01 loss: 0.5892 Lr: 0.04590
[2023-08-07 22:14:46,855 INFO misc.py line 115 22900] Train: [23/100][87/156] Data 0.001 (0.001) Batch 4.017 (3.448) Remain 11:34:20 loss: 0.7605 Lr: 0.04590
[2023-08-07 22:14:49,948 INFO misc.py line 115 22900] Train: [23/100][88/156] Data 0.001 (0.001) Batch 3.094 (3.444) Remain 11:33:26 loss: 0.5142 Lr: 0.04590
[2023-08-07 22:14:53,096 INFO misc.py line 115 22900] Train: [23/100][89/156] Data 0.001 (0.001) Batch 3.148 (3.441) Remain 11:32:41 loss: 0.5058 Lr: 0.04589
[2023-08-07 22:14:57,800 INFO misc.py line 115 22900] Train: [23/100][90/156] Data 0.001 (0.001) Batch 4.704 (3.455) Remain 11:35:33 loss: 0.8701 Lr: 0.04589
[2023-08-07 22:15:00,607 INFO misc.py line 115 22900] Train: [23/100][91/156] Data 0.001 (0.001) Batch 2.807 (3.448) Remain 11:34:01 loss: 0.5258 Lr: 0.04589
[2023-08-07 22:15:04,689 INFO misc.py line 115 22900] Train: [23/100][92/156] Data 0.001 (0.001) Batch 4.082 (3.455) Remain 11:35:23 loss: 0.8128 Lr: 0.04589
[2023-08-07 22:15:08,226 INFO misc.py line 115 22900] Train: [23/100][93/156] Data 0.001 (0.001) Batch 3.538 (3.456) Remain 11:35:31 loss: 0.5781 Lr: 0.04588
[2023-08-07 22:15:12,222 INFO misc.py line 115 22900] Train: [23/100][94/156] Data 0.001 (0.001) Batch 3.996 (3.462) Remain 11:36:39 loss: 0.7400 Lr: 0.04588
[2023-08-07 22:15:15,608 INFO misc.py line 115 22900] Train: [23/100][95/156] Data 0.001 (0.001) Batch 3.387 (3.461) Remain 11:36:26 loss: 0.6625 Lr: 0.04588
[2023-08-07 22:15:19,836 INFO misc.py line 115 22900] Train: [23/100][96/156] Data 0.001 (0.001) Batch 4.227 (3.469) Remain 11:38:02 loss: 0.6539 Lr: 0.04587
[2023-08-07 22:15:23,210 INFO misc.py line 115 22900] Train: [23/100][97/156] Data 0.001 (0.001) Batch 3.375 (3.468) Remain 11:37:46 loss: 0.5600 Lr: 0.04587
[2023-08-07 22:15:26,583 INFO misc.py line 115 22900] Train: [23/100][98/156] Data 0.001 (0.001) Batch 3.373 (3.467) Remain 11:37:30 loss: 0.6037 Lr: 0.04587
[2023-08-07 22:15:30,572 INFO misc.py line 115 22900] Train: [23/100][99/156] Data 0.001 (0.001) Batch 3.989 (3.473) Remain 11:38:33 loss: 0.4852 Lr: 0.04587
[2023-08-07 22:15:33,927 INFO misc.py line 115 22900] Train: [23/100][100/156] Data 0.001 (0.001) Batch 3.356 (3.472) Remain 11:38:14 loss: 0.7758 Lr: 0.04586
[2023-08-07 22:15:37,668 INFO misc.py line 115 22900] Train: [23/100][101/156] Data 0.001 (0.001) Batch 3.741 (3.474) Remain 11:38:44 loss: 0.4174 Lr: 0.04586
[2023-08-07 22:15:40,931 INFO misc.py line 115 22900] Train: [23/100][102/156] Data 0.001 (0.001) Batch 3.263 (3.472) Remain 11:38:15 loss: 0.5788 Lr: 0.04586
[2023-08-07 22:15:44,605 INFO misc.py line 115 22900] Train: [23/100][103/156] Data 0.001 (0.001) Batch 3.674 (3.474) Remain 11:38:36 loss: 0.6927 Lr: 0.04585
[2023-08-07 22:15:47,205 INFO misc.py line 115 22900] Train: [23/100][104/156] Data 0.001 (0.001) Batch 2.600 (3.466) Remain 11:36:48 loss: 0.3509 Lr: 0.04585
[2023-08-07 22:15:49,624 INFO misc.py line 115 22900] Train: [23/100][105/156] Data 0.001 (0.001) Batch 2.419 (3.455) Remain 11:34:41 loss: 0.4303 Lr: 0.04585
[2023-08-07 22:15:53,577 INFO misc.py line 115 22900] Train: [23/100][106/156] Data 0.001 (0.001) Batch 3.953 (3.460) Remain 11:35:35 loss: 0.7807 Lr: 0.04584
[2023-08-07 22:15:57,155 INFO misc.py line 115 22900] Train: [23/100][107/156] Data 0.001 (0.001) Batch 3.579 (3.461) Remain 11:35:46 loss: 0.5902 Lr: 0.04584
[2023-08-07 22:15:59,475 INFO misc.py line 115 22900] Train: [23/100][108/156] Data 0.001 (0.001) Batch 2.319 (3.450) Remain 11:33:31 loss: 0.2070 Lr: 0.04584
[2023-08-07 22:16:03,010 INFO misc.py line 115 22900] Train: [23/100][109/156] Data 0.001 (0.001) Batch 3.535 (3.451) Remain 11:33:37 loss: 0.5392 Lr: 0.04584
[2023-08-07 22:16:07,025 INFO misc.py line 115 22900] Train: [23/100][110/156] Data 0.001 (0.001) Batch 4.015 (3.456) Remain 11:34:37 loss: 0.7629 Lr: 0.04583
[2023-08-07 22:16:10,595 INFO misc.py line 115 22900] Train: [23/100][111/156] Data 0.001 (0.001) Batch 3.570 (3.458) Remain 11:34:47 loss: 0.5259 Lr: 0.04583
[2023-08-07 22:16:14,969 INFO misc.py line 115 22900] Train: [23/100][112/156] Data 0.001 (0.001) Batch 4.374 (3.466) Remain 11:36:25 loss: 0.8760 Lr: 0.04583
[2023-08-07 22:16:19,078 INFO misc.py line 115 22900] Train: [23/100][113/156] Data 0.001 (0.001) Batch 4.108 (3.472) Remain 11:37:32 loss: 0.4271 Lr: 0.04582
[2023-08-07 22:16:22,508 INFO misc.py line 115 22900] Train: [23/100][114/156] Data 0.001 (0.001) Batch 3.430 (3.471) Remain 11:37:24 loss: 0.5345 Lr: 0.04582
[2023-08-07 22:16:26,791 INFO misc.py line 115 22900] Train: [23/100][115/156] Data 0.001 (0.001) Batch 4.283 (3.479) Remain 11:38:47 loss: 0.5750 Lr: 0.04582
[2023-08-07 22:16:29,412 INFO misc.py line 115 22900] Train: [23/100][116/156] Data 0.001 (0.001) Batch 2.621 (3.471) Remain 11:37:12 loss: 0.5473 Lr: 0.04582
[2023-08-07 22:16:33,543 INFO misc.py line 115 22900] Train: [23/100][117/156] Data 0.001 (0.001) Batch 4.131 (3.477) Remain 11:38:19 loss: 0.6401 Lr: 0.04581
[2023-08-07 22:16:36,885 INFO misc.py line 115 22900] Train: [23/100][118/156] Data 0.001 (0.001) Batch 3.342 (3.476) Remain 11:38:01 loss: 0.5281 Lr: 0.04581
[2023-08-07 22:16:41,189 INFO misc.py line 115 22900] Train: [23/100][119/156] Data 0.001 (0.001) Batch 4.304 (3.483) Remain 11:39:24 loss: 0.6624 Lr: 0.04581
[2023-08-07 22:16:44,371 INFO misc.py line 115 22900] Train: [23/100][120/156] Data 0.001 (0.001) Batch 3.181 (3.480) Remain 11:38:49 loss: 0.4888 Lr: 0.04580
[2023-08-07 22:16:48,044 INFO misc.py line 115 22900] Train: [23/100][121/156] Data 0.001 (0.001) Batch 3.673 (3.482) Remain 11:39:05 loss: 0.6650 Lr: 0.04580
[2023-08-07 22:16:52,085 INFO misc.py line 115 22900] Train: [23/100][122/156] Data 0.001 (0.001) Batch 4.041 (3.487) Remain 11:39:59 loss: 0.5227 Lr: 0.04580
[2023-08-07 22:16:55,695 INFO misc.py line 115 22900] Train: [23/100][123/156] Data 0.001 (0.001) Batch 3.610 (3.488) Remain 11:40:07 loss: 0.5158 Lr: 0.04579
[2023-08-07 22:16:59,256 INFO misc.py line 115 22900] Train: [23/100][124/156] Data 0.001 (0.001) Batch 3.561 (3.488) Remain 11:40:11 loss: 0.4509 Lr: 0.04579
[2023-08-07 22:17:02,132 INFO misc.py line 115 22900] Train: [23/100][125/156] Data 0.001 (0.001) Batch 2.876 (3.483) Remain 11:39:07 loss: 0.7954 Lr: 0.04579
[2023-08-07 22:17:05,944 INFO misc.py line 115 22900] Train: [23/100][126/156] Data 0.001 (0.001) Batch 3.813 (3.486) Remain 11:39:36 loss: 0.7235 Lr: 0.04579
[2023-08-07 22:17:08,867 INFO misc.py line 115 22900] Train: [23/100][127/156] Data 0.001 (0.001) Batch 2.923 (3.481) Remain 11:38:38 loss: 0.4350 Lr: 0.04578
[2023-08-07 22:17:12,955 INFO misc.py line 115 22900] Train: [23/100][128/156] Data 0.001 (0.001) Batch 4.087 (3.486) Remain 11:39:33 loss: 0.4284 Lr: 0.04578
[2023-08-07 22:17:16,543 INFO misc.py line 115 22900] Train: [23/100][129/156] Data 0.001 (0.001) Batch 3.588 (3.487) Remain 11:39:39 loss: 0.3731 Lr: 0.04578
[2023-08-07 22:17:19,832 INFO misc.py line 115 22900] Train: [23/100][130/156] Data 0.001 (0.001) Batch 3.290 (3.485) Remain 11:39:17 loss: 0.4595 Lr: 0.04577
[2023-08-07 22:17:23,505 INFO misc.py line 115 22900] Train: [23/100][131/156] Data 0.001 (0.001) Batch 3.673 (3.487) Remain 11:39:31 loss: 0.4872 Lr: 0.04577
[2023-08-07 22:17:26,125 INFO misc.py line 115 22900] Train: [23/100][132/156] Data 0.001 (0.001) Batch 2.620 (3.480) Remain 11:38:07 loss: 0.4097 Lr: 0.04577
[2023-08-07 22:17:29,316 INFO misc.py line 115 22900] Train: [23/100][133/156] Data 0.001 (0.001) Batch 3.190 (3.478) Remain 11:37:36 loss: 0.3804 Lr: 0.04577
[2023-08-07 22:17:31,783 INFO misc.py line 115 22900] Train: [23/100][134/156] Data 0.001 (0.001) Batch 2.467 (3.470) Remain 11:36:00 loss: 0.3785 Lr: 0.04576
[2023-08-07 22:17:35,196 INFO misc.py line 115 22900] Train: [23/100][135/156] Data 0.001 (0.001) Batch 3.413 (3.470) Remain 11:35:51 loss: 0.5750 Lr: 0.04576
[2023-08-07 22:17:38,468 INFO misc.py line 115 22900] Train: [23/100][136/156] Data 0.001 (0.001) Batch 3.271 (3.468) Remain 11:35:30 loss: 0.3749 Lr: 0.04576
[2023-08-07 22:17:41,547 INFO misc.py line 115 22900] Train: [23/100][137/156] Data 0.002 (0.001) Batch 3.079 (3.465) Remain 11:34:52 loss: 0.4132 Lr: 0.04575
[2023-08-07 22:17:44,780 INFO misc.py line 115 22900] Train: [23/100][138/156] Data 0.001 (0.001) Batch 3.234 (3.464) Remain 11:34:28 loss: 0.4241 Lr: 0.04575
[2023-08-07 22:17:48,850 INFO misc.py line 115 22900] Train: [23/100][139/156] Data 0.001 (0.001) Batch 4.070 (3.468) Remain 11:35:18 loss: 0.4463 Lr: 0.04575
[2023-08-07 22:17:51,875 INFO misc.py line 115 22900] Train: [23/100][140/156] Data 0.001 (0.001) Batch 3.025 (3.465) Remain 11:34:35 loss: 0.4185 Lr: 0.04574
[2023-08-07 22:17:54,431 INFO misc.py line 115 22900] Train: [23/100][141/156] Data 0.001 (0.001) Batch 2.555 (3.458) Remain 11:33:13 loss: 0.6293 Lr: 0.04574
[2023-08-07 22:17:58,414 INFO misc.py line 115 22900] Train: [23/100][142/156] Data 0.002 (0.001) Batch 3.983 (3.462) Remain 11:33:55 loss: 0.5832 Lr: 0.04574
[2023-08-07 22:18:02,981 INFO misc.py line 115 22900] Train: [23/100][143/156] Data 0.001 (0.001) Batch 4.567 (3.470) Remain 11:35:26 loss: 0.9910 Lr: 0.04574
[2023-08-07 22:18:05,081 INFO misc.py line 115 22900] Train: [23/100][144/156] Data 0.001 (0.001) Batch 2.099 (3.460) Remain 11:33:26 loss: 0.2943 Lr: 0.04573
[2023-08-07 22:18:08,046 INFO misc.py line 115 22900] Train: [23/100][145/156] Data 0.001 (0.001) Batch 2.966 (3.457) Remain 11:32:40 loss: 0.4049 Lr: 0.04573
[2023-08-07 22:18:11,284 INFO misc.py line 115 22900] Train: [23/100][146/156] Data 0.001 (0.001) Batch 3.237 (3.455) Remain 11:32:18 loss: 0.4145 Lr: 0.04573
[2023-08-07 22:18:15,207 INFO misc.py line 115 22900] Train: [23/100][147/156] Data 0.001 (0.001) Batch 3.924 (3.458) Remain 11:32:54 loss: 0.4832 Lr: 0.04572
[2023-08-07 22:18:17,965 INFO misc.py line 115 22900] Train: [23/100][148/156] Data 0.001 (0.001) Batch 2.758 (3.454) Remain 11:31:53 loss: 0.4180 Lr: 0.04572
[2023-08-07 22:18:22,074 INFO misc.py line 115 22900] Train: [23/100][149/156] Data 0.001 (0.001) Batch 4.108 (3.458) Remain 11:32:43 loss: 0.4748 Lr: 0.04572
[2023-08-07 22:18:25,123 INFO misc.py line 115 22900] Train: [23/100][150/156] Data 0.001 (0.001) Batch 3.049 (3.455) Remain 11:32:06 loss: 0.3629 Lr: 0.04572
[2023-08-07 22:18:29,216 INFO misc.py line 115 22900] Train: [23/100][151/156] Data 0.001 (0.001) Batch 4.094 (3.460) Remain 11:32:54 loss: 0.5855 Lr: 0.04571
[2023-08-07 22:18:33,245 INFO misc.py line 115 22900] Train: [23/100][152/156] Data 0.001 (0.001) Batch 4.029 (3.463) Remain 11:33:37 loss: 0.2540 Lr: 0.04571
[2023-08-07 22:18:37,194 INFO misc.py line 115 22900] Train: [23/100][153/156] Data 0.001 (0.001) Batch 3.948 (3.467) Remain 11:34:12 loss: 0.5420 Lr: 0.04571
[2023-08-07 22:18:39,814 INFO misc.py line 115 22900] Train: [23/100][154/156] Data 0.001 (0.001) Batch 2.621 (3.461) Remain 11:33:01 loss: 0.8296 Lr: 0.04570
[2023-08-07 22:18:42,981 INFO misc.py line 115 22900] Train: [23/100][155/156] Data 0.001 (0.001) Batch 3.167 (3.459) Remain 11:32:35 loss: 0.4856 Lr: 0.04570
[2023-08-07 22:18:46,645 INFO misc.py line 115 22900] Train: [23/100][156/156] Data 0.001 (0.001) Batch 3.664 (3.461) Remain 11:32:47 loss: 0.4995 Lr: 0.04570
[2023-08-07 22:18:46,645 INFO misc.py line 129 22900] Train result: loss: 0.5433 
[2023-08-07 22:18:46,645 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 22:18:48,738 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.8453 
[2023-08-07 22:18:49,605 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.7514 
[2023-08-07 22:18:51,270 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.9225 
[2023-08-07 22:18:52,792 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.0865 
[2023-08-07 22:18:54,637 INFO evaluator.py line 122 22900] Test: [5/24] Loss 2.0075 
[2023-08-07 22:18:56,300 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.6483 
[2023-08-07 22:18:58,436 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.2204 
[2023-08-07 22:19:00,238 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.8156 
[2023-08-07 22:19:01,521 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.3029 
[2023-08-07 22:19:03,652 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3411 
[2023-08-07 22:19:04,178 INFO evaluator.py line 122 22900] Test: [11/24] Loss 0.8964 
[2023-08-07 22:19:05,709 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.8501 
[2023-08-07 22:19:08,420 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.0443 
[2023-08-07 22:19:10,102 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.8182 
[2023-08-07 22:19:12,122 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.3459 
[2023-08-07 22:19:14,833 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.0166 
[2023-08-07 22:19:17,540 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.2371 
[2023-08-07 22:19:19,387 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.7847 
[2023-08-07 22:19:20,138 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.3646 
[2023-08-07 22:19:21,023 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.9028 
[2023-08-07 22:19:23,286 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.5204 
[2023-08-07 22:19:25,249 INFO evaluator.py line 122 22900] Test: [22/24] Loss 2.0388 
[2023-08-07 22:19:27,094 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.9270 
[2023-08-07 22:19:29,031 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.8077 
[2023-08-07 22:19:29,080 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2034/0.2951/0.6769.
[2023-08-07 22:19:29,080 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6438/0.9479
[2023-08-07 22:19:29,080 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9472/0.9851
[2023-08-07 22:19:29,080 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1334/0.2745
[2023-08-07 22:19:29,080 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1258/0.3704
[2023-08-07 22:19:29,080 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6103/0.8105
[2023-08-07 22:19:29,080 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.2966/0.4125
[2023-08-07 22:19:29,080 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.4583/0.5931
[2023-08-07 22:19:29,080 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0876/0.0920
[2023-08-07 22:19:29,080 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1373/0.3310
[2023-08-07 22:19:29,080 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0557/0.0559
[2023-08-07 22:19:29,080 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 22:19:29,080 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0140/0.0148
[2023-08-07 22:19:29,080 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1469/0.2974
[2023-08-07 22:19:29,080 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0456/0.0532
[2023-08-07 22:19:29,080 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0066/0.0068
[2023-08-07 22:19:29,080 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-07 22:19:29,080 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0093/0.0093
[2023-08-07 22:19:29,080 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.2902/0.5345
[2023-08-07 22:19:29,080 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 22:19:29,080 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0590/0.1130
[2023-08-07 22:19:29,081 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 22:19:29,081 INFO misc.py line 152 22900] Currently Best mIoU: 0.2147
[2023-08-07 22:19:29,081 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 22:19:35,380 INFO misc.py line 115 22900] Train: [24/100][1/156] Data 1.364 (1.364) Batch 5.531 (5.531) Remain 18:27:07 loss: 0.6464 Lr: 0.04569
[2023-08-07 22:19:37,856 INFO misc.py line 115 22900] Train: [24/100][2/156] Data 0.001 (0.001) Batch 2.476 (2.476) Remain 08:15:33 loss: 0.3665 Lr: 0.04569
[2023-08-07 22:19:41,939 INFO misc.py line 115 22900] Train: [24/100][3/156] Data 0.001 (0.001) Batch 4.083 (4.083) Remain 13:37:10 loss: 0.4170 Lr: 0.04569
[2023-08-07 22:19:44,900 INFO misc.py line 115 22900] Train: [24/100][4/156] Data 0.001 (0.001) Batch 2.961 (2.961) Remain 09:52:38 loss: 0.2834 Lr: 0.04569
[2023-08-07 22:19:48,574 INFO misc.py line 115 22900] Train: [24/100][5/156] Data 0.001 (0.001) Batch 3.674 (3.318) Remain 11:03:55 loss: 0.4865 Lr: 0.04568
[2023-08-07 22:19:52,028 INFO misc.py line 115 22900] Train: [24/100][6/156] Data 0.001 (0.001) Batch 3.454 (3.363) Remain 11:12:58 loss: 0.3280 Lr: 0.04568
[2023-08-07 22:19:56,087 INFO misc.py line 115 22900] Train: [24/100][7/156] Data 0.001 (0.001) Batch 4.059 (3.537) Remain 11:47:43 loss: 1.0720 Lr: 0.04568
[2023-08-07 22:19:59,338 INFO misc.py line 115 22900] Train: [24/100][8/156] Data 0.001 (0.001) Batch 3.251 (3.480) Remain 11:36:13 loss: 0.5356 Lr: 0.04567
[2023-08-07 22:20:03,444 INFO misc.py line 115 22900] Train: [24/100][9/156] Data 0.001 (0.001) Batch 4.105 (3.584) Remain 11:57:01 loss: 0.5212 Lr: 0.04567
[2023-08-07 22:20:07,169 INFO misc.py line 115 22900] Train: [24/100][10/156] Data 0.001 (0.001) Batch 3.726 (3.604) Remain 12:01:00 loss: 0.5110 Lr: 0.04567
[2023-08-07 22:20:10,671 INFO misc.py line 115 22900] Train: [24/100][11/156] Data 0.001 (0.001) Batch 3.502 (3.592) Remain 11:58:22 loss: 0.3601 Lr: 0.04566
[2023-08-07 22:20:14,807 INFO misc.py line 115 22900] Train: [24/100][12/156] Data 0.001 (0.001) Batch 4.135 (3.652) Remain 12:10:24 loss: 0.6001 Lr: 0.04566
[2023-08-07 22:20:17,767 INFO misc.py line 115 22900] Train: [24/100][13/156] Data 0.001 (0.001) Batch 2.960 (3.583) Remain 11:56:30 loss: 0.5937 Lr: 0.04566
[2023-08-07 22:20:21,050 INFO misc.py line 115 22900] Train: [24/100][14/156] Data 0.001 (0.001) Batch 3.283 (3.556) Remain 11:50:59 loss: 0.4415 Lr: 0.04566
[2023-08-07 22:20:25,068 INFO misc.py line 115 22900] Train: [24/100][15/156] Data 0.001 (0.001) Batch 4.019 (3.594) Remain 11:58:39 loss: 0.5495 Lr: 0.04565
[2023-08-07 22:20:28,000 INFO misc.py line 115 22900] Train: [24/100][16/156] Data 0.001 (0.001) Batch 2.931 (3.543) Remain 11:48:23 loss: 0.4479 Lr: 0.04565
[2023-08-07 22:20:30,965 INFO misc.py line 115 22900] Train: [24/100][17/156] Data 0.001 (0.001) Batch 2.965 (3.502) Remain 11:40:05 loss: 0.4686 Lr: 0.04565
[2023-08-07 22:20:34,406 INFO misc.py line 115 22900] Train: [24/100][18/156] Data 0.001 (0.001) Batch 3.441 (3.498) Remain 11:39:13 loss: 0.5589 Lr: 0.04564
[2023-08-07 22:20:36,894 INFO misc.py line 115 22900] Train: [24/100][19/156] Data 0.001 (0.001) Batch 2.488 (3.435) Remain 11:26:32 loss: 0.4012 Lr: 0.04564
[2023-08-07 22:20:39,814 INFO misc.py line 115 22900] Train: [24/100][20/156] Data 0.001 (0.001) Batch 2.920 (3.404) Remain 11:20:26 loss: 0.3641 Lr: 0.04564
[2023-08-07 22:20:42,413 INFO misc.py line 115 22900] Train: [24/100][21/156] Data 0.001 (0.001) Batch 2.599 (3.360) Remain 11:11:26 loss: 0.3335 Lr: 0.04563
[2023-08-07 22:20:45,295 INFO misc.py line 115 22900] Train: [24/100][22/156] Data 0.001 (0.001) Batch 2.881 (3.335) Remain 11:06:20 loss: 0.6495 Lr: 0.04563
[2023-08-07 22:20:49,311 INFO misc.py line 115 22900] Train: [24/100][23/156] Data 0.001 (0.001) Batch 4.016 (3.369) Remain 11:13:06 loss: 0.5466 Lr: 0.04563
[2023-08-07 22:20:52,735 INFO misc.py line 115 22900] Train: [24/100][24/156] Data 0.001 (0.001) Batch 3.425 (3.371) Remain 11:13:34 loss: 0.4058 Lr: 0.04563
[2023-08-07 22:20:56,973 INFO misc.py line 115 22900] Train: [24/100][25/156] Data 0.001 (0.001) Batch 4.238 (3.411) Remain 11:21:23 loss: 0.7015 Lr: 0.04562
[2023-08-07 22:21:00,717 INFO misc.py line 115 22900] Train: [24/100][26/156] Data 0.001 (0.001) Batch 3.744 (3.425) Remain 11:24:13 loss: 0.7434 Lr: 0.04562
[2023-08-07 22:21:03,679 INFO misc.py line 115 22900] Train: [24/100][27/156] Data 0.001 (0.001) Batch 2.962 (3.406) Remain 11:20:19 loss: 0.3026 Lr: 0.04562
[2023-08-07 22:21:08,046 INFO misc.py line 115 22900] Train: [24/100][28/156] Data 0.001 (0.001) Batch 4.367 (3.444) Remain 11:27:56 loss: 0.7103 Lr: 0.04561
[2023-08-07 22:21:11,809 INFO misc.py line 115 22900] Train: [24/100][29/156] Data 0.001 (0.001) Batch 3.764 (3.457) Remain 11:30:20 loss: 0.4487 Lr: 0.04561
[2023-08-07 22:21:15,478 INFO misc.py line 115 22900] Train: [24/100][30/156] Data 0.001 (0.001) Batch 3.669 (3.464) Remain 11:31:50 loss: 0.5251 Lr: 0.04561
[2023-08-07 22:21:19,200 INFO misc.py line 115 22900] Train: [24/100][31/156] Data 0.001 (0.001) Batch 3.722 (3.474) Remain 11:33:37 loss: 0.3704 Lr: 0.04560
[2023-08-07 22:21:23,422 INFO misc.py line 115 22900] Train: [24/100][32/156] Data 0.001 (0.001) Batch 4.222 (3.499) Remain 11:38:43 loss: 0.5342 Lr: 0.04560
[2023-08-07 22:21:27,811 INFO misc.py line 115 22900] Train: [24/100][33/156] Data 0.001 (0.001) Batch 4.389 (3.529) Remain 11:44:34 loss: 0.7942 Lr: 0.04560
[2023-08-07 22:21:31,198 INFO misc.py line 115 22900] Train: [24/100][34/156] Data 0.001 (0.001) Batch 3.387 (3.525) Remain 11:43:36 loss: 0.5255 Lr: 0.04560
[2023-08-07 22:21:34,558 INFO misc.py line 115 22900] Train: [24/100][35/156] Data 0.001 (0.001) Batch 3.360 (3.519) Remain 11:42:31 loss: 0.5455 Lr: 0.04559
[2023-08-07 22:21:38,667 INFO misc.py line 115 22900] Train: [24/100][36/156] Data 0.001 (0.001) Batch 4.109 (3.537) Remain 11:46:01 loss: 0.7309 Lr: 0.04559
[2023-08-07 22:21:40,228 INFO misc.py line 115 22900] Train: [24/100][37/156] Data 0.001 (0.001) Batch 1.561 (3.479) Remain 11:34:22 loss: 0.1466 Lr: 0.04559
[2023-08-07 22:21:43,947 INFO misc.py line 115 22900] Train: [24/100][38/156] Data 0.001 (0.001) Batch 3.718 (3.486) Remain 11:35:40 loss: 0.5606 Lr: 0.04558
[2023-08-07 22:21:47,884 INFO misc.py line 115 22900] Train: [24/100][39/156] Data 0.001 (0.001) Batch 3.938 (3.498) Remain 11:38:07 loss: 0.5662 Lr: 0.04558
[2023-08-07 22:21:50,367 INFO misc.py line 115 22900] Train: [24/100][40/156] Data 0.001 (0.001) Batch 2.482 (3.471) Remain 11:32:35 loss: 0.2954 Lr: 0.04558
[2023-08-07 22:21:54,204 INFO misc.py line 115 22900] Train: [24/100][41/156] Data 0.001 (0.001) Batch 3.838 (3.481) Remain 11:34:27 loss: 0.3486 Lr: 0.04557
[2023-08-07 22:21:56,560 INFO misc.py line 115 22900] Train: [24/100][42/156] Data 0.001 (0.001) Batch 2.356 (3.452) Remain 11:28:38 loss: 0.4401 Lr: 0.04557
[2023-08-07 22:22:00,634 INFO misc.py line 115 22900] Train: [24/100][43/156] Data 0.001 (0.001) Batch 4.074 (3.467) Remain 11:31:41 loss: 0.3993 Lr: 0.04557
[2023-08-07 22:22:03,879 INFO misc.py line 115 22900] Train: [24/100][44/156] Data 0.001 (0.001) Batch 3.245 (3.462) Remain 11:30:32 loss: 0.4525 Lr: 0.04557
[2023-08-07 22:22:05,571 INFO misc.py line 115 22900] Train: [24/100][45/156] Data 0.001 (0.001) Batch 1.692 (3.420) Remain 11:22:04 loss: 0.4706 Lr: 0.04556
[2023-08-07 22:22:08,934 INFO misc.py line 115 22900] Train: [24/100][46/156] Data 0.001 (0.001) Batch 3.363 (3.418) Remain 11:21:45 loss: 0.4080 Lr: 0.04556
[2023-08-07 22:22:12,001 INFO misc.py line 115 22900] Train: [24/100][47/156] Data 0.001 (0.001) Batch 3.068 (3.411) Remain 11:20:06 loss: 0.7022 Lr: 0.04556
[2023-08-07 22:22:16,125 INFO misc.py line 115 22900] Train: [24/100][48/156] Data 0.001 (0.001) Batch 4.124 (3.426) Remain 11:23:13 loss: 0.7054 Lr: 0.04555
[2023-08-07 22:22:19,079 INFO misc.py line 115 22900] Train: [24/100][49/156] Data 0.001 (0.001) Batch 2.953 (3.416) Remain 11:21:06 loss: 0.4003 Lr: 0.04555
[2023-08-07 22:22:22,166 INFO misc.py line 115 22900] Train: [24/100][50/156] Data 0.001 (0.001) Batch 3.087 (3.409) Remain 11:19:39 loss: 0.3302 Lr: 0.04555
[2023-08-07 22:22:26,387 INFO misc.py line 115 22900] Train: [24/100][51/156] Data 0.001 (0.001) Batch 4.220 (3.426) Remain 11:22:58 loss: 0.4505 Lr: 0.04554
[2023-08-07 22:22:30,490 INFO misc.py line 115 22900] Train: [24/100][52/156] Data 0.001 (0.001) Batch 4.103 (3.440) Remain 11:25:40 loss: 0.6652 Lr: 0.04554
[2023-08-07 22:22:34,064 INFO misc.py line 115 22900] Train: [24/100][53/156] Data 0.001 (0.001) Batch 3.574 (3.443) Remain 11:26:08 loss: 0.5319 Lr: 0.04554
[2023-08-07 22:22:37,220 INFO misc.py line 115 22900] Train: [24/100][54/156] Data 0.001 (0.001) Batch 3.156 (3.437) Remain 11:24:58 loss: 0.6213 Lr: 0.04554
[2023-08-07 22:22:41,169 INFO misc.py line 115 22900] Train: [24/100][55/156] Data 0.001 (0.001) Batch 3.948 (3.447) Remain 11:26:52 loss: 0.6606 Lr: 0.04553
[2023-08-07 22:22:44,637 INFO misc.py line 115 22900] Train: [24/100][56/156] Data 0.001 (0.001) Batch 3.468 (3.447) Remain 11:26:53 loss: 0.4440 Lr: 0.04553
[2023-08-07 22:22:47,983 INFO misc.py line 115 22900] Train: [24/100][57/156] Data 0.001 (0.001) Batch 3.347 (3.445) Remain 11:26:28 loss: 0.5620 Lr: 0.04553
[2023-08-07 22:22:50,524 INFO misc.py line 115 22900] Train: [24/100][58/156] Data 0.001 (0.001) Batch 2.541 (3.429) Remain 11:23:08 loss: 0.3848 Lr: 0.04552
[2023-08-07 22:22:54,590 INFO misc.py line 115 22900] Train: [24/100][59/156] Data 0.001 (0.001) Batch 4.067 (3.440) Remain 11:25:20 loss: 0.6247 Lr: 0.04552
[2023-08-07 22:22:57,896 INFO misc.py line 115 22900] Train: [24/100][60/156] Data 0.001 (0.001) Batch 3.306 (3.438) Remain 11:24:49 loss: 0.3574 Lr: 0.04552
[2023-08-07 22:23:00,820 INFO misc.py line 115 22900] Train: [24/100][61/156] Data 0.001 (0.001) Batch 2.924 (3.429) Remain 11:22:59 loss: 0.5005 Lr: 0.04551
[2023-08-07 22:23:04,974 INFO misc.py line 115 22900] Train: [24/100][62/156] Data 0.001 (0.001) Batch 4.155 (3.441) Remain 11:25:23 loss: 0.6847 Lr: 0.04551
[2023-08-07 22:23:08,239 INFO misc.py line 115 22900] Train: [24/100][63/156] Data 0.001 (0.001) Batch 3.264 (3.438) Remain 11:24:44 loss: 0.5352 Lr: 0.04551
[2023-08-07 22:23:11,215 INFO misc.py line 115 22900] Train: [24/100][64/156] Data 0.001 (0.001) Batch 2.976 (3.431) Remain 11:23:10 loss: 0.4689 Lr: 0.04551
[2023-08-07 22:23:14,636 INFO misc.py line 115 22900] Train: [24/100][65/156] Data 0.001 (0.001) Batch 3.421 (3.431) Remain 11:23:05 loss: 0.4755 Lr: 0.04550
[2023-08-07 22:23:18,396 INFO misc.py line 115 22900] Train: [24/100][66/156] Data 0.001 (0.001) Batch 3.759 (3.436) Remain 11:24:04 loss: 0.5813 Lr: 0.04550
[2023-08-07 22:23:21,547 INFO misc.py line 115 22900] Train: [24/100][67/156] Data 0.001 (0.001) Batch 3.151 (3.431) Remain 11:23:07 loss: 0.5345 Lr: 0.04550
[2023-08-07 22:23:24,171 INFO misc.py line 115 22900] Train: [24/100][68/156] Data 0.001 (0.001) Batch 2.624 (3.419) Remain 11:20:36 loss: 0.3653 Lr: 0.04549
[2023-08-07 22:23:28,280 INFO misc.py line 115 22900] Train: [24/100][69/156] Data 0.001 (0.001) Batch 4.109 (3.429) Remain 11:22:37 loss: 0.5680 Lr: 0.04549
[2023-08-07 22:23:31,837 INFO misc.py line 115 22900] Train: [24/100][70/156] Data 0.001 (0.001) Batch 3.557 (3.431) Remain 11:22:56 loss: 0.6117 Lr: 0.04549
[2023-08-07 22:23:34,658 INFO misc.py line 115 22900] Train: [24/100][71/156] Data 0.001 (0.001) Batch 2.821 (3.422) Remain 11:21:06 loss: 0.6293 Lr: 0.04548
[2023-08-07 22:23:38,575 INFO misc.py line 115 22900] Train: [24/100][72/156] Data 0.001 (0.001) Batch 3.917 (3.430) Remain 11:22:28 loss: 0.3980 Lr: 0.04548
[2023-08-07 22:23:41,238 INFO misc.py line 115 22900] Train: [24/100][73/156] Data 0.001 (0.001) Batch 2.663 (3.419) Remain 11:20:14 loss: 0.9783 Lr: 0.04548
[2023-08-07 22:23:45,647 INFO misc.py line 115 22900] Train: [24/100][74/156] Data 0.001 (0.001) Batch 4.409 (3.433) Remain 11:22:57 loss: 0.6639 Lr: 0.04547
[2023-08-07 22:23:48,926 INFO misc.py line 115 22900] Train: [24/100][75/156] Data 0.001 (0.001) Batch 3.279 (3.430) Remain 11:22:28 loss: 0.6193 Lr: 0.04547
[2023-08-07 22:23:51,878 INFO misc.py line 115 22900] Train: [24/100][76/156] Data 0.001 (0.001) Batch 2.953 (3.424) Remain 11:21:06 loss: 0.5148 Lr: 0.04547
[2023-08-07 22:23:55,285 INFO misc.py line 115 22900] Train: [24/100][77/156] Data 0.001 (0.001) Batch 3.407 (3.424) Remain 11:21:00 loss: 0.5971 Lr: 0.04547
[2023-08-07 22:23:58,590 INFO misc.py line 115 22900] Train: [24/100][78/156] Data 0.001 (0.001) Batch 3.304 (3.422) Remain 11:20:38 loss: 0.6778 Lr: 0.04546
[2023-08-07 22:24:02,538 INFO misc.py line 115 22900] Train: [24/100][79/156] Data 0.001 (0.001) Batch 3.948 (3.429) Remain 11:21:57 loss: 0.5829 Lr: 0.04546
[2023-08-07 22:24:05,564 INFO misc.py line 115 22900] Train: [24/100][80/156] Data 0.001 (0.001) Batch 3.026 (3.424) Remain 11:20:51 loss: 0.4391 Lr: 0.04546
[2023-08-07 22:24:09,637 INFO misc.py line 115 22900] Train: [24/100][81/156] Data 0.001 (0.001) Batch 4.073 (3.432) Remain 11:22:27 loss: 0.6310 Lr: 0.04545
[2023-08-07 22:24:12,034 INFO misc.py line 115 22900] Train: [24/100][82/156] Data 0.001 (0.001) Batch 2.397 (3.419) Remain 11:19:47 loss: 0.4163 Lr: 0.04545
[2023-08-07 22:24:14,490 INFO misc.py line 115 22900] Train: [24/100][83/156] Data 0.001 (0.001) Batch 2.455 (3.407) Remain 11:17:20 loss: 0.2864 Lr: 0.04545
[2023-08-07 22:24:17,824 INFO misc.py line 115 22900] Train: [24/100][84/156] Data 0.001 (0.001) Batch 3.334 (3.406) Remain 11:17:06 loss: 0.5530 Lr: 0.04544
[2023-08-07 22:24:21,462 INFO misc.py line 115 22900] Train: [24/100][85/156] Data 0.001 (0.001) Batch 3.638 (3.409) Remain 11:17:36 loss: 0.5347 Lr: 0.04544
[2023-08-07 22:24:24,807 INFO misc.py line 115 22900] Train: [24/100][86/156] Data 0.001 (0.001) Batch 3.345 (3.408) Remain 11:17:24 loss: 0.5931 Lr: 0.04544
[2023-08-07 22:24:28,243 INFO misc.py line 115 22900] Train: [24/100][87/156] Data 0.001 (0.001) Batch 3.437 (3.408) Remain 11:17:25 loss: 0.4317 Lr: 0.04544
[2023-08-07 22:24:32,225 INFO misc.py line 115 22900] Train: [24/100][88/156] Data 0.001 (0.001) Batch 3.982 (3.415) Remain 11:18:42 loss: 0.4107 Lr: 0.04543
[2023-08-07 22:24:35,868 INFO misc.py line 115 22900] Train: [24/100][89/156] Data 0.001 (0.001) Batch 3.643 (3.418) Remain 11:19:10 loss: 0.4483 Lr: 0.04543
[2023-08-07 22:24:39,934 INFO misc.py line 115 22900] Train: [24/100][90/156] Data 0.001 (0.001) Batch 4.066 (3.425) Remain 11:20:35 loss: 0.5583 Lr: 0.04543
[2023-08-07 22:24:43,842 INFO misc.py line 115 22900] Train: [24/100][91/156] Data 0.001 (0.001) Batch 3.909 (3.431) Remain 11:21:37 loss: 0.3987 Lr: 0.04542
[2023-08-07 22:24:47,498 INFO misc.py line 115 22900] Train: [24/100][92/156] Data 0.001 (0.001) Batch 3.656 (3.433) Remain 11:22:04 loss: 0.3236 Lr: 0.04542
[2023-08-07 22:24:51,194 INFO misc.py line 115 22900] Train: [24/100][93/156] Data 0.001 (0.001) Batch 3.696 (3.436) Remain 11:22:35 loss: 0.5310 Lr: 0.04542
[2023-08-07 22:24:55,276 INFO misc.py line 115 22900] Train: [24/100][94/156] Data 0.001 (0.001) Batch 4.081 (3.443) Remain 11:23:56 loss: 0.6839 Lr: 0.04541
[2023-08-07 22:24:58,166 INFO misc.py line 115 22900] Train: [24/100][95/156] Data 0.001 (0.001) Batch 2.891 (3.437) Remain 11:22:41 loss: 0.5610 Lr: 0.04541
[2023-08-07 22:25:02,281 INFO misc.py line 115 22900] Train: [24/100][96/156] Data 0.001 (0.001) Batch 4.114 (3.445) Remain 11:24:05 loss: 0.5231 Lr: 0.04541
[2023-08-07 22:25:06,735 INFO misc.py line 115 22900] Train: [24/100][97/156] Data 0.001 (0.001) Batch 4.454 (3.455) Remain 11:26:09 loss: 0.7525 Lr: 0.04540
[2023-08-07 22:25:10,146 INFO misc.py line 115 22900] Train: [24/100][98/156] Data 0.001 (0.001) Batch 3.411 (3.455) Remain 11:26:00 loss: 0.3815 Lr: 0.04540
[2023-08-07 22:25:13,748 INFO misc.py line 115 22900] Train: [24/100][99/156] Data 0.001 (0.001) Batch 3.602 (3.456) Remain 11:26:15 loss: 0.4272 Lr: 0.04540
[2023-08-07 22:25:18,241 INFO misc.py line 115 22900] Train: [24/100][100/156] Data 0.001 (0.001) Batch 4.493 (3.467) Remain 11:28:19 loss: 0.6236 Lr: 0.04540
[2023-08-07 22:25:21,894 INFO misc.py line 115 22900] Train: [24/100][101/156] Data 0.001 (0.001) Batch 3.653 (3.469) Remain 11:28:38 loss: 0.7525 Lr: 0.04539
[2023-08-07 22:25:25,992 INFO misc.py line 115 22900] Train: [24/100][102/156] Data 0.001 (0.001) Batch 4.098 (3.475) Remain 11:29:50 loss: 0.4216 Lr: 0.04539
[2023-08-07 22:25:28,923 INFO misc.py line 115 22900] Train: [24/100][103/156] Data 0.001 (0.001) Batch 2.930 (3.470) Remain 11:28:42 loss: 0.3295 Lr: 0.04539
[2023-08-07 22:25:32,663 INFO misc.py line 115 22900] Train: [24/100][104/156] Data 0.001 (0.001) Batch 3.740 (3.473) Remain 11:29:10 loss: 0.6139 Lr: 0.04538
[2023-08-07 22:25:35,997 INFO misc.py line 115 22900] Train: [24/100][105/156] Data 0.001 (0.001) Batch 3.334 (3.471) Remain 11:28:51 loss: 0.3978 Lr: 0.04538
[2023-08-07 22:25:39,256 INFO misc.py line 115 22900] Train: [24/100][106/156] Data 0.001 (0.001) Batch 3.259 (3.469) Remain 11:28:23 loss: 0.5652 Lr: 0.04538
[2023-08-07 22:25:42,378 INFO misc.py line 115 22900] Train: [24/100][107/156] Data 0.001 (0.001) Batch 3.122 (3.466) Remain 11:27:39 loss: 0.5967 Lr: 0.04537
[2023-08-07 22:25:46,001 INFO misc.py line 115 22900] Train: [24/100][108/156] Data 0.001 (0.001) Batch 3.623 (3.467) Remain 11:27:54 loss: 0.5041 Lr: 0.04537
[2023-08-07 22:25:49,575 INFO misc.py line 115 22900] Train: [24/100][109/156] Data 0.001 (0.001) Batch 3.574 (3.468) Remain 11:28:02 loss: 0.5431 Lr: 0.04537
[2023-08-07 22:25:53,238 INFO misc.py line 115 22900] Train: [24/100][110/156] Data 0.001 (0.001) Batch 3.663 (3.470) Remain 11:28:20 loss: 0.5527 Lr: 0.04536
[2023-08-07 22:25:57,726 INFO misc.py line 115 22900] Train: [24/100][111/156] Data 0.001 (0.001) Batch 4.488 (3.480) Remain 11:30:09 loss: 0.6275 Lr: 0.04536
[2023-08-07 22:26:01,993 INFO misc.py line 115 22900] Train: [24/100][112/156] Data 0.001 (0.001) Batch 4.267 (3.487) Remain 11:31:32 loss: 0.6905 Lr: 0.04536
[2023-08-07 22:26:05,127 INFO misc.py line 115 22900] Train: [24/100][113/156] Data 0.001 (0.001) Batch 3.134 (3.484) Remain 11:30:50 loss: 0.5234 Lr: 0.04536
[2023-08-07 22:26:08,120 INFO misc.py line 115 22900] Train: [24/100][114/156] Data 0.001 (0.001) Batch 2.993 (3.479) Remain 11:29:54 loss: 0.2781 Lr: 0.04535
[2023-08-07 22:26:11,908 INFO misc.py line 115 22900] Train: [24/100][115/156] Data 0.001 (0.001) Batch 3.789 (3.482) Remain 11:30:23 loss: 0.6242 Lr: 0.04535
[2023-08-07 22:26:15,596 INFO misc.py line 115 22900] Train: [24/100][116/156] Data 0.001 (0.001) Batch 3.688 (3.484) Remain 11:30:42 loss: 0.4329 Lr: 0.04535
[2023-08-07 22:26:19,656 INFO misc.py line 115 22900] Train: [24/100][117/156] Data 0.001 (0.001) Batch 4.060 (3.489) Remain 11:31:38 loss: 0.7010 Lr: 0.04534
[2023-08-07 22:26:22,330 INFO misc.py line 115 22900] Train: [24/100][118/156] Data 0.001 (0.001) Batch 2.674 (3.482) Remain 11:30:10 loss: 0.5524 Lr: 0.04534
[2023-08-07 22:26:26,376 INFO misc.py line 115 22900] Train: [24/100][119/156] Data 0.001 (0.001) Batch 4.046 (3.487) Remain 11:31:05 loss: 0.5898 Lr: 0.04534
[2023-08-07 22:26:28,554 INFO misc.py line 115 22900] Train: [24/100][120/156] Data 0.001 (0.001) Batch 2.178 (3.475) Remain 11:28:48 loss: 0.2506 Lr: 0.04533
[2023-08-07 22:26:32,605 INFO misc.py line 115 22900] Train: [24/100][121/156] Data 0.001 (0.001) Batch 4.051 (3.480) Remain 11:29:43 loss: 0.5812 Lr: 0.04533
[2023-08-07 22:26:36,342 INFO misc.py line 115 22900] Train: [24/100][122/156] Data 0.001 (0.001) Batch 3.738 (3.482) Remain 11:30:05 loss: 0.6948 Lr: 0.04533
[2023-08-07 22:26:38,697 INFO misc.py line 115 22900] Train: [24/100][123/156] Data 0.001 (0.001) Batch 2.354 (3.473) Remain 11:28:10 loss: 0.1312 Lr: 0.04532
[2023-08-07 22:26:41,336 INFO misc.py line 115 22900] Train: [24/100][124/156] Data 0.001 (0.001) Batch 2.639 (3.466) Remain 11:26:44 loss: 0.3919 Lr: 0.04532
[2023-08-07 22:26:43,309 INFO misc.py line 115 22900] Train: [24/100][125/156] Data 0.001 (0.001) Batch 1.973 (3.454) Remain 11:24:15 loss: 0.4559 Lr: 0.04532
[2023-08-07 22:26:46,975 INFO misc.py line 115 22900] Train: [24/100][126/156] Data 0.001 (0.001) Batch 3.666 (3.456) Remain 11:24:33 loss: 0.4849 Lr: 0.04532
[2023-08-07 22:26:49,965 INFO misc.py line 115 22900] Train: [24/100][127/156] Data 0.001 (0.001) Batch 2.990 (3.452) Remain 11:23:44 loss: 0.4789 Lr: 0.04531
[2023-08-07 22:26:53,749 INFO misc.py line 115 22900] Train: [24/100][128/156] Data 0.001 (0.001) Batch 3.784 (3.454) Remain 11:24:13 loss: 0.5336 Lr: 0.04531
[2023-08-07 22:26:56,489 INFO misc.py line 115 22900] Train: [24/100][129/156] Data 0.001 (0.001) Batch 2.741 (3.449) Remain 11:23:02 loss: 0.3688 Lr: 0.04531
[2023-08-07 22:26:59,945 INFO misc.py line 115 22900] Train: [24/100][130/156] Data 0.001 (0.001) Batch 3.456 (3.449) Remain 11:22:59 loss: 0.3982 Lr: 0.04530
[2023-08-07 22:27:03,046 INFO misc.py line 115 22900] Train: [24/100][131/156] Data 0.001 (0.001) Batch 3.101 (3.446) Remain 11:22:23 loss: 0.1580 Lr: 0.04530
[2023-08-07 22:27:06,011 INFO misc.py line 115 22900] Train: [24/100][132/156] Data 0.001 (0.001) Batch 2.964 (3.442) Remain 11:21:35 loss: 0.2995 Lr: 0.04530
[2023-08-07 22:27:10,146 INFO misc.py line 115 22900] Train: [24/100][133/156] Data 0.001 (0.001) Batch 4.136 (3.448) Remain 11:22:35 loss: 0.5436 Lr: 0.04529
[2023-08-07 22:27:14,055 INFO misc.py line 115 22900] Train: [24/100][134/156] Data 0.001 (0.001) Batch 3.908 (3.451) Remain 11:23:14 loss: 0.7998 Lr: 0.04529
[2023-08-07 22:27:18,160 INFO misc.py line 115 22900] Train: [24/100][135/156] Data 0.001 (0.001) Batch 4.106 (3.456) Remain 11:24:09 loss: 0.6139 Lr: 0.04529
[2023-08-07 22:27:22,385 INFO misc.py line 115 22900] Train: [24/100][136/156] Data 0.001 (0.001) Batch 4.225 (3.462) Remain 11:25:14 loss: 0.5088 Lr: 0.04528
[2023-08-07 22:27:26,137 INFO misc.py line 115 22900] Train: [24/100][137/156] Data 0.001 (0.001) Batch 3.752 (3.464) Remain 11:25:36 loss: 0.4662 Lr: 0.04528
[2023-08-07 22:27:29,112 INFO misc.py line 115 22900] Train: [24/100][138/156] Data 0.001 (0.001) Batch 2.974 (3.461) Remain 11:24:50 loss: 0.2685 Lr: 0.04528
[2023-08-07 22:27:32,716 INFO misc.py line 115 22900] Train: [24/100][139/156] Data 0.001 (0.001) Batch 3.605 (3.462) Remain 11:24:59 loss: 0.5236 Lr: 0.04528
[2023-08-07 22:27:36,686 INFO misc.py line 115 22900] Train: [24/100][140/156] Data 0.001 (0.001) Batch 3.970 (3.465) Remain 11:25:40 loss: 0.9504 Lr: 0.04527
[2023-08-07 22:27:39,165 INFO misc.py line 115 22900] Train: [24/100][141/156] Data 0.001 (0.001) Batch 2.479 (3.458) Remain 11:24:11 loss: 0.3058 Lr: 0.04527
[2023-08-07 22:27:42,593 INFO misc.py line 115 22900] Train: [24/100][142/156] Data 0.001 (0.001) Batch 3.429 (3.458) Remain 11:24:05 loss: 0.5298 Lr: 0.04527
[2023-08-07 22:27:46,374 INFO misc.py line 115 22900] Train: [24/100][143/156] Data 0.001 (0.001) Batch 3.781 (3.460) Remain 11:24:29 loss: 0.5410 Lr: 0.04526
[2023-08-07 22:27:50,129 INFO misc.py line 115 22900] Train: [24/100][144/156] Data 0.001 (0.001) Batch 3.754 (3.462) Remain 11:24:51 loss: 0.4279 Lr: 0.04526
[2023-08-07 22:27:54,195 INFO misc.py line 115 22900] Train: [24/100][145/156] Data 0.001 (0.001) Batch 4.066 (3.467) Remain 11:25:38 loss: 0.6951 Lr: 0.04526
[2023-08-07 22:27:57,171 INFO misc.py line 115 22900] Train: [24/100][146/156] Data 0.001 (0.001) Batch 2.977 (3.463) Remain 11:24:53 loss: 0.5047 Lr: 0.04525
[2023-08-07 22:28:00,880 INFO misc.py line 115 22900] Train: [24/100][147/156] Data 0.001 (0.001) Batch 3.709 (3.465) Remain 11:25:10 loss: 0.9604 Lr: 0.04525
[2023-08-07 22:28:04,478 INFO misc.py line 115 22900] Train: [24/100][148/156] Data 0.001 (0.001) Batch 3.598 (3.466) Remain 11:25:18 loss: 0.5511 Lr: 0.04525
[2023-08-07 22:28:07,936 INFO misc.py line 115 22900] Train: [24/100][149/156] Data 0.001 (0.001) Batch 3.458 (3.466) Remain 11:25:13 loss: 0.4570 Lr: 0.04524
[2023-08-07 22:28:10,643 INFO misc.py line 115 22900] Train: [24/100][150/156] Data 0.001 (0.001) Batch 2.707 (3.461) Remain 11:24:09 loss: 0.5356 Lr: 0.04524
[2023-08-07 22:28:15,189 INFO misc.py line 115 22900] Train: [24/100][151/156] Data 0.001 (0.001) Batch 4.546 (3.468) Remain 11:25:32 loss: 0.6789 Lr: 0.04524
[2023-08-07 22:28:18,923 INFO misc.py line 115 22900] Train: [24/100][152/156] Data 0.001 (0.001) Batch 3.734 (3.470) Remain 11:25:50 loss: 0.5197 Lr: 0.04523
[2023-08-07 22:28:22,921 INFO misc.py line 115 22900] Train: [24/100][153/156] Data 0.001 (0.001) Batch 3.998 (3.473) Remain 11:26:28 loss: 0.6177 Lr: 0.04523
[2023-08-07 22:28:26,150 INFO misc.py line 115 22900] Train: [24/100][154/156] Data 0.001 (0.001) Batch 3.229 (3.472) Remain 11:26:06 loss: 0.6262 Lr: 0.04523
[2023-08-07 22:28:29,290 INFO misc.py line 115 22900] Train: [24/100][155/156] Data 0.001 (0.001) Batch 3.140 (3.469) Remain 11:25:36 loss: 0.5850 Lr: 0.04523
[2023-08-07 22:28:32,915 INFO misc.py line 115 22900] Train: [24/100][156/156] Data 0.001 (0.001) Batch 3.625 (3.470) Remain 11:25:45 loss: 0.4907 Lr: 0.04522
[2023-08-07 22:28:32,916 INFO misc.py line 129 22900] Train result: loss: 0.5197 
[2023-08-07 22:28:32,916 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 22:28:35,030 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.5360 
[2023-08-07 22:28:35,899 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.7191 
[2023-08-07 22:28:37,562 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.7588 
[2023-08-07 22:28:39,085 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.2630 
[2023-08-07 22:28:40,928 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.6515 
[2023-08-07 22:28:42,593 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.6180 
[2023-08-07 22:28:44,729 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.3350 
[2023-08-07 22:28:46,533 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.6714 
[2023-08-07 22:28:47,816 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.3121 
[2023-08-07 22:28:49,946 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.2739 
[2023-08-07 22:28:50,472 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.2263 
[2023-08-07 22:28:52,007 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.6686 
[2023-08-07 22:28:54,716 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.0388 
[2023-08-07 22:28:56,394 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.7510 
[2023-08-07 22:28:58,419 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.4258 
[2023-08-07 22:29:01,129 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.1909 
[2023-08-07 22:29:03,835 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.2507 
[2023-08-07 22:29:05,683 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.5527 
[2023-08-07 22:29:06,434 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.1309 
[2023-08-07 22:29:07,319 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.8783 
[2023-08-07 22:29:09,582 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.2385 
[2023-08-07 22:29:11,548 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.7553 
[2023-08-07 22:29:13,396 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.9277 
[2023-08-07 22:29:15,331 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.2803 
[2023-08-07 22:29:15,391 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2044/0.2952/0.6819.
[2023-08-07 22:29:15,391 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6633/0.9622
[2023-08-07 22:29:15,391 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9479/0.9911
[2023-08-07 22:29:15,391 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1554/0.3887
[2023-08-07 22:29:15,391 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1501/0.2366
[2023-08-07 22:29:15,391 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6051/0.7320
[2023-08-07 22:29:15,391 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.1987/0.2331
[2023-08-07 22:29:15,391 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.4641/0.7495
[2023-08-07 22:29:15,391 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0714/0.0752
[2023-08-07 22:29:15,391 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.0950/0.2304
[2023-08-07 22:29:15,391 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0873/0.0898
[2023-08-07 22:29:15,391 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 22:29:15,391 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0050/0.0051
[2023-08-07 22:29:15,391 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.2022/0.4527
[2023-08-07 22:29:15,391 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.1471/0.1747
[2023-08-07 22:29:15,391 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0378/0.0416
[2023-08-07 22:29:15,391 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0007/0.0007
[2023-08-07 22:29:15,391 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0012/0.0012
[2023-08-07 22:29:15,391 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.2053/0.4439
[2023-08-07 22:29:15,392 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 22:29:15,392 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0510/0.0947
[2023-08-07 22:29:15,392 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 22:29:15,392 INFO misc.py line 152 22900] Currently Best mIoU: 0.2147
[2023-08-07 22:29:15,392 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 22:29:21,999 INFO misc.py line 115 22900] Train: [25/100][1/156] Data 1.610 (1.610) Batch 5.821 (5.821) Remain 19:10:08 loss: 0.4584 Lr: 0.04522
[2023-08-07 22:29:25,911 INFO misc.py line 115 22900] Train: [25/100][2/156] Data 0.001 (0.001) Batch 3.912 (3.912) Remain 12:52:51 loss: 0.5643 Lr: 0.04522
[2023-08-07 22:29:28,692 INFO misc.py line 115 22900] Train: [25/100][3/156] Data 0.001 (0.001) Batch 2.781 (2.781) Remain 09:09:23 loss: 0.3767 Lr: 0.04521
[2023-08-07 22:29:31,152 INFO misc.py line 115 22900] Train: [25/100][4/156] Data 0.001 (0.001) Batch 2.461 (2.461) Remain 08:06:05 loss: 0.4140 Lr: 0.04521
[2023-08-07 22:29:35,348 INFO misc.py line 115 22900] Train: [25/100][5/156] Data 0.001 (0.001) Batch 4.195 (3.328) Remain 10:57:20 loss: 0.7597 Lr: 0.04521
[2023-08-07 22:29:38,192 INFO misc.py line 115 22900] Train: [25/100][6/156] Data 0.001 (0.001) Batch 2.844 (3.167) Remain 10:25:26 loss: 0.3484 Lr: 0.04520
[2023-08-07 22:29:42,121 INFO misc.py line 115 22900] Train: [25/100][7/156] Data 0.001 (0.001) Batch 3.929 (3.357) Remain 11:03:02 loss: 0.7335 Lr: 0.04520
[2023-08-07 22:29:45,581 INFO misc.py line 115 22900] Train: [25/100][8/156] Data 0.001 (0.001) Batch 3.460 (3.378) Remain 11:07:01 loss: 0.7157 Lr: 0.04520
[2023-08-07 22:29:49,137 INFO misc.py line 115 22900] Train: [25/100][9/156] Data 0.001 (0.001) Batch 3.556 (3.408) Remain 11:12:50 loss: 0.3606 Lr: 0.04519
[2023-08-07 22:29:52,869 INFO misc.py line 115 22900] Train: [25/100][10/156] Data 0.001 (0.001) Batch 3.732 (3.454) Remain 11:21:55 loss: 0.4384 Lr: 0.04519
[2023-08-07 22:29:56,948 INFO misc.py line 115 22900] Train: [25/100][11/156] Data 0.001 (0.001) Batch 4.079 (3.532) Remain 11:37:16 loss: 0.5652 Lr: 0.04519
[2023-08-07 22:30:00,961 INFO misc.py line 115 22900] Train: [25/100][12/156] Data 0.001 (0.001) Batch 4.012 (3.585) Remain 11:47:45 loss: 0.3298 Lr: 0.04518
[2023-08-07 22:30:05,565 INFO misc.py line 115 22900] Train: [25/100][13/156] Data 0.001 (0.001) Batch 4.605 (3.687) Remain 12:07:49 loss: 0.7088 Lr: 0.04518
[2023-08-07 22:30:09,157 INFO misc.py line 115 22900] Train: [25/100][14/156] Data 0.001 (0.001) Batch 3.592 (3.679) Remain 12:06:02 loss: 0.5297 Lr: 0.04518
[2023-08-07 22:30:12,848 INFO misc.py line 115 22900] Train: [25/100][15/156] Data 0.001 (0.001) Batch 3.691 (3.680) Remain 12:06:11 loss: 0.4624 Lr: 0.04518
[2023-08-07 22:30:15,221 INFO misc.py line 115 22900] Train: [25/100][16/156] Data 0.001 (0.001) Batch 2.372 (3.579) Remain 11:46:17 loss: 0.2884 Lr: 0.04517
[2023-08-07 22:30:19,105 INFO misc.py line 115 22900] Train: [25/100][17/156] Data 0.001 (0.001) Batch 3.884 (3.601) Remain 11:50:31 loss: 0.5268 Lr: 0.04517
[2023-08-07 22:30:23,115 INFO misc.py line 115 22900] Train: [25/100][18/156] Data 0.001 (0.001) Batch 4.010 (3.628) Remain 11:55:50 loss: 0.5128 Lr: 0.04517
[2023-08-07 22:30:26,104 INFO misc.py line 115 22900] Train: [25/100][19/156] Data 0.001 (0.001) Batch 2.989 (3.588) Remain 11:47:54 loss: 0.4328 Lr: 0.04516
[2023-08-07 22:30:29,270 INFO misc.py line 115 22900] Train: [25/100][20/156] Data 0.001 (0.001) Batch 3.166 (3.563) Remain 11:42:57 loss: 0.4640 Lr: 0.04516
[2023-08-07 22:30:32,883 INFO misc.py line 115 22900] Train: [25/100][21/156] Data 0.001 (0.001) Batch 3.613 (3.566) Remain 11:43:25 loss: 0.4603 Lr: 0.04516
[2023-08-07 22:30:35,505 INFO misc.py line 115 22900] Train: [25/100][22/156] Data 0.001 (0.001) Batch 2.622 (3.516) Remain 11:33:34 loss: 0.4076 Lr: 0.04515
[2023-08-07 22:30:38,858 INFO misc.py line 115 22900] Train: [25/100][23/156] Data 0.001 (0.001) Batch 3.353 (3.508) Remain 11:31:53 loss: 0.4869 Lr: 0.04515
[2023-08-07 22:30:42,931 INFO misc.py line 115 22900] Train: [25/100][24/156] Data 0.001 (0.001) Batch 4.073 (3.535) Remain 11:37:08 loss: 0.3801 Lr: 0.04515
[2023-08-07 22:30:47,023 INFO misc.py line 115 22900] Train: [25/100][25/156] Data 0.001 (0.001) Batch 4.092 (3.561) Remain 11:42:04 loss: 0.4500 Lr: 0.04514
[2023-08-07 22:30:50,543 INFO misc.py line 115 22900] Train: [25/100][26/156] Data 0.001 (0.001) Batch 3.520 (3.559) Remain 11:41:39 loss: 0.4470 Lr: 0.04514
[2023-08-07 22:30:52,944 INFO misc.py line 115 22900] Train: [25/100][27/156] Data 0.001 (0.001) Batch 2.401 (3.510) Remain 11:32:05 loss: 0.3397 Lr: 0.04514
[2023-08-07 22:30:54,953 INFO misc.py line 115 22900] Train: [25/100][28/156] Data 0.001 (0.001) Batch 2.009 (3.450) Remain 11:20:11 loss: 0.3697 Lr: 0.04513
[2023-08-07 22:30:58,256 INFO misc.py line 115 22900] Train: [25/100][29/156] Data 0.001 (0.001) Batch 3.303 (3.445) Remain 11:19:01 loss: 0.3086 Lr: 0.04513
[2023-08-07 22:31:00,391 INFO misc.py line 115 22900] Train: [25/100][30/156] Data 0.001 (0.001) Batch 2.136 (3.396) Remain 11:09:24 loss: 0.1166 Lr: 0.04513
[2023-08-07 22:31:03,874 INFO misc.py line 115 22900] Train: [25/100][31/156] Data 0.001 (0.001) Batch 3.483 (3.399) Remain 11:09:57 loss: 0.3082 Lr: 0.04513
[2023-08-07 22:31:07,929 INFO misc.py line 115 22900] Train: [25/100][32/156] Data 0.001 (0.001) Batch 4.055 (3.422) Remain 11:14:21 loss: 0.7277 Lr: 0.04512
[2023-08-07 22:31:11,644 INFO misc.py line 115 22900] Train: [25/100][33/156] Data 0.001 (0.001) Batch 3.715 (3.432) Remain 11:16:13 loss: 0.6599 Lr: 0.04512
[2023-08-07 22:31:14,615 INFO misc.py line 115 22900] Train: [25/100][34/156] Data 0.001 (0.001) Batch 2.970 (3.417) Remain 11:13:14 loss: 0.4350 Lr: 0.04512
[2023-08-07 22:31:17,809 INFO misc.py line 115 22900] Train: [25/100][35/156] Data 0.001 (0.001) Batch 3.194 (3.410) Remain 11:11:48 loss: 0.5015 Lr: 0.04511
[2023-08-07 22:31:21,925 INFO misc.py line 115 22900] Train: [25/100][36/156] Data 0.001 (0.001) Batch 4.116 (3.431) Remain 11:15:58 loss: 0.6631 Lr: 0.04511
[2023-08-07 22:31:25,748 INFO misc.py line 115 22900] Train: [25/100][37/156] Data 0.001 (0.001) Batch 3.823 (3.443) Remain 11:18:10 loss: 0.5651 Lr: 0.04511
[2023-08-07 22:31:29,300 INFO misc.py line 115 22900] Train: [25/100][38/156] Data 0.001 (0.001) Batch 3.552 (3.446) Remain 11:18:44 loss: 0.4581 Lr: 0.04510
[2023-08-07 22:31:32,170 INFO misc.py line 115 22900] Train: [25/100][39/156] Data 0.001 (0.001) Batch 2.870 (3.430) Remain 11:15:31 loss: 0.1922 Lr: 0.04510
[2023-08-07 22:31:35,822 INFO misc.py line 115 22900] Train: [25/100][40/156] Data 0.001 (0.001) Batch 3.652 (3.436) Remain 11:16:39 loss: 0.5215 Lr: 0.04510
[2023-08-07 22:31:38,768 INFO misc.py line 115 22900] Train: [25/100][41/156] Data 0.001 (0.001) Batch 2.946 (3.423) Remain 11:14:03 loss: 0.3354 Lr: 0.04509
[2023-08-07 22:31:43,268 INFO misc.py line 115 22900] Train: [25/100][42/156] Data 0.001 (0.001) Batch 4.500 (3.451) Remain 11:19:26 loss: 0.8849 Lr: 0.04509
[2023-08-07 22:31:47,263 INFO misc.py line 115 22900] Train: [25/100][43/156] Data 0.001 (0.001) Batch 3.995 (3.464) Remain 11:22:03 loss: 0.6172 Lr: 0.04509
[2023-08-07 22:31:50,162 INFO misc.py line 115 22900] Train: [25/100][44/156] Data 0.001 (0.001) Batch 2.900 (3.451) Remain 11:19:17 loss: 0.5542 Lr: 0.04508
[2023-08-07 22:31:54,744 INFO misc.py line 115 22900] Train: [25/100][45/156] Data 0.001 (0.001) Batch 4.581 (3.477) Remain 11:24:31 loss: 0.7438 Lr: 0.04508
[2023-08-07 22:31:58,768 INFO misc.py line 115 22900] Train: [25/100][46/156] Data 0.001 (0.001) Batch 4.024 (3.490) Remain 11:26:58 loss: 0.6482 Lr: 0.04508
[2023-08-07 22:32:02,099 INFO misc.py line 115 22900] Train: [25/100][47/156] Data 0.001 (0.001) Batch 3.331 (3.487) Remain 11:26:12 loss: 0.5989 Lr: 0.04507
[2023-08-07 22:32:05,913 INFO misc.py line 115 22900] Train: [25/100][48/156] Data 0.001 (0.001) Batch 3.814 (3.494) Remain 11:27:34 loss: 0.8210 Lr: 0.04507
[2023-08-07 22:32:08,690 INFO misc.py line 115 22900] Train: [25/100][49/156] Data 0.001 (0.001) Batch 2.777 (3.478) Remain 11:24:27 loss: 0.3483 Lr: 0.04507
[2023-08-07 22:32:12,163 INFO misc.py line 115 22900] Train: [25/100][50/156] Data 0.001 (0.001) Batch 3.474 (3.478) Remain 11:24:22 loss: 0.6239 Lr: 0.04507
[2023-08-07 22:32:15,340 INFO misc.py line 115 22900] Train: [25/100][51/156] Data 0.001 (0.001) Batch 3.177 (3.472) Remain 11:23:05 loss: 0.2759 Lr: 0.04506
[2023-08-07 22:32:18,712 INFO misc.py line 115 22900] Train: [25/100][52/156] Data 0.001 (0.001) Batch 3.372 (3.470) Remain 11:22:37 loss: 0.5450 Lr: 0.04506
[2023-08-07 22:32:23,241 INFO misc.py line 115 22900] Train: [25/100][53/156] Data 0.001 (0.001) Batch 4.529 (3.491) Remain 11:26:44 loss: 0.7090 Lr: 0.04506
[2023-08-07 22:32:27,014 INFO misc.py line 115 22900] Train: [25/100][54/156] Data 0.001 (0.001) Batch 3.772 (3.497) Remain 11:27:45 loss: 0.6381 Lr: 0.04505
[2023-08-07 22:32:30,870 INFO misc.py line 115 22900] Train: [25/100][55/156] Data 0.001 (0.001) Batch 3.856 (3.503) Remain 11:29:03 loss: 0.7103 Lr: 0.04505
[2023-08-07 22:32:34,786 INFO misc.py line 115 22900] Train: [25/100][56/156] Data 0.001 (0.001) Batch 3.917 (3.511) Remain 11:30:32 loss: 0.4738 Lr: 0.04505
[2023-08-07 22:32:39,224 INFO misc.py line 115 22900] Train: [25/100][57/156] Data 0.001 (0.001) Batch 4.438 (3.528) Remain 11:33:51 loss: 0.6459 Lr: 0.04504
[2023-08-07 22:32:42,285 INFO misc.py line 115 22900] Train: [25/100][58/156] Data 0.001 (0.001) Batch 3.061 (3.520) Remain 11:32:07 loss: 0.5836 Lr: 0.04504
[2023-08-07 22:32:46,035 INFO misc.py line 115 22900] Train: [25/100][59/156] Data 0.001 (0.001) Batch 3.750 (3.524) Remain 11:32:52 loss: 0.5058 Lr: 0.04504
[2023-08-07 22:32:49,696 INFO misc.py line 115 22900] Train: [25/100][60/156] Data 0.001 (0.001) Batch 3.661 (3.526) Remain 11:33:17 loss: 0.6365 Lr: 0.04503
[2023-08-07 22:32:53,443 INFO misc.py line 115 22900] Train: [25/100][61/156] Data 0.001 (0.001) Batch 3.747 (3.530) Remain 11:33:58 loss: 0.5239 Lr: 0.04503
[2023-08-07 22:32:56,319 INFO misc.py line 115 22900] Train: [25/100][62/156] Data 0.001 (0.001) Batch 2.877 (3.519) Remain 11:31:44 loss: 0.6343 Lr: 0.04503
[2023-08-07 22:32:58,723 INFO misc.py line 115 22900] Train: [25/100][63/156] Data 0.001 (0.001) Batch 2.403 (3.501) Remain 11:28:01 loss: 0.3378 Lr: 0.04502
[2023-08-07 22:33:02,395 INFO misc.py line 115 22900] Train: [25/100][64/156] Data 0.001 (0.001) Batch 3.672 (3.503) Remain 11:28:31 loss: 0.5005 Lr: 0.04502
[2023-08-07 22:33:05,946 INFO misc.py line 115 22900] Train: [25/100][65/156] Data 0.001 (0.001) Batch 3.551 (3.504) Remain 11:28:36 loss: 0.5471 Lr: 0.04502
[2023-08-07 22:33:10,103 INFO misc.py line 115 22900] Train: [25/100][66/156] Data 0.001 (0.001) Batch 4.157 (3.514) Remain 11:30:35 loss: 0.5849 Lr: 0.04501
[2023-08-07 22:33:13,900 INFO misc.py line 115 22900] Train: [25/100][67/156] Data 0.001 (0.001) Batch 3.797 (3.519) Remain 11:31:24 loss: 0.3669 Lr: 0.04501
[2023-08-07 22:33:17,429 INFO misc.py line 115 22900] Train: [25/100][68/156] Data 0.001 (0.001) Batch 3.529 (3.519) Remain 11:31:22 loss: 0.6034 Lr: 0.04501
[2023-08-07 22:33:22,026 INFO misc.py line 115 22900] Train: [25/100][69/156] Data 0.001 (0.001) Batch 4.597 (3.535) Remain 11:34:31 loss: 0.5755 Lr: 0.04501
[2023-08-07 22:33:25,414 INFO misc.py line 115 22900] Train: [25/100][70/156] Data 0.001 (0.001) Batch 3.389 (3.533) Remain 11:34:01 loss: 0.5933 Lr: 0.04500
[2023-08-07 22:33:28,615 INFO misc.py line 115 22900] Train: [25/100][71/156] Data 0.001 (0.001) Batch 3.201 (3.528) Remain 11:33:00 loss: 0.7958 Lr: 0.04500
[2023-08-07 22:33:32,657 INFO misc.py line 115 22900] Train: [25/100][72/156] Data 0.001 (0.001) Batch 4.042 (3.536) Remain 11:34:25 loss: 0.5479 Lr: 0.04500
[2023-08-07 22:33:35,727 INFO misc.py line 115 22900] Train: [25/100][73/156] Data 0.001 (0.001) Batch 3.070 (3.529) Remain 11:33:03 loss: 0.2450 Lr: 0.04499
[2023-08-07 22:33:39,753 INFO misc.py line 115 22900] Train: [25/100][74/156] Data 0.001 (0.001) Batch 4.026 (3.536) Remain 11:34:21 loss: 0.7393 Lr: 0.04499
[2023-08-07 22:33:43,733 INFO misc.py line 115 22900] Train: [25/100][75/156] Data 0.001 (0.001) Batch 3.981 (3.542) Remain 11:35:31 loss: 0.7524 Lr: 0.04499
[2023-08-07 22:33:47,039 INFO misc.py line 115 22900] Train: [25/100][76/156] Data 0.001 (0.001) Batch 3.306 (3.539) Remain 11:34:49 loss: 0.3765 Lr: 0.04498
[2023-08-07 22:33:49,658 INFO misc.py line 115 22900] Train: [25/100][77/156] Data 0.001 (0.001) Batch 2.620 (3.527) Remain 11:32:19 loss: 0.7865 Lr: 0.04498
[2023-08-07 22:33:53,080 INFO misc.py line 115 22900] Train: [25/100][78/156] Data 0.001 (0.001) Batch 3.422 (3.525) Remain 11:31:59 loss: 0.4982 Lr: 0.04498
[2023-08-07 22:33:57,750 INFO misc.py line 115 22900] Train: [25/100][79/156] Data 0.001 (0.001) Batch 4.669 (3.540) Remain 11:34:53 loss: 0.6089 Lr: 0.04497
[2023-08-07 22:34:01,013 INFO misc.py line 115 22900] Train: [25/100][80/156] Data 0.001 (0.001) Batch 3.264 (3.537) Remain 11:34:07 loss: 0.6005 Lr: 0.04497
[2023-08-07 22:34:03,765 INFO misc.py line 115 22900] Train: [25/100][81/156] Data 0.001 (0.001) Batch 2.752 (3.527) Remain 11:32:05 loss: 0.2982 Lr: 0.04497
[2023-08-07 22:34:06,407 INFO misc.py line 115 22900] Train: [25/100][82/156] Data 0.001 (0.001) Batch 2.642 (3.515) Remain 11:29:50 loss: 0.5185 Lr: 0.04496
[2023-08-07 22:34:09,397 INFO misc.py line 115 22900] Train: [25/100][83/156] Data 0.001 (0.001) Batch 2.990 (3.509) Remain 11:28:29 loss: 0.1818 Lr: 0.04496
[2023-08-07 22:34:13,314 INFO misc.py line 115 22900] Train: [25/100][84/156] Data 0.001 (0.001) Batch 3.917 (3.514) Remain 11:29:25 loss: 0.7257 Lr: 0.04496
[2023-08-07 22:34:17,125 INFO misc.py line 115 22900] Train: [25/100][85/156] Data 0.001 (0.001) Batch 3.811 (3.517) Remain 11:30:04 loss: 0.3057 Lr: 0.04495
[2023-08-07 22:34:20,246 INFO misc.py line 115 22900] Train: [25/100][86/156] Data 0.001 (0.001) Batch 3.121 (3.513) Remain 11:29:04 loss: 0.4284 Lr: 0.04495
[2023-08-07 22:34:23,548 INFO misc.py line 115 22900] Train: [25/100][87/156] Data 0.001 (0.001) Batch 3.302 (3.510) Remain 11:28:31 loss: 0.3397 Lr: 0.04495
[2023-08-07 22:34:27,468 INFO misc.py line 115 22900] Train: [25/100][88/156] Data 0.001 (0.001) Batch 3.920 (3.515) Remain 11:29:24 loss: 0.3918 Lr: 0.04494
[2023-08-07 22:34:31,010 INFO misc.py line 115 22900] Train: [25/100][89/156] Data 0.001 (0.001) Batch 3.542 (3.515) Remain 11:29:24 loss: 0.6607 Lr: 0.04494
[2023-08-07 22:34:34,966 INFO misc.py line 115 22900] Train: [25/100][90/156] Data 0.001 (0.001) Batch 3.956 (3.520) Remain 11:30:20 loss: 0.4458 Lr: 0.04494
[2023-08-07 22:34:37,931 INFO misc.py line 115 22900] Train: [25/100][91/156] Data 0.001 (0.001) Batch 2.965 (3.514) Remain 11:29:03 loss: 0.2578 Lr: 0.04494
[2023-08-07 22:34:41,274 INFO misc.py line 115 22900] Train: [25/100][92/156] Data 0.001 (0.001) Batch 3.343 (3.512) Remain 11:28:37 loss: 0.3386 Lr: 0.04493
[2023-08-07 22:34:44,734 INFO misc.py line 115 22900] Train: [25/100][93/156] Data 0.001 (0.001) Batch 3.459 (3.512) Remain 11:28:26 loss: 0.5325 Lr: 0.04493
[2023-08-07 22:34:48,398 INFO misc.py line 115 22900] Train: [25/100][94/156] Data 0.001 (0.001) Batch 3.664 (3.513) Remain 11:28:42 loss: 0.4034 Lr: 0.04493
[2023-08-07 22:34:51,987 INFO misc.py line 115 22900] Train: [25/100][95/156] Data 0.001 (0.001) Batch 3.590 (3.514) Remain 11:28:49 loss: 0.5720 Lr: 0.04492
[2023-08-07 22:34:56,044 INFO misc.py line 115 22900] Train: [25/100][96/156] Data 0.001 (0.001) Batch 4.057 (3.520) Remain 11:29:54 loss: 0.3752 Lr: 0.04492
[2023-08-07 22:34:59,310 INFO misc.py line 115 22900] Train: [25/100][97/156] Data 0.001 (0.001) Batch 3.265 (3.517) Remain 11:29:18 loss: 0.4309 Lr: 0.04492
[2023-08-07 22:35:03,032 INFO misc.py line 115 22900] Train: [25/100][98/156] Data 0.001 (0.001) Batch 3.722 (3.519) Remain 11:29:40 loss: 0.5995 Lr: 0.04491
[2023-08-07 22:35:07,015 INFO misc.py line 115 22900] Train: [25/100][99/156] Data 0.001 (0.001) Batch 3.984 (3.524) Remain 11:30:34 loss: 0.3558 Lr: 0.04491
[2023-08-07 22:35:10,605 INFO misc.py line 115 22900] Train: [25/100][100/156] Data 0.001 (0.001) Batch 3.590 (3.525) Remain 11:30:38 loss: 0.5013 Lr: 0.04491
[2023-08-07 22:35:14,005 INFO misc.py line 115 22900] Train: [25/100][101/156] Data 0.001 (0.001) Batch 3.400 (3.524) Remain 11:30:19 loss: 0.7973 Lr: 0.04490
[2023-08-07 22:35:17,199 INFO misc.py line 115 22900] Train: [25/100][102/156] Data 0.001 (0.001) Batch 3.194 (3.520) Remain 11:29:37 loss: 0.4589 Lr: 0.04490
[2023-08-07 22:35:20,024 INFO misc.py line 115 22900] Train: [25/100][103/156] Data 0.001 (0.001) Batch 2.825 (3.513) Remain 11:28:12 loss: 0.4940 Lr: 0.04490
[2023-08-07 22:35:23,364 INFO misc.py line 115 22900] Train: [25/100][104/156] Data 0.001 (0.001) Batch 3.340 (3.512) Remain 11:27:48 loss: 0.4147 Lr: 0.04489
[2023-08-07 22:35:26,044 INFO misc.py line 115 22900] Train: [25/100][105/156] Data 0.001 (0.001) Batch 2.680 (3.503) Remain 11:26:09 loss: 0.3789 Lr: 0.04489
[2023-08-07 22:35:29,182 INFO misc.py line 115 22900] Train: [25/100][106/156] Data 0.001 (0.001) Batch 3.138 (3.500) Remain 11:25:23 loss: 0.3991 Lr: 0.04489
[2023-08-07 22:35:32,668 INFO misc.py line 115 22900] Train: [25/100][107/156] Data 0.001 (0.001) Batch 3.487 (3.500) Remain 11:25:18 loss: 0.3042 Lr: 0.04488
[2023-08-07 22:35:36,776 INFO misc.py line 115 22900] Train: [25/100][108/156] Data 0.001 (0.001) Batch 4.108 (3.506) Remain 11:26:23 loss: 0.8691 Lr: 0.04488
[2023-08-07 22:35:40,713 INFO misc.py line 115 22900] Train: [25/100][109/156] Data 0.001 (0.001) Batch 3.937 (3.510) Remain 11:27:07 loss: 0.7586 Lr: 0.04488
[2023-08-07 22:35:43,926 INFO misc.py line 115 22900] Train: [25/100][110/156] Data 0.001 (0.001) Batch 3.213 (3.507) Remain 11:26:31 loss: 0.5351 Lr: 0.04487
[2023-08-07 22:35:46,069 INFO misc.py line 115 22900] Train: [25/100][111/156] Data 0.001 (0.001) Batch 2.143 (3.494) Remain 11:23:59 loss: 0.3437 Lr: 0.04487
[2023-08-07 22:35:49,502 INFO misc.py line 115 22900] Train: [25/100][112/156] Data 0.001 (0.001) Batch 3.433 (3.494) Remain 11:23:49 loss: 0.4539 Lr: 0.04487
[2023-08-07 22:35:52,660 INFO misc.py line 115 22900] Train: [25/100][113/156] Data 0.001 (0.001) Batch 3.158 (3.491) Remain 11:23:10 loss: 0.6461 Lr: 0.04486
[2023-08-07 22:35:56,494 INFO misc.py line 115 22900] Train: [25/100][114/156] Data 0.001 (0.001) Batch 3.834 (3.494) Remain 11:23:43 loss: 0.5782 Lr: 0.04486
[2023-08-07 22:35:59,941 INFO misc.py line 115 22900] Train: [25/100][115/156] Data 0.001 (0.001) Batch 3.447 (3.493) Remain 11:23:34 loss: 0.3233 Lr: 0.04486
[2023-08-07 22:36:03,989 INFO misc.py line 115 22900] Train: [25/100][116/156] Data 0.001 (0.001) Batch 4.048 (3.498) Remain 11:24:28 loss: 0.8426 Lr: 0.04485
[2023-08-07 22:36:07,980 INFO misc.py line 115 22900] Train: [25/100][117/156] Data 0.001 (0.001) Batch 3.991 (3.503) Remain 11:25:16 loss: 0.6087 Lr: 0.04485
[2023-08-07 22:36:11,113 INFO misc.py line 115 22900] Train: [25/100][118/156] Data 0.001 (0.001) Batch 3.133 (3.499) Remain 11:24:34 loss: 0.4078 Lr: 0.04485
[2023-08-07 22:36:15,188 INFO misc.py line 115 22900] Train: [25/100][119/156] Data 0.001 (0.001) Batch 4.076 (3.504) Remain 11:25:29 loss: 0.6954 Lr: 0.04485
[2023-08-07 22:36:18,465 INFO misc.py line 115 22900] Train: [25/100][120/156] Data 0.001 (0.001) Batch 3.277 (3.502) Remain 11:25:03 loss: 0.5710 Lr: 0.04484
[2023-08-07 22:36:20,692 INFO misc.py line 115 22900] Train: [25/100][121/156] Data 0.001 (0.001) Batch 2.227 (3.492) Remain 11:22:53 loss: 0.2520 Lr: 0.04484
[2023-08-07 22:36:24,148 INFO misc.py line 115 22900] Train: [25/100][122/156] Data 0.001 (0.001) Batch 3.456 (3.491) Remain 11:22:46 loss: 0.3729 Lr: 0.04484
[2023-08-07 22:36:26,487 INFO misc.py line 115 22900] Train: [25/100][123/156] Data 0.001 (0.001) Batch 2.339 (3.482) Remain 11:20:49 loss: 0.4208 Lr: 0.04483
[2023-08-07 22:36:29,464 INFO misc.py line 115 22900] Train: [25/100][124/156] Data 0.001 (0.001) Batch 2.977 (3.477) Remain 11:19:57 loss: 0.4187 Lr: 0.04483
[2023-08-07 22:36:32,396 INFO misc.py line 115 22900] Train: [25/100][125/156] Data 0.001 (0.001) Batch 2.932 (3.473) Remain 11:19:01 loss: 0.3597 Lr: 0.04483
[2023-08-07 22:36:35,792 INFO misc.py line 115 22900] Train: [25/100][126/156] Data 0.001 (0.001) Batch 3.396 (3.472) Remain 11:18:50 loss: 0.5651 Lr: 0.04482
[2023-08-07 22:36:39,076 INFO misc.py line 115 22900] Train: [25/100][127/156] Data 0.001 (0.001) Batch 3.284 (3.471) Remain 11:18:29 loss: 0.2630 Lr: 0.04482
[2023-08-07 22:36:42,594 INFO misc.py line 115 22900] Train: [25/100][128/156] Data 0.001 (0.001) Batch 3.518 (3.471) Remain 11:18:30 loss: 0.3506 Lr: 0.04482
[2023-08-07 22:36:45,558 INFO misc.py line 115 22900] Train: [25/100][129/156] Data 0.001 (0.001) Batch 2.963 (3.467) Remain 11:17:39 loss: 0.6262 Lr: 0.04481
[2023-08-07 22:36:49,195 INFO misc.py line 115 22900] Train: [25/100][130/156] Data 0.001 (0.001) Batch 3.637 (3.469) Remain 11:17:51 loss: 0.5562 Lr: 0.04481
[2023-08-07 22:36:52,228 INFO misc.py line 115 22900] Train: [25/100][131/156] Data 0.001 (0.001) Batch 3.033 (3.465) Remain 11:17:08 loss: 0.5060 Lr: 0.04481
[2023-08-07 22:36:56,482 INFO misc.py line 115 22900] Train: [25/100][132/156] Data 0.001 (0.001) Batch 4.254 (3.471) Remain 11:18:16 loss: 0.6202 Lr: 0.04480
[2023-08-07 22:37:00,568 INFO misc.py line 115 22900] Train: [25/100][133/156] Data 0.001 (0.001) Batch 4.085 (3.476) Remain 11:19:08 loss: 0.5649 Lr: 0.04480
[2023-08-07 22:37:04,341 INFO misc.py line 115 22900] Train: [25/100][134/156] Data 0.001 (0.001) Batch 3.774 (3.478) Remain 11:19:31 loss: 0.4353 Lr: 0.04480
[2023-08-07 22:37:08,176 INFO misc.py line 115 22900] Train: [25/100][135/156] Data 0.001 (0.001) Batch 3.835 (3.481) Remain 11:20:00 loss: 0.4804 Lr: 0.04479
[2023-08-07 22:37:11,677 INFO misc.py line 115 22900] Train: [25/100][136/156] Data 0.001 (0.001) Batch 3.501 (3.481) Remain 11:19:58 loss: 0.5052 Lr: 0.04479
[2023-08-07 22:37:15,558 INFO misc.py line 115 22900] Train: [25/100][137/156] Data 0.001 (0.001) Batch 3.881 (3.484) Remain 11:20:29 loss: 0.7216 Lr: 0.04479
[2023-08-07 22:37:19,071 INFO misc.py line 115 22900] Train: [25/100][138/156] Data 0.001 (0.001) Batch 3.513 (3.484) Remain 11:20:28 loss: 0.4032 Lr: 0.04478
[2023-08-07 22:37:22,144 INFO misc.py line 115 22900] Train: [25/100][139/156] Data 0.001 (0.001) Batch 3.072 (3.481) Remain 11:19:49 loss: 0.5357 Lr: 0.04478
[2023-08-07 22:37:25,982 INFO misc.py line 115 22900] Train: [25/100][140/156] Data 0.001 (0.001) Batch 3.838 (3.484) Remain 11:20:17 loss: 0.5368 Lr: 0.04478
[2023-08-07 22:37:29,474 INFO misc.py line 115 22900] Train: [25/100][141/156] Data 0.001 (0.001) Batch 3.492 (3.484) Remain 11:20:14 loss: 0.5552 Lr: 0.04477
[2023-08-07 22:37:32,127 INFO misc.py line 115 22900] Train: [25/100][142/156] Data 0.001 (0.001) Batch 2.653 (3.478) Remain 11:19:00 loss: 0.3819 Lr: 0.04477
[2023-08-07 22:37:36,723 INFO misc.py line 115 22900] Train: [25/100][143/156] Data 0.001 (0.001) Batch 4.596 (3.486) Remain 11:20:30 loss: 0.6095 Lr: 0.04477
[2023-08-07 22:37:39,137 INFO misc.py line 115 22900] Train: [25/100][144/156] Data 0.001 (0.001) Batch 2.414 (3.478) Remain 11:18:58 loss: 0.4808 Lr: 0.04476
[2023-08-07 22:37:43,215 INFO misc.py line 115 22900] Train: [25/100][145/156] Data 0.001 (0.001) Batch 4.078 (3.483) Remain 11:19:44 loss: 0.2880 Lr: 0.04476
[2023-08-07 22:37:46,851 INFO misc.py line 115 22900] Train: [25/100][146/156] Data 0.001 (0.001) Batch 3.635 (3.484) Remain 11:19:53 loss: 0.3439 Lr: 0.04476
[2023-08-07 22:37:50,057 INFO misc.py line 115 22900] Train: [25/100][147/156] Data 0.001 (0.001) Batch 3.206 (3.482) Remain 11:19:27 loss: 0.5678 Lr: 0.04475
[2023-08-07 22:37:53,794 INFO misc.py line 115 22900] Train: [25/100][148/156] Data 0.001 (0.001) Batch 3.737 (3.483) Remain 11:19:44 loss: 0.4750 Lr: 0.04475
[2023-08-07 22:37:57,160 INFO misc.py line 115 22900] Train: [25/100][149/156] Data 0.001 (0.001) Batch 3.367 (3.483) Remain 11:19:31 loss: 0.3453 Lr: 0.04475
[2023-08-07 22:38:01,242 INFO misc.py line 115 22900] Train: [25/100][150/156] Data 0.001 (0.001) Batch 4.082 (3.487) Remain 11:20:15 loss: 0.4948 Lr: 0.04474
[2023-08-07 22:38:04,924 INFO misc.py line 115 22900] Train: [25/100][151/156] Data 0.001 (0.001) Batch 3.682 (3.488) Remain 11:20:27 loss: 0.5357 Lr: 0.04474
[2023-08-07 22:38:08,238 INFO misc.py line 115 22900] Train: [25/100][152/156] Data 0.001 (0.001) Batch 3.314 (3.487) Remain 11:20:10 loss: 0.3622 Lr: 0.04474
[2023-08-07 22:38:12,812 INFO misc.py line 115 22900] Train: [25/100][153/156] Data 0.001 (0.001) Batch 4.573 (3.494) Remain 11:21:31 loss: 0.6984 Lr: 0.04474
[2023-08-07 22:38:16,452 INFO misc.py line 115 22900] Train: [25/100][154/156] Data 0.001 (0.001) Batch 3.640 (3.495) Remain 11:21:39 loss: 0.5272 Lr: 0.04473
[2023-08-07 22:38:20,004 INFO misc.py line 115 22900] Train: [25/100][155/156] Data 0.001 (0.001) Batch 3.552 (3.495) Remain 11:21:40 loss: 0.4392 Lr: 0.04473
[2023-08-07 22:38:22,973 INFO misc.py line 115 22900] Train: [25/100][156/156] Data 0.001 (0.001) Batch 2.969 (3.492) Remain 11:20:56 loss: 0.3277 Lr: 0.04473
[2023-08-07 22:38:22,974 INFO misc.py line 129 22900] Train result: loss: 0.5024 
[2023-08-07 22:38:22,974 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 22:38:25,086 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.8253 
[2023-08-07 22:38:25,954 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.5976 
[2023-08-07 22:38:27,618 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.9748 
[2023-08-07 22:38:29,138 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.1960 
[2023-08-07 22:38:30,984 INFO evaluator.py line 122 22900] Test: [5/24] Loss 2.0888 
[2023-08-07 22:38:32,649 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.4958 
[2023-08-07 22:38:34,786 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.3099 
[2023-08-07 22:38:36,589 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.9174 
[2023-08-07 22:38:37,874 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.2001 
[2023-08-07 22:38:40,005 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3736 
[2023-08-07 22:38:40,532 INFO evaluator.py line 122 22900] Test: [11/24] Loss 0.5940 
[2023-08-07 22:38:42,065 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.6686 
[2023-08-07 22:38:44,779 INFO evaluator.py line 122 22900] Test: [13/24] Loss 0.6853 
[2023-08-07 22:38:46,459 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.9025 
[2023-08-07 22:38:48,481 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.4077 
[2023-08-07 22:38:51,192 INFO evaluator.py line 122 22900] Test: [16/24] Loss 0.6926 
[2023-08-07 22:38:53,900 INFO evaluator.py line 122 22900] Test: [17/24] Loss 0.8812 
[2023-08-07 22:38:55,749 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.9352 
[2023-08-07 22:38:56,498 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.1029 
[2023-08-07 22:38:57,384 INFO evaluator.py line 122 22900] Test: [20/24] Loss 1.0127 
[2023-08-07 22:38:59,644 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3473 
[2023-08-07 22:39:01,610 INFO evaluator.py line 122 22900] Test: [22/24] Loss 2.2197 
[2023-08-07 22:39:03,456 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.1518 
[2023-08-07 22:39:05,391 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.6506 
[2023-08-07 22:39:05,445 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2107/0.3052/0.6921.
[2023-08-07 22:39:05,445 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6698/0.9384
[2023-08-07 22:39:05,445 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9441/0.9907
[2023-08-07 22:39:05,445 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1173/0.1887
[2023-08-07 22:39:05,445 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1422/0.1951
[2023-08-07 22:39:05,445 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.5704/0.8053
[2023-08-07 22:39:05,446 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.3484/0.4628
[2023-08-07 22:39:05,446 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5261/0.7749
[2023-08-07 22:39:05,446 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0818/0.0844
[2023-08-07 22:39:05,446 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1450/0.5321
[2023-08-07 22:39:05,446 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.1651/0.1767
[2023-08-07 22:39:05,446 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0298/0.1637
[2023-08-07 22:39:05,446 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0000/0.0000
[2023-08-07 22:39:05,446 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.0499/0.0555
[2023-08-07 22:39:05,446 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0141/0.0150
[2023-08-07 22:39:05,446 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0468/0.0486
[2023-08-07 22:39:05,446 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-07 22:39:05,446 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0158/0.0161
[2023-08-07 22:39:05,446 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.2703/0.5120
[2023-08-07 22:39:05,446 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 22:39:05,446 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0777/0.1436
[2023-08-07 22:39:05,446 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 22:39:05,446 INFO misc.py line 152 22900] Currently Best mIoU: 0.2147
[2023-08-07 22:39:05,446 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 22:39:10,050 INFO misc.py line 115 22900] Train: [26/100][1/156] Data 0.946 (0.946) Batch 3.826 (3.826) Remain 12:25:57 loss: 0.4051 Lr: 0.04472
[2023-08-07 22:39:12,779 INFO misc.py line 115 22900] Train: [26/100][2/156] Data 0.001 (0.001) Batch 2.729 (2.729) Remain 08:52:08 loss: 0.2861 Lr: 0.04472
[2023-08-07 22:39:16,339 INFO misc.py line 115 22900] Train: [26/100][3/156] Data 0.001 (0.001) Batch 3.560 (3.560) Remain 11:33:57 loss: 0.4642 Lr: 0.04472
[2023-08-07 22:39:20,053 INFO misc.py line 115 22900] Train: [26/100][4/156] Data 0.001 (0.001) Batch 3.714 (3.714) Remain 12:04:00 loss: 0.6800 Lr: 0.04471
[2023-08-07 22:39:21,416 INFO misc.py line 115 22900] Train: [26/100][5/156] Data 0.001 (0.001) Batch 1.363 (2.539) Remain 08:14:49 loss: 0.4023 Lr: 0.04471
[2023-08-07 22:39:24,260 INFO misc.py line 115 22900] Train: [26/100][6/156] Data 0.001 (0.001) Batch 2.844 (2.640) Remain 08:34:35 loss: 0.3827 Lr: 0.04471
[2023-08-07 22:39:27,458 INFO misc.py line 115 22900] Train: [26/100][7/156] Data 0.001 (0.001) Batch 3.198 (2.780) Remain 09:01:42 loss: 0.2504 Lr: 0.04470
[2023-08-07 22:39:31,123 INFO misc.py line 115 22900] Train: [26/100][8/156] Data 0.001 (0.001) Batch 3.665 (2.957) Remain 09:36:10 loss: 0.4923 Lr: 0.04470
[2023-08-07 22:39:35,216 INFO misc.py line 115 22900] Train: [26/100][9/156] Data 0.001 (0.001) Batch 4.093 (3.146) Remain 10:13:02 loss: 0.2898 Lr: 0.04470
[2023-08-07 22:39:39,769 INFO misc.py line 115 22900] Train: [26/100][10/156] Data 0.001 (0.001) Batch 4.552 (3.347) Remain 10:52:07 loss: 0.9209 Lr: 0.04469
[2023-08-07 22:39:42,122 INFO misc.py line 115 22900] Train: [26/100][11/156] Data 0.001 (0.001) Batch 2.353 (3.223) Remain 10:27:51 loss: 0.3923 Lr: 0.04469
[2023-08-07 22:39:44,509 INFO misc.py line 115 22900] Train: [26/100][12/156] Data 0.001 (0.001) Batch 2.388 (3.130) Remain 10:09:43 loss: 0.2321 Lr: 0.04469
[2023-08-07 22:39:47,875 INFO misc.py line 115 22900] Train: [26/100][13/156] Data 0.001 (0.001) Batch 3.366 (3.154) Remain 10:14:16 loss: 0.6456 Lr: 0.04468
[2023-08-07 22:39:50,797 INFO misc.py line 115 22900] Train: [26/100][14/156] Data 0.001 (0.001) Batch 2.921 (3.133) Remain 10:10:06 loss: 0.4898 Lr: 0.04468
[2023-08-07 22:39:54,006 INFO misc.py line 115 22900] Train: [26/100][15/156] Data 0.001 (0.001) Batch 3.209 (3.139) Remain 10:11:18 loss: 0.3950 Lr: 0.04468
[2023-08-07 22:39:57,747 INFO misc.py line 115 22900] Train: [26/100][16/156] Data 0.001 (0.001) Batch 3.740 (3.185) Remain 10:20:15 loss: 0.4041 Lr: 0.04467
[2023-08-07 22:40:01,779 INFO misc.py line 115 22900] Train: [26/100][17/156] Data 0.001 (0.001) Batch 4.033 (3.246) Remain 10:31:59 loss: 0.6315 Lr: 0.04467
[2023-08-07 22:40:05,814 INFO misc.py line 115 22900] Train: [26/100][18/156] Data 0.001 (0.001) Batch 4.035 (3.298) Remain 10:42:11 loss: 0.3944 Lr: 0.04467
[2023-08-07 22:40:09,887 INFO misc.py line 115 22900] Train: [26/100][19/156] Data 0.001 (0.001) Batch 4.072 (3.347) Remain 10:51:33 loss: 0.5081 Lr: 0.04466
[2023-08-07 22:40:12,807 INFO misc.py line 115 22900] Train: [26/100][20/156] Data 0.001 (0.001) Batch 2.921 (3.322) Remain 10:46:37 loss: 0.1571 Lr: 0.04466
[2023-08-07 22:40:17,702 INFO misc.py line 115 22900] Train: [26/100][21/156] Data 0.001 (0.001) Batch 4.894 (3.409) Remain 11:03:34 loss: 0.7635 Lr: 0.04466
[2023-08-07 22:40:20,700 INFO misc.py line 115 22900] Train: [26/100][22/156] Data 0.001 (0.001) Batch 2.999 (3.387) Remain 10:59:18 loss: 0.4288 Lr: 0.04465
[2023-08-07 22:40:24,124 INFO misc.py line 115 22900] Train: [26/100][23/156] Data 0.001 (0.001) Batch 3.424 (3.389) Remain 10:59:36 loss: 0.4592 Lr: 0.04465
[2023-08-07 22:40:27,438 INFO misc.py line 115 22900] Train: [26/100][24/156] Data 0.001 (0.001) Batch 3.314 (3.386) Remain 10:58:50 loss: 0.2993 Lr: 0.04465
[2023-08-07 22:40:31,441 INFO misc.py line 115 22900] Train: [26/100][25/156] Data 0.001 (0.001) Batch 4.004 (3.414) Remain 11:04:15 loss: 0.7074 Lr: 0.04464
[2023-08-07 22:40:35,320 INFO misc.py line 115 22900] Train: [26/100][26/156] Data 0.001 (0.001) Batch 3.879 (3.434) Remain 11:08:07 loss: 0.4844 Lr: 0.04464
[2023-08-07 22:40:39,283 INFO misc.py line 115 22900] Train: [26/100][27/156] Data 0.001 (0.001) Batch 3.963 (3.456) Remain 11:12:21 loss: 0.4616 Lr: 0.04464
[2023-08-07 22:40:43,073 INFO misc.py line 115 22900] Train: [26/100][28/156] Data 0.001 (0.001) Batch 3.790 (3.469) Remain 11:14:54 loss: 0.4781 Lr: 0.04463
[2023-08-07 22:40:47,029 INFO misc.py line 115 22900] Train: [26/100][29/156] Data 0.001 (0.001) Batch 3.957 (3.488) Remain 11:18:29 loss: 0.5451 Lr: 0.04463
[2023-08-07 22:40:49,156 INFO misc.py line 115 22900] Train: [26/100][30/156] Data 0.001 (0.001) Batch 2.127 (3.438) Remain 11:08:37 loss: 0.2747 Lr: 0.04463
[2023-08-07 22:40:52,897 INFO misc.py line 115 22900] Train: [26/100][31/156] Data 0.001 (0.001) Batch 3.741 (3.448) Remain 11:10:40 loss: 0.3798 Lr: 0.04462
[2023-08-07 22:40:56,899 INFO misc.py line 115 22900] Train: [26/100][32/156] Data 0.001 (0.001) Batch 4.002 (3.468) Remain 11:14:19 loss: 0.3857 Lr: 0.04462
[2023-08-07 22:40:59,961 INFO misc.py line 115 22900] Train: [26/100][33/156] Data 0.001 (0.001) Batch 3.061 (3.454) Remain 11:11:38 loss: 0.1918 Lr: 0.04462
[2023-08-07 22:41:03,409 INFO misc.py line 115 22900] Train: [26/100][34/156] Data 0.001 (0.001) Batch 3.449 (3.454) Remain 11:11:32 loss: 0.3551 Lr: 0.04461
[2023-08-07 22:41:07,422 INFO misc.py line 115 22900] Train: [26/100][35/156] Data 0.001 (0.001) Batch 4.012 (3.471) Remain 11:14:53 loss: 0.5289 Lr: 0.04461
[2023-08-07 22:41:10,636 INFO misc.py line 115 22900] Train: [26/100][36/156] Data 0.001 (0.001) Batch 3.215 (3.464) Remain 11:13:18 loss: 0.4263 Lr: 0.04461
[2023-08-07 22:41:13,840 INFO misc.py line 115 22900] Train: [26/100][37/156] Data 0.001 (0.001) Batch 3.204 (3.456) Remain 11:11:46 loss: 0.2532 Lr: 0.04460
[2023-08-07 22:41:17,375 INFO misc.py line 115 22900] Train: [26/100][38/156] Data 0.001 (0.001) Batch 3.534 (3.458) Remain 11:12:09 loss: 0.4107 Lr: 0.04460
[2023-08-07 22:41:20,847 INFO misc.py line 115 22900] Train: [26/100][39/156] Data 0.001 (0.001) Batch 3.472 (3.459) Remain 11:12:10 loss: 0.5180 Lr: 0.04460
[2023-08-07 22:41:23,917 INFO misc.py line 115 22900] Train: [26/100][40/156] Data 0.001 (0.001) Batch 3.069 (3.448) Remain 11:10:04 loss: 0.2920 Lr: 0.04459
[2023-08-07 22:41:28,006 INFO misc.py line 115 22900] Train: [26/100][41/156] Data 0.001 (0.001) Batch 4.089 (3.465) Remain 11:13:17 loss: 0.3877 Lr: 0.04459
[2023-08-07 22:41:31,382 INFO misc.py line 115 22900] Train: [26/100][42/156] Data 0.001 (0.001) Batch 3.377 (3.463) Remain 11:12:47 loss: 0.5359 Lr: 0.04459
[2023-08-07 22:41:35,387 INFO misc.py line 115 22900] Train: [26/100][43/156] Data 0.001 (0.001) Batch 4.005 (3.476) Remain 11:15:22 loss: 0.5203 Lr: 0.04458
[2023-08-07 22:41:38,602 INFO misc.py line 115 22900] Train: [26/100][44/156] Data 0.001 (0.001) Batch 3.215 (3.470) Remain 11:14:04 loss: 0.5036 Lr: 0.04458
[2023-08-07 22:41:41,326 INFO misc.py line 115 22900] Train: [26/100][45/156] Data 0.001 (0.001) Batch 2.724 (3.452) Remain 11:10:33 loss: 0.3345 Lr: 0.04458
[2023-08-07 22:41:44,497 INFO misc.py line 115 22900] Train: [26/100][46/156] Data 0.001 (0.001) Batch 3.171 (3.446) Remain 11:09:14 loss: 0.6014 Lr: 0.04457
[2023-08-07 22:41:47,905 INFO misc.py line 115 22900] Train: [26/100][47/156] Data 0.001 (0.001) Batch 3.408 (3.445) Remain 11:09:00 loss: 0.2137 Lr: 0.04457
[2023-08-07 22:41:51,266 INFO misc.py line 115 22900] Train: [26/100][48/156] Data 0.001 (0.001) Batch 3.361 (3.443) Remain 11:08:35 loss: 0.5438 Lr: 0.04457
[2023-08-07 22:41:53,351 INFO misc.py line 115 22900] Train: [26/100][49/156] Data 0.001 (0.001) Batch 2.085 (3.413) Remain 11:02:48 loss: 0.2225 Lr: 0.04456
[2023-08-07 22:41:57,334 INFO misc.py line 115 22900] Train: [26/100][50/156] Data 0.001 (0.001) Batch 3.983 (3.425) Remain 11:05:06 loss: 0.4276 Lr: 0.04456
[2023-08-07 22:41:59,546 INFO misc.py line 115 22900] Train: [26/100][51/156] Data 0.001 (0.001) Batch 2.212 (3.400) Remain 11:00:08 loss: 0.2059 Lr: 0.04456
[2023-08-07 22:42:02,296 INFO misc.py line 115 22900] Train: [26/100][52/156] Data 0.001 (0.001) Batch 2.750 (3.387) Remain 10:57:30 loss: 0.6101 Lr: 0.04455
[2023-08-07 22:42:06,094 INFO misc.py line 115 22900] Train: [26/100][53/156] Data 0.001 (0.001) Batch 3.798 (3.395) Remain 10:59:02 loss: 0.7155 Lr: 0.04455
[2023-08-07 22:42:09,548 INFO misc.py line 115 22900] Train: [26/100][54/156] Data 0.001 (0.001) Batch 3.454 (3.396) Remain 10:59:12 loss: 0.3570 Lr: 0.04455
[2023-08-07 22:42:12,616 INFO misc.py line 115 22900] Train: [26/100][55/156] Data 0.001 (0.001) Batch 3.068 (3.390) Remain 10:57:55 loss: 0.4970 Lr: 0.04454
[2023-08-07 22:42:17,227 INFO misc.py line 115 22900] Train: [26/100][56/156] Data 0.001 (0.001) Batch 4.612 (3.413) Remain 11:02:20 loss: 0.7105 Lr: 0.04454
[2023-08-07 22:42:20,908 INFO misc.py line 115 22900] Train: [26/100][57/156] Data 0.001 (0.001) Batch 3.680 (3.418) Remain 11:03:15 loss: 0.3990 Lr: 0.04454
[2023-08-07 22:42:23,820 INFO misc.py line 115 22900] Train: [26/100][58/156] Data 0.001 (0.001) Batch 2.912 (3.409) Remain 11:01:24 loss: 0.2133 Lr: 0.04453
[2023-08-07 22:42:27,851 INFO misc.py line 115 22900] Train: [26/100][59/156] Data 0.001 (0.001) Batch 4.031 (3.420) Remain 11:03:30 loss: 0.5272 Lr: 0.04453
[2023-08-07 22:42:31,433 INFO misc.py line 115 22900] Train: [26/100][60/156] Data 0.001 (0.001) Batch 3.583 (3.423) Remain 11:04:00 loss: 0.5636 Lr: 0.04453
[2023-08-07 22:42:34,606 INFO misc.py line 115 22900] Train: [26/100][61/156] Data 0.001 (0.001) Batch 3.173 (3.418) Remain 11:03:06 loss: 0.2563 Lr: 0.04453
[2023-08-07 22:42:38,714 INFO misc.py line 115 22900] Train: [26/100][62/156] Data 0.001 (0.001) Batch 4.108 (3.430) Remain 11:05:19 loss: 0.4666 Lr: 0.04452
[2023-08-07 22:42:42,197 INFO misc.py line 115 22900] Train: [26/100][63/156] Data 0.001 (0.001) Batch 3.483 (3.431) Remain 11:05:26 loss: 0.3508 Lr: 0.04452
[2023-08-07 22:42:44,855 INFO misc.py line 115 22900] Train: [26/100][64/156] Data 0.001 (0.001) Batch 2.658 (3.418) Remain 11:02:55 loss: 0.2716 Lr: 0.04452
[2023-08-07 22:42:48,772 INFO misc.py line 115 22900] Train: [26/100][65/156] Data 0.001 (0.001) Batch 3.917 (3.426) Remain 11:04:25 loss: 0.4545 Lr: 0.04451
[2023-08-07 22:42:52,276 INFO misc.py line 115 22900] Train: [26/100][66/156] Data 0.001 (0.001) Batch 3.504 (3.428) Remain 11:04:36 loss: 0.2993 Lr: 0.04451
[2023-08-07 22:42:55,775 INFO misc.py line 115 22900] Train: [26/100][67/156] Data 0.001 (0.001) Batch 3.499 (3.429) Remain 11:04:45 loss: 0.5877 Lr: 0.04451
[2023-08-07 22:42:59,872 INFO misc.py line 115 22900] Train: [26/100][68/156] Data 0.001 (0.001) Batch 4.097 (3.439) Remain 11:06:42 loss: 0.5023 Lr: 0.04450
[2023-08-07 22:43:02,931 INFO misc.py line 115 22900] Train: [26/100][69/156] Data 0.001 (0.001) Batch 3.059 (3.433) Remain 11:05:31 loss: 0.6035 Lr: 0.04450
[2023-08-07 22:43:07,096 INFO misc.py line 115 22900] Train: [26/100][70/156] Data 0.001 (0.001) Batch 4.165 (3.444) Remain 11:07:35 loss: 0.4531 Lr: 0.04450
[2023-08-07 22:43:09,882 INFO misc.py line 115 22900] Train: [26/100][71/156] Data 0.001 (0.001) Batch 2.787 (3.434) Remain 11:05:39 loss: 0.4342 Lr: 0.04449
[2023-08-07 22:43:13,860 INFO misc.py line 115 22900] Train: [26/100][72/156] Data 0.001 (0.001) Batch 3.978 (3.442) Remain 11:07:07 loss: 0.7500 Lr: 0.04449
[2023-08-07 22:43:17,842 INFO misc.py line 115 22900] Train: [26/100][73/156] Data 0.001 (0.001) Batch 3.982 (3.450) Remain 11:08:33 loss: 0.5687 Lr: 0.04449
[2023-08-07 22:43:21,042 INFO misc.py line 115 22900] Train: [26/100][74/156] Data 0.001 (0.001) Batch 3.201 (3.447) Remain 11:07:49 loss: 0.4736 Lr: 0.04448
[2023-08-07 22:43:25,095 INFO misc.py line 115 22900] Train: [26/100][75/156] Data 0.001 (0.001) Batch 4.052 (3.455) Remain 11:09:23 loss: 0.6383 Lr: 0.04448
[2023-08-07 22:43:28,648 INFO misc.py line 115 22900] Train: [26/100][76/156] Data 0.001 (0.001) Batch 3.553 (3.456) Remain 11:09:35 loss: 0.7727 Lr: 0.04448
[2023-08-07 22:43:32,849 INFO misc.py line 115 22900] Train: [26/100][77/156] Data 0.001 (0.001) Batch 4.201 (3.466) Remain 11:11:29 loss: 0.4791 Lr: 0.04447
[2023-08-07 22:43:36,001 INFO misc.py line 115 22900] Train: [26/100][78/156] Data 0.001 (0.001) Batch 3.152 (3.462) Remain 11:10:37 loss: 0.5636 Lr: 0.04447
[2023-08-07 22:43:39,394 INFO misc.py line 115 22900] Train: [26/100][79/156] Data 0.001 (0.001) Batch 3.393 (3.461) Remain 11:10:23 loss: 0.2969 Lr: 0.04447
[2023-08-07 22:43:43,332 INFO misc.py line 115 22900] Train: [26/100][80/156] Data 0.001 (0.001) Batch 3.937 (3.467) Remain 11:11:31 loss: 0.3803 Lr: 0.04446
[2023-08-07 22:43:47,247 INFO misc.py line 115 22900] Train: [26/100][81/156] Data 0.001 (0.001) Batch 3.915 (3.473) Remain 11:12:34 loss: 0.6288 Lr: 0.04446
[2023-08-07 22:43:50,960 INFO misc.py line 115 22900] Train: [26/100][82/156] Data 0.001 (0.001) Batch 3.714 (3.476) Remain 11:13:06 loss: 0.6619 Lr: 0.04446
[2023-08-07 22:43:54,418 INFO misc.py line 115 22900] Train: [26/100][83/156] Data 0.001 (0.001) Batch 3.457 (3.476) Remain 11:13:00 loss: 0.8070 Lr: 0.04445
[2023-08-07 22:43:58,289 INFO misc.py line 115 22900] Train: [26/100][84/156] Data 0.001 (0.001) Batch 3.872 (3.481) Remain 11:13:53 loss: 0.7594 Lr: 0.04445
[2023-08-07 22:44:02,185 INFO misc.py line 115 22900] Train: [26/100][85/156] Data 0.001 (0.001) Batch 3.896 (3.486) Remain 11:14:49 loss: 0.7195 Lr: 0.04445
[2023-08-07 22:44:05,517 INFO misc.py line 115 22900] Train: [26/100][86/156] Data 0.001 (0.001) Batch 3.332 (3.484) Remain 11:14:24 loss: 0.5496 Lr: 0.04444
[2023-08-07 22:44:08,412 INFO misc.py line 115 22900] Train: [26/100][87/156] Data 0.001 (0.001) Batch 2.894 (3.477) Remain 11:12:59 loss: 0.5919 Lr: 0.04444
[2023-08-07 22:44:09,966 INFO misc.py line 115 22900] Train: [26/100][88/156] Data 0.001 (0.001) Batch 1.554 (3.454) Remain 11:08:32 loss: 0.1046 Lr: 0.04444
[2023-08-07 22:44:12,871 INFO misc.py line 115 22900] Train: [26/100][89/156] Data 0.001 (0.001) Batch 2.905 (3.448) Remain 11:07:15 loss: 0.5471 Lr: 0.04443
[2023-08-07 22:44:16,053 INFO misc.py line 115 22900] Train: [26/100][90/156] Data 0.001 (0.001) Batch 3.182 (3.445) Remain 11:06:36 loss: 0.5449 Lr: 0.04443
[2023-08-07 22:44:19,227 INFO misc.py line 115 22900] Train: [26/100][91/156] Data 0.001 (0.001) Batch 3.174 (3.442) Remain 11:05:57 loss: 0.3845 Lr: 0.04443
[2023-08-07 22:44:22,905 INFO misc.py line 115 22900] Train: [26/100][92/156] Data 0.001 (0.001) Batch 3.678 (3.445) Remain 11:06:24 loss: 0.3622 Lr: 0.04442
[2023-08-07 22:44:26,939 INFO misc.py line 115 22900] Train: [26/100][93/156] Data 0.001 (0.001) Batch 4.034 (3.451) Remain 11:07:36 loss: 0.6398 Lr: 0.04442
[2023-08-07 22:44:30,392 INFO misc.py line 115 22900] Train: [26/100][94/156] Data 0.001 (0.001) Batch 3.453 (3.451) Remain 11:07:33 loss: 0.6058 Lr: 0.04442
[2023-08-07 22:44:33,336 INFO misc.py line 115 22900] Train: [26/100][95/156] Data 0.001 (0.001) Batch 2.944 (3.446) Remain 11:06:26 loss: 0.3417 Lr: 0.04441
[2023-08-07 22:44:36,667 INFO misc.py line 115 22900] Train: [26/100][96/156] Data 0.001 (0.001) Batch 3.331 (3.444) Remain 11:06:08 loss: 0.3533 Lr: 0.04441
[2023-08-07 22:44:39,536 INFO misc.py line 115 22900] Train: [26/100][97/156] Data 0.001 (0.001) Batch 2.869 (3.438) Remain 11:04:54 loss: 0.2602 Lr: 0.04441
[2023-08-07 22:44:43,017 INFO misc.py line 115 22900] Train: [26/100][98/156] Data 0.001 (0.001) Batch 3.481 (3.439) Remain 11:04:55 loss: 0.3924 Lr: 0.04440
[2023-08-07 22:44:46,955 INFO misc.py line 115 22900] Train: [26/100][99/156] Data 0.001 (0.001) Batch 3.937 (3.444) Remain 11:05:52 loss: 0.7516 Lr: 0.04440
[2023-08-07 22:44:49,638 INFO misc.py line 115 22900] Train: [26/100][100/156] Data 0.001 (0.001) Batch 2.684 (3.436) Remain 11:04:18 loss: 0.3999 Lr: 0.04440
[2023-08-07 22:44:52,896 INFO misc.py line 115 22900] Train: [26/100][101/156] Data 0.001 (0.001) Batch 3.257 (3.434) Remain 11:03:53 loss: 0.4243 Lr: 0.04439
[2023-08-07 22:44:55,913 INFO misc.py line 115 22900] Train: [26/100][102/156] Data 0.001 (0.001) Batch 3.018 (3.430) Remain 11:03:01 loss: 0.3410 Lr: 0.04439
[2023-08-07 22:45:00,513 INFO misc.py line 115 22900] Train: [26/100][103/156] Data 0.001 (0.001) Batch 4.600 (3.442) Remain 11:05:13 loss: 0.5137 Lr: 0.04439
[2023-08-07 22:45:04,523 INFO misc.py line 115 22900] Train: [26/100][104/156] Data 0.001 (0.001) Batch 4.009 (3.447) Remain 11:06:15 loss: 0.6551 Lr: 0.04438
[2023-08-07 22:45:08,255 INFO misc.py line 115 22900] Train: [26/100][105/156] Data 0.001 (0.001) Batch 3.732 (3.450) Remain 11:06:44 loss: 0.5236 Lr: 0.04438
[2023-08-07 22:45:10,867 INFO misc.py line 115 22900] Train: [26/100][106/156] Data 0.001 (0.001) Batch 2.612 (3.442) Remain 11:05:06 loss: 0.2898 Lr: 0.04438
[2023-08-07 22:45:14,334 INFO misc.py line 115 22900] Train: [26/100][107/156] Data 0.002 (0.001) Batch 3.467 (3.442) Remain 11:05:06 loss: 0.6197 Lr: 0.04437
[2023-08-07 22:45:17,230 INFO misc.py line 115 22900] Train: [26/100][108/156] Data 0.001 (0.001) Batch 2.896 (3.437) Remain 11:04:02 loss: 0.2935 Lr: 0.04437
[2023-08-07 22:45:20,763 INFO misc.py line 115 22900] Train: [26/100][109/156] Data 0.001 (0.001) Batch 3.534 (3.438) Remain 11:04:09 loss: 0.4877 Lr: 0.04437
[2023-08-07 22:45:23,537 INFO misc.py line 115 22900] Train: [26/100][110/156] Data 0.001 (0.001) Batch 2.773 (3.432) Remain 11:02:54 loss: 0.4071 Lr: 0.04436
[2023-08-07 22:45:25,315 INFO misc.py line 115 22900] Train: [26/100][111/156] Data 0.001 (0.001) Batch 1.778 (3.416) Remain 10:59:53 loss: 0.3410 Lr: 0.04436
[2023-08-07 22:45:27,443 INFO misc.py line 115 22900] Train: [26/100][112/156] Data 0.001 (0.001) Batch 2.128 (3.405) Remain 10:57:32 loss: 0.2616 Lr: 0.04436
[2023-08-07 22:45:31,317 INFO misc.py line 115 22900] Train: [26/100][113/156] Data 0.001 (0.001) Batch 3.874 (3.409) Remain 10:58:18 loss: 0.4947 Lr: 0.04435
[2023-08-07 22:45:34,394 INFO misc.py line 115 22900] Train: [26/100][114/156] Data 0.001 (0.001) Batch 3.077 (3.406) Remain 10:57:40 loss: 0.2026 Lr: 0.04435
[2023-08-07 22:45:38,007 INFO misc.py line 115 22900] Train: [26/100][115/156] Data 0.001 (0.001) Batch 3.613 (3.408) Remain 10:57:58 loss: 0.4083 Lr: 0.04435
[2023-08-07 22:45:41,905 INFO misc.py line 115 22900] Train: [26/100][116/156] Data 0.001 (0.001) Batch 3.898 (3.412) Remain 10:58:45 loss: 0.4535 Lr: 0.04434
[2023-08-07 22:45:46,248 INFO misc.py line 115 22900] Train: [26/100][117/156] Data 0.001 (0.001) Batch 4.344 (3.420) Remain 11:00:16 loss: 0.8189 Lr: 0.04434
[2023-08-07 22:45:49,514 INFO misc.py line 115 22900] Train: [26/100][118/156] Data 0.001 (0.001) Batch 3.266 (3.419) Remain 10:59:57 loss: 0.2353 Lr: 0.04433
[2023-08-07 22:45:52,995 INFO misc.py line 115 22900] Train: [26/100][119/156] Data 0.001 (0.001) Batch 3.481 (3.419) Remain 11:00:00 loss: 0.7760 Lr: 0.04433
[2023-08-07 22:45:57,022 INFO misc.py line 115 22900] Train: [26/100][120/156] Data 0.001 (0.001) Batch 4.028 (3.425) Remain 11:00:57 loss: 0.6650 Lr: 0.04433
[2023-08-07 22:46:01,088 INFO misc.py line 115 22900] Train: [26/100][121/156] Data 0.001 (0.001) Batch 4.065 (3.430) Remain 11:01:56 loss: 0.3944 Lr: 0.04432
[2023-08-07 22:46:05,691 INFO misc.py line 115 22900] Train: [26/100][122/156] Data 0.001 (0.001) Batch 4.603 (3.440) Remain 11:03:47 loss: 0.5140 Lr: 0.04432
[2023-08-07 22:46:09,005 INFO misc.py line 115 22900] Train: [26/100][123/156] Data 0.001 (0.001) Batch 3.314 (3.439) Remain 11:03:31 loss: 0.4679 Lr: 0.04432
[2023-08-07 22:46:13,167 INFO misc.py line 115 22900] Train: [26/100][124/156] Data 0.001 (0.001) Batch 4.163 (3.445) Remain 11:04:37 loss: 0.6037 Lr: 0.04431
[2023-08-07 22:46:16,893 INFO misc.py line 115 22900] Train: [26/100][125/156] Data 0.001 (0.001) Batch 3.726 (3.447) Remain 11:05:00 loss: 0.4301 Lr: 0.04431
[2023-08-07 22:46:21,389 INFO misc.py line 115 22900] Train: [26/100][126/156] Data 0.001 (0.001) Batch 4.496 (3.456) Remain 11:06:36 loss: 0.5632 Lr: 0.04431
[2023-08-07 22:46:24,865 INFO misc.py line 115 22900] Train: [26/100][127/156] Data 0.001 (0.001) Batch 3.476 (3.456) Remain 11:06:34 loss: 0.5430 Lr: 0.04430
[2023-08-07 22:46:28,893 INFO misc.py line 115 22900] Train: [26/100][128/156] Data 0.001 (0.001) Batch 4.028 (3.460) Remain 11:07:24 loss: 0.6684 Lr: 0.04430
[2023-08-07 22:46:32,948 INFO misc.py line 115 22900] Train: [26/100][129/156] Data 0.001 (0.001) Batch 4.055 (3.465) Remain 11:08:15 loss: 0.3313 Lr: 0.04430
[2023-08-07 22:46:36,468 INFO misc.py line 115 22900] Train: [26/100][130/156] Data 0.001 (0.001) Batch 3.520 (3.466) Remain 11:08:16 loss: 0.6384 Lr: 0.04429
[2023-08-07 22:46:39,324 INFO misc.py line 115 22900] Train: [26/100][131/156] Data 0.001 (0.001) Batch 2.855 (3.461) Remain 11:07:18 loss: 0.2970 Lr: 0.04429
[2023-08-07 22:46:42,823 INFO misc.py line 115 22900] Train: [26/100][132/156] Data 0.001 (0.001) Batch 3.500 (3.461) Remain 11:07:18 loss: 0.4306 Lr: 0.04429
[2023-08-07 22:46:46,898 INFO misc.py line 115 22900] Train: [26/100][133/156] Data 0.001 (0.001) Batch 4.075 (3.466) Remain 11:08:09 loss: 0.5230 Lr: 0.04428
[2023-08-07 22:46:50,479 INFO misc.py line 115 22900] Train: [26/100][134/156] Data 0.001 (0.001) Batch 3.581 (3.467) Remain 11:08:16 loss: 0.2415 Lr: 0.04428
[2023-08-07 22:46:54,519 INFO misc.py line 115 22900] Train: [26/100][135/156] Data 0.001 (0.001) Batch 4.040 (3.471) Remain 11:09:02 loss: 0.4711 Lr: 0.04428
[2023-08-07 22:46:57,931 INFO misc.py line 115 22900] Train: [26/100][136/156] Data 0.001 (0.001) Batch 3.412 (3.471) Remain 11:08:54 loss: 0.4269 Lr: 0.04427
[2023-08-07 22:46:59,882 INFO misc.py line 115 22900] Train: [26/100][137/156] Data 0.001 (0.001) Batch 1.951 (3.459) Remain 11:06:39 loss: 0.5610 Lr: 0.04427
[2023-08-07 22:47:02,523 INFO misc.py line 115 22900] Train: [26/100][138/156] Data 0.001 (0.001) Batch 2.641 (3.453) Remain 11:05:26 loss: 0.6848 Lr: 0.04427
[2023-08-07 22:47:05,704 INFO misc.py line 115 22900] Train: [26/100][139/156] Data 0.001 (0.001) Batch 3.181 (3.451) Remain 11:04:59 loss: 0.4665 Lr: 0.04426
[2023-08-07 22:47:09,683 INFO misc.py line 115 22900] Train: [26/100][140/156] Data 0.001 (0.001) Batch 3.978 (3.455) Remain 11:05:40 loss: 0.5961 Lr: 0.04426
[2023-08-07 22:47:12,642 INFO misc.py line 115 22900] Train: [26/100][141/156] Data 0.001 (0.001) Batch 2.960 (3.451) Remain 11:04:55 loss: 0.3004 Lr: 0.04426
[2023-08-07 22:47:17,159 INFO misc.py line 115 22900] Train: [26/100][142/156] Data 0.001 (0.001) Batch 4.517 (3.459) Remain 11:06:20 loss: 0.9612 Lr: 0.04425
[2023-08-07 22:47:20,852 INFO misc.py line 115 22900] Train: [26/100][143/156] Data 0.001 (0.001) Batch 3.693 (3.461) Remain 11:06:36 loss: 0.5187 Lr: 0.04425
[2023-08-07 22:47:24,228 INFO misc.py line 115 22900] Train: [26/100][144/156] Data 0.001 (0.001) Batch 3.375 (3.460) Remain 11:06:26 loss: 0.3697 Lr: 0.04425
[2023-08-07 22:47:26,899 INFO misc.py line 115 22900] Train: [26/100][145/156] Data 0.001 (0.001) Batch 2.672 (3.455) Remain 11:05:18 loss: 0.3627 Lr: 0.04424
[2023-08-07 22:47:30,868 INFO misc.py line 115 22900] Train: [26/100][146/156] Data 0.001 (0.001) Batch 3.969 (3.458) Remain 11:05:56 loss: 0.6648 Lr: 0.04424
[2023-08-07 22:47:35,098 INFO misc.py line 115 22900] Train: [26/100][147/156] Data 0.001 (0.001) Batch 4.230 (3.464) Remain 11:06:55 loss: 0.6772 Lr: 0.04424
[2023-08-07 22:47:38,284 INFO misc.py line 115 22900] Train: [26/100][148/156] Data 0.001 (0.001) Batch 3.186 (3.462) Remain 11:06:29 loss: 0.5437 Lr: 0.04423
[2023-08-07 22:47:41,808 INFO misc.py line 115 22900] Train: [26/100][149/156] Data 0.001 (0.001) Batch 3.524 (3.462) Remain 11:06:30 loss: 0.4926 Lr: 0.04423
[2023-08-07 22:47:45,830 INFO misc.py line 115 22900] Train: [26/100][150/156] Data 0.001 (0.001) Batch 4.022 (3.466) Remain 11:07:11 loss: 0.4245 Lr: 0.04423
[2023-08-07 22:47:49,201 INFO misc.py line 115 22900] Train: [26/100][151/156] Data 0.001 (0.001) Batch 3.371 (3.465) Remain 11:07:00 loss: 0.4599 Lr: 0.04422
[2023-08-07 22:47:52,684 INFO misc.py line 115 22900] Train: [26/100][152/156] Data 0.001 (0.001) Batch 3.482 (3.465) Remain 11:06:58 loss: 0.5429 Lr: 0.04422
[2023-08-07 22:47:55,651 INFO misc.py line 115 22900] Train: [26/100][153/156] Data 0.001 (0.001) Batch 2.967 (3.462) Remain 11:06:16 loss: 0.5344 Lr: 0.04422
[2023-08-07 22:47:59,720 INFO misc.py line 115 22900] Train: [26/100][154/156] Data 0.001 (0.001) Batch 4.070 (3.466) Remain 11:06:59 loss: 0.7199 Lr: 0.04421
[2023-08-07 22:48:03,791 INFO misc.py line 115 22900] Train: [26/100][155/156] Data 0.001 (0.001) Batch 4.070 (3.470) Remain 11:07:42 loss: 0.6336 Lr: 0.04421
[2023-08-07 22:48:07,467 INFO misc.py line 115 22900] Train: [26/100][156/156] Data 0.001 (0.001) Batch 3.676 (3.471) Remain 11:07:54 loss: 0.3436 Lr: 0.04421
[2023-08-07 22:48:07,467 INFO misc.py line 129 22900] Train result: loss: 0.4755 
[2023-08-07 22:48:07,467 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 22:48:09,602 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.0674 
[2023-08-07 22:48:10,471 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.7551 
[2023-08-07 22:48:12,136 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.9544 
[2023-08-07 22:48:13,660 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.4013 
[2023-08-07 22:48:15,506 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.8118 
[2023-08-07 22:48:17,169 INFO evaluator.py line 122 22900] Test: [6/24] Loss 1.1117 
[2023-08-07 22:48:19,308 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.8203 
[2023-08-07 22:48:21,115 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.2417 
[2023-08-07 22:48:22,396 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.5639 
[2023-08-07 22:48:24,527 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.5134 
[2023-08-07 22:48:25,053 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.6220 
[2023-08-07 22:48:26,586 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.8762 
[2023-08-07 22:48:29,298 INFO evaluator.py line 122 22900] Test: [13/24] Loss 0.9787 
[2023-08-07 22:48:30,978 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.9249 
[2023-08-07 22:48:33,000 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.4553 
[2023-08-07 22:48:35,711 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.1267 
[2023-08-07 22:48:38,418 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.2531 
[2023-08-07 22:48:40,265 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.7334 
[2023-08-07 22:48:41,014 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.1571 
[2023-08-07 22:48:41,900 INFO evaluator.py line 122 22900] Test: [20/24] Loss 1.0003 
[2023-08-07 22:48:44,161 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.4950 
[2023-08-07 22:48:46,125 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.9344 
[2023-08-07 22:48:47,969 INFO evaluator.py line 122 22900] Test: [23/24] Loss 3.1089 
[2023-08-07 22:48:49,904 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.9375 
[2023-08-07 22:48:49,957 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.1686/0.2486/0.6483.
[2023-08-07 22:48:49,957 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.5874/0.9677
[2023-08-07 22:48:49,957 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9324/0.9892
[2023-08-07 22:48:49,957 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1150/0.2406
[2023-08-07 22:48:49,957 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.0421/0.0437
[2023-08-07 22:48:49,957 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.5071/0.6128
[2023-08-07 22:48:49,958 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.0946/0.0963
[2023-08-07 22:48:49,958 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.4521/0.5976
[2023-08-07 22:48:49,958 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0185/0.0196
[2023-08-07 22:48:49,958 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1350/0.4234
[2023-08-07 22:48:49,958 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0967/0.1107
[2023-08-07 22:48:49,958 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 22:48:49,958 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0041/0.0041
[2023-08-07 22:48:49,958 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1964/0.5422
[2023-08-07 22:48:49,958 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0080/0.0086
[2023-08-07 22:48:49,958 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0424/0.0582
[2023-08-07 22:48:49,958 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-07 22:48:49,958 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0167/0.0179
[2023-08-07 22:48:49,958 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.0817/0.1821
[2023-08-07 22:48:49,958 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 22:48:49,958 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0418/0.0563
[2023-08-07 22:48:49,958 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 22:48:49,958 INFO misc.py line 152 22900] Currently Best mIoU: 0.2147
[2023-08-07 22:48:49,958 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 22:48:55,682 INFO misc.py line 115 22900] Train: [27/100][1/156] Data 1.421 (1.421) Batch 4.948 (4.948) Remain 15:51:55 loss: 0.4003 Lr: 0.04420
[2023-08-07 22:48:59,717 INFO misc.py line 115 22900] Train: [27/100][2/156] Data 0.001 (0.001) Batch 4.034 (4.034) Remain 12:56:01 loss: 0.4721 Lr: 0.04420
[2023-08-07 22:49:02,071 INFO misc.py line 115 22900] Train: [27/100][3/156] Data 0.001 (0.001) Batch 2.355 (2.355) Remain 07:32:54 loss: 0.5362 Lr: 0.04420
[2023-08-07 22:49:04,721 INFO misc.py line 115 22900] Train: [27/100][4/156] Data 0.001 (0.001) Batch 2.650 (2.650) Remain 08:29:42 loss: 0.6029 Lr: 0.04419
[2023-08-07 22:49:08,615 INFO misc.py line 115 22900] Train: [27/100][5/156] Data 0.001 (0.001) Batch 3.894 (3.272) Remain 10:29:14 loss: 0.6551 Lr: 0.04419
[2023-08-07 22:49:12,126 INFO misc.py line 115 22900] Train: [27/100][6/156] Data 0.001 (0.001) Batch 3.511 (3.352) Remain 10:44:32 loss: 0.3482 Lr: 0.04419
[2023-08-07 22:49:15,851 INFO misc.py line 115 22900] Train: [27/100][7/156] Data 0.001 (0.001) Batch 3.724 (3.445) Remain 11:02:23 loss: 0.5531 Lr: 0.04418
[2023-08-07 22:49:19,099 INFO misc.py line 115 22900] Train: [27/100][8/156] Data 0.001 (0.001) Batch 3.248 (3.405) Remain 10:54:45 loss: 0.3696 Lr: 0.04418
[2023-08-07 22:49:22,497 INFO misc.py line 115 22900] Train: [27/100][9/156] Data 0.001 (0.001) Batch 3.398 (3.404) Remain 10:54:28 loss: 0.3545 Lr: 0.04418
[2023-08-07 22:49:26,510 INFO misc.py line 115 22900] Train: [27/100][10/156] Data 0.001 (0.001) Batch 4.013 (3.491) Remain 11:11:08 loss: 0.5854 Lr: 0.04417
[2023-08-07 22:49:30,494 INFO misc.py line 115 22900] Train: [27/100][11/156] Data 0.001 (0.001) Batch 3.984 (3.553) Remain 11:22:54 loss: 0.4967 Lr: 0.04417
[2023-08-07 22:49:34,506 INFO misc.py line 115 22900] Train: [27/100][12/156] Data 0.001 (0.001) Batch 4.012 (3.604) Remain 11:32:40 loss: 0.4892 Lr: 0.04417
[2023-08-07 22:49:39,066 INFO misc.py line 115 22900] Train: [27/100][13/156] Data 0.001 (0.001) Batch 4.560 (3.699) Remain 11:50:58 loss: 0.5707 Lr: 0.04416
[2023-08-07 22:49:42,531 INFO misc.py line 115 22900] Train: [27/100][14/156] Data 0.001 (0.001) Batch 3.465 (3.678) Remain 11:46:49 loss: 0.4315 Lr: 0.04416
[2023-08-07 22:49:47,020 INFO misc.py line 115 22900] Train: [27/100][15/156] Data 0.001 (0.001) Batch 4.489 (3.746) Remain 11:59:44 loss: 0.6069 Lr: 0.04416
[2023-08-07 22:49:50,745 INFO misc.py line 115 22900] Train: [27/100][16/156] Data 0.001 (0.001) Batch 3.724 (3.744) Remain 11:59:22 loss: 0.3125 Lr: 0.04415
[2023-08-07 22:49:54,765 INFO misc.py line 115 22900] Train: [27/100][17/156] Data 0.001 (0.001) Batch 4.020 (3.764) Remain 12:03:05 loss: 0.4394 Lr: 0.04415
[2023-08-07 22:49:58,070 INFO misc.py line 115 22900] Train: [27/100][18/156] Data 0.001 (0.001) Batch 3.305 (3.733) Remain 11:57:09 loss: 0.6660 Lr: 0.04415
[2023-08-07 22:50:02,344 INFO misc.py line 115 22900] Train: [27/100][19/156] Data 0.001 (0.001) Batch 4.273 (3.767) Remain 12:03:35 loss: 0.5840 Lr: 0.04414
[2023-08-07 22:50:04,838 INFO misc.py line 115 22900] Train: [27/100][20/156] Data 0.001 (0.001) Batch 2.494 (3.692) Remain 11:49:08 loss: 0.3941 Lr: 0.04414
[2023-08-07 22:50:08,885 INFO misc.py line 115 22900] Train: [27/100][21/156] Data 0.001 (0.001) Batch 4.048 (3.712) Remain 11:52:52 loss: 0.6624 Lr: 0.04414
[2023-08-07 22:50:12,466 INFO misc.py line 115 22900] Train: [27/100][22/156] Data 0.001 (0.001) Batch 3.581 (3.705) Remain 11:51:28 loss: 0.5814 Lr: 0.04413
[2023-08-07 22:50:16,422 INFO misc.py line 115 22900] Train: [27/100][23/156] Data 0.001 (0.001) Batch 3.955 (3.718) Remain 11:53:49 loss: 0.7217 Lr: 0.04413
[2023-08-07 22:50:18,804 INFO misc.py line 115 22900] Train: [27/100][24/156] Data 0.001 (0.001) Batch 2.383 (3.654) Remain 11:41:33 loss: 0.3842 Lr: 0.04413
[2023-08-07 22:50:20,861 INFO misc.py line 115 22900] Train: [27/100][25/156] Data 0.001 (0.001) Batch 2.057 (3.581) Remain 11:27:33 loss: 0.1881 Lr: 0.04412
[2023-08-07 22:50:24,124 INFO misc.py line 115 22900] Train: [27/100][26/156] Data 0.001 (0.001) Batch 3.263 (3.568) Remain 11:24:50 loss: 0.4448 Lr: 0.04412
[2023-08-07 22:50:26,924 INFO misc.py line 115 22900] Train: [27/100][27/156] Data 0.001 (0.001) Batch 2.800 (3.536) Remain 11:18:38 loss: 0.4325 Lr: 0.04411
[2023-08-07 22:50:30,558 INFO misc.py line 115 22900] Train: [27/100][28/156] Data 0.001 (0.001) Batch 3.634 (3.539) Remain 11:19:20 loss: 0.4474 Lr: 0.04411
[2023-08-07 22:50:34,073 INFO misc.py line 115 22900] Train: [27/100][29/156] Data 0.001 (0.001) Batch 3.514 (3.539) Remain 11:19:06 loss: 0.4567 Lr: 0.04411
[2023-08-07 22:50:37,772 INFO misc.py line 115 22900] Train: [27/100][30/156] Data 0.001 (0.001) Batch 3.699 (3.544) Remain 11:20:11 loss: 0.3520 Lr: 0.04410
[2023-08-07 22:50:40,467 INFO misc.py line 115 22900] Train: [27/100][31/156] Data 0.001 (0.001) Batch 2.695 (3.514) Remain 11:14:18 loss: 0.4269 Lr: 0.04410
[2023-08-07 22:50:43,979 INFO misc.py line 115 22900] Train: [27/100][32/156] Data 0.001 (0.001) Batch 3.512 (3.514) Remain 11:14:13 loss: 0.3121 Lr: 0.04410
[2023-08-07 22:50:48,053 INFO misc.py line 115 22900] Train: [27/100][33/156] Data 0.001 (0.001) Batch 4.075 (3.533) Remain 11:17:45 loss: 0.4108 Lr: 0.04409
[2023-08-07 22:50:51,700 INFO misc.py line 115 22900] Train: [27/100][34/156] Data 0.001 (0.001) Batch 3.647 (3.536) Remain 11:18:24 loss: 0.5635 Lr: 0.04409
[2023-08-07 22:50:54,273 INFO misc.py line 115 22900] Train: [27/100][35/156] Data 0.001 (0.001) Batch 2.572 (3.506) Remain 11:12:34 loss: 0.2688 Lr: 0.04409
[2023-08-07 22:50:58,232 INFO misc.py line 115 22900] Train: [27/100][36/156] Data 0.001 (0.001) Batch 3.959 (3.520) Remain 11:15:08 loss: 0.7114 Lr: 0.04408
[2023-08-07 22:51:01,657 INFO misc.py line 115 22900] Train: [27/100][37/156] Data 0.001 (0.001) Batch 3.424 (3.517) Remain 11:14:32 loss: 0.3275 Lr: 0.04408
[2023-08-07 22:51:04,222 INFO misc.py line 115 22900] Train: [27/100][38/156] Data 0.001 (0.001) Batch 2.566 (3.490) Remain 11:09:16 loss: 0.4130 Lr: 0.04408
[2023-08-07 22:51:08,205 INFO misc.py line 115 22900] Train: [27/100][39/156] Data 0.001 (0.001) Batch 3.983 (3.504) Remain 11:11:50 loss: 0.8754 Lr: 0.04407
[2023-08-07 22:51:10,112 INFO misc.py line 115 22900] Train: [27/100][40/156] Data 0.001 (0.001) Batch 1.907 (3.461) Remain 11:03:30 loss: 0.5209 Lr: 0.04407
[2023-08-07 22:51:13,945 INFO misc.py line 115 22900] Train: [27/100][41/156] Data 0.001 (0.001) Batch 3.833 (3.470) Remain 11:05:19 loss: 0.7138 Lr: 0.04407
[2023-08-07 22:51:17,841 INFO misc.py line 115 22900] Train: [27/100][42/156] Data 0.001 (0.001) Batch 3.896 (3.481) Remain 11:07:21 loss: 0.5021 Lr: 0.04406
[2023-08-07 22:51:20,790 INFO misc.py line 115 22900] Train: [27/100][43/156] Data 0.001 (0.001) Batch 2.948 (3.468) Remain 11:04:45 loss: 0.2044 Lr: 0.04406
[2023-08-07 22:51:24,960 INFO misc.py line 115 22900] Train: [27/100][44/156] Data 0.001 (0.001) Batch 4.171 (3.485) Remain 11:07:58 loss: 0.5783 Lr: 0.04406
[2023-08-07 22:51:27,656 INFO misc.py line 115 22900] Train: [27/100][45/156] Data 0.001 (0.001) Batch 2.695 (3.466) Remain 11:04:18 loss: 0.6108 Lr: 0.04405
[2023-08-07 22:51:31,786 INFO misc.py line 115 22900] Train: [27/100][46/156] Data 0.001 (0.001) Batch 4.130 (3.482) Remain 11:07:13 loss: 0.7111 Lr: 0.04405
[2023-08-07 22:51:35,477 INFO misc.py line 115 22900] Train: [27/100][47/156] Data 0.001 (0.001) Batch 3.691 (3.486) Remain 11:08:04 loss: 0.4044 Lr: 0.04405
[2023-08-07 22:51:39,422 INFO misc.py line 115 22900] Train: [27/100][48/156] Data 0.001 (0.001) Batch 3.945 (3.497) Remain 11:09:57 loss: 0.6786 Lr: 0.04404
[2023-08-07 22:51:43,479 INFO misc.py line 115 22900] Train: [27/100][49/156] Data 0.001 (0.001) Batch 4.057 (3.509) Remain 11:12:14 loss: 0.8725 Lr: 0.04404
[2023-08-07 22:51:46,909 INFO misc.py line 115 22900] Train: [27/100][50/156] Data 0.001 (0.001) Batch 3.430 (3.507) Remain 11:11:51 loss: 0.8907 Lr: 0.04404
[2023-08-07 22:51:50,935 INFO misc.py line 115 22900] Train: [27/100][51/156] Data 0.001 (0.001) Batch 4.026 (3.518) Remain 11:13:52 loss: 0.5101 Lr: 0.04403
[2023-08-07 22:51:53,763 INFO misc.py line 115 22900] Train: [27/100][52/156] Data 0.001 (0.001) Batch 2.829 (3.504) Remain 11:11:07 loss: 0.4494 Lr: 0.04403
[2023-08-07 22:51:57,032 INFO misc.py line 115 22900] Train: [27/100][53/156] Data 0.001 (0.001) Batch 3.269 (3.499) Remain 11:10:09 loss: 0.3888 Lr: 0.04403
[2023-08-07 22:52:01,179 INFO misc.py line 115 22900] Train: [27/100][54/156] Data 0.001 (0.001) Batch 4.147 (3.512) Remain 11:12:31 loss: 0.6389 Lr: 0.04402
[2023-08-07 22:52:04,386 INFO misc.py line 115 22900] Train: [27/100][55/156] Data 0.001 (0.001) Batch 3.207 (3.506) Remain 11:11:20 loss: 0.5869 Lr: 0.04402
[2023-08-07 22:52:07,991 INFO misc.py line 115 22900] Train: [27/100][56/156] Data 0.001 (0.001) Batch 3.606 (3.508) Remain 11:11:39 loss: 0.3985 Lr: 0.04402
[2023-08-07 22:52:11,098 INFO misc.py line 115 22900] Train: [27/100][57/156] Data 0.001 (0.001) Batch 3.107 (3.500) Remain 11:10:10 loss: 0.3373 Lr: 0.04401
[2023-08-07 22:52:14,469 INFO misc.py line 115 22900] Train: [27/100][58/156] Data 0.001 (0.001) Batch 3.371 (3.498) Remain 11:09:39 loss: 0.4426 Lr: 0.04401
[2023-08-07 22:52:18,510 INFO misc.py line 115 22900] Train: [27/100][59/156] Data 0.001 (0.001) Batch 4.042 (3.508) Remain 11:11:27 loss: 0.5922 Lr: 0.04401
[2023-08-07 22:52:22,623 INFO misc.py line 115 22900] Train: [27/100][60/156] Data 0.001 (0.001) Batch 4.113 (3.518) Remain 11:13:25 loss: 0.5508 Lr: 0.04400
[2023-08-07 22:52:26,650 INFO misc.py line 115 22900] Train: [27/100][61/156] Data 0.001 (0.001) Batch 4.027 (3.527) Remain 11:15:03 loss: 0.7995 Lr: 0.04400
[2023-08-07 22:52:30,724 INFO misc.py line 115 22900] Train: [27/100][62/156] Data 0.001 (0.001) Batch 4.074 (3.536) Remain 11:16:46 loss: 0.4644 Lr: 0.04399
[2023-08-07 22:52:33,385 INFO misc.py line 115 22900] Train: [27/100][63/156] Data 0.001 (0.001) Batch 2.661 (3.522) Remain 11:13:54 loss: 0.3859 Lr: 0.04399
[2023-08-07 22:52:36,212 INFO misc.py line 115 22900] Train: [27/100][64/156] Data 0.001 (0.001) Batch 2.827 (3.511) Remain 11:11:40 loss: 0.3274 Lr: 0.04399
[2023-08-07 22:52:39,785 INFO misc.py line 115 22900] Train: [27/100][65/156] Data 0.001 (0.001) Batch 3.572 (3.512) Remain 11:11:48 loss: 0.4148 Lr: 0.04398
[2023-08-07 22:52:42,670 INFO misc.py line 115 22900] Train: [27/100][66/156] Data 0.001 (0.001) Batch 2.885 (3.502) Remain 11:09:51 loss: 0.5686 Lr: 0.04398
[2023-08-07 22:52:46,089 INFO misc.py line 115 22900] Train: [27/100][67/156] Data 0.001 (0.001) Batch 3.418 (3.500) Remain 11:09:32 loss: 0.5312 Lr: 0.04398
[2023-08-07 22:52:49,697 INFO misc.py line 115 22900] Train: [27/100][68/156] Data 0.001 (0.001) Batch 3.608 (3.502) Remain 11:09:48 loss: 0.5641 Lr: 0.04397
[2023-08-07 22:52:52,860 INFO misc.py line 115 22900] Train: [27/100][69/156] Data 0.001 (0.001) Batch 3.163 (3.497) Remain 11:08:45 loss: 0.3544 Lr: 0.04397
[2023-08-07 22:52:56,609 INFO misc.py line 115 22900] Train: [27/100][70/156] Data 0.001 (0.001) Batch 3.749 (3.501) Remain 11:09:25 loss: 0.5559 Lr: 0.04397
[2023-08-07 22:52:59,532 INFO misc.py line 115 22900] Train: [27/100][71/156] Data 0.001 (0.001) Batch 2.924 (3.492) Remain 11:07:44 loss: 0.1256 Lr: 0.04396
[2023-08-07 22:53:02,682 INFO misc.py line 115 22900] Train: [27/100][72/156] Data 0.001 (0.001) Batch 3.149 (3.487) Remain 11:06:44 loss: 0.5064 Lr: 0.04396
[2023-08-07 22:53:06,299 INFO misc.py line 115 22900] Train: [27/100][73/156] Data 0.001 (0.001) Batch 3.617 (3.489) Remain 11:07:01 loss: 0.5421 Lr: 0.04396
[2023-08-07 22:53:10,359 INFO misc.py line 115 22900] Train: [27/100][74/156] Data 0.001 (0.001) Batch 4.059 (3.497) Remain 11:08:30 loss: 0.4650 Lr: 0.04395
[2023-08-07 22:53:13,060 INFO misc.py line 115 22900] Train: [27/100][75/156] Data 0.001 (0.001) Batch 2.702 (3.486) Remain 11:06:20 loss: 0.4373 Lr: 0.04395
[2023-08-07 22:53:16,680 INFO misc.py line 115 22900] Train: [27/100][76/156] Data 0.001 (0.001) Batch 3.619 (3.488) Remain 11:06:37 loss: 0.6429 Lr: 0.04395
[2023-08-07 22:53:19,877 INFO misc.py line 115 22900] Train: [27/100][77/156] Data 0.001 (0.001) Batch 3.198 (3.484) Remain 11:05:49 loss: 0.4587 Lr: 0.04394
[2023-08-07 22:53:23,702 INFO misc.py line 115 22900] Train: [27/100][78/156] Data 0.001 (0.001) Batch 3.825 (3.488) Remain 11:06:38 loss: 0.5102 Lr: 0.04394
[2023-08-07 22:53:26,971 INFO misc.py line 115 22900] Train: [27/100][79/156] Data 0.001 (0.001) Batch 3.269 (3.486) Remain 11:06:01 loss: 0.6264 Lr: 0.04394
[2023-08-07 22:53:29,493 INFO misc.py line 115 22900] Train: [27/100][80/156] Data 0.001 (0.001) Batch 2.522 (3.473) Remain 11:03:34 loss: 0.4767 Lr: 0.04393
[2023-08-07 22:53:32,594 INFO misc.py line 115 22900] Train: [27/100][81/156] Data 0.001 (0.001) Batch 3.101 (3.468) Remain 11:02:36 loss: 0.4748 Lr: 0.04393
[2023-08-07 22:53:35,835 INFO misc.py line 115 22900] Train: [27/100][82/156] Data 0.001 (0.001) Batch 3.241 (3.465) Remain 11:02:00 loss: 0.4051 Lr: 0.04393
[2023-08-07 22:53:39,343 INFO misc.py line 115 22900] Train: [27/100][83/156] Data 0.001 (0.001) Batch 3.508 (3.466) Remain 11:02:02 loss: 0.6025 Lr: 0.04392
[2023-08-07 22:53:42,241 INFO misc.py line 115 22900] Train: [27/100][84/156] Data 0.001 (0.001) Batch 2.898 (3.459) Remain 11:00:38 loss: 0.3209 Lr: 0.04392
[2023-08-07 22:53:46,348 INFO misc.py line 115 22900] Train: [27/100][85/156] Data 0.001 (0.001) Batch 4.107 (3.467) Remain 11:02:06 loss: 0.4323 Lr: 0.04392
[2023-08-07 22:53:50,726 INFO misc.py line 115 22900] Train: [27/100][86/156] Data 0.001 (0.001) Batch 4.378 (3.478) Remain 11:04:08 loss: 0.6532 Lr: 0.04391
[2023-08-07 22:53:54,551 INFO misc.py line 115 22900] Train: [27/100][87/156] Data 0.001 (0.001) Batch 3.825 (3.482) Remain 11:04:52 loss: 0.3837 Lr: 0.04391
[2023-08-07 22:53:56,887 INFO misc.py line 115 22900] Train: [27/100][88/156] Data 0.001 (0.001) Batch 2.336 (3.468) Remain 11:02:14 loss: 0.3273 Lr: 0.04390
[2023-08-07 22:54:00,660 INFO misc.py line 115 22900] Train: [27/100][89/156] Data 0.001 (0.001) Batch 3.772 (3.472) Remain 11:02:51 loss: 0.4193 Lr: 0.04390
[2023-08-07 22:54:03,809 INFO misc.py line 115 22900] Train: [27/100][90/156] Data 0.001 (0.001) Batch 3.150 (3.468) Remain 11:02:05 loss: 0.3934 Lr: 0.04390
[2023-08-07 22:54:07,774 INFO misc.py line 115 22900] Train: [27/100][91/156] Data 0.001 (0.001) Batch 3.965 (3.474) Remain 11:03:06 loss: 0.7085 Lr: 0.04389
[2023-08-07 22:54:10,832 INFO misc.py line 115 22900] Train: [27/100][92/156] Data 0.001 (0.001) Batch 3.058 (3.469) Remain 11:02:09 loss: 0.4043 Lr: 0.04389
[2023-08-07 22:54:14,715 INFO misc.py line 115 22900] Train: [27/100][93/156] Data 0.001 (0.001) Batch 3.883 (3.474) Remain 11:02:58 loss: 0.4795 Lr: 0.04389
[2023-08-07 22:54:19,120 INFO misc.py line 115 22900] Train: [27/100][94/156] Data 0.001 (0.001) Batch 4.405 (3.484) Remain 11:04:52 loss: 0.7512 Lr: 0.04388
[2023-08-07 22:54:22,279 INFO misc.py line 115 22900] Train: [27/100][95/156] Data 0.001 (0.001) Batch 3.159 (3.481) Remain 11:04:08 loss: 0.4094 Lr: 0.04388
[2023-08-07 22:54:26,239 INFO misc.py line 115 22900] Train: [27/100][96/156] Data 0.001 (0.001) Batch 3.960 (3.486) Remain 11:05:04 loss: 0.5372 Lr: 0.04388
[2023-08-07 22:54:29,749 INFO misc.py line 115 22900] Train: [27/100][97/156] Data 0.001 (0.001) Batch 3.510 (3.486) Remain 11:05:03 loss: 0.5239 Lr: 0.04387
[2023-08-07 22:54:33,713 INFO misc.py line 115 22900] Train: [27/100][98/156] Data 0.001 (0.001) Batch 3.964 (3.491) Remain 11:05:57 loss: 0.3217 Lr: 0.04387
[2023-08-07 22:54:36,290 INFO misc.py line 115 22900] Train: [27/100][99/156] Data 0.001 (0.001) Batch 2.576 (3.481) Remain 11:04:05 loss: 0.3997 Lr: 0.04387
[2023-08-07 22:54:40,031 INFO misc.py line 115 22900] Train: [27/100][100/156] Data 0.001 (0.001) Batch 3.741 (3.484) Remain 11:04:32 loss: 0.5320 Lr: 0.04386
[2023-08-07 22:54:43,720 INFO misc.py line 115 22900] Train: [27/100][101/156] Data 0.001 (0.001) Batch 3.690 (3.486) Remain 11:04:52 loss: 0.4549 Lr: 0.04386
[2023-08-07 22:54:46,694 INFO misc.py line 115 22900] Train: [27/100][102/156] Data 0.001 (0.001) Batch 2.974 (3.481) Remain 11:03:50 loss: 0.1496 Lr: 0.04386
[2023-08-07 22:54:50,759 INFO misc.py line 115 22900] Train: [27/100][103/156] Data 0.001 (0.001) Batch 4.065 (3.487) Remain 11:04:53 loss: 1.0286 Lr: 0.04385
[2023-08-07 22:54:54,895 INFO misc.py line 115 22900] Train: [27/100][104/156] Data 0.001 (0.001) Batch 4.137 (3.493) Remain 11:06:03 loss: 0.4314 Lr: 0.04385
[2023-08-07 22:54:58,837 INFO misc.py line 115 22900] Train: [27/100][105/156] Data 0.001 (0.001) Batch 3.942 (3.498) Remain 11:06:50 loss: 0.5155 Lr: 0.04385
[2023-08-07 22:55:02,559 INFO misc.py line 115 22900] Train: [27/100][106/156] Data 0.001 (0.001) Batch 3.722 (3.500) Remain 11:07:11 loss: 0.5806 Lr: 0.04384
[2023-08-07 22:55:06,106 INFO misc.py line 115 22900] Train: [27/100][107/156] Data 0.001 (0.001) Batch 3.547 (3.500) Remain 11:07:13 loss: 0.3348 Lr: 0.04384
[2023-08-07 22:55:09,766 INFO misc.py line 115 22900] Train: [27/100][108/156] Data 0.001 (0.001) Batch 3.660 (3.502) Remain 11:07:27 loss: 0.4561 Lr: 0.04384
[2023-08-07 22:55:13,299 INFO misc.py line 115 22900] Train: [27/100][109/156] Data 0.001 (0.001) Batch 3.533 (3.502) Remain 11:07:27 loss: 0.2574 Lr: 0.04383
[2023-08-07 22:55:17,049 INFO misc.py line 115 22900] Train: [27/100][110/156] Data 0.001 (0.001) Batch 3.750 (3.504) Remain 11:07:50 loss: 0.4840 Lr: 0.04383
[2023-08-07 22:55:19,127 INFO misc.py line 115 22900] Train: [27/100][111/156] Data 0.001 (0.001) Batch 2.078 (3.491) Remain 11:05:15 loss: 0.3088 Lr: 0.04382
[2023-08-07 22:55:22,881 INFO misc.py line 115 22900] Train: [27/100][112/156] Data 0.001 (0.001) Batch 3.755 (3.494) Remain 11:05:39 loss: 0.5045 Lr: 0.04382
[2023-08-07 22:55:26,668 INFO misc.py line 115 22900] Train: [27/100][113/156] Data 0.001 (0.001) Batch 3.786 (3.496) Remain 11:06:06 loss: 0.4842 Lr: 0.04382
[2023-08-07 22:55:30,674 INFO misc.py line 115 22900] Train: [27/100][114/156] Data 0.001 (0.001) Batch 4.006 (3.501) Remain 11:06:55 loss: 0.5182 Lr: 0.04381
[2023-08-07 22:55:33,488 INFO misc.py line 115 22900] Train: [27/100][115/156] Data 0.001 (0.001) Batch 2.815 (3.495) Remain 11:05:42 loss: 0.3253 Lr: 0.04381
[2023-08-07 22:55:37,865 INFO misc.py line 115 22900] Train: [27/100][116/156] Data 0.001 (0.001) Batch 4.377 (3.503) Remain 11:07:07 loss: 0.7346 Lr: 0.04381
[2023-08-07 22:55:41,823 INFO misc.py line 115 22900] Train: [27/100][117/156] Data 0.001 (0.001) Batch 3.958 (3.507) Remain 11:07:49 loss: 0.3649 Lr: 0.04380
[2023-08-07 22:55:44,376 INFO misc.py line 115 22900] Train: [27/100][118/156] Data 0.001 (0.001) Batch 2.553 (3.498) Remain 11:06:11 loss: 0.4181 Lr: 0.04380
[2023-08-07 22:55:46,677 INFO misc.py line 115 22900] Train: [27/100][119/156] Data 0.001 (0.001) Batch 2.301 (3.488) Remain 11:04:10 loss: 0.3030 Lr: 0.04380
[2023-08-07 22:55:48,972 INFO misc.py line 115 22900] Train: [27/100][120/156] Data 0.001 (0.001) Batch 2.296 (3.478) Remain 11:02:10 loss: 0.2625 Lr: 0.04379
[2023-08-07 22:55:51,929 INFO misc.py line 115 22900] Train: [27/100][121/156] Data 0.001 (0.001) Batch 2.957 (3.473) Remain 11:01:16 loss: 0.4685 Lr: 0.04379
[2023-08-07 22:55:55,276 INFO misc.py line 115 22900] Train: [27/100][122/156] Data 0.001 (0.001) Batch 3.346 (3.472) Remain 11:01:00 loss: 0.3911 Lr: 0.04379
[2023-08-07 22:55:59,269 INFO misc.py line 115 22900] Train: [27/100][123/156] Data 0.001 (0.001) Batch 3.994 (3.477) Remain 11:01:46 loss: 0.6484 Lr: 0.04378
[2023-08-07 22:56:02,984 INFO misc.py line 115 22900] Train: [27/100][124/156] Data 0.001 (0.001) Batch 3.715 (3.479) Remain 11:02:05 loss: 0.8232 Lr: 0.04378
[2023-08-07 22:56:05,704 INFO misc.py line 115 22900] Train: [27/100][125/156] Data 0.001 (0.001) Batch 2.720 (3.472) Remain 11:00:51 loss: 0.2571 Lr: 0.04378
[2023-08-07 22:56:08,294 INFO misc.py line 115 22900] Train: [27/100][126/156] Data 0.001 (0.001) Batch 2.590 (3.465) Remain 10:59:25 loss: 0.5170 Lr: 0.04377
[2023-08-07 22:56:11,358 INFO misc.py line 115 22900] Train: [27/100][127/156] Data 0.001 (0.001) Batch 3.064 (3.462) Remain 10:58:45 loss: 0.2792 Lr: 0.04377
[2023-08-07 22:56:15,396 INFO misc.py line 115 22900] Train: [27/100][128/156] Data 0.001 (0.001) Batch 4.038 (3.467) Remain 10:59:34 loss: 0.6979 Lr: 0.04377
[2023-08-07 22:56:18,798 INFO misc.py line 115 22900] Train: [27/100][129/156] Data 0.001 (0.001) Batch 3.402 (3.466) Remain 10:59:25 loss: 0.4464 Lr: 0.04376
[2023-08-07 22:56:21,330 INFO misc.py line 115 22900] Train: [27/100][130/156] Data 0.001 (0.001) Batch 2.533 (3.459) Remain 10:57:57 loss: 0.3267 Lr: 0.04376
[2023-08-07 22:56:24,811 INFO misc.py line 115 22900] Train: [27/100][131/156] Data 0.001 (0.001) Batch 3.480 (3.459) Remain 10:57:56 loss: 0.4627 Lr: 0.04375
[2023-08-07 22:56:28,210 INFO misc.py line 115 22900] Train: [27/100][132/156] Data 0.001 (0.001) Batch 3.399 (3.458) Remain 10:57:47 loss: 0.3906 Lr: 0.04375
[2023-08-07 22:56:31,851 INFO misc.py line 115 22900] Train: [27/100][133/156] Data 0.001 (0.001) Batch 3.641 (3.460) Remain 10:58:00 loss: 0.2702 Lr: 0.04375
[2023-08-07 22:56:35,332 INFO misc.py line 115 22900] Train: [27/100][134/156] Data 0.001 (0.001) Batch 3.480 (3.460) Remain 10:57:58 loss: 0.4032 Lr: 0.04374
[2023-08-07 22:56:38,290 INFO misc.py line 115 22900] Train: [27/100][135/156] Data 0.001 (0.001) Batch 2.958 (3.456) Remain 10:57:11 loss: 0.5181 Lr: 0.04374
[2023-08-07 22:56:41,626 INFO misc.py line 115 22900] Train: [27/100][136/156] Data 0.001 (0.001) Batch 3.336 (3.455) Remain 10:56:58 loss: 0.4460 Lr: 0.04374
[2023-08-07 22:56:44,596 INFO misc.py line 115 22900] Train: [27/100][137/156] Data 0.001 (0.001) Batch 2.970 (3.452) Remain 10:56:13 loss: 0.5670 Lr: 0.04373
[2023-08-07 22:56:48,085 INFO misc.py line 115 22900] Train: [27/100][138/156] Data 0.001 (0.001) Batch 3.489 (3.452) Remain 10:56:12 loss: 0.5374 Lr: 0.04373
[2023-08-07 22:56:51,366 INFO misc.py line 115 22900] Train: [27/100][139/156] Data 0.001 (0.001) Batch 3.281 (3.451) Remain 10:55:55 loss: 0.6948 Lr: 0.04373
[2023-08-07 22:56:54,256 INFO misc.py line 115 22900] Train: [27/100][140/156] Data 0.001 (0.001) Batch 2.890 (3.447) Remain 10:55:05 loss: 0.3891 Lr: 0.04372
[2023-08-07 22:56:58,305 INFO misc.py line 115 22900] Train: [27/100][141/156] Data 0.001 (0.001) Batch 4.049 (3.451) Remain 10:55:51 loss: 0.8607 Lr: 0.04372
[2023-08-07 22:57:02,364 INFO misc.py line 115 22900] Train: [27/100][142/156] Data 0.001 (0.001) Batch 4.059 (3.455) Remain 10:56:37 loss: 0.3847 Lr: 0.04372
[2023-08-07 22:57:05,539 INFO misc.py line 115 22900] Train: [27/100][143/156] Data 0.001 (0.001) Batch 3.175 (3.453) Remain 10:56:11 loss: 0.4135 Lr: 0.04371
[2023-08-07 22:57:09,662 INFO misc.py line 115 22900] Train: [27/100][144/156] Data 0.001 (0.001) Batch 4.123 (3.458) Remain 10:57:02 loss: 0.5314 Lr: 0.04371
[2023-08-07 22:57:12,580 INFO misc.py line 115 22900] Train: [27/100][145/156] Data 0.001 (0.001) Batch 2.918 (3.454) Remain 10:56:15 loss: 0.3123 Lr: 0.04371
[2023-08-07 22:57:15,988 INFO misc.py line 115 22900] Train: [27/100][146/156] Data 0.001 (0.001) Batch 3.409 (3.454) Remain 10:56:08 loss: 0.4522 Lr: 0.04370
[2023-08-07 22:57:18,920 INFO misc.py line 115 22900] Train: [27/100][147/156] Data 0.001 (0.001) Batch 2.931 (3.450) Remain 10:55:23 loss: 0.4464 Lr: 0.04370
[2023-08-07 22:57:22,350 INFO misc.py line 115 22900] Train: [27/100][148/156] Data 0.001 (0.001) Batch 3.430 (3.450) Remain 10:55:18 loss: 0.6799 Lr: 0.04370
[2023-08-07 22:57:25,824 INFO misc.py line 115 22900] Train: [27/100][149/156] Data 0.001 (0.001) Batch 3.474 (3.450) Remain 10:55:16 loss: 0.6491 Lr: 0.04369
[2023-08-07 22:57:30,224 INFO misc.py line 115 22900] Train: [27/100][150/156] Data 0.001 (0.001) Batch 4.400 (3.457) Remain 10:56:27 loss: 0.4559 Lr: 0.04369
[2023-08-07 22:57:34,232 INFO misc.py line 115 22900] Train: [27/100][151/156] Data 0.001 (0.001) Batch 4.008 (3.461) Remain 10:57:05 loss: 0.8774 Lr: 0.04368
[2023-08-07 22:57:38,102 INFO misc.py line 115 22900] Train: [27/100][152/156] Data 0.001 (0.001) Batch 3.870 (3.463) Remain 10:57:33 loss: 0.4607 Lr: 0.04368
[2023-08-07 22:57:40,188 INFO misc.py line 115 22900] Train: [27/100][153/156] Data 0.001 (0.001) Batch 2.087 (3.454) Remain 10:55:45 loss: 0.2544 Lr: 0.04368
[2023-08-07 22:57:43,610 INFO misc.py line 115 22900] Train: [27/100][154/156] Data 0.001 (0.001) Batch 3.422 (3.454) Remain 10:55:39 loss: 0.5211 Lr: 0.04367
[2023-08-07 22:57:46,201 INFO misc.py line 115 22900] Train: [27/100][155/156] Data 0.001 (0.001) Batch 2.591 (3.448) Remain 10:54:31 loss: 0.3435 Lr: 0.04367
[2023-08-07 22:57:49,518 INFO misc.py line 115 22900] Train: [27/100][156/156] Data 0.001 (0.001) Batch 3.317 (3.447) Remain 10:54:18 loss: 0.3717 Lr: 0.04367
[2023-08-07 22:57:49,518 INFO misc.py line 129 22900] Train result: loss: 0.4880 
[2023-08-07 22:57:49,518 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 22:57:51,650 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.1064 
[2023-08-07 22:57:52,521 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.5490 
[2023-08-07 22:57:54,185 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.7530 
[2023-08-07 22:57:55,706 INFO evaluator.py line 122 22900] Test: [4/24] Loss 0.9964 
[2023-08-07 22:57:57,550 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.8344 
[2023-08-07 22:57:59,214 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.5879 
[2023-08-07 22:58:01,354 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.0048 
[2023-08-07 22:58:03,157 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.2471 
[2023-08-07 22:58:04,443 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.0407 
[2023-08-07 22:58:06,574 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3285 
[2023-08-07 22:58:07,100 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.1487 
[2023-08-07 22:58:08,633 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.7521 
[2023-08-07 22:58:11,343 INFO evaluator.py line 122 22900] Test: [13/24] Loss 0.8952 
[2023-08-07 22:58:13,023 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.6541 
[2023-08-07 22:58:15,049 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.4266 
[2023-08-07 22:58:17,758 INFO evaluator.py line 122 22900] Test: [16/24] Loss 0.9105 
[2023-08-07 22:58:20,465 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.2261 
[2023-08-07 22:58:22,313 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.7111 
[2023-08-07 22:58:23,062 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.1051 
[2023-08-07 22:58:23,948 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.8367 
[2023-08-07 22:58:26,208 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3251 
[2023-08-07 22:58:28,174 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.9111 
[2023-08-07 22:58:30,019 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.5958 
[2023-08-07 22:58:31,955 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.9216 
[2023-08-07 22:58:32,006 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2038/0.2971/0.6746.
[2023-08-07 22:58:32,006 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6353/0.9604
[2023-08-07 22:58:32,006 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9518/0.9886
[2023-08-07 22:58:32,006 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1383/0.2889
[2023-08-07 22:58:32,006 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1232/0.1690
[2023-08-07 22:58:32,006 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6019/0.8525
[2023-08-07 22:58:32,006 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.1757/0.1839
[2023-08-07 22:58:32,006 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5041/0.5491
[2023-08-07 22:58:32,006 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0400/0.0426
[2023-08-07 22:58:32,006 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1637/0.3951
[2023-08-07 22:58:32,006 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0250/0.0250
[2023-08-07 22:58:32,006 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 22:58:32,006 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0239/0.0246
[2023-08-07 22:58:32,006 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.2367/0.5629
[2023-08-07 22:58:32,006 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0032/0.0032
[2023-08-07 22:58:32,006 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0282/0.0299
[2023-08-07 22:58:32,006 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-07 22:58:32,006 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0351/0.0367
[2023-08-07 22:58:32,006 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.3094/0.6045
[2023-08-07 22:58:32,006 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 22:58:32,006 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0814/0.2240
[2023-08-07 22:58:32,007 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 22:58:32,007 INFO misc.py line 152 22900] Currently Best mIoU: 0.2147
[2023-08-07 22:58:32,007 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 22:58:38,433 INFO misc.py line 115 22900] Train: [28/100][1/156] Data 1.227 (1.227) Batch 5.639 (5.639) Remain 17:50:10 loss: 0.5304 Lr: 0.04366
[2023-08-07 22:58:42,220 INFO misc.py line 115 22900] Train: [28/100][2/156] Data 0.001 (0.001) Batch 3.787 (3.787) Remain 11:58:42 loss: 0.3510 Lr: 0.04366
[2023-08-07 22:58:46,772 INFO misc.py line 115 22900] Train: [28/100][3/156] Data 0.001 (0.001) Batch 4.552 (4.552) Remain 14:23:46 loss: 0.6477 Lr: 0.04366
[2023-08-07 22:58:50,369 INFO misc.py line 115 22900] Train: [28/100][4/156] Data 0.001 (0.001) Batch 3.597 (3.597) Remain 11:22:27 loss: 0.7172 Lr: 0.04365
[2023-08-07 22:58:53,856 INFO misc.py line 115 22900] Train: [28/100][5/156] Data 0.001 (0.001) Batch 3.487 (3.542) Remain 11:11:58 loss: 0.5422 Lr: 0.04365
[2023-08-07 22:58:57,970 INFO misc.py line 115 22900] Train: [28/100][6/156] Data 0.001 (0.001) Batch 4.114 (3.733) Remain 11:48:05 loss: 0.3611 Lr: 0.04365
[2023-08-07 22:59:01,043 INFO misc.py line 115 22900] Train: [28/100][7/156] Data 0.001 (0.001) Batch 3.072 (3.568) Remain 11:16:43 loss: 0.1528 Lr: 0.04364
[2023-08-07 22:59:05,097 INFO misc.py line 115 22900] Train: [28/100][8/156] Data 0.002 (0.001) Batch 4.054 (3.665) Remain 11:35:07 loss: 0.3333 Lr: 0.04364
[2023-08-07 22:59:09,142 INFO misc.py line 115 22900] Train: [28/100][9/156] Data 0.001 (0.001) Batch 4.044 (3.728) Remain 11:47:03 loss: 0.5993 Lr: 0.04364
[2023-08-07 22:59:12,624 INFO misc.py line 115 22900] Train: [28/100][10/156] Data 0.001 (0.001) Batch 3.482 (3.693) Remain 11:40:19 loss: 0.6537 Lr: 0.04363
[2023-08-07 22:59:15,173 INFO misc.py line 115 22900] Train: [28/100][11/156] Data 0.001 (0.001) Batch 2.549 (3.550) Remain 11:13:08 loss: 0.5642 Lr: 0.04363
[2023-08-07 22:59:19,150 INFO misc.py line 115 22900] Train: [28/100][12/156] Data 0.001 (0.001) Batch 3.978 (3.598) Remain 11:22:05 loss: 0.5016 Lr: 0.04362
[2023-08-07 22:59:21,629 INFO misc.py line 115 22900] Train: [28/100][13/156] Data 0.001 (0.001) Batch 2.479 (3.486) Remain 11:00:49 loss: 0.4513 Lr: 0.04362
[2023-08-07 22:59:24,587 INFO misc.py line 115 22900] Train: [28/100][14/156] Data 0.001 (0.001) Batch 2.958 (3.438) Remain 10:51:40 loss: 0.3501 Lr: 0.04362
[2023-08-07 22:59:28,729 INFO misc.py line 115 22900] Train: [28/100][15/156] Data 0.001 (0.001) Batch 4.143 (3.496) Remain 11:02:44 loss: 0.4241 Lr: 0.04361
[2023-08-07 22:59:32,982 INFO misc.py line 115 22900] Train: [28/100][16/156] Data 0.001 (0.001) Batch 4.253 (3.555) Remain 11:13:42 loss: 0.6247 Lr: 0.04361
[2023-08-07 22:59:36,031 INFO misc.py line 115 22900] Train: [28/100][17/156] Data 0.001 (0.001) Batch 3.049 (3.518) Remain 11:06:48 loss: 0.5158 Lr: 0.04361
[2023-08-07 22:59:38,685 INFO misc.py line 115 22900] Train: [28/100][18/156] Data 0.001 (0.001) Batch 2.653 (3.461) Remain 10:55:49 loss: 0.4027 Lr: 0.04360
[2023-08-07 22:59:42,299 INFO misc.py line 115 22900] Train: [28/100][19/156] Data 0.001 (0.001) Batch 3.615 (3.470) Remain 10:57:35 loss: 0.6018 Lr: 0.04360
[2023-08-07 22:59:47,021 INFO misc.py line 115 22900] Train: [28/100][20/156] Data 0.001 (0.001) Batch 4.722 (3.544) Remain 11:11:28 loss: 0.8948 Lr: 0.04360
[2023-08-07 22:59:50,338 INFO misc.py line 115 22900] Train: [28/100][21/156] Data 0.001 (0.001) Batch 3.317 (3.531) Remain 11:09:01 loss: 0.5806 Lr: 0.04359
[2023-08-07 22:59:54,336 INFO misc.py line 115 22900] Train: [28/100][22/156] Data 0.001 (0.001) Batch 3.998 (3.556) Remain 11:13:37 loss: 0.6068 Lr: 0.04359
[2023-08-07 22:59:57,960 INFO misc.py line 115 22900] Train: [28/100][23/156] Data 0.001 (0.001) Batch 3.624 (3.559) Remain 11:14:12 loss: 0.3650 Lr: 0.04359
[2023-08-07 23:00:01,940 INFO misc.py line 115 22900] Train: [28/100][24/156] Data 0.001 (0.001) Batch 3.980 (3.579) Remain 11:17:56 loss: 0.4291 Lr: 0.04358
[2023-08-07 23:00:05,233 INFO misc.py line 115 22900] Train: [28/100][25/156] Data 0.001 (0.001) Batch 3.293 (3.566) Remain 11:15:24 loss: 0.4445 Lr: 0.04358
[2023-08-07 23:00:09,362 INFO misc.py line 115 22900] Train: [28/100][26/156] Data 0.001 (0.001) Batch 4.128 (3.591) Remain 11:19:59 loss: 0.3781 Lr: 0.04358
[2023-08-07 23:00:12,712 INFO misc.py line 115 22900] Train: [28/100][27/156] Data 0.001 (0.001) Batch 3.350 (3.581) Remain 11:18:01 loss: 0.3099 Lr: 0.04357
[2023-08-07 23:00:15,526 INFO misc.py line 115 22900] Train: [28/100][28/156] Data 0.001 (0.001) Batch 2.814 (3.550) Remain 11:12:09 loss: 0.2231 Lr: 0.04357
[2023-08-07 23:00:19,027 INFO misc.py line 115 22900] Train: [28/100][29/156] Data 0.001 (0.001) Batch 3.500 (3.548) Remain 11:11:44 loss: 0.6768 Lr: 0.04356
[2023-08-07 23:00:22,563 INFO misc.py line 115 22900] Train: [28/100][30/156] Data 0.001 (0.001) Batch 3.536 (3.548) Remain 11:11:35 loss: 0.4436 Lr: 0.04356
[2023-08-07 23:00:26,684 INFO misc.py line 115 22900] Train: [28/100][31/156] Data 0.001 (0.001) Batch 4.121 (3.568) Remain 11:15:24 loss: 0.4941 Lr: 0.04356
[2023-08-07 23:00:29,981 INFO misc.py line 115 22900] Train: [28/100][32/156] Data 0.001 (0.001) Batch 3.297 (3.559) Remain 11:13:34 loss: 0.5150 Lr: 0.04355
[2023-08-07 23:00:34,025 INFO misc.py line 115 22900] Train: [28/100][33/156] Data 0.001 (0.001) Batch 4.044 (3.575) Remain 11:16:35 loss: 0.3043 Lr: 0.04355
[2023-08-07 23:00:37,309 INFO misc.py line 115 22900] Train: [28/100][34/156] Data 0.001 (0.001) Batch 3.284 (3.566) Remain 11:14:44 loss: 0.2590 Lr: 0.04355
[2023-08-07 23:00:41,528 INFO misc.py line 115 22900] Train: [28/100][35/156] Data 0.001 (0.001) Batch 4.219 (3.586) Remain 11:18:33 loss: 0.4743 Lr: 0.04354
[2023-08-07 23:00:44,282 INFO misc.py line 115 22900] Train: [28/100][36/156] Data 0.001 (0.001) Batch 2.754 (3.561) Remain 11:13:43 loss: 0.3692 Lr: 0.04354
[2023-08-07 23:00:47,396 INFO misc.py line 115 22900] Train: [28/100][37/156] Data 0.001 (0.001) Batch 3.114 (3.548) Remain 11:11:10 loss: 0.6138 Lr: 0.04354
[2023-08-07 23:00:51,022 INFO misc.py line 115 22900] Train: [28/100][38/156] Data 0.001 (0.001) Batch 3.626 (3.550) Remain 11:11:32 loss: 0.3987 Lr: 0.04353
[2023-08-07 23:00:55,025 INFO misc.py line 115 22900] Train: [28/100][39/156] Data 0.001 (0.001) Batch 4.003 (3.563) Remain 11:13:51 loss: 0.5330 Lr: 0.04353
[2023-08-07 23:00:58,610 INFO misc.py line 115 22900] Train: [28/100][40/156] Data 0.001 (0.001) Batch 3.584 (3.563) Remain 11:13:54 loss: 0.3017 Lr: 0.04353
[2023-08-07 23:01:03,075 INFO misc.py line 115 22900] Train: [28/100][41/156] Data 0.001 (0.001) Batch 4.465 (3.587) Remain 11:18:20 loss: 0.6951 Lr: 0.04352
[2023-08-07 23:01:06,461 INFO misc.py line 115 22900] Train: [28/100][42/156] Data 0.001 (0.001) Batch 3.386 (3.582) Remain 11:17:18 loss: 0.5635 Lr: 0.04352
[2023-08-07 23:01:08,484 INFO misc.py line 115 22900] Train: [28/100][43/156] Data 0.001 (0.001) Batch 2.023 (3.543) Remain 11:09:52 loss: 0.5172 Lr: 0.04351
[2023-08-07 23:01:11,907 INFO misc.py line 115 22900] Train: [28/100][44/156] Data 0.001 (0.001) Batch 3.424 (3.540) Remain 11:09:16 loss: 0.5133 Lr: 0.04351
[2023-08-07 23:01:13,909 INFO misc.py line 115 22900] Train: [28/100][45/156] Data 0.001 (0.001) Batch 2.002 (3.503) Remain 11:02:17 loss: 0.4384 Lr: 0.04351
[2023-08-07 23:01:17,841 INFO misc.py line 115 22900] Train: [28/100][46/156] Data 0.001 (0.001) Batch 3.932 (3.513) Remain 11:04:06 loss: 0.4027 Lr: 0.04350
[2023-08-07 23:01:21,591 INFO misc.py line 115 22900] Train: [28/100][47/156] Data 0.001 (0.001) Batch 3.750 (3.519) Remain 11:05:04 loss: 0.3854 Lr: 0.04350
[2023-08-07 23:01:25,715 INFO misc.py line 115 22900] Train: [28/100][48/156] Data 0.001 (0.001) Batch 4.125 (3.532) Remain 11:07:33 loss: 0.4264 Lr: 0.04350
[2023-08-07 23:01:29,088 INFO misc.py line 115 22900] Train: [28/100][49/156] Data 0.001 (0.001) Batch 3.373 (3.529) Remain 11:06:50 loss: 0.4510 Lr: 0.04349
[2023-08-07 23:01:31,506 INFO misc.py line 115 22900] Train: [28/100][50/156] Data 0.001 (0.001) Batch 2.418 (3.505) Remain 11:02:19 loss: 0.2027 Lr: 0.04349
[2023-08-07 23:01:35,498 INFO misc.py line 115 22900] Train: [28/100][51/156] Data 0.001 (0.001) Batch 3.993 (3.515) Remain 11:04:10 loss: 0.5991 Lr: 0.04349
[2023-08-07 23:01:38,613 INFO misc.py line 115 22900] Train: [28/100][52/156] Data 0.001 (0.001) Batch 3.115 (3.507) Remain 11:02:34 loss: 0.4504 Lr: 0.04348
[2023-08-07 23:01:42,471 INFO misc.py line 115 22900] Train: [28/100][53/156] Data 0.001 (0.001) Batch 3.858 (3.514) Remain 11:03:50 loss: 0.4690 Lr: 0.04348
[2023-08-07 23:01:45,989 INFO misc.py line 115 22900] Train: [28/100][54/156] Data 0.001 (0.001) Batch 3.517 (3.514) Remain 11:03:48 loss: 0.5979 Lr: 0.04348
[2023-08-07 23:01:50,023 INFO misc.py line 115 22900] Train: [28/100][55/156] Data 0.001 (0.001) Batch 4.034 (3.524) Remain 11:05:38 loss: 0.3433 Lr: 0.04347
[2023-08-07 23:01:53,008 INFO misc.py line 115 22900] Train: [28/100][56/156] Data 0.001 (0.001) Batch 2.985 (3.514) Remain 11:03:39 loss: 0.3647 Lr: 0.04347
[2023-08-07 23:01:56,988 INFO misc.py line 115 22900] Train: [28/100][57/156] Data 0.001 (0.001) Batch 3.980 (3.523) Remain 11:05:13 loss: 0.5882 Lr: 0.04346
[2023-08-07 23:02:01,079 INFO misc.py line 115 22900] Train: [28/100][58/156] Data 0.001 (0.001) Batch 4.091 (3.533) Remain 11:07:07 loss: 0.4663 Lr: 0.04346
[2023-08-07 23:02:04,776 INFO misc.py line 115 22900] Train: [28/100][59/156] Data 0.001 (0.001) Batch 3.697 (3.536) Remain 11:07:36 loss: 0.5391 Lr: 0.04346
[2023-08-07 23:02:08,044 INFO misc.py line 115 22900] Train: [28/100][60/156] Data 0.001 (0.001) Batch 3.268 (3.531) Remain 11:06:40 loss: 0.3018 Lr: 0.04345
[2023-08-07 23:02:11,686 INFO misc.py line 115 22900] Train: [28/100][61/156] Data 0.001 (0.001) Batch 3.642 (3.533) Remain 11:06:58 loss: 0.4955 Lr: 0.04345
[2023-08-07 23:02:14,811 INFO misc.py line 115 22900] Train: [28/100][62/156] Data 0.001 (0.001) Batch 3.125 (3.526) Remain 11:05:36 loss: 0.4962 Lr: 0.04345
[2023-08-07 23:02:18,029 INFO misc.py line 115 22900] Train: [28/100][63/156] Data 0.001 (0.001) Batch 3.218 (3.521) Remain 11:04:34 loss: 0.2493 Lr: 0.04344
[2023-08-07 23:02:21,622 INFO misc.py line 115 22900] Train: [28/100][64/156] Data 0.001 (0.001) Batch 3.593 (3.522) Remain 11:04:44 loss: 0.2789 Lr: 0.04344
[2023-08-07 23:02:25,438 INFO misc.py line 115 22900] Train: [28/100][65/156] Data 0.001 (0.001) Batch 3.816 (3.527) Remain 11:05:34 loss: 0.4964 Lr: 0.04344
[2023-08-07 23:02:28,269 INFO misc.py line 115 22900] Train: [28/100][66/156] Data 0.001 (0.001) Batch 2.831 (3.516) Remain 11:03:26 loss: 0.4629 Lr: 0.04343
[2023-08-07 23:02:30,873 INFO misc.py line 115 22900] Train: [28/100][67/156] Data 0.001 (0.001) Batch 2.604 (3.502) Remain 11:00:41 loss: 0.3927 Lr: 0.04343
[2023-08-07 23:02:34,132 INFO misc.py line 115 22900] Train: [28/100][68/156] Data 0.001 (0.001) Batch 3.259 (3.498) Remain 10:59:55 loss: 0.3455 Lr: 0.04343
[2023-08-07 23:02:37,752 INFO misc.py line 115 22900] Train: [28/100][69/156] Data 0.001 (0.001) Batch 3.621 (3.500) Remain 11:00:13 loss: 0.3190 Lr: 0.04342
[2023-08-07 23:02:40,696 INFO misc.py line 115 22900] Train: [28/100][70/156] Data 0.001 (0.001) Batch 2.943 (3.491) Remain 10:58:35 loss: 0.8705 Lr: 0.04342
[2023-08-07 23:02:44,180 INFO misc.py line 115 22900] Train: [28/100][71/156] Data 0.001 (0.001) Batch 3.484 (3.491) Remain 10:58:30 loss: 0.7313 Lr: 0.04341
[2023-08-07 23:02:47,900 INFO misc.py line 115 22900] Train: [28/100][72/156] Data 0.001 (0.001) Batch 3.720 (3.495) Remain 10:59:04 loss: 0.4359 Lr: 0.04341
[2023-08-07 23:02:51,468 INFO misc.py line 115 22900] Train: [28/100][73/156] Data 0.001 (0.001) Batch 3.568 (3.496) Remain 10:59:13 loss: 0.5223 Lr: 0.04341
[2023-08-07 23:02:54,942 INFO misc.py line 115 22900] Train: [28/100][74/156] Data 0.001 (0.001) Batch 3.474 (3.495) Remain 10:59:06 loss: 0.4962 Lr: 0.04340
[2023-08-07 23:02:58,898 INFO misc.py line 115 22900] Train: [28/100][75/156] Data 0.001 (0.001) Batch 3.956 (3.502) Remain 11:00:15 loss: 0.4573 Lr: 0.04340
[2023-08-07 23:03:02,561 INFO misc.py line 115 22900] Train: [28/100][76/156] Data 0.001 (0.001) Batch 3.664 (3.504) Remain 11:00:36 loss: 0.4155 Lr: 0.04340
[2023-08-07 23:03:06,432 INFO misc.py line 115 22900] Train: [28/100][77/156] Data 0.001 (0.001) Batch 3.871 (3.509) Remain 11:01:29 loss: 0.5370 Lr: 0.04339
[2023-08-07 23:03:10,277 INFO misc.py line 115 22900] Train: [28/100][78/156] Data 0.001 (0.001) Batch 3.845 (3.513) Remain 11:02:16 loss: 0.5030 Lr: 0.04339
[2023-08-07 23:03:13,110 INFO misc.py line 115 22900] Train: [28/100][79/156] Data 0.001 (0.001) Batch 2.832 (3.504) Remain 11:00:31 loss: 0.4729 Lr: 0.04339
[2023-08-07 23:03:17,245 INFO misc.py line 115 22900] Train: [28/100][80/156] Data 0.001 (0.001) Batch 4.135 (3.513) Remain 11:02:00 loss: 0.6163 Lr: 0.04338
[2023-08-07 23:03:21,236 INFO misc.py line 115 22900] Train: [28/100][81/156] Data 0.001 (0.001) Batch 3.991 (3.519) Remain 11:03:06 loss: 0.3679 Lr: 0.04338
[2023-08-07 23:03:24,756 INFO misc.py line 115 22900] Train: [28/100][82/156] Data 0.001 (0.001) Batch 3.520 (3.519) Remain 11:03:03 loss: 0.4210 Lr: 0.04338
[2023-08-07 23:03:28,367 INFO misc.py line 115 22900] Train: [28/100][83/156] Data 0.001 (0.001) Batch 3.610 (3.520) Remain 11:03:12 loss: 0.2609 Lr: 0.04337
[2023-08-07 23:03:29,851 INFO misc.py line 115 22900] Train: [28/100][84/156] Data 0.001 (0.001) Batch 1.484 (3.495) Remain 10:58:25 loss: 0.2496 Lr: 0.04337
[2023-08-07 23:03:33,532 INFO misc.py line 115 22900] Train: [28/100][85/156] Data 0.001 (0.001) Batch 3.681 (3.497) Remain 10:58:47 loss: 0.7745 Lr: 0.04336
[2023-08-07 23:03:36,622 INFO misc.py line 115 22900] Train: [28/100][86/156] Data 0.001 (0.001) Batch 3.091 (3.492) Remain 10:57:48 loss: 0.5490 Lr: 0.04336
[2023-08-07 23:03:39,870 INFO misc.py line 115 22900] Train: [28/100][87/156] Data 0.001 (0.001) Batch 3.248 (3.489) Remain 10:57:12 loss: 0.3648 Lr: 0.04336
[2023-08-07 23:03:42,952 INFO misc.py line 115 22900] Train: [28/100][88/156] Data 0.001 (0.001) Batch 3.082 (3.484) Remain 10:56:14 loss: 0.1889 Lr: 0.04335
[2023-08-07 23:03:45,691 INFO misc.py line 115 22900] Train: [28/100][89/156] Data 0.001 (0.001) Batch 2.739 (3.476) Remain 10:54:33 loss: 0.4263 Lr: 0.04335
[2023-08-07 23:03:49,655 INFO misc.py line 115 22900] Train: [28/100][90/156] Data 0.001 (0.001) Batch 3.963 (3.481) Remain 10:55:32 loss: 0.5191 Lr: 0.04335
[2023-08-07 23:03:52,665 INFO misc.py line 115 22900] Train: [28/100][91/156] Data 0.001 (0.001) Batch 3.011 (3.476) Remain 10:54:28 loss: 0.3495 Lr: 0.04334
[2023-08-07 23:03:55,893 INFO misc.py line 115 22900] Train: [28/100][92/156] Data 0.001 (0.001) Batch 3.228 (3.473) Remain 10:53:54 loss: 0.4231 Lr: 0.04334
[2023-08-07 23:04:00,086 INFO misc.py line 115 22900] Train: [28/100][93/156] Data 0.001 (0.001) Batch 4.193 (3.481) Remain 10:55:20 loss: 0.3668 Lr: 0.04334
[2023-08-07 23:04:02,699 INFO misc.py line 115 22900] Train: [28/100][94/156] Data 0.001 (0.001) Batch 2.613 (3.472) Remain 10:53:29 loss: 0.3574 Lr: 0.04333
[2023-08-07 23:04:06,010 INFO misc.py line 115 22900] Train: [28/100][95/156] Data 0.001 (0.001) Batch 3.311 (3.470) Remain 10:53:06 loss: 0.3408 Lr: 0.04333
[2023-08-07 23:04:08,550 INFO misc.py line 115 22900] Train: [28/100][96/156] Data 0.001 (0.001) Batch 2.540 (3.460) Remain 10:51:10 loss: 0.3370 Lr: 0.04332
[2023-08-07 23:04:11,109 INFO misc.py line 115 22900] Train: [28/100][97/156] Data 0.001 (0.001) Batch 2.559 (3.450) Remain 10:49:18 loss: 0.4802 Lr: 0.04332
[2023-08-07 23:04:14,949 INFO misc.py line 115 22900] Train: [28/100][98/156] Data 0.001 (0.001) Batch 3.840 (3.454) Remain 10:50:01 loss: 0.3824 Lr: 0.04332
[2023-08-07 23:04:18,524 INFO misc.py line 115 22900] Train: [28/100][99/156] Data 0.001 (0.001) Batch 3.575 (3.456) Remain 10:50:11 loss: 0.3371 Lr: 0.04331
[2023-08-07 23:04:22,063 INFO misc.py line 115 22900] Train: [28/100][100/156] Data 0.001 (0.001) Batch 3.539 (3.457) Remain 10:50:18 loss: 0.5241 Lr: 0.04331
[2023-08-07 23:04:24,753 INFO misc.py line 115 22900] Train: [28/100][101/156] Data 0.001 (0.001) Batch 2.690 (3.449) Remain 10:48:46 loss: 0.2382 Lr: 0.04331
[2023-08-07 23:04:27,927 INFO misc.py line 115 22900] Train: [28/100][102/156] Data 0.001 (0.001) Batch 3.174 (3.446) Remain 10:48:11 loss: 0.3867 Lr: 0.04330
[2023-08-07 23:04:31,416 INFO misc.py line 115 22900] Train: [28/100][103/156] Data 0.001 (0.001) Batch 3.489 (3.446) Remain 10:48:12 loss: 0.6070 Lr: 0.04330
[2023-08-07 23:04:34,894 INFO misc.py line 115 22900] Train: [28/100][104/156] Data 0.001 (0.001) Batch 3.478 (3.447) Remain 10:48:13 loss: 0.3961 Lr: 0.04330
[2023-08-07 23:04:39,023 INFO misc.py line 115 22900] Train: [28/100][105/156] Data 0.001 (0.001) Batch 4.129 (3.453) Remain 10:49:25 loss: 0.5305 Lr: 0.04329
[2023-08-07 23:04:43,088 INFO misc.py line 115 22900] Train: [28/100][106/156] Data 0.001 (0.001) Batch 4.065 (3.459) Remain 10:50:28 loss: 0.5978 Lr: 0.04329
[2023-08-07 23:04:46,049 INFO misc.py line 115 22900] Train: [28/100][107/156] Data 0.001 (0.001) Batch 2.961 (3.455) Remain 10:49:31 loss: 0.3491 Lr: 0.04329
[2023-08-07 23:04:49,945 INFO misc.py line 115 22900] Train: [28/100][108/156] Data 0.001 (0.001) Batch 3.896 (3.459) Remain 10:50:15 loss: 0.7308 Lr: 0.04328
[2023-08-07 23:04:54,439 INFO misc.py line 115 22900] Train: [28/100][109/156] Data 0.001 (0.001) Batch 4.494 (3.469) Remain 10:52:01 loss: 0.6981 Lr: 0.04328
[2023-08-07 23:04:58,695 INFO misc.py line 115 22900] Train: [28/100][110/156] Data 0.001 (0.001) Batch 4.257 (3.476) Remain 10:53:21 loss: 0.4821 Lr: 0.04327
[2023-08-07 23:05:01,014 INFO misc.py line 115 22900] Train: [28/100][111/156] Data 0.001 (0.001) Batch 2.318 (3.465) Remain 10:51:16 loss: 0.3377 Lr: 0.04327
[2023-08-07 23:05:05,060 INFO misc.py line 115 22900] Train: [28/100][112/156] Data 0.001 (0.001) Batch 4.047 (3.471) Remain 10:52:13 loss: 0.4311 Lr: 0.04327
[2023-08-07 23:05:08,076 INFO misc.py line 115 22900] Train: [28/100][113/156] Data 0.001 (0.001) Batch 3.016 (3.466) Remain 10:51:23 loss: 0.3884 Lr: 0.04326
[2023-08-07 23:05:11,795 INFO misc.py line 115 22900] Train: [28/100][114/156] Data 0.001 (0.001) Batch 3.719 (3.469) Remain 10:51:45 loss: 0.4931 Lr: 0.04326
[2023-08-07 23:05:14,833 INFO misc.py line 115 22900] Train: [28/100][115/156] Data 0.001 (0.001) Batch 3.038 (3.465) Remain 10:50:59 loss: 0.2584 Lr: 0.04326
[2023-08-07 23:05:18,372 INFO misc.py line 115 22900] Train: [28/100][116/156] Data 0.001 (0.001) Batch 3.539 (3.465) Remain 10:51:02 loss: 0.4092 Lr: 0.04325
[2023-08-07 23:05:21,953 INFO misc.py line 115 22900] Train: [28/100][117/156] Data 0.001 (0.001) Batch 3.581 (3.466) Remain 10:51:10 loss: 0.9051 Lr: 0.04325
[2023-08-07 23:05:25,272 INFO misc.py line 115 22900] Train: [28/100][118/156] Data 0.001 (0.001) Batch 3.319 (3.465) Remain 10:50:52 loss: 0.2649 Lr: 0.04325
[2023-08-07 23:05:28,761 INFO misc.py line 115 22900] Train: [28/100][119/156] Data 0.001 (0.001) Batch 3.489 (3.465) Remain 10:50:51 loss: 0.4277 Lr: 0.04324
[2023-08-07 23:05:31,406 INFO misc.py line 115 22900] Train: [28/100][120/156] Data 0.001 (0.001) Batch 2.644 (3.458) Remain 10:49:29 loss: 0.1984 Lr: 0.04324
[2023-08-07 23:05:34,791 INFO misc.py line 115 22900] Train: [28/100][121/156] Data 0.001 (0.001) Batch 3.385 (3.458) Remain 10:49:18 loss: 0.6035 Lr: 0.04323
[2023-08-07 23:05:38,493 INFO misc.py line 115 22900] Train: [28/100][122/156] Data 0.001 (0.001) Batch 3.702 (3.460) Remain 10:49:38 loss: 0.7419 Lr: 0.04323
[2023-08-07 23:05:41,842 INFO misc.py line 115 22900] Train: [28/100][123/156] Data 0.001 (0.001) Batch 3.350 (3.459) Remain 10:49:24 loss: 0.4004 Lr: 0.04323
[2023-08-07 23:05:44,798 INFO misc.py line 115 22900] Train: [28/100][124/156] Data 0.001 (0.001) Batch 2.955 (3.455) Remain 10:48:34 loss: 0.2956 Lr: 0.04322
[2023-08-07 23:05:48,604 INFO misc.py line 115 22900] Train: [28/100][125/156] Data 0.001 (0.001) Batch 3.806 (3.458) Remain 10:49:03 loss: 0.5106 Lr: 0.04322
[2023-08-07 23:05:51,409 INFO misc.py line 115 22900] Train: [28/100][126/156] Data 0.001 (0.001) Batch 2.805 (3.452) Remain 10:48:00 loss: 0.2480 Lr: 0.04322
[2023-08-07 23:05:55,165 INFO misc.py line 115 22900] Train: [28/100][127/156] Data 0.001 (0.001) Batch 3.756 (3.455) Remain 10:48:24 loss: 0.7766 Lr: 0.04321
[2023-08-07 23:05:58,484 INFO misc.py line 115 22900] Train: [28/100][128/156] Data 0.001 (0.001) Batch 3.319 (3.454) Remain 10:48:08 loss: 0.4990 Lr: 0.04321
[2023-08-07 23:06:02,173 INFO misc.py line 115 22900] Train: [28/100][129/156] Data 0.001 (0.001) Batch 3.689 (3.456) Remain 10:48:26 loss: 0.5818 Lr: 0.04321
[2023-08-07 23:06:06,174 INFO misc.py line 115 22900] Train: [28/100][130/156] Data 0.001 (0.001) Batch 4.002 (3.460) Remain 10:49:11 loss: 0.5268 Lr: 0.04320
[2023-08-07 23:06:09,529 INFO misc.py line 115 22900] Train: [28/100][131/156] Data 0.001 (0.001) Batch 3.355 (3.459) Remain 10:48:58 loss: 0.4686 Lr: 0.04320
[2023-08-07 23:06:12,001 INFO misc.py line 115 22900] Train: [28/100][132/156] Data 0.001 (0.001) Batch 2.471 (3.451) Remain 10:47:28 loss: 0.2248 Lr: 0.04319
[2023-08-07 23:06:14,693 INFO misc.py line 115 22900] Train: [28/100][133/156] Data 0.001 (0.001) Batch 2.692 (3.446) Remain 10:46:19 loss: 0.2288 Lr: 0.04319
[2023-08-07 23:06:18,779 INFO misc.py line 115 22900] Train: [28/100][134/156] Data 0.001 (0.001) Batch 4.086 (3.450) Remain 10:47:11 loss: 0.6775 Lr: 0.04319
[2023-08-07 23:06:21,449 INFO misc.py line 115 22900] Train: [28/100][135/156] Data 0.001 (0.001) Batch 2.670 (3.445) Remain 10:46:01 loss: 0.5574 Lr: 0.04318
[2023-08-07 23:06:25,211 INFO misc.py line 115 22900] Train: [28/100][136/156] Data 0.001 (0.001) Batch 3.762 (3.447) Remain 10:46:24 loss: 0.3359 Lr: 0.04318
[2023-08-07 23:06:28,375 INFO misc.py line 115 22900] Train: [28/100][137/156] Data 0.001 (0.001) Batch 3.165 (3.445) Remain 10:45:57 loss: 0.5484 Lr: 0.04318
[2023-08-07 23:06:31,504 INFO misc.py line 115 22900] Train: [28/100][138/156] Data 0.001 (0.001) Batch 3.129 (3.442) Remain 10:45:27 loss: 0.4544 Lr: 0.04317
[2023-08-07 23:06:35,380 INFO misc.py line 115 22900] Train: [28/100][139/156] Data 0.001 (0.001) Batch 3.876 (3.446) Remain 10:46:00 loss: 0.4860 Lr: 0.04317
[2023-08-07 23:06:38,480 INFO misc.py line 115 22900] Train: [28/100][140/156] Data 0.001 (0.001) Batch 3.099 (3.443) Remain 10:45:28 loss: 0.3405 Lr: 0.04317
[2023-08-07 23:06:41,952 INFO misc.py line 115 22900] Train: [28/100][141/156] Data 0.001 (0.001) Batch 3.472 (3.443) Remain 10:45:27 loss: 0.4444 Lr: 0.04316
[2023-08-07 23:06:46,431 INFO misc.py line 115 22900] Train: [28/100][142/156] Data 0.001 (0.001) Batch 4.479 (3.451) Remain 10:46:47 loss: 0.4990 Lr: 0.04316
[2023-08-07 23:06:49,348 INFO misc.py line 115 22900] Train: [28/100][143/156] Data 0.001 (0.001) Batch 2.917 (3.447) Remain 10:46:01 loss: 0.4264 Lr: 0.04315
[2023-08-07 23:06:52,706 INFO misc.py line 115 22900] Train: [28/100][144/156] Data 0.001 (0.001) Batch 3.357 (3.446) Remain 10:45:50 loss: 0.3689 Lr: 0.04315
[2023-08-07 23:06:55,333 INFO misc.py line 115 22900] Train: [28/100][145/156] Data 0.001 (0.001) Batch 2.628 (3.441) Remain 10:44:42 loss: 0.2747 Lr: 0.04315
[2023-08-07 23:07:00,093 INFO misc.py line 115 22900] Train: [28/100][146/156] Data 0.001 (0.001) Batch 4.760 (3.450) Remain 10:46:22 loss: 0.6512 Lr: 0.04314
[2023-08-07 23:07:04,450 INFO misc.py line 115 22900] Train: [28/100][147/156] Data 0.001 (0.001) Batch 4.357 (3.456) Remain 10:47:29 loss: 0.6098 Lr: 0.04314
[2023-08-07 23:07:07,502 INFO misc.py line 115 22900] Train: [28/100][148/156] Data 0.001 (0.001) Batch 3.052 (3.453) Remain 10:46:55 loss: 0.2441 Lr: 0.04314
[2023-08-07 23:07:11,334 INFO misc.py line 115 22900] Train: [28/100][149/156] Data 0.001 (0.001) Batch 3.832 (3.456) Remain 10:47:20 loss: 0.4443 Lr: 0.04313
[2023-08-07 23:07:14,228 INFO misc.py line 115 22900] Train: [28/100][150/156] Data 0.001 (0.001) Batch 2.894 (3.452) Remain 10:46:34 loss: 0.3393 Lr: 0.04313
[2023-08-07 23:07:17,697 INFO misc.py line 115 22900] Train: [28/100][151/156] Data 0.001 (0.001) Batch 3.469 (3.452) Remain 10:46:32 loss: 0.2470 Lr: 0.04313
[2023-08-07 23:07:20,978 INFO misc.py line 115 22900] Train: [28/100][152/156] Data 0.001 (0.001) Batch 3.281 (3.451) Remain 10:46:15 loss: 0.2860 Lr: 0.04312
[2023-08-07 23:07:24,259 INFO misc.py line 115 22900] Train: [28/100][153/156] Data 0.001 (0.001) Batch 3.281 (3.450) Remain 10:45:59 loss: 0.2904 Lr: 0.04312
[2023-08-07 23:07:28,062 INFO misc.py line 115 22900] Train: [28/100][154/156] Data 0.001 (0.001) Batch 3.803 (3.452) Remain 10:46:22 loss: 0.4356 Lr: 0.04311
[2023-08-07 23:07:30,350 INFO misc.py line 115 22900] Train: [28/100][155/156] Data 0.001 (0.001) Batch 2.288 (3.445) Remain 10:44:53 loss: 0.2107 Lr: 0.04311
[2023-08-07 23:07:33,459 INFO misc.py line 115 22900] Train: [28/100][156/156] Data 0.001 (0.001) Batch 3.108 (3.442) Remain 10:44:24 loss: 0.4806 Lr: 0.04311
[2023-08-07 23:07:33,459 INFO misc.py line 129 22900] Train result: loss: 0.4536 
[2023-08-07 23:07:33,459 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 23:07:35,578 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.9058 
[2023-08-07 23:07:36,448 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.6071 
[2023-08-07 23:07:38,113 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.8703 
[2023-08-07 23:07:39,634 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.1441 
[2023-08-07 23:07:41,480 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.6738 
[2023-08-07 23:07:43,142 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.7399 
[2023-08-07 23:07:45,282 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.2583 
[2023-08-07 23:07:47,085 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.9804 
[2023-08-07 23:07:48,368 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.3468 
[2023-08-07 23:07:50,499 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3744 
[2023-08-07 23:07:51,025 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.2687 
[2023-08-07 23:07:52,557 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.8158 
[2023-08-07 23:07:55,268 INFO evaluator.py line 122 22900] Test: [13/24] Loss 0.8785 
[2023-08-07 23:07:56,947 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.7964 
[2023-08-07 23:07:58,969 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.4120 
[2023-08-07 23:08:01,680 INFO evaluator.py line 122 22900] Test: [16/24] Loss 0.8935 
[2023-08-07 23:08:04,385 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.0621 
[2023-08-07 23:08:06,232 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.4376 
[2023-08-07 23:08:06,979 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.2769 
[2023-08-07 23:08:07,863 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.8738 
[2023-08-07 23:08:10,126 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3603 
[2023-08-07 23:08:12,092 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.6224 
[2023-08-07 23:08:13,940 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.5544 
[2023-08-07 23:08:15,877 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.6019 
[2023-08-07 23:08:15,925 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2082/0.2903/0.6763.
[2023-08-07 23:08:15,925 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6270/0.9312
[2023-08-07 23:08:15,925 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9344/0.9920
[2023-08-07 23:08:15,925 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.0994/0.1850
[2023-08-07 23:08:15,925 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1021/0.1399
[2023-08-07 23:08:15,925 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6155/0.8103
[2023-08-07 23:08:15,925 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.1016/0.1046
[2023-08-07 23:08:15,925 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5461/0.6404
[2023-08-07 23:08:15,925 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0901/0.0989
[2023-08-07 23:08:15,925 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1453/0.4945
[2023-08-07 23:08:15,925 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.1040/0.1074
[2023-08-07 23:08:15,925 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 23:08:15,925 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0026/0.0026
[2023-08-07 23:08:15,925 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.2886/0.5674
[2023-08-07 23:08:15,925 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0816/0.0919
[2023-08-07 23:08:15,925 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0224/0.0242
[2023-08-07 23:08:15,925 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-07 23:08:15,925 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0329/0.0340
[2023-08-07 23:08:15,925 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.2795/0.3655
[2023-08-07 23:08:15,925 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 23:08:15,925 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0902/0.2154
[2023-08-07 23:08:15,925 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 23:08:15,926 INFO misc.py line 152 22900] Currently Best mIoU: 0.2147
[2023-08-07 23:08:15,926 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 23:08:22,472 INFO misc.py line 115 22900] Train: [29/100][1/156] Data 1.524 (1.524) Batch 5.743 (5.743) Remain 17:55:01 loss: 0.6174 Lr: 0.04310
[2023-08-07 23:08:25,266 INFO misc.py line 115 22900] Train: [29/100][2/156] Data 0.001 (0.001) Batch 2.794 (2.794) Remain 08:42:53 loss: 0.2049 Lr: 0.04310
[2023-08-07 23:08:28,940 INFO misc.py line 115 22900] Train: [29/100][3/156] Data 0.001 (0.001) Batch 3.674 (3.674) Remain 11:27:34 loss: 0.4091 Lr: 0.04310
[2023-08-07 23:08:33,205 INFO misc.py line 115 22900] Train: [29/100][4/156] Data 0.001 (0.001) Batch 4.265 (4.265) Remain 13:18:06 loss: 0.5992 Lr: 0.04309
[2023-08-07 23:08:36,965 INFO misc.py line 115 22900] Train: [29/100][5/156] Data 0.001 (0.001) Batch 3.760 (4.013) Remain 12:30:49 loss: 0.2989 Lr: 0.04309
[2023-08-07 23:08:39,780 INFO misc.py line 115 22900] Train: [29/100][6/156] Data 0.001 (0.001) Batch 2.815 (3.613) Remain 11:16:02 loss: 0.2879 Lr: 0.04309
[2023-08-07 23:08:44,081 INFO misc.py line 115 22900] Train: [29/100][7/156] Data 0.001 (0.001) Batch 4.302 (3.785) Remain 11:48:10 loss: 0.5137 Lr: 0.04308
[2023-08-07 23:08:48,059 INFO misc.py line 115 22900] Train: [29/100][8/156] Data 0.001 (0.001) Batch 3.978 (3.824) Remain 11:55:18 loss: 0.4353 Lr: 0.04308
[2023-08-07 23:08:52,055 INFO misc.py line 115 22900] Train: [29/100][9/156] Data 0.001 (0.001) Batch 3.996 (3.852) Remain 12:00:36 loss: 0.3805 Lr: 0.04307
[2023-08-07 23:08:56,142 INFO misc.py line 115 22900] Train: [29/100][10/156] Data 0.001 (0.001) Batch 4.087 (3.886) Remain 12:06:48 loss: 0.4948 Lr: 0.04307
[2023-08-07 23:08:59,176 INFO misc.py line 115 22900] Train: [29/100][11/156] Data 0.001 (0.001) Batch 3.034 (3.779) Remain 11:46:49 loss: 0.3905 Lr: 0.04307
[2023-08-07 23:09:03,176 INFO misc.py line 115 22900] Train: [29/100][12/156] Data 0.001 (0.001) Batch 4.001 (3.804) Remain 11:51:21 loss: 0.3834 Lr: 0.04306
[2023-08-07 23:09:06,458 INFO misc.py line 115 22900] Train: [29/100][13/156] Data 0.001 (0.001) Batch 3.282 (3.752) Remain 11:41:32 loss: 0.5376 Lr: 0.04306
[2023-08-07 23:09:09,394 INFO misc.py line 115 22900] Train: [29/100][14/156] Data 0.001 (0.001) Batch 2.936 (3.678) Remain 11:27:35 loss: 0.4262 Lr: 0.04306
[2023-08-07 23:09:13,115 INFO misc.py line 115 22900] Train: [29/100][15/156] Data 0.001 (0.001) Batch 3.721 (3.681) Remain 11:28:12 loss: 0.2983 Lr: 0.04305
[2023-08-07 23:09:15,993 INFO misc.py line 115 22900] Train: [29/100][16/156] Data 0.001 (0.001) Batch 2.878 (3.619) Remain 11:16:36 loss: 0.4855 Lr: 0.04305
[2023-08-07 23:09:18,660 INFO misc.py line 115 22900] Train: [29/100][17/156] Data 0.001 (0.001) Batch 2.667 (3.551) Remain 11:03:49 loss: 0.3207 Lr: 0.04304
[2023-08-07 23:09:21,892 INFO misc.py line 115 22900] Train: [29/100][18/156] Data 0.001 (0.001) Batch 3.232 (3.530) Remain 10:59:46 loss: 0.4949 Lr: 0.04304
[2023-08-07 23:09:25,902 INFO misc.py line 115 22900] Train: [29/100][19/156] Data 0.001 (0.001) Batch 4.010 (3.560) Remain 11:05:19 loss: 0.4843 Lr: 0.04304
[2023-08-07 23:09:30,453 INFO misc.py line 115 22900] Train: [29/100][20/156] Data 0.001 (0.001) Batch 4.550 (3.618) Remain 11:16:09 loss: 0.4912 Lr: 0.04303
[2023-08-07 23:09:33,849 INFO misc.py line 115 22900] Train: [29/100][21/156] Data 0.001 (0.001) Batch 3.397 (3.606) Remain 11:13:47 loss: 0.5071 Lr: 0.04303
[2023-08-07 23:09:38,098 INFO misc.py line 115 22900] Train: [29/100][22/156] Data 0.001 (0.001) Batch 4.248 (3.640) Remain 11:20:03 loss: 0.7222 Lr: 0.04303
[2023-08-07 23:09:41,368 INFO misc.py line 115 22900] Train: [29/100][23/156] Data 0.001 (0.001) Batch 3.271 (3.621) Remain 11:16:32 loss: 0.4565 Lr: 0.04302
[2023-08-07 23:09:45,364 INFO misc.py line 115 22900] Train: [29/100][24/156] Data 0.001 (0.001) Batch 3.995 (3.639) Remain 11:19:48 loss: 0.4136 Lr: 0.04302
[2023-08-07 23:09:47,932 INFO misc.py line 115 22900] Train: [29/100][25/156] Data 0.001 (0.001) Batch 2.568 (3.591) Remain 11:10:39 loss: 0.3396 Lr: 0.04302
[2023-08-07 23:09:50,976 INFO misc.py line 115 22900] Train: [29/100][26/156] Data 0.001 (0.001) Batch 3.044 (3.567) Remain 11:06:09 loss: 0.3043 Lr: 0.04301
[2023-08-07 23:09:54,639 INFO misc.py line 115 22900] Train: [29/100][27/156] Data 0.001 (0.001) Batch 3.663 (3.571) Remain 11:06:50 loss: 0.5236 Lr: 0.04301
[2023-08-07 23:09:57,635 INFO misc.py line 115 22900] Train: [29/100][28/156] Data 0.001 (0.001) Batch 2.996 (3.548) Remain 11:02:29 loss: 0.3497 Lr: 0.04300
[2023-08-07 23:10:01,175 INFO misc.py line 115 22900] Train: [29/100][29/156] Data 0.001 (0.001) Batch 3.540 (3.547) Remain 11:02:22 loss: 0.3391 Lr: 0.04300
[2023-08-07 23:10:04,465 INFO misc.py line 115 22900] Train: [29/100][30/156] Data 0.001 (0.001) Batch 3.291 (3.538) Remain 11:00:32 loss: 0.5557 Lr: 0.04300
[2023-08-07 23:10:08,532 INFO misc.py line 115 22900] Train: [29/100][31/156] Data 0.001 (0.001) Batch 4.066 (3.557) Remain 11:04:00 loss: 0.4712 Lr: 0.04299
[2023-08-07 23:10:12,168 INFO misc.py line 115 22900] Train: [29/100][32/156] Data 0.001 (0.001) Batch 3.637 (3.560) Remain 11:04:27 loss: 0.3919 Lr: 0.04299
[2023-08-07 23:10:15,523 INFO misc.py line 115 22900] Train: [29/100][33/156] Data 0.001 (0.001) Batch 3.354 (3.553) Remain 11:03:07 loss: 0.3293 Lr: 0.04299
[2023-08-07 23:10:19,816 INFO misc.py line 115 22900] Train: [29/100][34/156] Data 0.001 (0.001) Batch 4.294 (3.577) Remain 11:07:31 loss: 0.9088 Lr: 0.04298
[2023-08-07 23:10:23,885 INFO misc.py line 115 22900] Train: [29/100][35/156] Data 0.001 (0.001) Batch 4.069 (3.592) Remain 11:10:20 loss: 0.3661 Lr: 0.04298
[2023-08-07 23:10:27,838 INFO misc.py line 115 22900] Train: [29/100][36/156] Data 0.001 (0.001) Batch 3.953 (3.603) Remain 11:12:18 loss: 0.5099 Lr: 0.04297
[2023-08-07 23:10:31,087 INFO misc.py line 115 22900] Train: [29/100][37/156] Data 0.001 (0.001) Batch 3.249 (3.593) Remain 11:10:18 loss: 0.4623 Lr: 0.04297
[2023-08-07 23:10:34,722 INFO misc.py line 115 22900] Train: [29/100][38/156] Data 0.001 (0.001) Batch 3.635 (3.594) Remain 11:10:28 loss: 0.5660 Lr: 0.04297
[2023-08-07 23:10:38,117 INFO misc.py line 115 22900] Train: [29/100][39/156] Data 0.001 (0.001) Batch 3.394 (3.588) Remain 11:09:23 loss: 0.4796 Lr: 0.04296
[2023-08-07 23:10:42,230 INFO misc.py line 115 22900] Train: [29/100][40/156] Data 0.001 (0.001) Batch 4.113 (3.602) Remain 11:11:58 loss: 0.4636 Lr: 0.04296
[2023-08-07 23:10:46,244 INFO misc.py line 115 22900] Train: [29/100][41/156] Data 0.001 (0.001) Batch 4.014 (3.613) Remain 11:13:56 loss: 0.4087 Lr: 0.04296
[2023-08-07 23:10:50,230 INFO misc.py line 115 22900] Train: [29/100][42/156] Data 0.001 (0.001) Batch 3.986 (3.623) Remain 11:15:39 loss: 0.5458 Lr: 0.04295
[2023-08-07 23:10:52,972 INFO misc.py line 115 22900] Train: [29/100][43/156] Data 0.001 (0.001) Batch 2.742 (3.601) Remain 11:11:29 loss: 0.1805 Lr: 0.04295
[2023-08-07 23:10:56,506 INFO misc.py line 115 22900] Train: [29/100][44/156] Data 0.001 (0.001) Batch 3.534 (3.599) Remain 11:11:07 loss: 0.5547 Lr: 0.04295
[2023-08-07 23:11:00,232 INFO misc.py line 115 22900] Train: [29/100][45/156] Data 0.001 (0.001) Batch 3.726 (3.602) Remain 11:11:37 loss: 0.5322 Lr: 0.04294
[2023-08-07 23:11:04,245 INFO misc.py line 115 22900] Train: [29/100][46/156] Data 0.001 (0.001) Batch 4.013 (3.612) Remain 11:13:21 loss: 0.3572 Lr: 0.04294
[2023-08-07 23:11:07,519 INFO misc.py line 115 22900] Train: [29/100][47/156] Data 0.001 (0.001) Batch 3.274 (3.604) Remain 11:11:51 loss: 0.4540 Lr: 0.04293
[2023-08-07 23:11:10,303 INFO misc.py line 115 22900] Train: [29/100][48/156] Data 0.001 (0.001) Batch 2.783 (3.586) Remain 11:08:24 loss: 0.4706 Lr: 0.04293
[2023-08-07 23:11:13,900 INFO misc.py line 115 22900] Train: [29/100][49/156] Data 0.001 (0.001) Batch 3.597 (3.586) Remain 11:08:23 loss: 0.6080 Lr: 0.04293
[2023-08-07 23:11:17,202 INFO misc.py line 115 22900] Train: [29/100][50/156] Data 0.001 (0.001) Batch 3.302 (3.580) Remain 11:07:12 loss: 0.5326 Lr: 0.04292
[2023-08-07 23:11:20,843 INFO misc.py line 115 22900] Train: [29/100][51/156] Data 0.001 (0.001) Batch 3.641 (3.581) Remain 11:07:22 loss: 0.7115 Lr: 0.04292
[2023-08-07 23:11:23,920 INFO misc.py line 115 22900] Train: [29/100][52/156] Data 0.001 (0.001) Batch 3.076 (3.571) Remain 11:05:23 loss: 0.3815 Lr: 0.04292
[2023-08-07 23:11:27,054 INFO misc.py line 115 22900] Train: [29/100][53/156] Data 0.001 (0.001) Batch 3.135 (3.562) Remain 11:03:42 loss: 0.2714 Lr: 0.04291
[2023-08-07 23:11:29,404 INFO misc.py line 115 22900] Train: [29/100][54/156] Data 0.001 (0.001) Batch 2.349 (3.539) Remain 10:59:13 loss: 0.3121 Lr: 0.04291
[2023-08-07 23:11:30,964 INFO misc.py line 115 22900] Train: [29/100][55/156] Data 0.001 (0.001) Batch 1.561 (3.500) Remain 10:52:04 loss: 0.0644 Lr: 0.04290
[2023-08-07 23:11:34,374 INFO misc.py line 115 22900] Train: [29/100][56/156] Data 0.001 (0.001) Batch 3.410 (3.499) Remain 10:51:42 loss: 0.3220 Lr: 0.04290
[2023-08-07 23:11:36,997 INFO misc.py line 115 22900] Train: [29/100][57/156] Data 0.001 (0.001) Batch 2.623 (3.483) Remain 10:48:37 loss: 0.3079 Lr: 0.04290
[2023-08-07 23:11:40,725 INFO misc.py line 115 22900] Train: [29/100][58/156] Data 0.001 (0.001) Batch 3.728 (3.487) Remain 10:49:23 loss: 0.6020 Lr: 0.04289
[2023-08-07 23:11:44,331 INFO misc.py line 115 22900] Train: [29/100][59/156] Data 0.001 (0.001) Batch 3.606 (3.489) Remain 10:49:43 loss: 0.3467 Lr: 0.04289
[2023-08-07 23:11:47,489 INFO misc.py line 115 22900] Train: [29/100][60/156] Data 0.001 (0.001) Batch 3.159 (3.483) Remain 10:48:35 loss: 0.3568 Lr: 0.04289
[2023-08-07 23:11:51,074 INFO misc.py line 115 22900] Train: [29/100][61/156] Data 0.001 (0.001) Batch 3.584 (3.485) Remain 10:48:51 loss: 0.4880 Lr: 0.04288
[2023-08-07 23:11:54,408 INFO misc.py line 115 22900] Train: [29/100][62/156] Data 0.001 (0.001) Batch 3.335 (3.483) Remain 10:48:19 loss: 0.3660 Lr: 0.04288
[2023-08-07 23:11:58,286 INFO misc.py line 115 22900] Train: [29/100][63/156] Data 0.001 (0.001) Batch 3.877 (3.489) Remain 10:49:29 loss: 0.3220 Lr: 0.04288
[2023-08-07 23:12:01,530 INFO misc.py line 115 22900] Train: [29/100][64/156] Data 0.001 (0.001) Batch 3.244 (3.485) Remain 10:48:41 loss: 0.4744 Lr: 0.04287
[2023-08-07 23:12:05,137 INFO misc.py line 115 22900] Train: [29/100][65/156] Data 0.001 (0.001) Batch 3.607 (3.487) Remain 10:48:59 loss: 0.2432 Lr: 0.04287
[2023-08-07 23:12:08,516 INFO misc.py line 115 22900] Train: [29/100][66/156] Data 0.001 (0.001) Batch 3.379 (3.485) Remain 10:48:37 loss: 0.3644 Lr: 0.04286
[2023-08-07 23:12:10,971 INFO misc.py line 115 22900] Train: [29/100][67/156] Data 0.001 (0.001) Batch 2.455 (3.469) Remain 10:45:33 loss: 0.3380 Lr: 0.04286
[2023-08-07 23:12:14,733 INFO misc.py line 115 22900] Train: [29/100][68/156] Data 0.001 (0.001) Batch 3.763 (3.474) Remain 10:46:20 loss: 0.4227 Lr: 0.04286
[2023-08-07 23:12:17,531 INFO misc.py line 115 22900] Train: [29/100][69/156] Data 0.001 (0.001) Batch 2.798 (3.464) Remain 10:44:23 loss: 0.1578 Lr: 0.04285
[2023-08-07 23:12:21,019 INFO misc.py line 115 22900] Train: [29/100][70/156] Data 0.001 (0.001) Batch 3.488 (3.464) Remain 10:44:23 loss: 0.4242 Lr: 0.04285
[2023-08-07 23:12:24,921 INFO misc.py line 115 22900] Train: [29/100][71/156] Data 0.001 (0.001) Batch 3.902 (3.470) Remain 10:45:32 loss: 0.3173 Lr: 0.04285
[2023-08-07 23:12:28,469 INFO misc.py line 115 22900] Train: [29/100][72/156] Data 0.001 (0.001) Batch 3.547 (3.471) Remain 10:45:41 loss: 0.4184 Lr: 0.04284
[2023-08-07 23:12:31,835 INFO misc.py line 115 22900] Train: [29/100][73/156] Data 0.001 (0.001) Batch 3.367 (3.470) Remain 10:45:21 loss: 0.3899 Lr: 0.04284
[2023-08-07 23:12:35,656 INFO misc.py line 115 22900] Train: [29/100][74/156] Data 0.001 (0.001) Batch 3.820 (3.475) Remain 10:46:12 loss: 0.4340 Lr: 0.04283
[2023-08-07 23:12:38,524 INFO misc.py line 115 22900] Train: [29/100][75/156] Data 0.001 (0.001) Batch 2.868 (3.466) Remain 10:44:35 loss: 0.3071 Lr: 0.04283
[2023-08-07 23:12:41,304 INFO misc.py line 115 22900] Train: [29/100][76/156] Data 0.001 (0.001) Batch 2.781 (3.457) Remain 10:42:46 loss: 0.3252 Lr: 0.04283
[2023-08-07 23:12:45,036 INFO misc.py line 115 22900] Train: [29/100][77/156] Data 0.001 (0.001) Batch 3.731 (3.461) Remain 10:43:24 loss: 0.5147 Lr: 0.04282
[2023-08-07 23:12:48,711 INFO misc.py line 115 22900] Train: [29/100][78/156] Data 0.001 (0.001) Batch 3.675 (3.464) Remain 10:43:53 loss: 0.7252 Lr: 0.04282
[2023-08-07 23:12:52,242 INFO misc.py line 115 22900] Train: [29/100][79/156] Data 0.001 (0.001) Batch 3.531 (3.464) Remain 10:43:59 loss: 0.3711 Lr: 0.04282
[2023-08-07 23:12:55,844 INFO misc.py line 115 22900] Train: [29/100][80/156] Data 0.001 (0.001) Batch 3.602 (3.466) Remain 10:44:15 loss: 0.3228 Lr: 0.04281
[2023-08-07 23:12:58,029 INFO misc.py line 115 22900] Train: [29/100][81/156] Data 0.001 (0.001) Batch 2.185 (3.450) Remain 10:41:09 loss: 0.1954 Lr: 0.04281
[2023-08-07 23:13:01,725 INFO misc.py line 115 22900] Train: [29/100][82/156] Data 0.001 (0.001) Batch 3.696 (3.453) Remain 10:41:40 loss: 0.8071 Lr: 0.04280
[2023-08-07 23:13:04,134 INFO misc.py line 115 22900] Train: [29/100][83/156] Data 0.001 (0.001) Batch 2.408 (3.440) Remain 10:39:11 loss: 0.4649 Lr: 0.04280
[2023-08-07 23:13:07,785 INFO misc.py line 115 22900] Train: [29/100][84/156] Data 0.001 (0.001) Batch 3.651 (3.443) Remain 10:39:37 loss: 0.4238 Lr: 0.04280
[2023-08-07 23:13:11,089 INFO misc.py line 115 22900] Train: [29/100][85/156] Data 0.001 (0.001) Batch 3.304 (3.441) Remain 10:39:15 loss: 0.7678 Lr: 0.04279
[2023-08-07 23:13:14,247 INFO misc.py line 115 22900] Train: [29/100][86/156] Data 0.001 (0.001) Batch 3.158 (3.437) Remain 10:38:33 loss: 0.6141 Lr: 0.04279
[2023-08-07 23:13:18,263 INFO misc.py line 115 22900] Train: [29/100][87/156] Data 0.001 (0.001) Batch 4.017 (3.444) Remain 10:39:47 loss: 0.6964 Lr: 0.04279
[2023-08-07 23:13:21,700 INFO misc.py line 115 22900] Train: [29/100][88/156] Data 0.001 (0.001) Batch 3.437 (3.444) Remain 10:39:42 loss: 0.5214 Lr: 0.04278
[2023-08-07 23:13:25,384 INFO misc.py line 115 22900] Train: [29/100][89/156] Data 0.001 (0.001) Batch 3.684 (3.447) Remain 10:40:10 loss: 0.3460 Lr: 0.04278
[2023-08-07 23:13:29,324 INFO misc.py line 115 22900] Train: [29/100][90/156] Data 0.001 (0.001) Batch 3.940 (3.453) Remain 10:41:09 loss: 0.3361 Lr: 0.04277
[2023-08-07 23:13:33,270 INFO misc.py line 115 22900] Train: [29/100][91/156] Data 0.001 (0.001) Batch 3.946 (3.458) Remain 10:42:08 loss: 0.3939 Lr: 0.04277
[2023-08-07 23:13:37,699 INFO misc.py line 115 22900] Train: [29/100][92/156] Data 0.001 (0.001) Batch 4.429 (3.469) Remain 10:44:06 loss: 0.6107 Lr: 0.04277
[2023-08-07 23:13:40,988 INFO misc.py line 115 22900] Train: [29/100][93/156] Data 0.001 (0.001) Batch 3.289 (3.467) Remain 10:43:41 loss: 0.4518 Lr: 0.04276
[2023-08-07 23:13:43,898 INFO misc.py line 115 22900] Train: [29/100][94/156] Data 0.001 (0.001) Batch 2.910 (3.461) Remain 10:42:29 loss: 0.1894 Lr: 0.04276
[2023-08-07 23:13:47,329 INFO misc.py line 115 22900] Train: [29/100][95/156] Data 0.001 (0.001) Batch 3.431 (3.461) Remain 10:42:22 loss: 0.4857 Lr: 0.04276
[2023-08-07 23:13:50,716 INFO misc.py line 115 22900] Train: [29/100][96/156] Data 0.001 (0.001) Batch 3.387 (3.460) Remain 10:42:10 loss: 0.5185 Lr: 0.04275
[2023-08-07 23:13:54,527 INFO misc.py line 115 22900] Train: [29/100][97/156] Data 0.001 (0.001) Batch 3.811 (3.464) Remain 10:42:48 loss: 0.4298 Lr: 0.04275
[2023-08-07 23:13:58,960 INFO misc.py line 115 22900] Train: [29/100][98/156] Data 0.001 (0.001) Batch 4.433 (3.474) Remain 10:44:38 loss: 0.8723 Lr: 0.04275
[2023-08-07 23:14:02,412 INFO misc.py line 115 22900] Train: [29/100][99/156] Data 0.001 (0.001) Batch 3.452 (3.474) Remain 10:44:32 loss: 0.6079 Lr: 0.04274
[2023-08-07 23:14:05,507 INFO misc.py line 115 22900] Train: [29/100][100/156] Data 0.001 (0.001) Batch 3.095 (3.470) Remain 10:43:45 loss: 0.4279 Lr: 0.04274
[2023-08-07 23:14:09,519 INFO misc.py line 115 22900] Train: [29/100][101/156] Data 0.001 (0.001) Batch 4.012 (3.475) Remain 10:44:43 loss: 0.5384 Lr: 0.04273
[2023-08-07 23:14:13,400 INFO misc.py line 115 22900] Train: [29/100][102/156] Data 0.001 (0.001) Batch 3.880 (3.479) Remain 10:45:25 loss: 0.3457 Lr: 0.04273
[2023-08-07 23:14:17,024 INFO misc.py line 115 22900] Train: [29/100][103/156] Data 0.001 (0.001) Batch 3.625 (3.481) Remain 10:45:38 loss: 0.4591 Lr: 0.04273
[2023-08-07 23:14:19,310 INFO misc.py line 115 22900] Train: [29/100][104/156] Data 0.001 (0.001) Batch 2.286 (3.469) Remain 10:43:23 loss: 0.2044 Lr: 0.04272
[2023-08-07 23:14:21,442 INFO misc.py line 115 22900] Train: [29/100][105/156] Data 0.001 (0.001) Batch 2.132 (3.456) Remain 10:40:53 loss: 0.3364 Lr: 0.04272
[2023-08-07 23:14:24,149 INFO misc.py line 115 22900] Train: [29/100][106/156] Data 0.001 (0.001) Batch 2.707 (3.449) Remain 10:39:29 loss: 0.5306 Lr: 0.04272
[2023-08-07 23:14:27,046 INFO misc.py line 115 22900] Train: [29/100][107/156] Data 0.001 (0.001) Batch 2.896 (3.443) Remain 10:38:26 loss: 0.2394 Lr: 0.04271
[2023-08-07 23:14:31,086 INFO misc.py line 115 22900] Train: [29/100][108/156] Data 0.001 (0.001) Batch 4.041 (3.449) Remain 10:39:26 loss: 0.7057 Lr: 0.04271
[2023-08-07 23:14:34,902 INFO misc.py line 115 22900] Train: [29/100][109/156] Data 0.001 (0.001) Batch 3.815 (3.452) Remain 10:40:01 loss: 0.4412 Lr: 0.04270
[2023-08-07 23:14:37,282 INFO misc.py line 115 22900] Train: [29/100][110/156] Data 0.001 (0.001) Batch 2.381 (3.442) Remain 10:38:06 loss: 0.4423 Lr: 0.04270
[2023-08-07 23:14:40,490 INFO misc.py line 115 22900] Train: [29/100][111/156] Data 0.001 (0.001) Batch 3.208 (3.440) Remain 10:37:39 loss: 0.5880 Lr: 0.04270
[2023-08-07 23:14:44,149 INFO misc.py line 115 22900] Train: [29/100][112/156] Data 0.001 (0.001) Batch 3.659 (3.442) Remain 10:37:58 loss: 0.5209 Lr: 0.04269
[2023-08-07 23:14:46,816 INFO misc.py line 115 22900] Train: [29/100][113/156] Data 0.001 (0.001) Batch 2.667 (3.435) Remain 10:36:36 loss: 0.2713 Lr: 0.04269
[2023-08-07 23:14:50,517 INFO misc.py line 115 22900] Train: [29/100][114/156] Data 0.001 (0.001) Batch 3.701 (3.438) Remain 10:36:59 loss: 0.3712 Lr: 0.04269
[2023-08-07 23:14:54,177 INFO misc.py line 115 22900] Train: [29/100][115/156] Data 0.001 (0.001) Batch 3.660 (3.440) Remain 10:37:18 loss: 0.5201 Lr: 0.04268
[2023-08-07 23:14:57,910 INFO misc.py line 115 22900] Train: [29/100][116/156] Data 0.001 (0.001) Batch 3.733 (3.442) Remain 10:37:43 loss: 0.4951 Lr: 0.04268
[2023-08-07 23:15:01,526 INFO misc.py line 115 22900] Train: [29/100][117/156] Data 0.001 (0.001) Batch 3.616 (3.444) Remain 10:37:57 loss: 0.4150 Lr: 0.04267
[2023-08-07 23:15:04,688 INFO misc.py line 115 22900] Train: [29/100][118/156] Data 0.001 (0.001) Batch 3.162 (3.441) Remain 10:37:26 loss: 0.5034 Lr: 0.04267
[2023-08-07 23:15:07,940 INFO misc.py line 115 22900] Train: [29/100][119/156] Data 0.001 (0.001) Batch 3.252 (3.440) Remain 10:37:04 loss: 0.5060 Lr: 0.04267
[2023-08-07 23:15:12,444 INFO misc.py line 115 22900] Train: [29/100][120/156] Data 0.001 (0.001) Batch 4.504 (3.449) Remain 10:38:42 loss: 0.6584 Lr: 0.04266
[2023-08-07 23:15:17,135 INFO misc.py line 115 22900] Train: [29/100][121/156] Data 0.001 (0.001) Batch 4.691 (3.459) Remain 10:40:36 loss: 0.5654 Lr: 0.04266
[2023-08-07 23:15:20,549 INFO misc.py line 115 22900] Train: [29/100][122/156] Data 0.001 (0.001) Batch 3.413 (3.459) Remain 10:40:28 loss: 0.6004 Lr: 0.04266
[2023-08-07 23:15:24,445 INFO misc.py line 115 22900] Train: [29/100][123/156] Data 0.001 (0.001) Batch 3.896 (3.463) Remain 10:41:05 loss: 0.4804 Lr: 0.04265
[2023-08-07 23:15:27,221 INFO misc.py line 115 22900] Train: [29/100][124/156] Data 0.001 (0.001) Batch 2.776 (3.457) Remain 10:39:58 loss: 0.2559 Lr: 0.04265
[2023-08-07 23:15:30,796 INFO misc.py line 115 22900] Train: [29/100][125/156] Data 0.001 (0.001) Batch 3.574 (3.458) Remain 10:40:06 loss: 0.4602 Lr: 0.04264
[2023-08-07 23:15:34,877 INFO misc.py line 115 22900] Train: [29/100][126/156] Data 0.001 (0.001) Batch 4.081 (3.463) Remain 10:40:58 loss: 0.5282 Lr: 0.04264
[2023-08-07 23:15:38,056 INFO misc.py line 115 22900] Train: [29/100][127/156] Data 0.001 (0.001) Batch 3.179 (3.461) Remain 10:40:30 loss: 0.4810 Lr: 0.04264
[2023-08-07 23:15:42,081 INFO misc.py line 115 22900] Train: [29/100][128/156] Data 0.001 (0.001) Batch 4.025 (3.465) Remain 10:41:16 loss: 0.4838 Lr: 0.04263
[2023-08-07 23:15:45,720 INFO misc.py line 115 22900] Train: [29/100][129/156] Data 0.001 (0.001) Batch 3.639 (3.467) Remain 10:41:28 loss: 0.4641 Lr: 0.04263
[2023-08-07 23:15:49,504 INFO misc.py line 115 22900] Train: [29/100][130/156] Data 0.001 (0.001) Batch 3.785 (3.469) Remain 10:41:52 loss: 0.4465 Lr: 0.04263
[2023-08-07 23:15:53,107 INFO misc.py line 115 22900] Train: [29/100][131/156] Data 0.001 (0.001) Batch 3.602 (3.470) Remain 10:42:01 loss: 0.2882 Lr: 0.04262
[2023-08-07 23:15:56,000 INFO misc.py line 115 22900] Train: [29/100][132/156] Data 0.001 (0.001) Batch 2.893 (3.466) Remain 10:41:07 loss: 0.3226 Lr: 0.04262
[2023-08-07 23:15:59,988 INFO misc.py line 115 22900] Train: [29/100][133/156] Data 0.001 (0.001) Batch 3.987 (3.470) Remain 10:41:49 loss: 0.4270 Lr: 0.04261
[2023-08-07 23:16:03,792 INFO misc.py line 115 22900] Train: [29/100][134/156] Data 0.001 (0.001) Batch 3.804 (3.472) Remain 10:42:13 loss: 0.3834 Lr: 0.04261
[2023-08-07 23:16:08,450 INFO misc.py line 115 22900] Train: [29/100][135/156] Data 0.001 (0.001) Batch 4.658 (3.481) Remain 10:43:50 loss: 0.8726 Lr: 0.04261
[2023-08-07 23:16:12,309 INFO misc.py line 115 22900] Train: [29/100][136/156] Data 0.001 (0.001) Batch 3.859 (3.484) Remain 10:44:18 loss: 0.5739 Lr: 0.04260
[2023-08-07 23:16:15,746 INFO misc.py line 115 22900] Train: [29/100][137/156] Data 0.001 (0.001) Batch 3.437 (3.484) Remain 10:44:10 loss: 0.4230 Lr: 0.04260
[2023-08-07 23:16:19,312 INFO misc.py line 115 22900] Train: [29/100][138/156] Data 0.001 (0.001) Batch 3.566 (3.484) Remain 10:44:14 loss: 0.2945 Lr: 0.04260
[2023-08-07 23:16:22,811 INFO misc.py line 115 22900] Train: [29/100][139/156] Data 0.001 (0.001) Batch 3.500 (3.484) Remain 10:44:11 loss: 0.6140 Lr: 0.04259
[2023-08-07 23:16:26,447 INFO misc.py line 115 22900] Train: [29/100][140/156] Data 0.001 (0.001) Batch 3.636 (3.485) Remain 10:44:20 loss: 0.5272 Lr: 0.04259
[2023-08-07 23:16:29,294 INFO misc.py line 115 22900] Train: [29/100][141/156] Data 0.001 (0.001) Batch 2.846 (3.481) Remain 10:43:25 loss: 0.4368 Lr: 0.04258
[2023-08-07 23:16:32,791 INFO misc.py line 115 22900] Train: [29/100][142/156] Data 0.001 (0.001) Batch 3.497 (3.481) Remain 10:43:23 loss: 0.6709 Lr: 0.04258
[2023-08-07 23:16:36,343 INFO misc.py line 115 22900] Train: [29/100][143/156] Data 0.001 (0.001) Batch 3.552 (3.481) Remain 10:43:25 loss: 0.3668 Lr: 0.04258
[2023-08-07 23:16:39,526 INFO misc.py line 115 22900] Train: [29/100][144/156] Data 0.001 (0.001) Batch 3.183 (3.479) Remain 10:42:58 loss: 0.2264 Lr: 0.04257
[2023-08-07 23:16:43,621 INFO misc.py line 115 22900] Train: [29/100][145/156] Data 0.001 (0.001) Batch 4.095 (3.484) Remain 10:43:43 loss: 0.6194 Lr: 0.04257
[2023-08-07 23:16:47,728 INFO misc.py line 115 22900] Train: [29/100][146/156] Data 0.001 (0.001) Batch 4.107 (3.488) Remain 10:44:28 loss: 0.3623 Lr: 0.04256
[2023-08-07 23:16:51,654 INFO misc.py line 115 22900] Train: [29/100][147/156] Data 0.001 (0.001) Batch 3.927 (3.491) Remain 10:44:58 loss: 0.3953 Lr: 0.04256
[2023-08-07 23:16:55,204 INFO misc.py line 115 22900] Train: [29/100][148/156] Data 0.001 (0.001) Batch 3.550 (3.491) Remain 10:44:59 loss: 0.2636 Lr: 0.04256
[2023-08-07 23:16:58,433 INFO misc.py line 115 22900] Train: [29/100][149/156] Data 0.001 (0.001) Batch 3.229 (3.490) Remain 10:44:36 loss: 0.2782 Lr: 0.04255
[2023-08-07 23:17:02,915 INFO misc.py line 115 22900] Train: [29/100][150/156] Data 0.001 (0.001) Batch 4.482 (3.496) Remain 10:45:47 loss: 1.1083 Lr: 0.04255
[2023-08-07 23:17:06,014 INFO misc.py line 115 22900] Train: [29/100][151/156] Data 0.001 (0.001) Batch 3.100 (3.494) Remain 10:45:14 loss: 0.2389 Lr: 0.04255
[2023-08-07 23:17:09,182 INFO misc.py line 115 22900] Train: [29/100][152/156] Data 0.001 (0.001) Batch 3.167 (3.492) Remain 10:44:46 loss: 0.3275 Lr: 0.04254
[2023-08-07 23:17:11,709 INFO misc.py line 115 22900] Train: [29/100][153/156] Data 0.001 (0.001) Batch 2.528 (3.485) Remain 10:43:31 loss: 0.2905 Lr: 0.04254
[2023-08-07 23:17:14,132 INFO misc.py line 115 22900] Train: [29/100][154/156] Data 0.001 (0.001) Batch 2.422 (3.478) Remain 10:42:10 loss: 0.3507 Lr: 0.04253
[2023-08-07 23:17:16,743 INFO misc.py line 115 22900] Train: [29/100][155/156] Data 0.001 (0.001) Batch 2.611 (3.472) Remain 10:41:03 loss: 0.4348 Lr: 0.04253
[2023-08-07 23:17:19,153 INFO misc.py line 115 22900] Train: [29/100][156/156] Data 0.001 (0.001) Batch 2.410 (3.465) Remain 10:39:43 loss: 0.2663 Lr: 0.04253
[2023-08-07 23:17:19,153 INFO misc.py line 129 22900] Train result: loss: 0.4452 
[2023-08-07 23:17:19,154 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 23:17:21,252 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.8436 
[2023-08-07 23:17:22,123 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.5637 
[2023-08-07 23:17:23,787 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.7717 
[2023-08-07 23:17:25,309 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.4581 
[2023-08-07 23:17:27,153 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.7520 
[2023-08-07 23:17:28,818 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.7304 
[2023-08-07 23:17:30,954 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.3028 
[2023-08-07 23:17:32,760 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.9619 
[2023-08-07 23:17:34,043 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.5981 
[2023-08-07 23:17:36,173 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3515 
[2023-08-07 23:17:36,698 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.6025 
[2023-08-07 23:17:38,231 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.7567 
[2023-08-07 23:17:40,945 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.1337 
[2023-08-07 23:17:42,625 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.8625 
[2023-08-07 23:17:44,648 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.4981 
[2023-08-07 23:17:47,359 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.2307 
[2023-08-07 23:17:50,065 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.4725 
[2023-08-07 23:17:51,910 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.5719 
[2023-08-07 23:17:52,661 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.0199 
[2023-08-07 23:17:53,545 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.7977 
[2023-08-07 23:17:55,806 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3169 
[2023-08-07 23:17:57,768 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.7939 
[2023-08-07 23:17:59,613 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.6983 
[2023-08-07 23:18:01,549 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.3160 
[2023-08-07 23:18:01,601 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2196/0.3114/0.6705.
[2023-08-07 23:18:01,601 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6532/0.9462
[2023-08-07 23:18:01,601 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9432/0.9934
[2023-08-07 23:18:01,602 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1408/0.4874
[2023-08-07 23:18:01,602 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1278/0.3485
[2023-08-07 23:18:01,602 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6037/0.6702
[2023-08-07 23:18:01,602 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.3212/0.4420
[2023-08-07 23:18:01,602 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.4667/0.6211
[2023-08-07 23:18:01,602 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0699/0.0748
[2023-08-07 23:18:01,602 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.0889/0.2662
[2023-08-07 23:18:01,602 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0570/0.0576
[2023-08-07 23:18:01,602 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0245/0.0375
[2023-08-07 23:18:01,602 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0059/0.0063
[2023-08-07 23:18:01,602 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1743/0.3215
[2023-08-07 23:18:01,602 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0997/0.1279
[2023-08-07 23:18:01,602 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0034/0.0035
[2023-08-07 23:18:01,602 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0011/0.0012
[2023-08-07 23:18:01,602 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.1803/0.2050
[2023-08-07 23:18:01,602 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.3542/0.5055
[2023-08-07 23:18:01,602 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 23:18:01,602 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0766/0.1119
[2023-08-07 23:18:01,602 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 23:18:01,602 INFO misc.py line 150 22900] Best validation mIoU updated to: 0.2196
[2023-08-07 23:18:01,602 INFO misc.py line 152 22900] Currently Best mIoU: 0.2196
[2023-08-07 23:18:01,602 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 23:18:07,370 INFO misc.py line 115 22900] Train: [30/100][1/156] Data 1.120 (1.120) Batch 4.731 (4.731) Remain 14:33:16 loss: 0.3020 Lr: 0.04252
[2023-08-07 23:18:09,438 INFO misc.py line 115 22900] Train: [30/100][2/156] Data 0.001 (0.001) Batch 2.068 (2.068) Remain 06:21:39 loss: 0.2376 Lr: 0.04252
[2023-08-07 23:18:12,339 INFO misc.py line 115 22900] Train: [30/100][3/156] Data 0.001 (0.001) Batch 2.901 (2.901) Remain 08:55:26 loss: 0.2928 Lr: 0.04252
[2023-08-07 23:18:16,015 INFO misc.py line 115 22900] Train: [30/100][4/156] Data 0.001 (0.001) Batch 3.675 (3.675) Remain 11:18:12 loss: 0.4405 Lr: 0.04251
[2023-08-07 23:18:20,367 INFO misc.py line 115 22900] Train: [30/100][5/156] Data 0.001 (0.001) Batch 4.353 (4.014) Remain 12:20:38 loss: 0.6530 Lr: 0.04251
[2023-08-07 23:18:22,779 INFO misc.py line 115 22900] Train: [30/100][6/156] Data 0.001 (0.001) Batch 2.412 (3.480) Remain 10:42:02 loss: 0.1672 Lr: 0.04250
[2023-08-07 23:18:26,872 INFO misc.py line 115 22900] Train: [30/100][7/156] Data 0.001 (0.001) Batch 4.092 (3.633) Remain 11:10:14 loss: 0.5950 Lr: 0.04250
[2023-08-07 23:18:31,407 INFO misc.py line 115 22900] Train: [30/100][8/156] Data 0.001 (0.001) Batch 4.535 (3.813) Remain 11:43:27 loss: 0.5094 Lr: 0.04250
[2023-08-07 23:18:34,947 INFO misc.py line 115 22900] Train: [30/100][9/156] Data 0.001 (0.001) Batch 3.540 (3.768) Remain 11:34:58 loss: 0.4858 Lr: 0.04249
[2023-08-07 23:18:38,483 INFO misc.py line 115 22900] Train: [30/100][10/156] Data 0.001 (0.001) Batch 3.537 (3.735) Remain 11:28:49 loss: 0.2913 Lr: 0.04249
[2023-08-07 23:18:41,996 INFO misc.py line 115 22900] Train: [30/100][11/156] Data 0.001 (0.001) Batch 3.513 (3.707) Remain 11:23:39 loss: 0.5270 Lr: 0.04249
[2023-08-07 23:18:45,836 INFO misc.py line 115 22900] Train: [30/100][12/156] Data 0.001 (0.001) Batch 3.840 (3.722) Remain 11:26:18 loss: 0.3785 Lr: 0.04248
[2023-08-07 23:18:50,338 INFO misc.py line 115 22900] Train: [30/100][13/156] Data 0.001 (0.001) Batch 4.502 (3.800) Remain 11:40:37 loss: 0.5350 Lr: 0.04248
[2023-08-07 23:18:53,656 INFO misc.py line 115 22900] Train: [30/100][14/156] Data 0.001 (0.001) Batch 3.318 (3.756) Remain 11:32:29 loss: 0.4569 Lr: 0.04247
[2023-08-07 23:18:57,816 INFO misc.py line 115 22900] Train: [30/100][15/156] Data 0.001 (0.001) Batch 4.160 (3.790) Remain 11:38:38 loss: 0.5356 Lr: 0.04247
[2023-08-07 23:19:00,324 INFO misc.py line 115 22900] Train: [30/100][16/156] Data 0.001 (0.001) Batch 2.508 (3.691) Remain 11:20:23 loss: 0.2503 Lr: 0.04247
[2023-08-07 23:19:02,323 INFO misc.py line 115 22900] Train: [30/100][17/156] Data 0.001 (0.001) Batch 1.999 (3.570) Remain 10:58:03 loss: 0.1713 Lr: 0.04246
[2023-08-07 23:19:05,880 INFO misc.py line 115 22900] Train: [30/100][18/156] Data 0.001 (0.001) Batch 3.557 (3.569) Remain 10:57:50 loss: 0.6068 Lr: 0.04246
[2023-08-07 23:19:10,010 INFO misc.py line 115 22900] Train: [30/100][19/156] Data 0.001 (0.001) Batch 4.130 (3.604) Remain 11:04:14 loss: 0.6556 Lr: 0.04246
[2023-08-07 23:19:13,600 INFO misc.py line 115 22900] Train: [30/100][20/156] Data 0.001 (0.001) Batch 3.590 (3.604) Remain 11:04:01 loss: 0.6527 Lr: 0.04245
[2023-08-07 23:19:17,120 INFO misc.py line 115 22900] Train: [30/100][21/156] Data 0.001 (0.001) Batch 3.520 (3.599) Remain 11:03:06 loss: 0.5777 Lr: 0.04245
[2023-08-07 23:19:20,533 INFO misc.py line 115 22900] Train: [30/100][22/156] Data 0.001 (0.001) Batch 3.413 (3.589) Remain 11:01:14 loss: 0.3713 Lr: 0.04244
[2023-08-07 23:19:24,051 INFO misc.py line 115 22900] Train: [30/100][23/156] Data 0.001 (0.001) Batch 3.517 (3.586) Remain 11:00:31 loss: 0.4948 Lr: 0.04244
[2023-08-07 23:19:27,345 INFO misc.py line 115 22900] Train: [30/100][24/156] Data 0.001 (0.001) Batch 3.294 (3.572) Remain 10:57:54 loss: 0.4967 Lr: 0.04244
[2023-08-07 23:19:31,084 INFO misc.py line 115 22900] Train: [30/100][25/156] Data 0.001 (0.001) Batch 3.739 (3.579) Remain 10:59:14 loss: 0.2410 Lr: 0.04243
[2023-08-07 23:19:34,919 INFO misc.py line 115 22900] Train: [30/100][26/156] Data 0.001 (0.001) Batch 3.835 (3.590) Remain 11:01:14 loss: 0.3716 Lr: 0.04243
[2023-08-07 23:19:37,149 INFO misc.py line 115 22900] Train: [30/100][27/156] Data 0.001 (0.001) Batch 2.230 (3.534) Remain 10:50:44 loss: 0.3451 Lr: 0.04242
[2023-08-07 23:19:41,063 INFO misc.py line 115 22900] Train: [30/100][28/156] Data 0.001 (0.001) Batch 3.913 (3.549) Remain 10:53:28 loss: 0.3871 Lr: 0.04242
[2023-08-07 23:19:44,268 INFO misc.py line 115 22900] Train: [30/100][29/156] Data 0.009 (0.001) Batch 3.205 (3.536) Remain 10:50:59 loss: 0.3094 Lr: 0.04242
[2023-08-07 23:19:48,294 INFO misc.py line 115 22900] Train: [30/100][30/156] Data 0.001 (0.001) Batch 4.026 (3.554) Remain 10:54:16 loss: 0.6478 Lr: 0.04241
[2023-08-07 23:19:51,727 INFO misc.py line 115 22900] Train: [30/100][31/156] Data 0.001 (0.001) Batch 3.433 (3.550) Remain 10:53:24 loss: 0.4712 Lr: 0.04241
[2023-08-07 23:19:55,358 INFO misc.py line 115 22900] Train: [30/100][32/156] Data 0.001 (0.001) Batch 3.631 (3.552) Remain 10:53:52 loss: 0.4941 Lr: 0.04241
[2023-08-07 23:19:58,778 INFO misc.py line 115 22900] Train: [30/100][33/156] Data 0.001 (0.001) Batch 3.420 (3.548) Remain 10:53:00 loss: 0.5943 Lr: 0.04240
[2023-08-07 23:20:01,571 INFO misc.py line 115 22900] Train: [30/100][34/156] Data 0.001 (0.001) Batch 2.793 (3.524) Remain 10:48:27 loss: 0.3922 Lr: 0.04240
[2023-08-07 23:20:04,817 INFO misc.py line 115 22900] Train: [30/100][35/156] Data 0.001 (0.001) Batch 3.246 (3.515) Remain 10:46:48 loss: 0.3759 Lr: 0.04239
[2023-08-07 23:20:07,944 INFO misc.py line 115 22900] Train: [30/100][36/156] Data 0.001 (0.001) Batch 3.127 (3.503) Remain 10:44:34 loss: 0.4285 Lr: 0.04239
[2023-08-07 23:20:12,281 INFO misc.py line 115 22900] Train: [30/100][37/156] Data 0.001 (0.001) Batch 4.337 (3.528) Remain 10:49:02 loss: 0.7193 Lr: 0.04239
[2023-08-07 23:20:16,615 INFO misc.py line 115 22900] Train: [30/100][38/156] Data 0.001 (0.001) Batch 4.334 (3.551) Remain 10:53:12 loss: 0.5631 Lr: 0.04238
[2023-08-07 23:20:20,405 INFO misc.py line 115 22900] Train: [30/100][39/156] Data 0.001 (0.001) Batch 3.790 (3.557) Remain 10:54:22 loss: 0.5902 Lr: 0.04238
[2023-08-07 23:20:23,775 INFO misc.py line 115 22900] Train: [30/100][40/156] Data 0.001 (0.001) Batch 3.370 (3.552) Remain 10:53:23 loss: 0.3320 Lr: 0.04238
[2023-08-07 23:20:27,795 INFO misc.py line 115 22900] Train: [30/100][41/156] Data 0.001 (0.001) Batch 4.020 (3.565) Remain 10:55:35 loss: 0.8975 Lr: 0.04237
[2023-08-07 23:20:30,484 INFO misc.py line 115 22900] Train: [30/100][42/156] Data 0.001 (0.001) Batch 2.689 (3.542) Remain 10:51:24 loss: 0.3008 Lr: 0.04237
[2023-08-07 23:20:33,053 INFO misc.py line 115 22900] Train: [30/100][43/156] Data 0.001 (0.001) Batch 2.569 (3.518) Remain 10:46:52 loss: 0.2147 Lr: 0.04236
[2023-08-07 23:20:35,924 INFO misc.py line 115 22900] Train: [30/100][44/156] Data 0.001 (0.001) Batch 2.870 (3.502) Remain 10:43:54 loss: 0.2462 Lr: 0.04236
[2023-08-07 23:20:39,129 INFO misc.py line 115 22900] Train: [30/100][45/156] Data 0.001 (0.001) Batch 3.205 (3.495) Remain 10:42:33 loss: 0.6516 Lr: 0.04236
[2023-08-07 23:20:42,477 INFO misc.py line 115 22900] Train: [30/100][46/156] Data 0.001 (0.001) Batch 3.348 (3.492) Remain 10:41:51 loss: 0.4316 Lr: 0.04235
[2023-08-07 23:20:45,641 INFO misc.py line 115 22900] Train: [30/100][47/156] Data 0.001 (0.001) Batch 3.164 (3.484) Remain 10:40:26 loss: 0.5708 Lr: 0.04235
[2023-08-07 23:20:49,011 INFO misc.py line 115 22900] Train: [30/100][48/156] Data 0.001 (0.001) Batch 3.371 (3.482) Remain 10:39:55 loss: 0.3535 Lr: 0.04234
[2023-08-07 23:20:52,113 INFO misc.py line 115 22900] Train: [30/100][49/156] Data 0.001 (0.001) Batch 3.101 (3.473) Remain 10:38:20 loss: 0.1629 Lr: 0.04234
[2023-08-07 23:20:56,281 INFO misc.py line 115 22900] Train: [30/100][50/156] Data 0.001 (0.001) Batch 4.168 (3.488) Remain 10:41:00 loss: 0.5300 Lr: 0.04234
[2023-08-07 23:21:00,378 INFO misc.py line 115 22900] Train: [30/100][51/156] Data 0.001 (0.001) Batch 4.096 (3.501) Remain 10:43:16 loss: 0.3806 Lr: 0.04233
[2023-08-07 23:21:03,168 INFO misc.py line 115 22900] Train: [30/100][52/156] Data 0.001 (0.001) Batch 2.791 (3.486) Remain 10:40:33 loss: 0.3063 Lr: 0.04233
[2023-08-07 23:21:04,989 INFO misc.py line 115 22900] Train: [30/100][53/156] Data 0.001 (0.001) Batch 1.820 (3.453) Remain 10:34:22 loss: 0.3672 Lr: 0.04233
[2023-08-07 23:21:08,327 INFO misc.py line 115 22900] Train: [30/100][54/156] Data 0.001 (0.001) Batch 3.339 (3.451) Remain 10:33:54 loss: 0.3553 Lr: 0.04232
[2023-08-07 23:21:11,533 INFO misc.py line 115 22900] Train: [30/100][55/156] Data 0.001 (0.001) Batch 3.206 (3.446) Remain 10:32:58 loss: 0.3256 Lr: 0.04232
[2023-08-07 23:21:14,966 INFO misc.py line 115 22900] Train: [30/100][56/156] Data 0.001 (0.001) Batch 3.432 (3.446) Remain 10:32:52 loss: 0.4812 Lr: 0.04231
[2023-08-07 23:21:18,595 INFO misc.py line 115 22900] Train: [30/100][57/156] Data 0.001 (0.001) Batch 3.629 (3.449) Remain 10:33:26 loss: 0.3025 Lr: 0.04231
[2023-08-07 23:21:22,308 INFO misc.py line 115 22900] Train: [30/100][58/156] Data 0.001 (0.001) Batch 3.713 (3.454) Remain 10:34:15 loss: 0.5536 Lr: 0.04231
[2023-08-07 23:21:25,630 INFO misc.py line 115 22900] Train: [30/100][59/156] Data 0.001 (0.001) Batch 3.322 (3.452) Remain 10:33:46 loss: 0.3539 Lr: 0.04230
[2023-08-07 23:21:29,612 INFO misc.py line 115 22900] Train: [30/100][60/156] Data 0.001 (0.001) Batch 3.983 (3.461) Remain 10:35:25 loss: 0.4986 Lr: 0.04230
[2023-08-07 23:21:32,209 INFO misc.py line 115 22900] Train: [30/100][61/156] Data 0.001 (0.001) Batch 2.597 (3.446) Remain 10:32:38 loss: 0.3263 Lr: 0.04230
[2023-08-07 23:21:36,434 INFO misc.py line 115 22900] Train: [30/100][62/156] Data 0.001 (0.001) Batch 4.225 (3.459) Remain 10:34:59 loss: 0.7698 Lr: 0.04229
[2023-08-07 23:21:40,423 INFO misc.py line 115 22900] Train: [30/100][63/156] Data 0.001 (0.001) Batch 3.989 (3.468) Remain 10:36:33 loss: 0.4565 Lr: 0.04229
[2023-08-07 23:21:43,832 INFO misc.py line 115 22900] Train: [30/100][64/156] Data 0.001 (0.001) Batch 3.409 (3.467) Remain 10:36:19 loss: 0.2618 Lr: 0.04228
[2023-08-07 23:21:47,597 INFO misc.py line 115 22900] Train: [30/100][65/156] Data 0.001 (0.001) Batch 3.765 (3.472) Remain 10:37:09 loss: 0.2083 Lr: 0.04228
[2023-08-07 23:21:50,197 INFO misc.py line 115 22900] Train: [30/100][66/156] Data 0.001 (0.001) Batch 2.599 (3.458) Remain 10:34:33 loss: 0.2764 Lr: 0.04228
[2023-08-07 23:21:52,817 INFO misc.py line 115 22900] Train: [30/100][67/156] Data 0.001 (0.001) Batch 2.620 (3.445) Remain 10:32:05 loss: 0.1747 Lr: 0.04227
[2023-08-07 23:21:56,777 INFO misc.py line 115 22900] Train: [30/100][68/156] Data 0.001 (0.001) Batch 3.960 (3.453) Remain 10:33:29 loss: 0.3391 Lr: 0.04227
[2023-08-07 23:22:00,575 INFO misc.py line 115 22900] Train: [30/100][69/156] Data 0.001 (0.001) Batch 3.798 (3.458) Remain 10:34:23 loss: 0.4713 Lr: 0.04226
[2023-08-07 23:22:02,726 INFO misc.py line 115 22900] Train: [30/100][70/156] Data 0.001 (0.001) Batch 2.151 (3.439) Remain 10:30:45 loss: 0.1158 Lr: 0.04226
[2023-08-07 23:22:06,756 INFO misc.py line 115 22900] Train: [30/100][71/156] Data 0.001 (0.001) Batch 4.029 (3.447) Remain 10:32:17 loss: 0.6700 Lr: 0.04226
[2023-08-07 23:22:10,429 INFO misc.py line 115 22900] Train: [30/100][72/156] Data 0.001 (0.001) Batch 3.673 (3.451) Remain 10:32:50 loss: 0.3961 Lr: 0.04225
[2023-08-07 23:22:13,419 INFO misc.py line 115 22900] Train: [30/100][73/156] Data 0.001 (0.001) Batch 2.991 (3.444) Remain 10:31:34 loss: 0.2895 Lr: 0.04225
[2023-08-07 23:22:16,264 INFO misc.py line 115 22900] Train: [30/100][74/156] Data 0.001 (0.001) Batch 2.845 (3.436) Remain 10:29:58 loss: 0.3037 Lr: 0.04225
[2023-08-07 23:22:20,513 INFO misc.py line 115 22900] Train: [30/100][75/156] Data 0.001 (0.001) Batch 4.249 (3.447) Remain 10:31:58 loss: 0.4675 Lr: 0.04224
[2023-08-07 23:22:24,475 INFO misc.py line 115 22900] Train: [30/100][76/156] Data 0.001 (0.001) Batch 3.962 (3.454) Remain 10:33:13 loss: 0.4048 Lr: 0.04224
[2023-08-07 23:22:28,303 INFO misc.py line 115 22900] Train: [30/100][77/156] Data 0.001 (0.001) Batch 3.828 (3.459) Remain 10:34:05 loss: 0.2975 Lr: 0.04223
[2023-08-07 23:22:31,785 INFO misc.py line 115 22900] Train: [30/100][78/156] Data 0.001 (0.001) Batch 3.482 (3.459) Remain 10:34:05 loss: 0.2851 Lr: 0.04223
[2023-08-07 23:22:34,784 INFO misc.py line 115 22900] Train: [30/100][79/156] Data 0.001 (0.001) Batch 3.000 (3.453) Remain 10:32:55 loss: 0.3139 Lr: 0.04223
[2023-08-07 23:22:37,273 INFO misc.py line 115 22900] Train: [30/100][80/156] Data 0.001 (0.001) Batch 2.489 (3.441) Remain 10:30:33 loss: 0.3822 Lr: 0.04222
[2023-08-07 23:22:40,787 INFO misc.py line 115 22900] Train: [30/100][81/156] Data 0.001 (0.001) Batch 3.513 (3.442) Remain 10:30:40 loss: 0.4558 Lr: 0.04222
[2023-08-07 23:22:43,753 INFO misc.py line 115 22900] Train: [30/100][82/156] Data 0.001 (0.001) Batch 2.967 (3.436) Remain 10:29:31 loss: 0.3293 Lr: 0.04221
[2023-08-07 23:22:47,786 INFO misc.py line 115 22900] Train: [30/100][83/156] Data 0.001 (0.001) Batch 4.032 (3.443) Remain 10:30:49 loss: 0.6127 Lr: 0.04221
[2023-08-07 23:22:51,272 INFO misc.py line 115 22900] Train: [30/100][84/156] Data 0.001 (0.001) Batch 3.486 (3.444) Remain 10:30:52 loss: 0.2007 Lr: 0.04221
[2023-08-07 23:22:54,690 INFO misc.py line 115 22900] Train: [30/100][85/156] Data 0.001 (0.001) Batch 3.418 (3.443) Remain 10:30:45 loss: 0.6808 Lr: 0.04220
[2023-08-07 23:22:58,396 INFO misc.py line 115 22900] Train: [30/100][86/156] Data 0.001 (0.001) Batch 3.706 (3.446) Remain 10:31:16 loss: 0.3258 Lr: 0.04220
[2023-08-07 23:23:01,576 INFO misc.py line 115 22900] Train: [30/100][87/156] Data 0.001 (0.001) Batch 3.180 (3.443) Remain 10:30:38 loss: 0.5176 Lr: 0.04220
[2023-08-07 23:23:04,270 INFO misc.py line 115 22900] Train: [30/100][88/156] Data 0.001 (0.001) Batch 2.694 (3.434) Remain 10:28:57 loss: 0.1625 Lr: 0.04219
[2023-08-07 23:23:07,877 INFO misc.py line 115 22900] Train: [30/100][89/156] Data 0.001 (0.001) Batch 3.607 (3.436) Remain 10:29:16 loss: 0.6373 Lr: 0.04219
[2023-08-07 23:23:11,652 INFO misc.py line 115 22900] Train: [30/100][90/156] Data 0.001 (0.001) Batch 3.775 (3.440) Remain 10:29:55 loss: 0.4774 Lr: 0.04218
[2023-08-07 23:23:15,964 INFO misc.py line 115 22900] Train: [30/100][91/156] Data 0.001 (0.001) Batch 4.312 (3.450) Remain 10:31:41 loss: 0.4895 Lr: 0.04218
[2023-08-07 23:23:19,391 INFO misc.py line 115 22900] Train: [30/100][92/156] Data 0.001 (0.001) Batch 3.427 (3.450) Remain 10:31:34 loss: 0.2555 Lr: 0.04218
[2023-08-07 23:23:22,379 INFO misc.py line 115 22900] Train: [30/100][93/156] Data 0.001 (0.001) Batch 2.988 (3.445) Remain 10:30:35 loss: 0.3508 Lr: 0.04217
[2023-08-07 23:23:25,817 INFO misc.py line 115 22900] Train: [30/100][94/156] Data 0.001 (0.001) Batch 3.438 (3.445) Remain 10:30:30 loss: 0.6213 Lr: 0.04217
[2023-08-07 23:23:29,007 INFO misc.py line 115 22900] Train: [30/100][95/156] Data 0.001 (0.001) Batch 3.190 (3.442) Remain 10:29:56 loss: 0.3635 Lr: 0.04216
[2023-08-07 23:23:33,362 INFO misc.py line 115 22900] Train: [30/100][96/156] Data 0.001 (0.001) Batch 4.355 (3.452) Remain 10:31:41 loss: 0.5172 Lr: 0.04216
[2023-08-07 23:23:36,728 INFO misc.py line 115 22900] Train: [30/100][97/156] Data 0.001 (0.001) Batch 3.367 (3.451) Remain 10:31:27 loss: 0.3828 Lr: 0.04216
[2023-08-07 23:23:40,535 INFO misc.py line 115 22900] Train: [30/100][98/156] Data 0.001 (0.001) Batch 3.806 (3.455) Remain 10:32:05 loss: 0.4649 Lr: 0.04215
[2023-08-07 23:23:45,019 INFO misc.py line 115 22900] Train: [30/100][99/156] Data 0.001 (0.001) Batch 4.484 (3.465) Remain 10:33:59 loss: 0.4903 Lr: 0.04215
[2023-08-07 23:23:48,193 INFO misc.py line 115 22900] Train: [30/100][100/156] Data 0.001 (0.001) Batch 3.174 (3.462) Remain 10:33:23 loss: 0.3311 Lr: 0.04215
[2023-08-07 23:23:51,764 INFO misc.py line 115 22900] Train: [30/100][101/156] Data 0.001 (0.001) Batch 3.571 (3.464) Remain 10:33:32 loss: 0.3042 Lr: 0.04214
[2023-08-07 23:23:56,544 INFO misc.py line 115 22900] Train: [30/100][102/156] Data 0.001 (0.001) Batch 4.780 (3.477) Remain 10:35:54 loss: 0.5165 Lr: 0.04214
[2023-08-07 23:23:59,642 INFO misc.py line 115 22900] Train: [30/100][103/156] Data 0.001 (0.001) Batch 3.098 (3.473) Remain 10:35:09 loss: 0.4822 Lr: 0.04213
[2023-08-07 23:24:02,467 INFO misc.py line 115 22900] Train: [30/100][104/156] Data 0.001 (0.001) Batch 2.825 (3.467) Remain 10:33:55 loss: 0.3165 Lr: 0.04213
[2023-08-07 23:24:06,551 INFO misc.py line 115 22900] Train: [30/100][105/156] Data 0.001 (0.001) Batch 4.084 (3.473) Remain 10:34:58 loss: 0.7253 Lr: 0.04213
[2023-08-07 23:24:09,301 INFO misc.py line 115 22900] Train: [30/100][106/156] Data 0.001 (0.001) Batch 2.750 (3.466) Remain 10:33:38 loss: 0.2952 Lr: 0.04212
[2023-08-07 23:24:13,385 INFO misc.py line 115 22900] Train: [30/100][107/156] Data 0.001 (0.001) Batch 4.083 (3.472) Remain 10:34:39 loss: 0.3081 Lr: 0.04212
[2023-08-07 23:24:16,209 INFO misc.py line 115 22900] Train: [30/100][108/156] Data 0.001 (0.001) Batch 2.824 (3.465) Remain 10:33:28 loss: 0.3712 Lr: 0.04211
[2023-08-07 23:24:19,484 INFO misc.py line 115 22900] Train: [30/100][109/156] Data 0.001 (0.001) Batch 3.275 (3.464) Remain 10:33:05 loss: 0.3769 Lr: 0.04211
[2023-08-07 23:24:23,609 INFO misc.py line 115 22900] Train: [30/100][110/156] Data 0.001 (0.001) Batch 4.124 (3.470) Remain 10:34:09 loss: 0.3113 Lr: 0.04211
[2023-08-07 23:24:26,130 INFO misc.py line 115 22900] Train: [30/100][111/156] Data 0.001 (0.001) Batch 2.521 (3.461) Remain 10:32:30 loss: 0.1137 Lr: 0.04210
[2023-08-07 23:24:30,507 INFO misc.py line 115 22900] Train: [30/100][112/156] Data 0.001 (0.001) Batch 4.378 (3.469) Remain 10:33:58 loss: 0.4326 Lr: 0.04210
[2023-08-07 23:24:33,800 INFO misc.py line 115 22900] Train: [30/100][113/156] Data 0.001 (0.001) Batch 3.293 (3.468) Remain 10:33:37 loss: 0.4151 Lr: 0.04210
[2023-08-07 23:24:36,871 INFO misc.py line 115 22900] Train: [30/100][114/156] Data 0.001 (0.001) Batch 3.071 (3.464) Remain 10:32:55 loss: 0.3818 Lr: 0.04209
[2023-08-07 23:24:40,454 INFO misc.py line 115 22900] Train: [30/100][115/156] Data 0.001 (0.001) Batch 3.583 (3.465) Remain 10:33:03 loss: 0.3171 Lr: 0.04209
[2023-08-07 23:24:44,455 INFO misc.py line 115 22900] Train: [30/100][116/156] Data 0.001 (0.001) Batch 4.000 (3.470) Remain 10:33:51 loss: 0.7795 Lr: 0.04208
[2023-08-07 23:24:48,101 INFO misc.py line 115 22900] Train: [30/100][117/156] Data 0.001 (0.001) Batch 3.646 (3.472) Remain 10:34:05 loss: 0.4652 Lr: 0.04208
[2023-08-07 23:24:51,552 INFO misc.py line 115 22900] Train: [30/100][118/156] Data 0.001 (0.001) Batch 3.451 (3.471) Remain 10:33:59 loss: 0.3820 Lr: 0.04208
[2023-08-07 23:24:54,867 INFO misc.py line 115 22900] Train: [30/100][119/156] Data 0.001 (0.001) Batch 3.315 (3.470) Remain 10:33:41 loss: 0.2800 Lr: 0.04207
[2023-08-07 23:24:58,608 INFO misc.py line 115 22900] Train: [30/100][120/156] Data 0.001 (0.001) Batch 3.740 (3.472) Remain 10:34:03 loss: 0.4658 Lr: 0.04207
[2023-08-07 23:25:02,145 INFO misc.py line 115 22900] Train: [30/100][121/156] Data 0.001 (0.001) Batch 3.537 (3.473) Remain 10:34:05 loss: 0.5428 Lr: 0.04206
[2023-08-07 23:25:04,863 INFO misc.py line 115 22900] Train: [30/100][122/156] Data 0.001 (0.001) Batch 2.719 (3.467) Remain 10:32:52 loss: 0.2639 Lr: 0.04206
[2023-08-07 23:25:08,904 INFO misc.py line 115 22900] Train: [30/100][123/156] Data 0.001 (0.001) Batch 4.041 (3.471) Remain 10:33:41 loss: 0.3363 Lr: 0.04206
[2023-08-07 23:25:12,678 INFO misc.py line 115 22900] Train: [30/100][124/156] Data 0.002 (0.001) Batch 3.773 (3.474) Remain 10:34:05 loss: 0.5537 Lr: 0.04205
[2023-08-07 23:25:16,060 INFO misc.py line 115 22900] Train: [30/100][125/156] Data 0.001 (0.001) Batch 3.383 (3.473) Remain 10:33:54 loss: 0.4885 Lr: 0.04205
[2023-08-07 23:25:18,007 INFO misc.py line 115 22900] Train: [30/100][126/156] Data 0.001 (0.001) Batch 1.947 (3.461) Remain 10:31:34 loss: 0.4637 Lr: 0.04204
[2023-08-07 23:25:22,123 INFO misc.py line 115 22900] Train: [30/100][127/156] Data 0.001 (0.001) Batch 4.116 (3.466) Remain 10:32:29 loss: 0.4395 Lr: 0.04204
[2023-08-07 23:25:24,394 INFO misc.py line 115 22900] Train: [30/100][128/156] Data 0.001 (0.001) Batch 2.270 (3.456) Remain 10:30:41 loss: 0.1705 Lr: 0.04204
[2023-08-07 23:25:28,487 INFO misc.py line 115 22900] Train: [30/100][129/156] Data 0.001 (0.001) Batch 4.094 (3.461) Remain 10:31:32 loss: 0.4208 Lr: 0.04203
[2023-08-07 23:25:32,477 INFO misc.py line 115 22900] Train: [30/100][130/156] Data 0.001 (0.001) Batch 3.989 (3.466) Remain 10:32:14 loss: 0.4206 Lr: 0.04203
[2023-08-07 23:25:36,010 INFO misc.py line 115 22900] Train: [30/100][131/156] Data 0.001 (0.001) Batch 3.534 (3.466) Remain 10:32:17 loss: 0.3914 Lr: 0.04203
[2023-08-07 23:25:40,045 INFO misc.py line 115 22900] Train: [30/100][132/156] Data 0.001 (0.001) Batch 4.035 (3.471) Remain 10:33:02 loss: 0.5084 Lr: 0.04202
[2023-08-07 23:25:43,208 INFO misc.py line 115 22900] Train: [30/100][133/156] Data 0.001 (0.001) Batch 3.162 (3.468) Remain 10:32:32 loss: 0.5299 Lr: 0.04202
[2023-08-07 23:25:46,381 INFO misc.py line 115 22900] Train: [30/100][134/156] Data 0.001 (0.001) Batch 3.173 (3.466) Remain 10:32:04 loss: 0.4402 Lr: 0.04201
[2023-08-07 23:25:50,061 INFO misc.py line 115 22900] Train: [30/100][135/156] Data 0.001 (0.001) Batch 3.681 (3.468) Remain 10:32:18 loss: 0.2940 Lr: 0.04201
[2023-08-07 23:25:53,482 INFO misc.py line 115 22900] Train: [30/100][136/156] Data 0.001 (0.001) Batch 3.420 (3.467) Remain 10:32:11 loss: 0.4818 Lr: 0.04201
[2023-08-07 23:25:56,027 INFO misc.py line 115 22900] Train: [30/100][137/156] Data 0.001 (0.001) Batch 2.545 (3.460) Remain 10:30:52 loss: 0.3770 Lr: 0.04200
[2023-08-07 23:25:59,883 INFO misc.py line 115 22900] Train: [30/100][138/156] Data 0.001 (0.001) Batch 3.856 (3.463) Remain 10:31:21 loss: 0.3690 Lr: 0.04200
[2023-08-07 23:26:03,088 INFO misc.py line 115 22900] Train: [30/100][139/156] Data 0.001 (0.001) Batch 3.205 (3.461) Remain 10:30:57 loss: 0.3318 Lr: 0.04199
[2023-08-07 23:26:07,106 INFO misc.py line 115 22900] Train: [30/100][140/156] Data 0.001 (0.001) Batch 4.018 (3.465) Remain 10:31:38 loss: 0.4323 Lr: 0.04199
[2023-08-07 23:26:09,937 INFO misc.py line 115 22900] Train: [30/100][141/156] Data 0.001 (0.001) Batch 2.830 (3.461) Remain 10:30:44 loss: 0.3065 Lr: 0.04199
[2023-08-07 23:26:14,133 INFO misc.py line 115 22900] Train: [30/100][142/156] Data 0.001 (0.001) Batch 4.196 (3.466) Remain 10:31:38 loss: 0.6680 Lr: 0.04198
[2023-08-07 23:26:18,655 INFO misc.py line 115 22900] Train: [30/100][143/156] Data 0.001 (0.001) Batch 4.522 (3.474) Remain 10:32:57 loss: 0.5410 Lr: 0.04198
[2023-08-07 23:26:22,430 INFO misc.py line 115 22900] Train: [30/100][144/156] Data 0.001 (0.001) Batch 3.775 (3.476) Remain 10:33:17 loss: 0.4933 Lr: 0.04197
[2023-08-07 23:26:24,726 INFO misc.py line 115 22900] Train: [30/100][145/156] Data 0.001 (0.001) Batch 2.297 (3.468) Remain 10:31:43 loss: 0.4479 Lr: 0.04197
[2023-08-07 23:26:28,816 INFO misc.py line 115 22900] Train: [30/100][146/156] Data 0.001 (0.001) Batch 4.090 (3.472) Remain 10:32:27 loss: 0.6231 Lr: 0.04197
[2023-08-07 23:26:32,922 INFO misc.py line 115 22900] Train: [30/100][147/156] Data 0.001 (0.001) Batch 4.106 (3.476) Remain 10:33:12 loss: 0.5977 Lr: 0.04196
[2023-08-07 23:26:36,113 INFO misc.py line 115 22900] Train: [30/100][148/156] Data 0.001 (0.001) Batch 3.191 (3.474) Remain 10:32:47 loss: 0.5190 Lr: 0.04196
[2023-08-07 23:26:39,656 INFO misc.py line 115 22900] Train: [30/100][149/156] Data 0.001 (0.001) Batch 3.543 (3.475) Remain 10:32:48 loss: 0.2993 Lr: 0.04196
[2023-08-07 23:26:42,901 INFO misc.py line 115 22900] Train: [30/100][150/156] Data 0.001 (0.001) Batch 3.245 (3.473) Remain 10:32:28 loss: 0.3922 Lr: 0.04195
[2023-08-07 23:26:46,979 INFO misc.py line 115 22900] Train: [30/100][151/156] Data 0.001 (0.001) Batch 4.078 (3.477) Remain 10:33:09 loss: 0.5810 Lr: 0.04195
[2023-08-07 23:26:50,389 INFO misc.py line 115 22900] Train: [30/100][152/156] Data 0.001 (0.001) Batch 3.410 (3.477) Remain 10:33:01 loss: 0.6794 Lr: 0.04194
[2023-08-07 23:26:53,783 INFO misc.py line 115 22900] Train: [30/100][153/156] Data 0.001 (0.001) Batch 3.394 (3.476) Remain 10:32:51 loss: 0.3176 Lr: 0.04194
[2023-08-07 23:26:57,665 INFO misc.py line 115 22900] Train: [30/100][154/156] Data 0.001 (0.001) Batch 3.882 (3.479) Remain 10:33:17 loss: 0.5042 Lr: 0.04194
[2023-08-07 23:27:01,194 INFO misc.py line 115 22900] Train: [30/100][155/156] Data 0.001 (0.001) Batch 3.529 (3.479) Remain 10:33:17 loss: 0.5190 Lr: 0.04193
[2023-08-07 23:27:04,907 INFO misc.py line 115 22900] Train: [30/100][156/156] Data 0.001 (0.001) Batch 3.712 (3.481) Remain 10:33:30 loss: 0.3822 Lr: 0.04193
[2023-08-07 23:27:04,907 INFO misc.py line 129 22900] Train result: loss: 0.4249 
[2023-08-07 23:27:04,907 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 23:27:06,996 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.7067 
[2023-08-07 23:27:07,866 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.5594 
[2023-08-07 23:27:09,530 INFO evaluator.py line 122 22900] Test: [3/24] Loss 1.2384 
[2023-08-07 23:27:11,051 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.1972 
[2023-08-07 23:27:12,898 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.8013 
[2023-08-07 23:27:14,561 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.6474 
[2023-08-07 23:27:16,696 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.2503 
[2023-08-07 23:27:18,503 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.7891 
[2023-08-07 23:27:19,786 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.4277 
[2023-08-07 23:27:21,916 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.4642 
[2023-08-07 23:27:22,441 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.2037 
[2023-08-07 23:27:23,975 INFO evaluator.py line 122 22900] Test: [12/24] Loss 1.0337 
[2023-08-07 23:27:26,684 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.1115 
[2023-08-07 23:27:28,365 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.8777 
[2023-08-07 23:27:30,387 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.4573 
[2023-08-07 23:27:33,095 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.1743 
[2023-08-07 23:27:35,799 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.4014 
[2023-08-07 23:27:37,644 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.5227 
[2023-08-07 23:27:38,393 INFO evaluator.py line 122 22900] Test: [19/24] Loss 0.9940 
[2023-08-07 23:27:39,278 INFO evaluator.py line 122 22900] Test: [20/24] Loss 1.0603 
[2023-08-07 23:27:41,541 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3449 
[2023-08-07 23:27:43,505 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.8007 
[2023-08-07 23:27:45,354 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.7699 
[2023-08-07 23:27:47,288 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.3774 
[2023-08-07 23:27:47,338 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2148/0.3032/0.6774.
[2023-08-07 23:27:47,338 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6502/0.9255
[2023-08-07 23:27:47,338 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9252/0.9910
[2023-08-07 23:27:47,338 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1287/0.2573
[2023-08-07 23:27:47,338 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1017/0.1961
[2023-08-07 23:27:47,338 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6022/0.8526
[2023-08-07 23:27:47,338 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.1643/0.1747
[2023-08-07 23:27:47,338 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.3668/0.3963
[2023-08-07 23:27:47,338 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.2217/0.2915
[2023-08-07 23:27:47,338 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1222/0.2485
[2023-08-07 23:27:47,338 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0302/0.0303
[2023-08-07 23:27:47,338 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0051/0.0098
[2023-08-07 23:27:47,338 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0024/0.0024
[2023-08-07 23:27:47,338 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1775/0.5538
[2023-08-07 23:27:47,338 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.2208/0.3317
[2023-08-07 23:27:47,338 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0535/0.0824
[2023-08-07 23:27:47,338 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0996/0.1029
[2023-08-07 23:27:47,338 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0332/0.0476
[2023-08-07 23:27:47,338 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.2990/0.4198
[2023-08-07 23:27:47,338 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 23:27:47,338 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0915/0.1508
[2023-08-07 23:27:47,338 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 23:27:47,339 INFO misc.py line 152 22900] Currently Best mIoU: 0.2196
[2023-08-07 23:27:47,339 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 23:27:52,583 INFO misc.py line 115 22900] Train: [31/100][1/156] Data 0.892 (0.892) Batch 4.469 (4.469) Remain 13:33:19 loss: 0.3117 Lr: 0.04192
[2023-08-07 23:27:56,960 INFO misc.py line 115 22900] Train: [31/100][2/156] Data 0.001 (0.001) Batch 4.377 (4.377) Remain 13:16:27 loss: 0.6320 Lr: 0.04192
[2023-08-07 23:28:00,273 INFO misc.py line 115 22900] Train: [31/100][3/156] Data 0.001 (0.001) Batch 3.314 (3.314) Remain 10:02:54 loss: 0.3791 Lr: 0.04192
[2023-08-07 23:28:03,550 INFO misc.py line 115 22900] Train: [31/100][4/156] Data 0.001 (0.001) Batch 3.277 (3.277) Remain 09:56:14 loss: 0.2308 Lr: 0.04191
[2023-08-07 23:28:06,757 INFO misc.py line 115 22900] Train: [31/100][5/156] Data 0.001 (0.001) Batch 3.207 (3.242) Remain 09:49:46 loss: 0.4999 Lr: 0.04191
[2023-08-07 23:28:10,705 INFO misc.py line 115 22900] Train: [31/100][6/156] Data 0.001 (0.001) Batch 3.947 (3.477) Remain 10:32:29 loss: 0.3438 Lr: 0.04190
[2023-08-07 23:28:14,426 INFO misc.py line 115 22900] Train: [31/100][7/156] Data 0.001 (0.001) Batch 3.721 (3.538) Remain 10:43:31 loss: 0.5077 Lr: 0.04190
[2023-08-07 23:28:17,133 INFO misc.py line 115 22900] Train: [31/100][8/156] Data 0.001 (0.001) Batch 2.708 (3.372) Remain 10:13:15 loss: 0.1390 Lr: 0.04190
[2023-08-07 23:28:20,170 INFO misc.py line 115 22900] Train: [31/100][9/156] Data 0.001 (0.001) Batch 3.036 (3.316) Remain 10:03:02 loss: 0.4093 Lr: 0.04189
[2023-08-07 23:28:23,515 INFO misc.py line 115 22900] Train: [31/100][10/156] Data 0.001 (0.001) Batch 3.346 (3.320) Remain 10:03:44 loss: 0.7122 Lr: 0.04189
[2023-08-07 23:28:26,192 INFO misc.py line 115 22900] Train: [31/100][11/156] Data 0.001 (0.001) Batch 2.677 (3.240) Remain 09:49:03 loss: 0.3904 Lr: 0.04189
[2023-08-07 23:28:29,642 INFO misc.py line 115 22900] Train: [31/100][12/156] Data 0.001 (0.001) Batch 3.450 (3.263) Remain 09:53:14 loss: 0.1986 Lr: 0.04188
[2023-08-07 23:28:32,690 INFO misc.py line 115 22900] Train: [31/100][13/156] Data 0.001 (0.001) Batch 3.048 (3.242) Remain 09:49:17 loss: 0.3131 Lr: 0.04188
[2023-08-07 23:28:36,135 INFO misc.py line 115 22900] Train: [31/100][14/156] Data 0.001 (0.001) Batch 3.445 (3.260) Remain 09:52:35 loss: 0.5835 Lr: 0.04187
[2023-08-07 23:28:40,088 INFO misc.py line 115 22900] Train: [31/100][15/156] Data 0.001 (0.001) Batch 3.953 (3.318) Remain 10:03:01 loss: 0.3533 Lr: 0.04187
[2023-08-07 23:28:44,742 INFO misc.py line 115 22900] Train: [31/100][16/156] Data 0.001 (0.001) Batch 4.654 (3.421) Remain 10:21:39 loss: 0.4746 Lr: 0.04187
[2023-08-07 23:28:48,091 INFO misc.py line 115 22900] Train: [31/100][17/156] Data 0.001 (0.001) Batch 3.349 (3.416) Remain 10:20:40 loss: 0.1941 Lr: 0.04186
[2023-08-07 23:28:51,727 INFO misc.py line 115 22900] Train: [31/100][18/156] Data 0.001 (0.001) Batch 3.635 (3.430) Remain 10:23:16 loss: 0.2756 Lr: 0.04186
[2023-08-07 23:28:55,417 INFO misc.py line 115 22900] Train: [31/100][19/156] Data 0.001 (0.001) Batch 3.690 (3.446) Remain 10:26:09 loss: 0.5331 Lr: 0.04185
[2023-08-07 23:28:59,160 INFO misc.py line 115 22900] Train: [31/100][20/156] Data 0.001 (0.001) Batch 3.743 (3.464) Remain 10:29:16 loss: 0.4838 Lr: 0.04185
[2023-08-07 23:29:01,550 INFO misc.py line 115 22900] Train: [31/100][21/156] Data 0.001 (0.001) Batch 2.390 (3.404) Remain 10:18:22 loss: 0.1695 Lr: 0.04185
[2023-08-07 23:29:05,102 INFO misc.py line 115 22900] Train: [31/100][22/156] Data 0.001 (0.001) Batch 3.552 (3.412) Remain 10:19:44 loss: 0.2968 Lr: 0.04184
[2023-08-07 23:29:08,833 INFO misc.py line 115 22900] Train: [31/100][23/156] Data 0.001 (0.001) Batch 3.731 (3.428) Remain 10:22:34 loss: 0.3981 Lr: 0.04184
[2023-08-07 23:29:12,224 INFO misc.py line 115 22900] Train: [31/100][24/156] Data 0.001 (0.001) Batch 3.391 (3.426) Remain 10:22:12 loss: 0.4897 Lr: 0.04183
[2023-08-07 23:29:15,407 INFO misc.py line 115 22900] Train: [31/100][25/156] Data 0.001 (0.001) Batch 3.183 (3.415) Remain 10:20:08 loss: 0.2851 Lr: 0.04183
[2023-08-07 23:29:18,242 INFO misc.py line 115 22900] Train: [31/100][26/156] Data 0.001 (0.001) Batch 2.835 (3.390) Remain 10:15:30 loss: 0.3916 Lr: 0.04183
[2023-08-07 23:29:21,083 INFO misc.py line 115 22900] Train: [31/100][27/156] Data 0.001 (0.001) Batch 2.841 (3.367) Remain 10:11:17 loss: 0.3220 Lr: 0.04182
[2023-08-07 23:29:23,789 INFO misc.py line 115 22900] Train: [31/100][28/156] Data 0.001 (0.001) Batch 2.706 (3.341) Remain 10:06:26 loss: 0.1639 Lr: 0.04182
[2023-08-07 23:29:27,881 INFO misc.py line 115 22900] Train: [31/100][29/156] Data 0.001 (0.001) Batch 4.092 (3.370) Remain 10:11:37 loss: 0.4087 Lr: 0.04181
[2023-08-07 23:29:30,966 INFO misc.py line 115 22900] Train: [31/100][30/156] Data 0.001 (0.001) Batch 3.085 (3.359) Remain 10:09:39 loss: 0.2698 Lr: 0.04181
[2023-08-07 23:29:34,587 INFO misc.py line 115 22900] Train: [31/100][31/156] Data 0.001 (0.001) Batch 3.621 (3.368) Remain 10:11:17 loss: 0.3337 Lr: 0.04181
[2023-08-07 23:29:38,854 INFO misc.py line 115 22900] Train: [31/100][32/156] Data 0.001 (0.001) Batch 4.267 (3.399) Remain 10:16:51 loss: 0.4297 Lr: 0.04180
[2023-08-07 23:29:41,593 INFO misc.py line 115 22900] Train: [31/100][33/156] Data 0.001 (0.001) Batch 2.739 (3.377) Remain 10:12:49 loss: 0.2895 Lr: 0.04180
[2023-08-07 23:29:45,730 INFO misc.py line 115 22900] Train: [31/100][34/156] Data 0.001 (0.001) Batch 4.137 (3.402) Remain 10:17:12 loss: 0.5307 Lr: 0.04180
[2023-08-07 23:29:49,269 INFO misc.py line 115 22900] Train: [31/100][35/156] Data 0.001 (0.001) Batch 3.538 (3.406) Remain 10:17:55 loss: 0.7526 Lr: 0.04179
[2023-08-07 23:29:52,647 INFO misc.py line 115 22900] Train: [31/100][36/156] Data 0.001 (0.001) Batch 3.378 (3.405) Remain 10:17:42 loss: 0.2952 Lr: 0.04179
[2023-08-07 23:29:55,263 INFO misc.py line 115 22900] Train: [31/100][37/156] Data 0.001 (0.001) Batch 2.617 (3.382) Remain 10:13:27 loss: 0.3187 Lr: 0.04178
[2023-08-07 23:29:58,559 INFO misc.py line 115 22900] Train: [31/100][38/156] Data 0.001 (0.001) Batch 3.296 (3.380) Remain 10:12:56 loss: 0.2142 Lr: 0.04178
[2023-08-07 23:30:02,080 INFO misc.py line 115 22900] Train: [31/100][39/156] Data 0.001 (0.001) Batch 3.521 (3.384) Remain 10:13:36 loss: 0.2919 Lr: 0.04178
[2023-08-07 23:30:05,685 INFO misc.py line 115 22900] Train: [31/100][40/156] Data 0.001 (0.001) Batch 3.605 (3.390) Remain 10:14:37 loss: 0.4092 Lr: 0.04177
[2023-08-07 23:30:07,978 INFO misc.py line 115 22900] Train: [31/100][41/156] Data 0.001 (0.001) Batch 2.293 (3.361) Remain 10:09:20 loss: 0.1588 Lr: 0.04177
[2023-08-07 23:30:12,049 INFO misc.py line 115 22900] Train: [31/100][42/156] Data 0.001 (0.001) Batch 4.071 (3.379) Remain 10:12:35 loss: 0.4027 Lr: 0.04176
[2023-08-07 23:30:15,292 INFO misc.py line 115 22900] Train: [31/100][43/156] Data 0.001 (0.001) Batch 3.243 (3.375) Remain 10:11:54 loss: 0.4055 Lr: 0.04176
[2023-08-07 23:30:18,612 INFO misc.py line 115 22900] Train: [31/100][44/156] Data 0.001 (0.001) Batch 3.320 (3.374) Remain 10:11:36 loss: 0.8617 Lr: 0.04176
[2023-08-07 23:30:22,527 INFO misc.py line 115 22900] Train: [31/100][45/156] Data 0.001 (0.001) Batch 3.915 (3.387) Remain 10:13:53 loss: 0.3515 Lr: 0.04175
[2023-08-07 23:30:26,629 INFO misc.py line 115 22900] Train: [31/100][46/156] Data 0.001 (0.001) Batch 4.102 (3.404) Remain 10:16:50 loss: 0.6926 Lr: 0.04175
[2023-08-07 23:30:30,028 INFO misc.py line 115 22900] Train: [31/100][47/156] Data 0.001 (0.001) Batch 3.399 (3.404) Remain 10:16:46 loss: 0.4273 Lr: 0.04174
[2023-08-07 23:30:32,624 INFO misc.py line 115 22900] Train: [31/100][48/156] Data 0.001 (0.001) Batch 2.596 (3.386) Remain 10:13:27 loss: 0.3109 Lr: 0.04174
[2023-08-07 23:30:36,506 INFO misc.py line 115 22900] Train: [31/100][49/156] Data 0.001 (0.001) Batch 3.883 (3.396) Remain 10:15:21 loss: 0.4791 Lr: 0.04174
[2023-08-07 23:30:40,649 INFO misc.py line 115 22900] Train: [31/100][50/156] Data 0.001 (0.001) Batch 4.142 (3.412) Remain 10:18:11 loss: 0.7300 Lr: 0.04173
[2023-08-07 23:30:44,608 INFO misc.py line 115 22900] Train: [31/100][51/156] Data 0.001 (0.001) Batch 3.959 (3.424) Remain 10:20:11 loss: 0.6236 Lr: 0.04173
[2023-08-07 23:30:47,727 INFO misc.py line 115 22900] Train: [31/100][52/156] Data 0.001 (0.001) Batch 3.119 (3.417) Remain 10:19:00 loss: 0.3410 Lr: 0.04172
[2023-08-07 23:30:51,855 INFO misc.py line 115 22900] Train: [31/100][53/156] Data 0.001 (0.001) Batch 4.128 (3.432) Remain 10:21:31 loss: 0.7089 Lr: 0.04172
[2023-08-07 23:30:55,997 INFO misc.py line 115 22900] Train: [31/100][54/156] Data 0.001 (0.001) Batch 4.142 (3.446) Remain 10:23:59 loss: 0.3151 Lr: 0.04172
[2023-08-07 23:31:00,498 INFO misc.py line 115 22900] Train: [31/100][55/156] Data 0.001 (0.001) Batch 4.500 (3.466) Remain 10:27:36 loss: 0.3972 Lr: 0.04171
[2023-08-07 23:31:04,923 INFO misc.py line 115 22900] Train: [31/100][56/156] Data 0.001 (0.001) Batch 4.425 (3.484) Remain 10:30:49 loss: 0.5926 Lr: 0.04171
[2023-08-07 23:31:08,340 INFO misc.py line 115 22900] Train: [31/100][57/156] Data 0.001 (0.001) Batch 3.417 (3.483) Remain 10:30:32 loss: 0.3787 Lr: 0.04170
[2023-08-07 23:31:10,946 INFO misc.py line 115 22900] Train: [31/100][58/156] Data 0.001 (0.001) Batch 2.606 (3.467) Remain 10:27:36 loss: 0.2159 Lr: 0.04170
[2023-08-07 23:31:13,746 INFO misc.py line 115 22900] Train: [31/100][59/156] Data 0.001 (0.001) Batch 2.800 (3.455) Remain 10:25:23 loss: 0.1955 Lr: 0.04170
[2023-08-07 23:31:17,414 INFO misc.py line 115 22900] Train: [31/100][60/156] Data 0.001 (0.001) Batch 3.668 (3.459) Remain 10:26:00 loss: 0.4960 Lr: 0.04169
[2023-08-07 23:31:20,946 INFO misc.py line 115 22900] Train: [31/100][61/156] Data 0.001 (0.001) Batch 3.533 (3.460) Remain 10:26:10 loss: 0.3864 Lr: 0.04169
[2023-08-07 23:31:24,988 INFO misc.py line 115 22900] Train: [31/100][62/156] Data 0.001 (0.001) Batch 4.042 (3.470) Remain 10:27:54 loss: 0.5216 Lr: 0.04168
[2023-08-07 23:31:28,325 INFO misc.py line 115 22900] Train: [31/100][63/156] Data 0.001 (0.001) Batch 3.337 (3.468) Remain 10:27:26 loss: 0.4750 Lr: 0.04168
[2023-08-07 23:31:31,670 INFO misc.py line 115 22900] Train: [31/100][64/156] Data 0.001 (0.001) Batch 3.346 (3.466) Remain 10:27:01 loss: 0.4388 Lr: 0.04168
[2023-08-07 23:31:35,300 INFO misc.py line 115 22900] Train: [31/100][65/156] Data 0.001 (0.001) Batch 3.629 (3.468) Remain 10:27:26 loss: 0.3383 Lr: 0.04167
[2023-08-07 23:31:38,190 INFO misc.py line 115 22900] Train: [31/100][66/156] Data 0.001 (0.001) Batch 2.890 (3.459) Remain 10:25:43 loss: 0.4531 Lr: 0.04167
[2023-08-07 23:31:42,839 INFO misc.py line 115 22900] Train: [31/100][67/156] Data 0.001 (0.001) Batch 4.649 (3.478) Remain 10:29:02 loss: 1.0039 Lr: 0.04167
[2023-08-07 23:31:46,073 INFO misc.py line 115 22900] Train: [31/100][68/156] Data 0.001 (0.001) Batch 3.234 (3.474) Remain 10:28:18 loss: 0.3583 Lr: 0.04166
[2023-08-07 23:31:49,282 INFO misc.py line 115 22900] Train: [31/100][69/156] Data 0.001 (0.001) Batch 3.209 (3.470) Remain 10:27:31 loss: 0.5972 Lr: 0.04166
[2023-08-07 23:31:51,245 INFO misc.py line 115 22900] Train: [31/100][70/156] Data 0.001 (0.001) Batch 1.962 (3.447) Remain 10:23:23 loss: 0.4123 Lr: 0.04165
[2023-08-07 23:31:54,321 INFO misc.py line 115 22900] Train: [31/100][71/156] Data 0.001 (0.001) Batch 3.076 (3.442) Remain 10:22:20 loss: 0.5742 Lr: 0.04165
[2023-08-07 23:31:57,138 INFO misc.py line 115 22900] Train: [31/100][72/156] Data 0.001 (0.001) Batch 2.817 (3.433) Remain 10:20:39 loss: 0.1523 Lr: 0.04165
[2023-08-07 23:32:00,672 INFO misc.py line 115 22900] Train: [31/100][73/156] Data 0.001 (0.001) Batch 3.534 (3.434) Remain 10:20:51 loss: 0.4816 Lr: 0.04164
[2023-08-07 23:32:03,337 INFO misc.py line 115 22900] Train: [31/100][74/156] Data 0.001 (0.001) Batch 2.664 (3.423) Remain 10:18:50 loss: 0.2384 Lr: 0.04164
[2023-08-07 23:32:07,395 INFO misc.py line 115 22900] Train: [31/100][75/156] Data 0.001 (0.001) Batch 4.059 (3.432) Remain 10:20:22 loss: 0.7475 Lr: 0.04163
[2023-08-07 23:32:11,408 INFO misc.py line 115 22900] Train: [31/100][76/156] Data 0.001 (0.001) Batch 4.013 (3.440) Remain 10:21:45 loss: 0.7442 Lr: 0.04163
[2023-08-07 23:32:15,016 INFO misc.py line 115 22900] Train: [31/100][77/156] Data 0.001 (0.001) Batch 3.608 (3.442) Remain 10:22:06 loss: 0.4284 Lr: 0.04163
[2023-08-07 23:32:18,765 INFO misc.py line 115 22900] Train: [31/100][78/156] Data 0.001 (0.001) Batch 3.750 (3.447) Remain 10:22:47 loss: 0.5201 Lr: 0.04162
[2023-08-07 23:32:21,982 INFO misc.py line 115 22900] Train: [31/100][79/156] Data 0.001 (0.001) Batch 3.216 (3.444) Remain 10:22:11 loss: 0.2149 Lr: 0.04162
[2023-08-07 23:32:25,352 INFO misc.py line 115 22900] Train: [31/100][80/156] Data 0.001 (0.001) Batch 3.370 (3.443) Remain 10:21:57 loss: 0.4045 Lr: 0.04161
[2023-08-07 23:32:28,617 INFO misc.py line 115 22900] Train: [31/100][81/156] Data 0.001 (0.001) Batch 3.265 (3.440) Remain 10:21:29 loss: 0.2681 Lr: 0.04161
[2023-08-07 23:32:31,098 INFO misc.py line 115 22900] Train: [31/100][82/156] Data 0.001 (0.001) Batch 2.481 (3.428) Remain 10:19:14 loss: 0.4541 Lr: 0.04161
[2023-08-07 23:32:35,091 INFO misc.py line 115 22900] Train: [31/100][83/156] Data 0.001 (0.001) Batch 3.993 (3.435) Remain 10:20:27 loss: 0.3885 Lr: 0.04160
[2023-08-07 23:32:39,094 INFO misc.py line 115 22900] Train: [31/100][84/156] Data 0.001 (0.001) Batch 4.003 (3.442) Remain 10:21:40 loss: 0.4739 Lr: 0.04160
[2023-08-07 23:32:42,740 INFO misc.py line 115 22900] Train: [31/100][85/156] Data 0.001 (0.001) Batch 3.646 (3.445) Remain 10:22:03 loss: 0.3826 Lr: 0.04159
[2023-08-07 23:32:47,167 INFO misc.py line 115 22900] Train: [31/100][86/156] Data 0.001 (0.001) Batch 4.427 (3.457) Remain 10:24:08 loss: 0.5434 Lr: 0.04159
[2023-08-07 23:32:50,602 INFO misc.py line 115 22900] Train: [31/100][87/156] Data 0.001 (0.001) Batch 3.435 (3.456) Remain 10:24:02 loss: 0.4402 Lr: 0.04159
[2023-08-07 23:32:53,487 INFO misc.py line 115 22900] Train: [31/100][88/156] Data 0.001 (0.001) Batch 2.885 (3.450) Remain 10:22:45 loss: 0.3582 Lr: 0.04158
[2023-08-07 23:32:57,072 INFO misc.py line 115 22900] Train: [31/100][89/156] Data 0.001 (0.001) Batch 3.585 (3.451) Remain 10:22:59 loss: 0.3745 Lr: 0.04158
[2023-08-07 23:33:00,640 INFO misc.py line 115 22900] Train: [31/100][90/156] Data 0.001 (0.001) Batch 3.568 (3.452) Remain 10:23:10 loss: 0.5736 Lr: 0.04157
[2023-08-07 23:33:03,428 INFO misc.py line 115 22900] Train: [31/100][91/156] Data 0.001 (0.001) Batch 2.787 (3.445) Remain 10:21:45 loss: 0.1433 Lr: 0.04157
[2023-08-07 23:33:06,851 INFO misc.py line 115 22900] Train: [31/100][92/156] Data 0.001 (0.001) Batch 3.423 (3.445) Remain 10:21:39 loss: 0.3650 Lr: 0.04157
[2023-08-07 23:33:10,890 INFO misc.py line 115 22900] Train: [31/100][93/156] Data 0.001 (0.001) Batch 4.039 (3.451) Remain 10:22:47 loss: 0.4467 Lr: 0.04156
[2023-08-07 23:33:14,306 INFO misc.py line 115 22900] Train: [31/100][94/156] Data 0.001 (0.001) Batch 3.416 (3.451) Remain 10:22:39 loss: 0.4150 Lr: 0.04156
[2023-08-07 23:33:17,643 INFO misc.py line 115 22900] Train: [31/100][95/156] Data 0.001 (0.001) Batch 3.337 (3.450) Remain 10:22:22 loss: 0.5354 Lr: 0.04155
[2023-08-07 23:33:20,753 INFO misc.py line 115 22900] Train: [31/100][96/156] Data 0.001 (0.001) Batch 3.110 (3.446) Remain 10:21:39 loss: 0.4131 Lr: 0.04155
[2023-08-07 23:33:23,459 INFO misc.py line 115 22900] Train: [31/100][97/156] Data 0.001 (0.001) Batch 2.706 (3.438) Remain 10:20:11 loss: 0.3175 Lr: 0.04155
[2023-08-07 23:33:27,030 INFO misc.py line 115 22900] Train: [31/100][98/156] Data 0.001 (0.001) Batch 3.571 (3.440) Remain 10:20:22 loss: 0.4137 Lr: 0.04154
[2023-08-07 23:33:30,477 INFO misc.py line 115 22900] Train: [31/100][99/156] Data 0.001 (0.001) Batch 3.447 (3.440) Remain 10:20:20 loss: 0.4841 Lr: 0.04154
[2023-08-07 23:33:33,922 INFO misc.py line 115 22900] Train: [31/100][100/156] Data 0.001 (0.001) Batch 3.445 (3.440) Remain 10:20:17 loss: 0.2706 Lr: 0.04153
[2023-08-07 23:33:37,584 INFO misc.py line 115 22900] Train: [31/100][101/156] Data 0.001 (0.001) Batch 3.662 (3.442) Remain 10:20:38 loss: 0.6138 Lr: 0.04153
[2023-08-07 23:33:41,733 INFO misc.py line 115 22900] Train: [31/100][102/156] Data 0.001 (0.001) Batch 4.149 (3.449) Remain 10:21:52 loss: 0.6139 Lr: 0.04153
[2023-08-07 23:33:44,950 INFO misc.py line 115 22900] Train: [31/100][103/156] Data 0.001 (0.001) Batch 3.216 (3.447) Remain 10:21:23 loss: 0.2512 Lr: 0.04152
[2023-08-07 23:33:48,957 INFO misc.py line 115 22900] Train: [31/100][104/156] Data 0.001 (0.001) Batch 4.008 (3.452) Remain 10:22:20 loss: 0.4699 Lr: 0.04152
[2023-08-07 23:33:51,933 INFO misc.py line 115 22900] Train: [31/100][105/156] Data 0.001 (0.001) Batch 2.976 (3.448) Remain 10:21:26 loss: 0.3230 Lr: 0.04151
[2023-08-07 23:33:55,795 INFO misc.py line 115 22900] Train: [31/100][106/156] Data 0.001 (0.001) Batch 3.862 (3.452) Remain 10:22:06 loss: 0.3124 Lr: 0.04151
[2023-08-07 23:33:58,665 INFO misc.py line 115 22900] Train: [31/100][107/156] Data 0.001 (0.001) Batch 2.870 (3.446) Remain 10:21:02 loss: 0.2914 Lr: 0.04151
[2023-08-07 23:34:01,645 INFO misc.py line 115 22900] Train: [31/100][108/156] Data 0.001 (0.001) Batch 2.980 (3.442) Remain 10:20:10 loss: 0.3421 Lr: 0.04150
[2023-08-07 23:34:05,346 INFO misc.py line 115 22900] Train: [31/100][109/156] Data 0.001 (0.001) Batch 3.701 (3.444) Remain 10:20:33 loss: 0.3980 Lr: 0.04150
[2023-08-07 23:34:08,348 INFO misc.py line 115 22900] Train: [31/100][110/156] Data 0.001 (0.001) Batch 3.002 (3.440) Remain 10:19:45 loss: 0.3603 Lr: 0.04149
[2023-08-07 23:34:12,411 INFO misc.py line 115 22900] Train: [31/100][111/156] Data 0.001 (0.001) Batch 4.063 (3.446) Remain 10:20:44 loss: 0.7536 Lr: 0.04149
[2023-08-07 23:34:16,680 INFO misc.py line 115 22900] Train: [31/100][112/156] Data 0.001 (0.001) Batch 4.269 (3.453) Remain 10:22:02 loss: 0.4059 Lr: 0.04149
[2023-08-07 23:34:20,251 INFO misc.py line 115 22900] Train: [31/100][113/156] Data 0.001 (0.001) Batch 3.570 (3.454) Remain 10:22:11 loss: 0.5632 Lr: 0.04148
[2023-08-07 23:34:24,114 INFO misc.py line 115 22900] Train: [31/100][114/156] Data 0.001 (0.001) Batch 3.863 (3.458) Remain 10:22:47 loss: 0.6283 Lr: 0.04148
[2023-08-07 23:34:28,118 INFO misc.py line 115 22900] Train: [31/100][115/156] Data 0.001 (0.001) Batch 4.004 (3.463) Remain 10:23:36 loss: 0.7311 Lr: 0.04147
[2023-08-07 23:34:30,339 INFO misc.py line 115 22900] Train: [31/100][116/156] Data 0.001 (0.001) Batch 2.221 (3.452) Remain 10:21:34 loss: 0.2250 Lr: 0.04147
[2023-08-07 23:34:35,019 INFO misc.py line 115 22900] Train: [31/100][117/156] Data 0.001 (0.001) Batch 4.680 (3.463) Remain 10:23:27 loss: 0.8033 Lr: 0.04147
[2023-08-07 23:34:39,016 INFO misc.py line 115 22900] Train: [31/100][118/156] Data 0.005 (0.001) Batch 3.997 (3.467) Remain 10:24:14 loss: 0.3537 Lr: 0.04146
[2023-08-07 23:34:41,868 INFO misc.py line 115 22900] Train: [31/100][119/156] Data 0.001 (0.001) Batch 2.852 (3.462) Remain 10:23:13 loss: 0.2043 Lr: 0.04146
[2023-08-07 23:34:45,574 INFO misc.py line 115 22900] Train: [31/100][120/156] Data 0.001 (0.001) Batch 3.705 (3.464) Remain 10:23:32 loss: 0.3455 Lr: 0.04145
[2023-08-07 23:34:49,011 INFO misc.py line 115 22900] Train: [31/100][121/156] Data 0.001 (0.001) Batch 3.438 (3.464) Remain 10:23:26 loss: 0.3860 Lr: 0.04145
[2023-08-07 23:34:52,101 INFO misc.py line 115 22900] Train: [31/100][122/156] Data 0.001 (0.001) Batch 3.090 (3.461) Remain 10:22:49 loss: 0.4197 Lr: 0.04145
[2023-08-07 23:34:54,468 INFO misc.py line 115 22900] Train: [31/100][123/156] Data 0.001 (0.001) Batch 2.367 (3.452) Remain 10:21:07 loss: 0.2868 Lr: 0.04144
[2023-08-07 23:34:57,931 INFO misc.py line 115 22900] Train: [31/100][124/156] Data 0.001 (0.001) Batch 3.463 (3.452) Remain 10:21:04 loss: 0.3097 Lr: 0.04144
[2023-08-07 23:35:01,150 INFO misc.py line 115 22900] Train: [31/100][125/156] Data 0.001 (0.001) Batch 3.219 (3.450) Remain 10:20:40 loss: 0.4065 Lr: 0.04143
[2023-08-07 23:35:03,832 INFO misc.py line 115 22900] Train: [31/100][126/156] Data 0.001 (0.001) Batch 2.682 (3.444) Remain 10:19:29 loss: 0.3076 Lr: 0.04143
[2023-08-07 23:35:07,118 INFO misc.py line 115 22900] Train: [31/100][127/156] Data 0.001 (0.001) Batch 3.286 (3.442) Remain 10:19:12 loss: 0.5370 Lr: 0.04143
[2023-08-07 23:35:11,096 INFO misc.py line 115 22900] Train: [31/100][128/156] Data 0.001 (0.001) Batch 3.977 (3.447) Remain 10:19:55 loss: 0.3500 Lr: 0.04142
[2023-08-07 23:35:13,732 INFO misc.py line 115 22900] Train: [31/100][129/156] Data 0.001 (0.001) Batch 2.637 (3.440) Remain 10:18:42 loss: 0.3514 Lr: 0.04142
[2023-08-07 23:35:17,112 INFO misc.py line 115 22900] Train: [31/100][130/156] Data 0.001 (0.001) Batch 3.380 (3.440) Remain 10:18:34 loss: 0.2686 Lr: 0.04141
[2023-08-07 23:35:21,141 INFO misc.py line 115 22900] Train: [31/100][131/156] Data 0.001 (0.001) Batch 4.029 (3.444) Remain 10:19:20 loss: 0.5324 Lr: 0.04141
[2023-08-07 23:35:25,237 INFO misc.py line 115 22900] Train: [31/100][132/156] Data 0.001 (0.001) Batch 4.096 (3.449) Remain 10:20:11 loss: 0.5366 Lr: 0.04141
[2023-08-07 23:35:28,576 INFO misc.py line 115 22900] Train: [31/100][133/156] Data 0.001 (0.001) Batch 3.339 (3.448) Remain 10:19:58 loss: 0.4509 Lr: 0.04140
[2023-08-07 23:35:31,439 INFO misc.py line 115 22900] Train: [31/100][134/156] Data 0.001 (0.001) Batch 2.863 (3.444) Remain 10:19:07 loss: 0.5045 Lr: 0.04140
[2023-08-07 23:35:35,444 INFO misc.py line 115 22900] Train: [31/100][135/156] Data 0.001 (0.001) Batch 4.004 (3.448) Remain 10:19:49 loss: 0.3220 Lr: 0.04139
[2023-08-07 23:35:39,229 INFO misc.py line 115 22900] Train: [31/100][136/156] Data 0.001 (0.001) Batch 3.785 (3.451) Remain 10:20:13 loss: 0.3919 Lr: 0.04139
[2023-08-07 23:35:42,712 INFO misc.py line 115 22900] Train: [31/100][137/156] Data 0.001 (0.001) Batch 3.483 (3.451) Remain 10:20:12 loss: 0.3268 Lr: 0.04139
[2023-08-07 23:35:46,387 INFO misc.py line 115 22900] Train: [31/100][138/156] Data 0.001 (0.001) Batch 3.674 (3.453) Remain 10:20:26 loss: 0.3362 Lr: 0.04138
[2023-08-07 23:35:50,506 INFO misc.py line 115 22900] Train: [31/100][139/156] Data 0.001 (0.001) Batch 4.120 (3.458) Remain 10:21:16 loss: 0.4608 Lr: 0.04138
[2023-08-07 23:35:54,603 INFO misc.py line 115 22900] Train: [31/100][140/156] Data 0.001 (0.001) Batch 4.096 (3.462) Remain 10:22:03 loss: 0.5982 Lr: 0.04137
[2023-08-07 23:35:57,472 INFO misc.py line 115 22900] Train: [31/100][141/156] Data 0.001 (0.001) Batch 2.869 (3.458) Remain 10:21:13 loss: 0.1216 Lr: 0.04137
[2023-08-07 23:36:00,517 INFO misc.py line 115 22900] Train: [31/100][142/156] Data 0.001 (0.001) Batch 3.046 (3.455) Remain 10:20:37 loss: 0.4407 Lr: 0.04137
[2023-08-07 23:36:04,481 INFO misc.py line 115 22900] Train: [31/100][143/156] Data 0.001 (0.001) Batch 3.964 (3.459) Remain 10:21:13 loss: 0.4756 Lr: 0.04136
[2023-08-07 23:36:07,733 INFO misc.py line 115 22900] Train: [31/100][144/156] Data 0.001 (0.001) Batch 3.252 (3.457) Remain 10:20:54 loss: 0.4432 Lr: 0.04136
[2023-08-07 23:36:11,828 INFO misc.py line 115 22900] Train: [31/100][145/156] Data 0.001 (0.001) Batch 4.095 (3.462) Remain 10:21:39 loss: 0.3820 Lr: 0.04135
[2023-08-07 23:36:14,868 INFO misc.py line 115 22900] Train: [31/100][146/156] Data 0.001 (0.001) Batch 3.039 (3.459) Remain 10:21:04 loss: 0.5195 Lr: 0.04135
[2023-08-07 23:36:18,497 INFO misc.py line 115 22900] Train: [31/100][147/156] Data 0.001 (0.001) Batch 3.630 (3.460) Remain 10:21:13 loss: 0.4611 Lr: 0.04135
[2023-08-07 23:36:21,466 INFO misc.py line 115 22900] Train: [31/100][148/156] Data 0.001 (0.001) Batch 2.969 (3.457) Remain 10:20:33 loss: 0.1919 Lr: 0.04134
[2023-08-07 23:36:23,236 INFO misc.py line 115 22900] Train: [31/100][149/156] Data 0.001 (0.001) Batch 1.770 (3.445) Remain 10:18:25 loss: 0.3728 Lr: 0.04134
[2023-08-07 23:36:25,648 INFO misc.py line 115 22900] Train: [31/100][150/156] Data 0.001 (0.001) Batch 2.411 (3.438) Remain 10:17:06 loss: 0.2196 Lr: 0.04133
[2023-08-07 23:36:29,288 INFO misc.py line 115 22900] Train: [31/100][151/156] Data 0.001 (0.001) Batch 3.641 (3.439) Remain 10:17:17 loss: 0.8649 Lr: 0.04133
[2023-08-07 23:36:33,073 INFO misc.py line 115 22900] Train: [31/100][152/156] Data 0.001 (0.001) Batch 3.785 (3.442) Remain 10:17:39 loss: 0.3956 Lr: 0.04133
[2023-08-07 23:36:36,178 INFO misc.py line 115 22900] Train: [31/100][153/156] Data 0.001 (0.001) Batch 3.105 (3.439) Remain 10:17:11 loss: 0.3058 Lr: 0.04132
[2023-08-07 23:36:39,182 INFO misc.py line 115 22900] Train: [31/100][154/156] Data 0.001 (0.001) Batch 3.003 (3.436) Remain 10:16:37 loss: 0.3836 Lr: 0.04132
[2023-08-07 23:36:42,899 INFO misc.py line 115 22900] Train: [31/100][155/156] Data 0.001 (0.001) Batch 3.717 (3.438) Remain 10:16:53 loss: 0.3628 Lr: 0.04131
[2023-08-07 23:36:45,635 INFO misc.py line 115 22900] Train: [31/100][156/156] Data 0.001 (0.001) Batch 2.736 (3.434) Remain 10:16:00 loss: 0.4120 Lr: 0.04131
[2023-08-07 23:36:45,635 INFO misc.py line 129 22900] Train result: loss: 0.4172 
[2023-08-07 23:36:45,636 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 23:36:47,740 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.9590 
[2023-08-07 23:36:48,610 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.5290 
[2023-08-07 23:36:50,273 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.6211 
[2023-08-07 23:36:51,793 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.4793 
[2023-08-07 23:36:53,639 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.5557 
[2023-08-07 23:36:55,302 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.8050 
[2023-08-07 23:36:57,439 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.1944 
[2023-08-07 23:36:59,244 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.0305 
[2023-08-07 23:37:00,527 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.5559 
[2023-08-07 23:37:02,657 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3313 
[2023-08-07 23:37:03,183 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.3688 
[2023-08-07 23:37:04,715 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.6327 
[2023-08-07 23:37:07,425 INFO evaluator.py line 122 22900] Test: [13/24] Loss 0.9562 
[2023-08-07 23:37:09,104 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.6029 
[2023-08-07 23:37:11,126 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.5045 
[2023-08-07 23:37:13,837 INFO evaluator.py line 122 22900] Test: [16/24] Loss 0.9877 
[2023-08-07 23:37:16,545 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.1476 
[2023-08-07 23:37:18,392 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.2406 
[2023-08-07 23:37:19,139 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.2232 
[2023-08-07 23:37:20,024 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.7225 
[2023-08-07 23:37:22,284 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3746 
[2023-08-07 23:37:24,248 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.4915 
[2023-08-07 23:37:26,095 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.6913 
[2023-08-07 23:37:28,031 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.8173 
[2023-08-07 23:37:28,092 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2356/0.3449/0.6743.
[2023-08-07 23:37:28,092 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6463/0.9487
[2023-08-07 23:37:28,092 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9444/0.9884
[2023-08-07 23:37:28,092 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1488/0.4403
[2023-08-07 23:37:28,092 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1230/0.2304
[2023-08-07 23:37:28,092 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6220/0.7378
[2023-08-07 23:37:28,092 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.2914/0.3469
[2023-08-07 23:37:28,092 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.3760/0.4168
[2023-08-07 23:37:28,092 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1127/0.1271
[2023-08-07 23:37:28,092 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1862/0.5274
[2023-08-07 23:37:28,092 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0872/0.0889
[2023-08-07 23:37:28,092 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 23:37:28,092 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0284/0.0316
[2023-08-07 23:37:28,092 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.2112/0.6534
[2023-08-07 23:37:28,092 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0051/0.0052
[2023-08-07 23:37:28,092 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0133/0.0170
[2023-08-07 23:37:28,092 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.3976/0.4647
[2023-08-07 23:37:28,092 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.1872/0.2546
[2023-08-07 23:37:28,092 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.2527/0.4813
[2023-08-07 23:37:28,092 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 23:37:28,092 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0776/0.1385
[2023-08-07 23:37:28,093 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 23:37:28,093 INFO misc.py line 150 22900] Best validation mIoU updated to: 0.2356
[2023-08-07 23:37:28,093 INFO misc.py line 152 22900] Currently Best mIoU: 0.2356
[2023-08-07 23:37:28,093 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 23:37:34,708 INFO misc.py line 115 22900] Train: [32/100][1/156] Data 1.702 (1.702) Batch 5.559 (5.559) Remain 16:37:06 loss: 0.5293 Lr: 0.04131
[2023-08-07 23:37:38,097 INFO misc.py line 115 22900] Train: [32/100][2/156] Data 0.001 (0.001) Batch 3.389 (3.389) Remain 10:07:49 loss: 0.4107 Lr: 0.04130
[2023-08-07 23:37:41,074 INFO misc.py line 115 22900] Train: [32/100][3/156] Data 0.001 (0.001) Batch 2.977 (2.977) Remain 08:53:58 loss: 0.4988 Lr: 0.04130
[2023-08-07 23:37:44,971 INFO misc.py line 115 22900] Train: [32/100][4/156] Data 0.001 (0.001) Batch 3.897 (3.897) Remain 11:38:55 loss: 0.5591 Lr: 0.04129
[2023-08-07 23:37:47,529 INFO misc.py line 115 22900] Train: [32/100][5/156] Data 0.005 (0.003) Batch 2.558 (3.228) Remain 09:38:45 loss: 0.2200 Lr: 0.04129
[2023-08-07 23:37:51,406 INFO misc.py line 115 22900] Train: [32/100][6/156] Data 0.001 (0.002) Batch 3.877 (3.444) Remain 10:17:30 loss: 0.4618 Lr: 0.04129
[2023-08-07 23:37:54,386 INFO misc.py line 115 22900] Train: [32/100][7/156] Data 0.001 (0.002) Batch 2.981 (3.328) Remain 09:56:40 loss: 0.2023 Lr: 0.04128
[2023-08-07 23:37:57,889 INFO misc.py line 115 22900] Train: [32/100][8/156] Data 0.001 (0.002) Batch 3.503 (3.363) Remain 10:02:53 loss: 0.3768 Lr: 0.04128
[2023-08-07 23:38:01,766 INFO misc.py line 115 22900] Train: [32/100][9/156] Data 0.001 (0.002) Batch 3.876 (3.449) Remain 10:18:10 loss: 0.3751 Lr: 0.04127
[2023-08-07 23:38:05,087 INFO misc.py line 115 22900] Train: [32/100][10/156] Data 0.001 (0.002) Batch 3.322 (3.430) Remain 10:14:51 loss: 0.2678 Lr: 0.04127
[2023-08-07 23:38:07,670 INFO misc.py line 115 22900] Train: [32/100][11/156] Data 0.001 (0.002) Batch 2.583 (3.325) Remain 09:55:48 loss: 0.1868 Lr: 0.04127
[2023-08-07 23:38:11,181 INFO misc.py line 115 22900] Train: [32/100][12/156] Data 0.001 (0.002) Batch 3.511 (3.345) Remain 09:59:28 loss: 0.4260 Lr: 0.04126
[2023-08-07 23:38:13,635 INFO misc.py line 115 22900] Train: [32/100][13/156] Data 0.001 (0.001) Batch 2.454 (3.256) Remain 09:43:26 loss: 0.0860 Lr: 0.04126
[2023-08-07 23:38:17,612 INFO misc.py line 115 22900] Train: [32/100][14/156] Data 0.001 (0.001) Batch 3.976 (3.322) Remain 09:55:07 loss: 0.4117 Lr: 0.04125
[2023-08-07 23:38:20,184 INFO misc.py line 115 22900] Train: [32/100][15/156] Data 0.001 (0.001) Batch 2.572 (3.259) Remain 09:43:52 loss: 0.2607 Lr: 0.04125
[2023-08-07 23:38:24,183 INFO misc.py line 115 22900] Train: [32/100][16/156] Data 0.001 (0.001) Batch 3.999 (3.316) Remain 09:54:01 loss: 0.4802 Lr: 0.04125
[2023-08-07 23:38:26,988 INFO misc.py line 115 22900] Train: [32/100][17/156] Data 0.001 (0.001) Batch 2.805 (3.280) Remain 09:47:25 loss: 0.1977 Lr: 0.04124
[2023-08-07 23:38:30,253 INFO misc.py line 115 22900] Train: [32/100][18/156] Data 0.001 (0.001) Batch 3.265 (3.279) Remain 09:47:11 loss: 0.1486 Lr: 0.04124
[2023-08-07 23:38:33,267 INFO misc.py line 115 22900] Train: [32/100][19/156] Data 0.001 (0.001) Batch 3.014 (3.262) Remain 09:44:10 loss: 0.3847 Lr: 0.04123
[2023-08-07 23:38:37,228 INFO misc.py line 115 22900] Train: [32/100][20/156] Data 0.001 (0.001) Batch 3.961 (3.303) Remain 09:51:29 loss: 0.4033 Lr: 0.04123
[2023-08-07 23:38:41,162 INFO misc.py line 115 22900] Train: [32/100][21/156] Data 0.001 (0.001) Batch 3.934 (3.338) Remain 09:57:42 loss: 0.4237 Lr: 0.04123
[2023-08-07 23:38:45,359 INFO misc.py line 115 22900] Train: [32/100][22/156] Data 0.001 (0.001) Batch 4.196 (3.383) Remain 10:05:44 loss: 0.4188 Lr: 0.04122
[2023-08-07 23:38:49,352 INFO misc.py line 115 22900] Train: [32/100][23/156] Data 0.001 (0.001) Batch 3.994 (3.414) Remain 10:11:09 loss: 0.4838 Lr: 0.04122
[2023-08-07 23:38:53,494 INFO misc.py line 115 22900] Train: [32/100][24/156] Data 0.001 (0.001) Batch 4.142 (3.449) Remain 10:17:17 loss: 0.3222 Lr: 0.04121
[2023-08-07 23:38:56,458 INFO misc.py line 115 22900] Train: [32/100][25/156] Data 0.001 (0.001) Batch 2.963 (3.427) Remain 10:13:17 loss: 0.5139 Lr: 0.04121
[2023-08-07 23:38:59,506 INFO misc.py line 115 22900] Train: [32/100][26/156] Data 0.001 (0.001) Batch 3.048 (3.410) Remain 10:10:17 loss: 0.3370 Lr: 0.04121
[2023-08-07 23:39:02,138 INFO misc.py line 115 22900] Train: [32/100][27/156] Data 0.001 (0.001) Batch 2.632 (3.378) Remain 10:04:25 loss: 0.3803 Lr: 0.04120
[2023-08-07 23:39:05,329 INFO misc.py line 115 22900] Train: [32/100][28/156] Data 0.001 (0.001) Batch 3.191 (3.370) Remain 10:03:02 loss: 0.4394 Lr: 0.04120
[2023-08-07 23:39:09,434 INFO misc.py line 115 22900] Train: [32/100][29/156] Data 0.001 (0.001) Batch 4.105 (3.398) Remain 10:08:02 loss: 0.4440 Lr: 0.04119
[2023-08-07 23:39:13,410 INFO misc.py line 115 22900] Train: [32/100][30/156] Data 0.001 (0.001) Batch 3.976 (3.420) Remain 10:11:48 loss: 0.4126 Lr: 0.04119
[2023-08-07 23:39:16,971 INFO misc.py line 115 22900] Train: [32/100][31/156] Data 0.001 (0.001) Batch 3.561 (3.425) Remain 10:12:39 loss: 0.1968 Lr: 0.04119
[2023-08-07 23:39:19,260 INFO misc.py line 115 22900] Train: [32/100][32/156] Data 0.001 (0.001) Batch 2.289 (3.386) Remain 10:05:35 loss: 0.3477 Lr: 0.04118
[2023-08-07 23:39:22,921 INFO misc.py line 115 22900] Train: [32/100][33/156] Data 0.001 (0.001) Batch 3.661 (3.395) Remain 10:07:10 loss: 0.2429 Lr: 0.04118
[2023-08-07 23:39:26,464 INFO misc.py line 115 22900] Train: [32/100][34/156] Data 0.001 (0.001) Batch 3.543 (3.400) Remain 10:07:58 loss: 0.3293 Lr: 0.04117
[2023-08-07 23:39:30,208 INFO misc.py line 115 22900] Train: [32/100][35/156] Data 0.001 (0.001) Batch 3.744 (3.410) Remain 10:09:50 loss: 0.4355 Lr: 0.04117
[2023-08-07 23:39:33,852 INFO misc.py line 115 22900] Train: [32/100][36/156] Data 0.001 (0.001) Batch 3.645 (3.418) Remain 10:11:03 loss: 0.3106 Lr: 0.04117
[2023-08-07 23:39:35,955 INFO misc.py line 115 22900] Train: [32/100][37/156] Data 0.001 (0.001) Batch 2.103 (3.379) Remain 10:04:04 loss: 0.2123 Lr: 0.04116
[2023-08-07 23:39:39,285 INFO misc.py line 115 22900] Train: [32/100][38/156] Data 0.001 (0.001) Batch 3.330 (3.377) Remain 10:03:46 loss: 0.4119 Lr: 0.04116
[2023-08-07 23:39:42,063 INFO misc.py line 115 22900] Train: [32/100][39/156] Data 0.001 (0.001) Batch 2.778 (3.361) Remain 10:00:44 loss: 0.3290 Lr: 0.04115
[2023-08-07 23:39:45,605 INFO misc.py line 115 22900] Train: [32/100][40/156] Data 0.001 (0.001) Batch 3.542 (3.366) Remain 10:01:33 loss: 0.3496 Lr: 0.04115
[2023-08-07 23:39:49,193 INFO misc.py line 115 22900] Train: [32/100][41/156] Data 0.001 (0.001) Batch 3.588 (3.372) Remain 10:02:33 loss: 0.4748 Lr: 0.04115
[2023-08-07 23:39:52,724 INFO misc.py line 115 22900] Train: [32/100][42/156] Data 0.001 (0.001) Batch 3.531 (3.376) Remain 10:03:13 loss: 0.2819 Lr: 0.04114
[2023-08-07 23:39:55,579 INFO misc.py line 115 22900] Train: [32/100][43/156] Data 0.001 (0.001) Batch 2.855 (3.363) Remain 10:00:50 loss: 0.2334 Lr: 0.04114
[2023-08-07 23:39:58,992 INFO misc.py line 115 22900] Train: [32/100][44/156] Data 0.001 (0.001) Batch 3.413 (3.364) Remain 10:01:00 loss: 0.3106 Lr: 0.04113
[2023-08-07 23:40:03,352 INFO misc.py line 115 22900] Train: [32/100][45/156] Data 0.001 (0.001) Batch 4.361 (3.388) Remain 10:05:11 loss: 0.6362 Lr: 0.04113
[2023-08-07 23:40:07,867 INFO misc.py line 115 22900] Train: [32/100][46/156] Data 0.001 (0.001) Batch 4.514 (3.414) Remain 10:09:48 loss: 0.6209 Lr: 0.04113
[2023-08-07 23:40:09,814 INFO misc.py line 115 22900] Train: [32/100][47/156] Data 0.001 (0.001) Batch 1.947 (3.380) Remain 10:03:48 loss: 0.2525 Lr: 0.04112
[2023-08-07 23:40:12,495 INFO misc.py line 115 22900] Train: [32/100][48/156] Data 0.001 (0.001) Batch 2.681 (3.365) Remain 10:00:58 loss: 0.3433 Lr: 0.04112
[2023-08-07 23:40:15,406 INFO misc.py line 115 22900] Train: [32/100][49/156] Data 0.001 (0.001) Batch 2.910 (3.355) Remain 09:59:09 loss: 0.3928 Lr: 0.04111
[2023-08-07 23:40:19,507 INFO misc.py line 115 22900] Train: [32/100][50/156] Data 0.001 (0.001) Batch 4.101 (3.371) Remain 10:01:56 loss: 0.2762 Lr: 0.04111
[2023-08-07 23:40:21,513 INFO misc.py line 115 22900] Train: [32/100][51/156] Data 0.001 (0.001) Batch 2.006 (3.342) Remain 09:56:48 loss: 0.0773 Lr: 0.04110
[2023-08-07 23:40:25,219 INFO misc.py line 115 22900] Train: [32/100][52/156] Data 0.001 (0.001) Batch 3.706 (3.350) Remain 09:58:04 loss: 0.4988 Lr: 0.04110
[2023-08-07 23:40:28,652 INFO misc.py line 115 22900] Train: [32/100][53/156] Data 0.001 (0.001) Batch 3.432 (3.352) Remain 09:58:18 loss: 0.3511 Lr: 0.04110
[2023-08-07 23:40:32,665 INFO misc.py line 115 22900] Train: [32/100][54/156] Data 0.001 (0.001) Batch 4.013 (3.365) Remain 10:00:34 loss: 0.4347 Lr: 0.04109
[2023-08-07 23:40:36,778 INFO misc.py line 115 22900] Train: [32/100][55/156] Data 0.001 (0.001) Batch 4.113 (3.379) Remain 10:03:04 loss: 0.3558 Lr: 0.04109
[2023-08-07 23:40:39,979 INFO misc.py line 115 22900] Train: [32/100][56/156] Data 0.001 (0.001) Batch 3.200 (3.376) Remain 10:02:25 loss: 0.3884 Lr: 0.04108
[2023-08-07 23:40:43,331 INFO misc.py line 115 22900] Train: [32/100][57/156] Data 0.001 (0.001) Batch 3.352 (3.375) Remain 10:02:17 loss: 0.4209 Lr: 0.04108
[2023-08-07 23:40:46,228 INFO misc.py line 115 22900] Train: [32/100][58/156] Data 0.001 (0.001) Batch 2.898 (3.366) Remain 10:00:41 loss: 0.3200 Lr: 0.04108
[2023-08-07 23:40:50,065 INFO misc.py line 115 22900] Train: [32/100][59/156] Data 0.001 (0.001) Batch 3.837 (3.375) Remain 10:02:07 loss: 0.3859 Lr: 0.04107
[2023-08-07 23:40:54,203 INFO misc.py line 115 22900] Train: [32/100][60/156] Data 0.001 (0.001) Batch 4.138 (3.388) Remain 10:04:27 loss: 0.3879 Lr: 0.04107
[2023-08-07 23:40:56,692 INFO misc.py line 115 22900] Train: [32/100][61/156] Data 0.001 (0.001) Batch 2.490 (3.373) Remain 10:01:38 loss: 0.0697 Lr: 0.04106
[2023-08-07 23:40:59,412 INFO misc.py line 115 22900] Train: [32/100][62/156] Data 0.001 (0.001) Batch 2.720 (3.362) Remain 09:59:36 loss: 0.3294 Lr: 0.04106
[2023-08-07 23:41:03,465 INFO misc.py line 115 22900] Train: [32/100][63/156] Data 0.001 (0.001) Batch 4.053 (3.373) Remain 10:01:36 loss: 0.4657 Lr: 0.04106
[2023-08-07 23:41:07,348 INFO misc.py line 115 22900] Train: [32/100][64/156] Data 0.001 (0.001) Batch 3.883 (3.382) Remain 10:03:02 loss: 0.3654 Lr: 0.04105
[2023-08-07 23:41:11,395 INFO misc.py line 115 22900] Train: [32/100][65/156] Data 0.001 (0.001) Batch 4.047 (3.392) Remain 10:04:53 loss: 0.3428 Lr: 0.04105
[2023-08-07 23:41:15,397 INFO misc.py line 115 22900] Train: [32/100][66/156] Data 0.001 (0.001) Batch 4.003 (3.402) Remain 10:06:34 loss: 0.4753 Lr: 0.04104
[2023-08-07 23:41:18,662 INFO misc.py line 115 22900] Train: [32/100][67/156] Data 0.001 (0.001) Batch 3.264 (3.400) Remain 10:06:07 loss: 0.2930 Lr: 0.04104
[2023-08-07 23:41:22,604 INFO misc.py line 115 22900] Train: [32/100][68/156] Data 0.001 (0.001) Batch 3.942 (3.408) Remain 10:07:33 loss: 0.5310 Lr: 0.04104
[2023-08-07 23:41:26,647 INFO misc.py line 115 22900] Train: [32/100][69/156] Data 0.001 (0.001) Batch 4.043 (3.418) Remain 10:09:13 loss: 0.3368 Lr: 0.04103
[2023-08-07 23:41:30,633 INFO misc.py line 115 22900] Train: [32/100][70/156] Data 0.001 (0.001) Batch 3.986 (3.426) Remain 10:10:40 loss: 0.6165 Lr: 0.04103
[2023-08-07 23:41:33,557 INFO misc.py line 115 22900] Train: [32/100][71/156] Data 0.001 (0.001) Batch 2.924 (3.419) Remain 10:09:18 loss: 0.2642 Lr: 0.04102
[2023-08-07 23:41:37,847 INFO misc.py line 115 22900] Train: [32/100][72/156] Data 0.001 (0.001) Batch 4.290 (3.431) Remain 10:11:29 loss: 0.5340 Lr: 0.04102
[2023-08-07 23:41:40,659 INFO misc.py line 115 22900] Train: [32/100][73/156] Data 0.001 (0.001) Batch 2.812 (3.423) Remain 10:09:51 loss: 0.2200 Lr: 0.04102
[2023-08-07 23:41:43,781 INFO misc.py line 115 22900] Train: [32/100][74/156] Data 0.001 (0.001) Batch 3.122 (3.418) Remain 10:09:02 loss: 0.3772 Lr: 0.04101
[2023-08-07 23:41:46,261 INFO misc.py line 115 22900] Train: [32/100][75/156] Data 0.001 (0.001) Batch 2.480 (3.405) Remain 10:06:40 loss: 0.3741 Lr: 0.04101
[2023-08-07 23:41:49,954 INFO misc.py line 115 22900] Train: [32/100][76/156] Data 0.001 (0.001) Batch 3.693 (3.409) Remain 10:07:18 loss: 0.2495 Lr: 0.04100
[2023-08-07 23:41:53,596 INFO misc.py line 115 22900] Train: [32/100][77/156] Data 0.001 (0.001) Batch 3.642 (3.412) Remain 10:07:48 loss: 0.2683 Lr: 0.04100
[2023-08-07 23:41:57,198 INFO misc.py line 115 22900] Train: [32/100][78/156] Data 0.001 (0.001) Batch 3.602 (3.415) Remain 10:08:12 loss: 0.4824 Lr: 0.04100
[2023-08-07 23:42:00,448 INFO misc.py line 115 22900] Train: [32/100][79/156] Data 0.001 (0.001) Batch 3.250 (3.413) Remain 10:07:45 loss: 0.5978 Lr: 0.04099
[2023-08-07 23:42:04,500 INFO misc.py line 115 22900] Train: [32/100][80/156] Data 0.001 (0.001) Batch 4.052 (3.421) Remain 10:09:11 loss: 0.5708 Lr: 0.04099
[2023-08-07 23:42:06,357 INFO misc.py line 115 22900] Train: [32/100][81/156] Data 0.001 (0.001) Batch 1.857 (3.401) Remain 10:05:33 loss: 0.0986 Lr: 0.04098
[2023-08-07 23:42:09,127 INFO misc.py line 115 22900] Train: [32/100][82/156] Data 0.001 (0.001) Batch 2.771 (3.393) Remain 10:04:04 loss: 0.3313 Lr: 0.04098
[2023-08-07 23:42:12,492 INFO misc.py line 115 22900] Train: [32/100][83/156] Data 0.001 (0.001) Batch 3.364 (3.393) Remain 10:03:57 loss: 0.4498 Lr: 0.04097
[2023-08-07 23:42:15,717 INFO misc.py line 115 22900] Train: [32/100][84/156] Data 0.001 (0.001) Batch 3.225 (3.391) Remain 10:03:32 loss: 0.3412 Lr: 0.04097
[2023-08-07 23:42:19,413 INFO misc.py line 115 22900] Train: [32/100][85/156] Data 0.001 (0.001) Batch 3.696 (3.394) Remain 10:04:08 loss: 0.4054 Lr: 0.04097
[2023-08-07 23:42:22,805 INFO misc.py line 115 22900] Train: [32/100][86/156] Data 0.001 (0.001) Batch 3.391 (3.394) Remain 10:04:04 loss: 0.5323 Lr: 0.04096
[2023-08-07 23:42:26,602 INFO misc.py line 115 22900] Train: [32/100][87/156] Data 0.001 (0.001) Batch 3.797 (3.399) Remain 10:04:52 loss: 0.8021 Lr: 0.04096
[2023-08-07 23:42:28,795 INFO misc.py line 115 22900] Train: [32/100][88/156] Data 0.001 (0.001) Batch 2.193 (3.385) Remain 10:02:17 loss: 0.1724 Lr: 0.04095
[2023-08-07 23:42:32,897 INFO misc.py line 115 22900] Train: [32/100][89/156] Data 0.001 (0.001) Batch 4.102 (3.393) Remain 10:03:43 loss: 0.3676 Lr: 0.04095
[2023-08-07 23:42:36,816 INFO misc.py line 115 22900] Train: [32/100][90/156] Data 0.001 (0.001) Batch 3.919 (3.399) Remain 10:04:44 loss: 0.4578 Lr: 0.04095
[2023-08-07 23:42:40,385 INFO misc.py line 115 22900] Train: [32/100][91/156] Data 0.001 (0.001) Batch 3.569 (3.401) Remain 10:05:01 loss: 0.4161 Lr: 0.04094
[2023-08-07 23:42:43,919 INFO misc.py line 115 22900] Train: [32/100][92/156] Data 0.001 (0.001) Batch 3.534 (3.403) Remain 10:05:14 loss: 0.7059 Lr: 0.04094
[2023-08-07 23:42:47,153 INFO misc.py line 115 22900] Train: [32/100][93/156] Data 0.001 (0.001) Batch 3.234 (3.401) Remain 10:04:50 loss: 0.2508 Lr: 0.04093
[2023-08-07 23:42:50,628 INFO misc.py line 115 22900] Train: [32/100][94/156] Data 0.001 (0.001) Batch 3.475 (3.402) Remain 10:04:56 loss: 0.3995 Lr: 0.04093
[2023-08-07 23:42:54,196 INFO misc.py line 115 22900] Train: [32/100][95/156] Data 0.001 (0.001) Batch 3.568 (3.404) Remain 10:05:11 loss: 0.4162 Lr: 0.04093
[2023-08-07 23:42:57,686 INFO misc.py line 115 22900] Train: [32/100][96/156] Data 0.001 (0.001) Batch 3.490 (3.404) Remain 10:05:18 loss: 0.4162 Lr: 0.04092
[2023-08-07 23:43:00,914 INFO misc.py line 115 22900] Train: [32/100][97/156] Data 0.001 (0.001) Batch 3.228 (3.403) Remain 10:04:55 loss: 0.3879 Lr: 0.04092
[2023-08-07 23:43:04,983 INFO misc.py line 115 22900] Train: [32/100][98/156] Data 0.001 (0.001) Batch 4.069 (3.410) Remain 10:06:06 loss: 0.5077 Lr: 0.04091
[2023-08-07 23:43:08,580 INFO misc.py line 115 22900] Train: [32/100][99/156] Data 0.001 (0.001) Batch 3.597 (3.412) Remain 10:06:23 loss: 0.5767 Lr: 0.04091
[2023-08-07 23:43:12,018 INFO misc.py line 115 22900] Train: [32/100][100/156] Data 0.001 (0.001) Batch 3.438 (3.412) Remain 10:06:23 loss: 0.3382 Lr: 0.04091
[2023-08-07 23:43:15,963 INFO misc.py line 115 22900] Train: [32/100][101/156] Data 0.001 (0.001) Batch 3.946 (3.417) Remain 10:07:18 loss: 0.4235 Lr: 0.04090
[2023-08-07 23:43:19,734 INFO misc.py line 115 22900] Train: [32/100][102/156] Data 0.001 (0.001) Batch 3.771 (3.421) Remain 10:07:52 loss: 0.3691 Lr: 0.04090
[2023-08-07 23:43:24,581 INFO misc.py line 115 22900] Train: [32/100][103/156] Data 0.001 (0.001) Batch 4.847 (3.435) Remain 10:10:21 loss: 0.8842 Lr: 0.04089
[2023-08-07 23:43:28,539 INFO misc.py line 115 22900] Train: [32/100][104/156] Data 0.001 (0.001) Batch 3.957 (3.440) Remain 10:11:13 loss: 0.4351 Lr: 0.04089
[2023-08-07 23:43:32,118 INFO misc.py line 115 22900] Train: [32/100][105/156] Data 0.001 (0.001) Batch 3.580 (3.442) Remain 10:11:24 loss: 0.2993 Lr: 0.04089
[2023-08-07 23:43:36,260 INFO misc.py line 115 22900] Train: [32/100][106/156] Data 0.001 (0.001) Batch 4.142 (3.448) Remain 10:12:33 loss: 0.2858 Lr: 0.04088
[2023-08-07 23:43:39,566 INFO misc.py line 115 22900] Train: [32/100][107/156] Data 0.001 (0.001) Batch 3.306 (3.447) Remain 10:12:15 loss: 0.2985 Lr: 0.04088
[2023-08-07 23:43:44,089 INFO misc.py line 115 22900] Train: [32/100][108/156] Data 0.001 (0.001) Batch 4.522 (3.457) Remain 10:14:00 loss: 0.5470 Lr: 0.04087
[2023-08-07 23:43:48,189 INFO misc.py line 115 22900] Train: [32/100][109/156] Data 0.001 (0.001) Batch 4.100 (3.463) Remain 10:15:02 loss: 0.4435 Lr: 0.04087
[2023-08-07 23:43:52,648 INFO misc.py line 115 22900] Train: [32/100][110/156] Data 0.001 (0.001) Batch 4.459 (3.473) Remain 10:16:37 loss: 0.6013 Lr: 0.04086
[2023-08-07 23:43:56,645 INFO misc.py line 115 22900] Train: [32/100][111/156] Data 0.001 (0.001) Batch 3.997 (3.478) Remain 10:17:25 loss: 0.4819 Lr: 0.04086
[2023-08-07 23:43:59,116 INFO misc.py line 115 22900] Train: [32/100][112/156] Data 0.001 (0.001) Batch 2.471 (3.468) Remain 10:15:44 loss: 0.2052 Lr: 0.04086
[2023-08-07 23:44:02,714 INFO misc.py line 115 22900] Train: [32/100][113/156] Data 0.001 (0.001) Batch 3.598 (3.469) Remain 10:15:53 loss: 0.4215 Lr: 0.04085
[2023-08-07 23:44:06,173 INFO misc.py line 115 22900] Train: [32/100][114/156] Data 0.001 (0.001) Batch 3.458 (3.469) Remain 10:15:48 loss: 0.3930 Lr: 0.04085
[2023-08-07 23:44:09,986 INFO misc.py line 115 22900] Train: [32/100][115/156] Data 0.001 (0.001) Batch 3.814 (3.472) Remain 10:16:17 loss: 0.5499 Lr: 0.04084
[2023-08-07 23:44:13,529 INFO misc.py line 115 22900] Train: [32/100][116/156] Data 0.001 (0.001) Batch 3.543 (3.473) Remain 10:16:21 loss: 0.3358 Lr: 0.04084
[2023-08-07 23:44:17,577 INFO misc.py line 115 22900] Train: [32/100][117/156] Data 0.001 (0.001) Batch 4.047 (3.478) Remain 10:17:11 loss: 0.2547 Lr: 0.04084
[2023-08-07 23:44:21,188 INFO misc.py line 115 22900] Train: [32/100][118/156] Data 0.001 (0.001) Batch 3.611 (3.479) Remain 10:17:20 loss: 0.2237 Lr: 0.04083
[2023-08-07 23:44:25,155 INFO misc.py line 115 22900] Train: [32/100][119/156] Data 0.001 (0.001) Batch 3.967 (3.483) Remain 10:18:01 loss: 0.2274 Lr: 0.04083
[2023-08-07 23:44:28,499 INFO misc.py line 115 22900] Train: [32/100][120/156] Data 0.001 (0.001) Batch 3.344 (3.482) Remain 10:17:45 loss: 0.2527 Lr: 0.04082
[2023-08-07 23:44:31,639 INFO misc.py line 115 22900] Train: [32/100][121/156] Data 0.001 (0.001) Batch 3.141 (3.479) Remain 10:17:10 loss: 0.4378 Lr: 0.04082
[2023-08-07 23:44:34,112 INFO misc.py line 115 22900] Train: [32/100][122/156] Data 0.001 (0.001) Batch 2.473 (3.471) Remain 10:15:37 loss: 0.3534 Lr: 0.04082
[2023-08-07 23:44:36,978 INFO misc.py line 115 22900] Train: [32/100][123/156] Data 0.001 (0.001) Batch 2.866 (3.466) Remain 10:14:40 loss: 0.1456 Lr: 0.04081
[2023-08-07 23:44:40,620 INFO misc.py line 115 22900] Train: [32/100][124/156] Data 0.001 (0.001) Batch 3.641 (3.467) Remain 10:14:52 loss: 0.3224 Lr: 0.04081
[2023-08-07 23:44:44,664 INFO misc.py line 115 22900] Train: [32/100][125/156] Data 0.001 (0.001) Batch 4.044 (3.472) Remain 10:15:39 loss: 0.2845 Lr: 0.04080
[2023-08-07 23:44:48,190 INFO misc.py line 115 22900] Train: [32/100][126/156] Data 0.001 (0.001) Batch 3.526 (3.472) Remain 10:15:40 loss: 0.4081 Lr: 0.04080
[2023-08-07 23:44:52,082 INFO misc.py line 115 22900] Train: [32/100][127/156] Data 0.001 (0.001) Batch 3.892 (3.476) Remain 10:16:12 loss: 0.2379 Lr: 0.04079
[2023-08-07 23:44:56,128 INFO misc.py line 115 22900] Train: [32/100][128/156] Data 0.001 (0.001) Batch 4.046 (3.480) Remain 10:16:57 loss: 0.4860 Lr: 0.04079
[2023-08-07 23:45:00,143 INFO misc.py line 115 22900] Train: [32/100][129/156] Data 0.001 (0.001) Batch 4.015 (3.485) Remain 10:17:39 loss: 0.4837 Lr: 0.04079
[2023-08-07 23:45:02,539 INFO misc.py line 115 22900] Train: [32/100][130/156] Data 0.001 (0.001) Batch 2.396 (3.476) Remain 10:16:04 loss: 0.1242 Lr: 0.04078
[2023-08-07 23:45:05,503 INFO misc.py line 115 22900] Train: [32/100][131/156] Data 0.001 (0.001) Batch 2.964 (3.472) Remain 10:15:18 loss: 0.3450 Lr: 0.04078
[2023-08-07 23:45:08,655 INFO misc.py line 115 22900] Train: [32/100][132/156] Data 0.001 (0.001) Batch 3.152 (3.470) Remain 10:14:48 loss: 0.5115 Lr: 0.04077
[2023-08-07 23:45:12,693 INFO misc.py line 115 22900] Train: [32/100][133/156] Data 0.001 (0.001) Batch 4.038 (3.474) Remain 10:15:31 loss: 0.3029 Lr: 0.04077
[2023-08-07 23:45:16,350 INFO misc.py line 115 22900] Train: [32/100][134/156] Data 0.001 (0.001) Batch 3.657 (3.475) Remain 10:15:43 loss: 0.4684 Lr: 0.04077
[2023-08-07 23:45:18,250 INFO misc.py line 115 22900] Train: [32/100][135/156] Data 0.001 (0.001) Batch 1.900 (3.463) Remain 10:13:33 loss: 0.2477 Lr: 0.04076
[2023-08-07 23:45:21,313 INFO misc.py line 115 22900] Train: [32/100][136/156] Data 0.001 (0.001) Batch 3.063 (3.460) Remain 10:12:57 loss: 0.1549 Lr: 0.04076
[2023-08-07 23:45:24,058 INFO misc.py line 115 22900] Train: [32/100][137/156] Data 0.001 (0.001) Batch 2.745 (3.455) Remain 10:11:57 loss: 0.3293 Lr: 0.04075
[2023-08-07 23:45:28,546 INFO misc.py line 115 22900] Train: [32/100][138/156] Data 0.001 (0.001) Batch 4.488 (3.463) Remain 10:13:15 loss: 0.5838 Lr: 0.04075
[2023-08-07 23:45:32,605 INFO misc.py line 115 22900] Train: [32/100][139/156] Data 0.001 (0.001) Batch 4.059 (3.467) Remain 10:13:58 loss: 0.7766 Lr: 0.04075
[2023-08-07 23:45:36,698 INFO misc.py line 115 22900] Train: [32/100][140/156] Data 0.001 (0.001) Batch 4.093 (3.472) Remain 10:14:43 loss: 0.2911 Lr: 0.04074
[2023-08-07 23:45:40,134 INFO misc.py line 115 22900] Train: [32/100][141/156] Data 0.001 (0.001) Batch 3.437 (3.471) Remain 10:14:37 loss: 0.3433 Lr: 0.04074
[2023-08-07 23:45:44,225 INFO misc.py line 115 22900] Train: [32/100][142/156] Data 0.001 (0.001) Batch 4.091 (3.476) Remain 10:15:21 loss: 0.6497 Lr: 0.04073
[2023-08-07 23:45:47,897 INFO misc.py line 115 22900] Train: [32/100][143/156] Data 0.001 (0.001) Batch 3.671 (3.477) Remain 10:15:32 loss: 0.3489 Lr: 0.04073
[2023-08-07 23:45:51,184 INFO misc.py line 115 22900] Train: [32/100][144/156] Data 0.001 (0.001) Batch 3.287 (3.476) Remain 10:15:14 loss: 0.2813 Lr: 0.04072
[2023-08-07 23:45:53,998 INFO misc.py line 115 22900] Train: [32/100][145/156] Data 0.001 (0.001) Batch 2.814 (3.471) Remain 10:14:21 loss: 0.1493 Lr: 0.04072
[2023-08-07 23:45:57,310 INFO misc.py line 115 22900] Train: [32/100][146/156] Data 0.001 (0.001) Batch 3.312 (3.470) Remain 10:14:06 loss: 0.2998 Lr: 0.04072
[2023-08-07 23:46:01,202 INFO misc.py line 115 22900] Train: [32/100][147/156] Data 0.001 (0.001) Batch 3.892 (3.473) Remain 10:14:33 loss: 0.4830 Lr: 0.04071
[2023-08-07 23:46:04,275 INFO misc.py line 115 22900] Train: [32/100][148/156] Data 0.001 (0.001) Batch 3.074 (3.470) Remain 10:14:01 loss: 0.4806 Lr: 0.04071
[2023-08-07 23:46:07,617 INFO misc.py line 115 22900] Train: [32/100][149/156] Data 0.001 (0.001) Batch 3.342 (3.469) Remain 10:13:48 loss: 0.4118 Lr: 0.04070
[2023-08-07 23:46:10,961 INFO misc.py line 115 22900] Train: [32/100][150/156] Data 0.001 (0.001) Batch 3.344 (3.469) Remain 10:13:35 loss: 0.4139 Lr: 0.04070
[2023-08-07 23:46:13,716 INFO misc.py line 115 22900] Train: [32/100][151/156] Data 0.001 (0.001) Batch 2.755 (3.464) Remain 10:12:41 loss: 0.3352 Lr: 0.04070
[2023-08-07 23:46:17,513 INFO misc.py line 115 22900] Train: [32/100][152/156] Data 0.001 (0.001) Batch 3.797 (3.466) Remain 10:13:01 loss: 0.4125 Lr: 0.04069
[2023-08-07 23:46:21,175 INFO misc.py line 115 22900] Train: [32/100][153/156] Data 0.001 (0.001) Batch 3.663 (3.467) Remain 10:13:11 loss: 0.2986 Lr: 0.04069
[2023-08-07 23:46:24,358 INFO misc.py line 115 22900] Train: [32/100][154/156] Data 0.001 (0.001) Batch 3.183 (3.465) Remain 10:12:48 loss: 0.4798 Lr: 0.04068
[2023-08-07 23:46:27,911 INFO misc.py line 115 22900] Train: [32/100][155/156] Data 0.001 (0.001) Batch 3.553 (3.466) Remain 10:12:51 loss: 0.5092 Lr: 0.04068
[2023-08-07 23:46:31,905 INFO misc.py line 115 22900] Train: [32/100][156/156] Data 0.001 (0.001) Batch 3.994 (3.469) Remain 10:13:24 loss: 0.4416 Lr: 0.04068
[2023-08-07 23:46:31,906 INFO misc.py line 129 22900] Train result: loss: 0.3806 
[2023-08-07 23:46:31,906 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 23:46:34,003 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.8318 
[2023-08-07 23:46:34,872 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.5731 
[2023-08-07 23:46:36,535 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.6605 
[2023-08-07 23:46:38,058 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.3682 
[2023-08-07 23:46:39,900 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.3626 
[2023-08-07 23:46:41,564 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.5980 
[2023-08-07 23:46:43,702 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.3621 
[2023-08-07 23:46:45,506 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.9110 
[2023-08-07 23:46:46,788 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.5026 
[2023-08-07 23:46:48,918 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.4297 
[2023-08-07 23:46:49,443 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.1003 
[2023-08-07 23:46:50,977 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.6254 
[2023-08-07 23:46:53,687 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.2389 
[2023-08-07 23:46:55,367 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.6658 
[2023-08-07 23:46:57,389 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.4839 
[2023-08-07 23:47:00,097 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.2959 
[2023-08-07 23:47:02,801 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.3849 
[2023-08-07 23:47:04,648 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.0370 
[2023-08-07 23:47:05,396 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.0759 
[2023-08-07 23:47:06,281 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.7593 
[2023-08-07 23:47:08,542 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3921 
[2023-08-07 23:47:10,506 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.1859 
[2023-08-07 23:47:12,352 INFO evaluator.py line 122 22900] Test: [23/24] Loss 3.2072 
[2023-08-07 23:47:14,287 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.6211 
[2023-08-07 23:47:14,337 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2353/0.3312/0.6859.
[2023-08-07 23:47:14,337 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6781/0.9173
[2023-08-07 23:47:14,337 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9520/0.9872
[2023-08-07 23:47:14,337 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1268/0.3827
[2023-08-07 23:47:14,337 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.0462/0.0506
[2023-08-07 23:47:14,337 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.5837/0.6131
[2023-08-07 23:47:14,337 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.3508/0.5260
[2023-08-07 23:47:14,338 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5178/0.7571
[2023-08-07 23:47:14,338 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.2723/0.3634
[2023-08-07 23:47:14,338 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1512/0.3140
[2023-08-07 23:47:14,338 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0268/0.0269
[2023-08-07 23:47:14,338 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-07 23:47:14,338 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0002/0.0002
[2023-08-07 23:47:14,338 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1517/0.4003
[2023-08-07 23:47:14,338 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0818/0.0964
[2023-08-07 23:47:14,338 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0556/0.1047
[2023-08-07 23:47:14,338 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.2038/0.2330
[2023-08-07 23:47:14,338 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.1182/0.1289
[2023-08-07 23:47:14,338 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.2986/0.5229
[2023-08-07 23:47:14,338 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 23:47:14,338 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0907/0.2000
[2023-08-07 23:47:14,338 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 23:47:14,338 INFO misc.py line 152 22900] Currently Best mIoU: 0.2356
[2023-08-07 23:47:14,338 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 23:47:19,874 INFO misc.py line 115 22900] Train: [33/100][1/156] Data 0.771 (0.771) Batch 4.743 (4.743) Remain 13:58:33 loss: 0.2586 Lr: 0.04067
[2023-08-07 23:47:22,098 INFO misc.py line 115 22900] Train: [33/100][2/156] Data 0.001 (0.001) Batch 2.224 (2.224) Remain 06:33:08 loss: 0.3455 Lr: 0.04067
[2023-08-07 23:47:25,933 INFO misc.py line 115 22900] Train: [33/100][3/156] Data 0.001 (0.001) Batch 3.835 (3.835) Remain 11:17:49 loss: 0.3525 Lr: 0.04066
[2023-08-07 23:47:29,371 INFO misc.py line 115 22900] Train: [33/100][4/156] Data 0.001 (0.001) Batch 3.439 (3.439) Remain 10:07:44 loss: 0.1577 Lr: 0.04066
[2023-08-07 23:47:33,179 INFO misc.py line 115 22900] Train: [33/100][5/156] Data 0.001 (0.001) Batch 3.807 (3.623) Remain 10:40:15 loss: 0.5309 Lr: 0.04065
[2023-08-07 23:47:37,179 INFO misc.py line 115 22900] Train: [33/100][6/156] Data 0.001 (0.001) Batch 4.000 (3.749) Remain 11:02:23 loss: 0.5679 Lr: 0.04065
[2023-08-07 23:47:41,282 INFO misc.py line 115 22900] Train: [33/100][7/156] Data 0.001 (0.001) Batch 4.103 (3.837) Remain 11:17:58 loss: 0.3838 Lr: 0.04065
[2023-08-07 23:47:44,311 INFO misc.py line 115 22900] Train: [33/100][8/156] Data 0.001 (0.001) Batch 3.029 (3.676) Remain 10:49:21 loss: 0.4036 Lr: 0.04064
[2023-08-07 23:47:48,373 INFO misc.py line 115 22900] Train: [33/100][9/156] Data 0.001 (0.001) Batch 4.062 (3.740) Remain 11:00:40 loss: 0.5724 Lr: 0.04064
[2023-08-07 23:47:52,616 INFO misc.py line 115 22900] Train: [33/100][10/156] Data 0.001 (0.001) Batch 4.243 (3.812) Remain 11:13:17 loss: 0.5029 Lr: 0.04063
[2023-08-07 23:47:56,748 INFO misc.py line 115 22900] Train: [33/100][11/156] Data 0.001 (0.001) Batch 4.133 (3.852) Remain 11:20:18 loss: 0.4955 Lr: 0.04063
[2023-08-07 23:47:59,319 INFO misc.py line 115 22900] Train: [33/100][12/156] Data 0.001 (0.001) Batch 2.571 (3.710) Remain 10:55:06 loss: 0.3813 Lr: 0.04063
[2023-08-07 23:48:02,840 INFO misc.py line 115 22900] Train: [33/100][13/156] Data 0.001 (0.001) Batch 3.521 (3.691) Remain 10:51:43 loss: 0.3795 Lr: 0.04062
[2023-08-07 23:48:06,547 INFO misc.py line 115 22900] Train: [33/100][14/156] Data 0.001 (0.001) Batch 3.707 (3.692) Remain 10:51:55 loss: 0.2922 Lr: 0.04062
[2023-08-07 23:48:09,837 INFO misc.py line 115 22900] Train: [33/100][15/156] Data 0.001 (0.001) Batch 3.290 (3.659) Remain 10:45:56 loss: 0.3688 Lr: 0.04061
[2023-08-07 23:48:13,188 INFO misc.py line 115 22900] Train: [33/100][16/156] Data 0.001 (0.001) Batch 3.351 (3.635) Remain 10:41:41 loss: 0.4526 Lr: 0.04061
[2023-08-07 23:48:17,185 INFO misc.py line 115 22900] Train: [33/100][17/156] Data 0.001 (0.001) Batch 3.997 (3.661) Remain 10:46:12 loss: 0.3939 Lr: 0.04061
[2023-08-07 23:48:21,487 INFO misc.py line 115 22900] Train: [33/100][18/156] Data 0.001 (0.001) Batch 4.303 (3.704) Remain 10:53:41 loss: 0.7951 Lr: 0.04060
[2023-08-07 23:48:24,420 INFO misc.py line 115 22900] Train: [33/100][19/156] Data 0.001 (0.001) Batch 2.932 (3.655) Remain 10:45:07 loss: 0.4571 Lr: 0.04060
[2023-08-07 23:48:28,382 INFO misc.py line 115 22900] Train: [33/100][20/156] Data 0.001 (0.001) Batch 3.963 (3.674) Remain 10:48:15 loss: 0.4079 Lr: 0.04059
[2023-08-07 23:48:31,537 INFO misc.py line 115 22900] Train: [33/100][21/156] Data 0.001 (0.001) Batch 3.154 (3.645) Remain 10:43:05 loss: 0.6238 Lr: 0.04059
[2023-08-07 23:48:35,537 INFO misc.py line 115 22900] Train: [33/100][22/156] Data 0.001 (0.001) Batch 4.001 (3.663) Remain 10:46:20 loss: 0.5977 Lr: 0.04058
[2023-08-07 23:48:38,275 INFO misc.py line 115 22900] Train: [33/100][23/156] Data 0.001 (0.001) Batch 2.738 (3.617) Remain 10:38:07 loss: 0.3826 Lr: 0.04058
[2023-08-07 23:48:41,581 INFO misc.py line 115 22900] Train: [33/100][24/156] Data 0.001 (0.001) Batch 3.306 (3.602) Remain 10:35:26 loss: 0.2984 Lr: 0.04058
[2023-08-07 23:48:45,588 INFO misc.py line 115 22900] Train: [33/100][25/156] Data 0.001 (0.001) Batch 4.007 (3.621) Remain 10:38:37 loss: 0.4825 Lr: 0.04057
[2023-08-07 23:48:49,621 INFO misc.py line 115 22900] Train: [33/100][26/156] Data 0.001 (0.001) Batch 4.033 (3.639) Remain 10:41:43 loss: 0.4304 Lr: 0.04057
[2023-08-07 23:48:53,242 INFO misc.py line 115 22900] Train: [33/100][27/156] Data 0.001 (0.001) Batch 3.621 (3.638) Remain 10:41:32 loss: 0.3047 Lr: 0.04056
[2023-08-07 23:48:56,706 INFO misc.py line 115 22900] Train: [33/100][28/156] Data 0.001 (0.001) Batch 3.464 (3.631) Remain 10:40:15 loss: 0.2721 Lr: 0.04056
[2023-08-07 23:49:00,167 INFO misc.py line 115 22900] Train: [33/100][29/156] Data 0.001 (0.001) Batch 3.462 (3.624) Remain 10:39:02 loss: 0.3131 Lr: 0.04056
[2023-08-07 23:49:03,673 INFO misc.py line 115 22900] Train: [33/100][30/156] Data 0.001 (0.001) Batch 3.506 (3.620) Remain 10:38:12 loss: 0.1891 Lr: 0.04055
[2023-08-07 23:49:07,773 INFO misc.py line 115 22900] Train: [33/100][31/156] Data 0.001 (0.001) Batch 4.100 (3.637) Remain 10:41:10 loss: 0.5002 Lr: 0.04055
[2023-08-07 23:49:10,856 INFO misc.py line 115 22900] Train: [33/100][32/156] Data 0.001 (0.001) Batch 3.083 (3.618) Remain 10:37:44 loss: 0.4279 Lr: 0.04054
[2023-08-07 23:49:14,440 INFO misc.py line 115 22900] Train: [33/100][33/156] Data 0.001 (0.001) Batch 3.584 (3.617) Remain 10:37:28 loss: 0.4217 Lr: 0.04054
[2023-08-07 23:49:17,059 INFO misc.py line 115 22900] Train: [33/100][34/156] Data 0.001 (0.001) Batch 2.619 (3.585) Remain 10:31:44 loss: 0.4203 Lr: 0.04053
[2023-08-07 23:49:21,124 INFO misc.py line 115 22900] Train: [33/100][35/156] Data 0.001 (0.001) Batch 4.066 (3.600) Remain 10:34:20 loss: 0.2742 Lr: 0.04053
[2023-08-07 23:49:25,137 INFO misc.py line 115 22900] Train: [33/100][36/156] Data 0.001 (0.001) Batch 4.012 (3.612) Remain 10:36:28 loss: 0.5203 Lr: 0.04053
[2023-08-07 23:49:27,784 INFO misc.py line 115 22900] Train: [33/100][37/156] Data 0.001 (0.001) Batch 2.647 (3.584) Remain 10:31:24 loss: 0.1657 Lr: 0.04052
[2023-08-07 23:49:30,869 INFO misc.py line 115 22900] Train: [33/100][38/156] Data 0.001 (0.001) Batch 3.086 (3.570) Remain 10:28:50 loss: 0.2169 Lr: 0.04052
[2023-08-07 23:49:34,608 INFO misc.py line 115 22900] Train: [33/100][39/156] Data 0.001 (0.001) Batch 3.739 (3.574) Remain 10:29:37 loss: 0.3233 Lr: 0.04051
[2023-08-07 23:49:39,335 INFO misc.py line 115 22900] Train: [33/100][40/156] Data 0.001 (0.001) Batch 4.727 (3.605) Remain 10:35:02 loss: 0.6339 Lr: 0.04051
[2023-08-07 23:49:42,146 INFO misc.py line 115 22900] Train: [33/100][41/156] Data 0.001 (0.001) Batch 2.811 (3.585) Remain 10:31:18 loss: 0.3924 Lr: 0.04051
[2023-08-07 23:49:46,082 INFO misc.py line 115 22900] Train: [33/100][42/156] Data 0.001 (0.001) Batch 3.935 (3.594) Remain 10:32:49 loss: 0.5072 Lr: 0.04050
[2023-08-07 23:49:49,582 INFO misc.py line 115 22900] Train: [33/100][43/156] Data 0.001 (0.001) Batch 3.500 (3.591) Remain 10:32:21 loss: 0.3107 Lr: 0.04050
[2023-08-07 23:49:53,782 INFO misc.py line 115 22900] Train: [33/100][44/156] Data 0.001 (0.001) Batch 4.200 (3.606) Remain 10:34:54 loss: 0.4803 Lr: 0.04049
[2023-08-07 23:49:55,706 INFO misc.py line 115 22900] Train: [33/100][45/156] Data 0.001 (0.001) Batch 1.924 (3.566) Remain 10:27:47 loss: 0.2593 Lr: 0.04049
[2023-08-07 23:49:59,012 INFO misc.py line 115 22900] Train: [33/100][46/156] Data 0.001 (0.001) Batch 3.306 (3.560) Remain 10:26:40 loss: 0.2750 Lr: 0.04048
[2023-08-07 23:50:03,413 INFO misc.py line 115 22900] Train: [33/100][47/156] Data 0.001 (0.001) Batch 4.401 (3.579) Remain 10:29:58 loss: 0.5884 Lr: 0.04048
[2023-08-07 23:50:06,948 INFO misc.py line 115 22900] Train: [33/100][48/156] Data 0.001 (0.001) Batch 3.535 (3.578) Remain 10:29:44 loss: 0.2731 Lr: 0.04048
[2023-08-07 23:50:10,512 INFO misc.py line 115 22900] Train: [33/100][49/156] Data 0.001 (0.001) Batch 3.564 (3.578) Remain 10:29:38 loss: 0.3706 Lr: 0.04047
[2023-08-07 23:50:13,220 INFO misc.py line 115 22900] Train: [33/100][50/156] Data 0.001 (0.001) Batch 2.707 (3.559) Remain 10:26:19 loss: 0.3416 Lr: 0.04047
[2023-08-07 23:50:15,505 INFO misc.py line 115 22900] Train: [33/100][51/156] Data 0.001 (0.001) Batch 2.285 (3.533) Remain 10:21:35 loss: 0.0538 Lr: 0.04046
[2023-08-07 23:50:19,300 INFO misc.py line 115 22900] Train: [33/100][52/156] Data 0.001 (0.001) Batch 3.795 (3.538) Remain 10:22:28 loss: 0.5794 Lr: 0.04046
[2023-08-07 23:50:22,665 INFO misc.py line 115 22900] Train: [33/100][53/156] Data 0.001 (0.001) Batch 3.366 (3.535) Remain 10:21:48 loss: 0.2935 Lr: 0.04046
[2023-08-07 23:50:25,625 INFO misc.py line 115 22900] Train: [33/100][54/156] Data 0.001 (0.001) Batch 2.959 (3.523) Remain 10:19:45 loss: 0.3595 Lr: 0.04045
[2023-08-07 23:50:28,271 INFO misc.py line 115 22900] Train: [33/100][55/156] Data 0.001 (0.001) Batch 2.646 (3.506) Remain 10:16:44 loss: 0.2338 Lr: 0.04045
[2023-08-07 23:50:32,391 INFO misc.py line 115 22900] Train: [33/100][56/156] Data 0.001 (0.001) Batch 4.121 (3.518) Remain 10:18:42 loss: 0.5595 Lr: 0.04044
[2023-08-07 23:50:35,610 INFO misc.py line 115 22900] Train: [33/100][57/156] Data 0.001 (0.001) Batch 3.219 (3.513) Remain 10:17:40 loss: 0.5283 Lr: 0.04044
[2023-08-07 23:50:39,163 INFO misc.py line 115 22900] Train: [33/100][58/156] Data 0.001 (0.001) Batch 3.553 (3.513) Remain 10:17:45 loss: 0.3580 Lr: 0.04043
[2023-08-07 23:50:42,532 INFO misc.py line 115 22900] Train: [33/100][59/156] Data 0.001 (0.001) Batch 3.369 (3.511) Remain 10:17:14 loss: 0.4345 Lr: 0.04043
[2023-08-07 23:50:46,416 INFO misc.py line 115 22900] Train: [33/100][60/156] Data 0.001 (0.001) Batch 3.884 (3.517) Remain 10:18:20 loss: 0.6461 Lr: 0.04043
[2023-08-07 23:50:49,811 INFO misc.py line 115 22900] Train: [33/100][61/156] Data 0.001 (0.001) Batch 3.395 (3.515) Remain 10:17:54 loss: 0.3133 Lr: 0.04042
[2023-08-07 23:50:53,430 INFO misc.py line 115 22900] Train: [33/100][62/156] Data 0.001 (0.001) Batch 3.619 (3.517) Remain 10:18:09 loss: 0.3534 Lr: 0.04042
[2023-08-07 23:50:56,349 INFO misc.py line 115 22900] Train: [33/100][63/156] Data 0.001 (0.001) Batch 2.919 (3.507) Remain 10:16:20 loss: 0.4303 Lr: 0.04041
[2023-08-07 23:51:00,378 INFO misc.py line 115 22900] Train: [33/100][64/156] Data 0.001 (0.001) Batch 4.029 (3.515) Remain 10:17:47 loss: 0.2972 Lr: 0.04041
[2023-08-07 23:51:03,875 INFO misc.py line 115 22900] Train: [33/100][65/156] Data 0.001 (0.001) Batch 3.497 (3.515) Remain 10:17:40 loss: 0.3794 Lr: 0.04041
[2023-08-07 23:51:07,904 INFO misc.py line 115 22900] Train: [33/100][66/156] Data 0.001 (0.001) Batch 4.029 (3.523) Remain 10:19:03 loss: 0.4627 Lr: 0.04040
[2023-08-07 23:51:11,176 INFO misc.py line 115 22900] Train: [33/100][67/156] Data 0.001 (0.001) Batch 3.272 (3.519) Remain 10:18:18 loss: 0.2242 Lr: 0.04040
[2023-08-07 23:51:13,651 INFO misc.py line 115 22900] Train: [33/100][68/156] Data 0.001 (0.001) Batch 2.475 (3.503) Remain 10:15:25 loss: 0.2548 Lr: 0.04039
[2023-08-07 23:51:17,565 INFO misc.py line 115 22900] Train: [33/100][69/156] Data 0.001 (0.001) Batch 3.914 (3.510) Remain 10:16:27 loss: 0.4003 Lr: 0.04039
[2023-08-07 23:51:21,653 INFO misc.py line 115 22900] Train: [33/100][70/156] Data 0.001 (0.001) Batch 4.088 (3.518) Remain 10:17:54 loss: 0.4781 Lr: 0.04038
[2023-08-07 23:51:25,730 INFO misc.py line 115 22900] Train: [33/100][71/156] Data 0.001 (0.001) Batch 4.077 (3.526) Remain 10:19:17 loss: 0.3176 Lr: 0.04038
[2023-08-07 23:51:28,608 INFO misc.py line 115 22900] Train: [33/100][72/156] Data 0.001 (0.001) Batch 2.878 (3.517) Remain 10:17:35 loss: 0.1681 Lr: 0.04038
[2023-08-07 23:51:32,001 INFO misc.py line 115 22900] Train: [33/100][73/156] Data 0.001 (0.001) Batch 3.393 (3.515) Remain 10:17:13 loss: 0.2402 Lr: 0.04037
[2023-08-07 23:51:36,072 INFO misc.py line 115 22900] Train: [33/100][74/156] Data 0.001 (0.001) Batch 4.071 (3.523) Remain 10:18:32 loss: 0.4619 Lr: 0.04037
[2023-08-07 23:51:39,763 INFO misc.py line 115 22900] Train: [33/100][75/156] Data 0.001 (0.001) Batch 3.690 (3.525) Remain 10:18:53 loss: 0.2922 Lr: 0.04036
[2023-08-07 23:51:43,278 INFO misc.py line 115 22900] Train: [33/100][76/156] Data 0.001 (0.001) Batch 3.515 (3.525) Remain 10:18:48 loss: 0.2576 Lr: 0.04036
[2023-08-07 23:51:46,006 INFO misc.py line 115 22900] Train: [33/100][77/156] Data 0.001 (0.001) Batch 2.728 (3.515) Remain 10:16:51 loss: 0.3200 Lr: 0.04036
[2023-08-07 23:51:49,642 INFO misc.py line 115 22900] Train: [33/100][78/156] Data 0.001 (0.001) Batch 3.636 (3.516) Remain 10:17:04 loss: 0.5310 Lr: 0.04035
[2023-08-07 23:51:53,457 INFO misc.py line 115 22900] Train: [33/100][79/156] Data 0.001 (0.001) Batch 3.815 (3.520) Remain 10:17:42 loss: 0.4007 Lr: 0.04035
[2023-08-07 23:51:57,296 INFO misc.py line 115 22900] Train: [33/100][80/156] Data 0.001 (0.001) Batch 3.840 (3.524) Remain 10:18:22 loss: 0.3704 Lr: 0.04034
[2023-08-07 23:52:00,675 INFO misc.py line 115 22900] Train: [33/100][81/156] Data 0.001 (0.001) Batch 3.378 (3.522) Remain 10:17:59 loss: 0.2535 Lr: 0.04034
[2023-08-07 23:52:04,733 INFO misc.py line 115 22900] Train: [33/100][82/156] Data 0.001 (0.001) Batch 4.058 (3.529) Remain 10:19:07 loss: 0.5953 Lr: 0.04033
[2023-08-07 23:52:07,589 INFO misc.py line 115 22900] Train: [33/100][83/156] Data 0.001 (0.001) Batch 2.857 (3.521) Remain 10:17:35 loss: 0.1860 Lr: 0.04033
[2023-08-07 23:52:10,789 INFO misc.py line 115 22900] Train: [33/100][84/156] Data 0.001 (0.001) Batch 3.200 (3.517) Remain 10:16:50 loss: 0.2913 Lr: 0.04033
[2023-08-07 23:52:14,831 INFO misc.py line 115 22900] Train: [33/100][85/156] Data 0.001 (0.001) Batch 4.042 (3.523) Remain 10:17:54 loss: 0.4257 Lr: 0.04032
[2023-08-07 23:52:18,945 INFO misc.py line 115 22900] Train: [33/100][86/156] Data 0.001 (0.001) Batch 4.113 (3.530) Remain 10:19:05 loss: 0.2963 Lr: 0.04032
[2023-08-07 23:52:21,861 INFO misc.py line 115 22900] Train: [33/100][87/156] Data 0.001 (0.001) Batch 2.916 (3.523) Remain 10:17:44 loss: 0.1636 Lr: 0.04031
[2023-08-07 23:52:25,563 INFO misc.py line 115 22900] Train: [33/100][88/156] Data 0.001 (0.001) Batch 3.702 (3.525) Remain 10:18:03 loss: 0.2006 Lr: 0.04031
[2023-08-07 23:52:29,079 INFO misc.py line 115 22900] Train: [33/100][89/156] Data 0.001 (0.001) Batch 3.516 (3.525) Remain 10:17:58 loss: 0.3848 Lr: 0.04031
[2023-08-07 23:52:33,467 INFO misc.py line 115 22900] Train: [33/100][90/156] Data 0.001 (0.001) Batch 4.388 (3.535) Remain 10:19:39 loss: 0.4091 Lr: 0.04030
[2023-08-07 23:52:37,549 INFO misc.py line 115 22900] Train: [33/100][91/156] Data 0.001 (0.001) Batch 4.083 (3.541) Remain 10:20:41 loss: 0.5144 Lr: 0.04030
[2023-08-07 23:52:42,475 INFO misc.py line 115 22900] Train: [33/100][92/156] Data 0.001 (0.001) Batch 4.926 (3.557) Remain 10:23:21 loss: 0.8561 Lr: 0.04029
[2023-08-07 23:52:46,052 INFO misc.py line 115 22900] Train: [33/100][93/156] Data 0.001 (0.001) Batch 3.577 (3.557) Remain 10:23:20 loss: 0.2994 Lr: 0.04029
[2023-08-07 23:52:49,577 INFO misc.py line 115 22900] Train: [33/100][94/156] Data 0.001 (0.001) Batch 3.525 (3.557) Remain 10:23:13 loss: 0.3070 Lr: 0.04028
[2023-08-07 23:52:51,975 INFO misc.py line 115 22900] Train: [33/100][95/156] Data 0.001 (0.001) Batch 2.398 (3.544) Remain 10:20:57 loss: 0.2330 Lr: 0.04028
[2023-08-07 23:52:54,834 INFO misc.py line 115 22900] Train: [33/100][96/156] Data 0.001 (0.001) Batch 2.859 (3.537) Remain 10:19:36 loss: 0.3404 Lr: 0.04028
[2023-08-07 23:52:58,950 INFO misc.py line 115 22900] Train: [33/100][97/156] Data 0.001 (0.001) Batch 4.116 (3.543) Remain 10:20:37 loss: 0.5541 Lr: 0.04027
[2023-08-07 23:53:01,224 INFO misc.py line 115 22900] Train: [33/100][98/156] Data 0.001 (0.001) Batch 2.274 (3.529) Remain 10:18:13 loss: 0.1975 Lr: 0.04027
[2023-08-07 23:53:04,412 INFO misc.py line 115 22900] Train: [33/100][99/156] Data 0.001 (0.001) Batch 3.188 (3.526) Remain 10:17:32 loss: 0.2687 Lr: 0.04026
[2023-08-07 23:53:07,298 INFO misc.py line 115 22900] Train: [33/100][100/156] Data 0.001 (0.001) Batch 2.887 (3.519) Remain 10:16:20 loss: 0.2517 Lr: 0.04026
[2023-08-07 23:53:10,764 INFO misc.py line 115 22900] Train: [33/100][101/156] Data 0.001 (0.001) Batch 3.466 (3.519) Remain 10:16:10 loss: 0.6002 Lr: 0.04025
[2023-08-07 23:53:13,426 INFO misc.py line 115 22900] Train: [33/100][102/156] Data 0.001 (0.001) Batch 2.662 (3.510) Remain 10:14:36 loss: 0.2165 Lr: 0.04025
[2023-08-07 23:53:16,433 INFO misc.py line 115 22900] Train: [33/100][103/156] Data 0.001 (0.001) Batch 3.007 (3.505) Remain 10:13:40 loss: 0.1603 Lr: 0.04025
[2023-08-07 23:53:19,085 INFO misc.py line 115 22900] Train: [33/100][104/156] Data 0.001 (0.001) Batch 2.651 (3.497) Remain 10:12:07 loss: 0.3937 Lr: 0.04024
[2023-08-07 23:53:23,153 INFO misc.py line 115 22900] Train: [33/100][105/156] Data 0.001 (0.001) Batch 4.068 (3.502) Remain 10:13:03 loss: 0.6150 Lr: 0.04024
[2023-08-07 23:53:26,631 INFO misc.py line 115 22900] Train: [33/100][106/156] Data 0.001 (0.001) Batch 3.478 (3.502) Remain 10:12:57 loss: 0.4772 Lr: 0.04023
[2023-08-07 23:53:30,005 INFO misc.py line 115 22900] Train: [33/100][107/156] Data 0.001 (0.001) Batch 3.374 (3.501) Remain 10:12:40 loss: 0.6120 Lr: 0.04023
[2023-08-07 23:53:32,718 INFO misc.py line 115 22900] Train: [33/100][108/156] Data 0.001 (0.001) Batch 2.713 (3.493) Remain 10:11:18 loss: 0.1741 Lr: 0.04023
[2023-08-07 23:53:36,685 INFO misc.py line 115 22900] Train: [33/100][109/156] Data 0.001 (0.001) Batch 3.968 (3.498) Remain 10:12:01 loss: 0.5786 Lr: 0.04022
[2023-08-07 23:53:40,402 INFO misc.py line 115 22900] Train: [33/100][110/156] Data 0.001 (0.001) Batch 3.717 (3.500) Remain 10:12:19 loss: 0.3359 Lr: 0.04022
[2023-08-07 23:53:43,416 INFO misc.py line 115 22900] Train: [33/100][111/156] Data 0.001 (0.001) Batch 3.014 (3.495) Remain 10:11:29 loss: 0.3085 Lr: 0.04021
[2023-08-07 23:53:45,442 INFO misc.py line 115 22900] Train: [33/100][112/156] Data 0.001 (0.001) Batch 2.026 (3.482) Remain 10:09:04 loss: 0.3798 Lr: 0.04021
[2023-08-07 23:53:47,898 INFO misc.py line 115 22900] Train: [33/100][113/156] Data 0.001 (0.001) Batch 2.456 (3.472) Remain 10:07:22 loss: 0.2260 Lr: 0.04020
[2023-08-07 23:53:50,803 INFO misc.py line 115 22900] Train: [33/100][114/156] Data 0.001 (0.001) Batch 2.905 (3.467) Remain 10:06:25 loss: 0.0847 Lr: 0.04020
[2023-08-07 23:53:54,082 INFO misc.py line 115 22900] Train: [33/100][115/156] Data 0.001 (0.001) Batch 3.278 (3.466) Remain 10:06:04 loss: 0.4090 Lr: 0.04020
[2023-08-07 23:53:58,094 INFO misc.py line 115 22900] Train: [33/100][116/156] Data 0.001 (0.001) Batch 4.012 (3.470) Remain 10:06:51 loss: 0.5404 Lr: 0.04019
[2023-08-07 23:54:01,668 INFO misc.py line 115 22900] Train: [33/100][117/156] Data 0.001 (0.001) Batch 3.574 (3.471) Remain 10:06:58 loss: 0.4456 Lr: 0.04019
[2023-08-07 23:54:05,028 INFO misc.py line 115 22900] Train: [33/100][118/156] Data 0.001 (0.001) Batch 3.360 (3.470) Remain 10:06:44 loss: 0.2003 Lr: 0.04018
[2023-08-07 23:54:08,649 INFO misc.py line 115 22900] Train: [33/100][119/156] Data 0.001 (0.001) Batch 3.621 (3.472) Remain 10:06:54 loss: 0.3896 Lr: 0.04018
[2023-08-07 23:54:12,738 INFO misc.py line 115 22900] Train: [33/100][120/156] Data 0.001 (0.001) Batch 4.090 (3.477) Remain 10:07:46 loss: 0.2565 Lr: 0.04017
[2023-08-07 23:54:16,629 INFO misc.py line 115 22900] Train: [33/100][121/156] Data 0.001 (0.001) Batch 3.891 (3.480) Remain 10:08:19 loss: 0.4809 Lr: 0.04017
[2023-08-07 23:54:19,341 INFO misc.py line 115 22900] Train: [33/100][122/156] Data 0.001 (0.001) Batch 2.712 (3.474) Remain 10:07:08 loss: 0.3635 Lr: 0.04017
[2023-08-07 23:54:23,369 INFO misc.py line 115 22900] Train: [33/100][123/156] Data 0.001 (0.001) Batch 4.028 (3.479) Remain 10:07:53 loss: 0.2623 Lr: 0.04016
[2023-08-07 23:54:27,431 INFO misc.py line 115 22900] Train: [33/100][124/156] Data 0.001 (0.001) Batch 4.062 (3.483) Remain 10:08:40 loss: 0.4679 Lr: 0.04016
[2023-08-07 23:54:30,767 INFO misc.py line 115 22900] Train: [33/100][125/156] Data 0.001 (0.001) Batch 3.337 (3.482) Remain 10:08:24 loss: 0.2826 Lr: 0.04015
[2023-08-07 23:54:33,381 INFO misc.py line 115 22900] Train: [33/100][126/156] Data 0.001 (0.001) Batch 2.614 (3.475) Remain 10:07:06 loss: 0.4788 Lr: 0.04015
[2023-08-07 23:54:37,541 INFO misc.py line 115 22900] Train: [33/100][127/156] Data 0.001 (0.001) Batch 4.160 (3.481) Remain 10:08:01 loss: 0.5706 Lr: 0.04015
[2023-08-07 23:54:41,508 INFO misc.py line 115 22900] Train: [33/100][128/156] Data 0.001 (0.001) Batch 3.966 (3.485) Remain 10:08:38 loss: 0.4272 Lr: 0.04014
[2023-08-07 23:54:45,515 INFO misc.py line 115 22900] Train: [33/100][129/156] Data 0.001 (0.001) Batch 4.007 (3.489) Remain 10:09:18 loss: 0.3704 Lr: 0.04014
[2023-08-07 23:54:49,293 INFO misc.py line 115 22900] Train: [33/100][130/156] Data 0.001 (0.001) Batch 3.778 (3.491) Remain 10:09:38 loss: 0.1690 Lr: 0.04013
[2023-08-07 23:54:51,806 INFO misc.py line 115 22900] Train: [33/100][131/156] Data 0.001 (0.001) Batch 2.513 (3.483) Remain 10:08:15 loss: 0.3977 Lr: 0.04013
[2023-08-07 23:54:55,456 INFO misc.py line 115 22900] Train: [33/100][132/156] Data 0.001 (0.001) Batch 3.650 (3.485) Remain 10:08:25 loss: 0.4443 Lr: 0.04012
[2023-08-07 23:54:58,488 INFO misc.py line 115 22900] Train: [33/100][133/156] Data 0.001 (0.001) Batch 3.032 (3.481) Remain 10:07:45 loss: 0.4342 Lr: 0.04012
[2023-08-07 23:55:02,575 INFO misc.py line 115 22900] Train: [33/100][134/156] Data 0.001 (0.001) Batch 4.087 (3.486) Remain 10:08:30 loss: 0.3615 Lr: 0.04012
[2023-08-07 23:55:05,938 INFO misc.py line 115 22900] Train: [33/100][135/156] Data 0.001 (0.001) Batch 3.363 (3.485) Remain 10:08:17 loss: 0.3124 Lr: 0.04011
[2023-08-07 23:55:09,148 INFO misc.py line 115 22900] Train: [33/100][136/156] Data 0.001 (0.001) Batch 3.209 (3.483) Remain 10:07:52 loss: 0.3360 Lr: 0.04011
[2023-08-07 23:55:11,714 INFO misc.py line 115 22900] Train: [33/100][137/156] Data 0.001 (0.001) Batch 2.566 (3.476) Remain 10:06:36 loss: 0.2527 Lr: 0.04010
[2023-08-07 23:55:14,472 INFO misc.py line 115 22900] Train: [33/100][138/156] Data 0.001 (0.001) Batch 2.759 (3.471) Remain 10:05:37 loss: 0.0649 Lr: 0.04010
[2023-08-07 23:55:18,950 INFO misc.py line 115 22900] Train: [33/100][139/156] Data 0.001 (0.001) Batch 4.478 (3.478) Remain 10:06:51 loss: 0.6395 Lr: 0.04009
[2023-08-07 23:55:23,007 INFO misc.py line 115 22900] Train: [33/100][140/156] Data 0.001 (0.001) Batch 4.057 (3.482) Remain 10:07:32 loss: 0.3903 Lr: 0.04009
[2023-08-07 23:55:27,016 INFO misc.py line 115 22900] Train: [33/100][141/156] Data 0.001 (0.001) Batch 4.009 (3.486) Remain 10:08:09 loss: 0.4135 Lr: 0.04009
[2023-08-07 23:55:30,705 INFO misc.py line 115 22900] Train: [33/100][142/156] Data 0.001 (0.001) Batch 3.689 (3.488) Remain 10:08:20 loss: 0.3872 Lr: 0.04008
[2023-08-07 23:55:33,506 INFO misc.py line 115 22900] Train: [33/100][143/156] Data 0.001 (0.001) Batch 2.801 (3.483) Remain 10:07:26 loss: 0.2959 Lr: 0.04008
[2023-08-07 23:55:36,977 INFO misc.py line 115 22900] Train: [33/100][144/156] Data 0.001 (0.001) Batch 3.471 (3.483) Remain 10:07:21 loss: 0.7707 Lr: 0.04007
[2023-08-07 23:55:41,090 INFO misc.py line 115 22900] Train: [33/100][145/156] Data 0.001 (0.001) Batch 4.113 (3.487) Remain 10:08:04 loss: 0.4037 Lr: 0.04007
[2023-08-07 23:55:45,090 INFO misc.py line 115 22900] Train: [33/100][146/156] Data 0.001 (0.001) Batch 4.000 (3.491) Remain 10:08:38 loss: 0.7174 Lr: 0.04007
[2023-08-07 23:55:48,395 INFO misc.py line 115 22900] Train: [33/100][147/156] Data 0.001 (0.001) Batch 3.305 (3.489) Remain 10:08:21 loss: 0.3509 Lr: 0.04006
[2023-08-07 23:55:51,060 INFO misc.py line 115 22900] Train: [33/100][148/156] Data 0.001 (0.001) Batch 2.665 (3.484) Remain 10:07:18 loss: 0.2580 Lr: 0.04006
[2023-08-07 23:55:53,594 INFO misc.py line 115 22900] Train: [33/100][149/156] Data 0.001 (0.001) Batch 2.534 (3.477) Remain 10:06:07 loss: 0.2611 Lr: 0.04005
[2023-08-07 23:55:56,876 INFO misc.py line 115 22900] Train: [33/100][150/156] Data 0.001 (0.001) Batch 3.282 (3.476) Remain 10:05:49 loss: 0.2819 Lr: 0.04005
[2023-08-07 23:55:59,966 INFO misc.py line 115 22900] Train: [33/100][151/156] Data 0.001 (0.001) Batch 3.090 (3.473) Remain 10:05:19 loss: 0.2429 Lr: 0.04004
[2023-08-07 23:56:03,674 INFO misc.py line 115 22900] Train: [33/100][152/156] Data 0.001 (0.001) Batch 3.707 (3.475) Remain 10:05:32 loss: 0.4277 Lr: 0.04004
[2023-08-07 23:56:06,867 INFO misc.py line 115 22900] Train: [33/100][153/156] Data 0.001 (0.001) Batch 3.193 (3.473) Remain 10:05:09 loss: 0.6693 Lr: 0.04004
[2023-08-07 23:56:09,608 INFO misc.py line 115 22900] Train: [33/100][154/156] Data 0.001 (0.001) Batch 2.742 (3.468) Remain 10:04:14 loss: 0.3376 Lr: 0.04003
[2023-08-07 23:56:12,926 INFO misc.py line 115 22900] Train: [33/100][155/156] Data 0.001 (0.001) Batch 3.317 (3.467) Remain 10:04:01 loss: 0.3563 Lr: 0.04003
[2023-08-07 23:56:15,563 INFO misc.py line 115 22900] Train: [33/100][156/156] Data 0.001 (0.001) Batch 2.637 (3.462) Remain 10:03:00 loss: 0.4170 Lr: 0.04002
[2023-08-07 23:56:15,563 INFO misc.py line 129 22900] Train result: loss: 0.3822 
[2023-08-07 23:56:15,563 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-07 23:56:17,669 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.0124 
[2023-08-07 23:56:18,538 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.7811 
[2023-08-07 23:56:20,201 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.8283 
[2023-08-07 23:56:21,721 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.4880 
[2023-08-07 23:56:23,566 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.3618 
[2023-08-07 23:56:25,227 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.6990 
[2023-08-07 23:56:27,363 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.8648 
[2023-08-07 23:56:29,166 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.9215 
[2023-08-07 23:56:30,449 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.5511 
[2023-08-07 23:56:32,579 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3733 
[2023-08-07 23:56:33,104 INFO evaluator.py line 122 22900] Test: [11/24] Loss 0.8779 
[2023-08-07 23:56:34,637 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.7859 
[2023-08-07 23:56:37,348 INFO evaluator.py line 122 22900] Test: [13/24] Loss 0.9642 
[2023-08-07 23:56:39,029 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.8463 
[2023-08-07 23:56:41,052 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.4512 
[2023-08-07 23:56:43,761 INFO evaluator.py line 122 22900] Test: [16/24] Loss 0.9240 
[2023-08-07 23:56:46,469 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.3841 
[2023-08-07 23:56:48,315 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.4581 
[2023-08-07 23:56:49,063 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.1449 
[2023-08-07 23:56:49,949 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.8333 
[2023-08-07 23:56:52,211 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.2356 
[2023-08-07 23:56:54,174 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.6057 
[2023-08-07 23:56:56,022 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.3927 
[2023-08-07 23:56:57,958 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.8171 
[2023-08-07 23:56:58,011 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2331/0.3543/0.6758.
[2023-08-07 23:56:58,011 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6498/0.9235
[2023-08-07 23:56:58,011 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9377/0.9880
[2023-08-07 23:56:58,011 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1374/0.4218
[2023-08-07 23:56:58,011 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1134/0.4172
[2023-08-07 23:56:58,011 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6507/0.7521
[2023-08-07 23:56:58,011 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.3690/0.4758
[2023-08-07 23:56:58,011 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5067/0.5608
[2023-08-07 23:56:58,011 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0854/0.0937
[2023-08-07 23:56:58,011 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1473/0.3247
[2023-08-07 23:56:58,011 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.1194/0.1218
[2023-08-07 23:56:58,011 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0422/0.7976
[2023-08-07 23:56:58,011 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0101/0.0103
[2023-08-07 23:56:58,011 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1749/0.2471
[2023-08-07 23:56:58,011 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0070/0.0074
[2023-08-07 23:56:58,011 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0406/0.0619
[2023-08-07 23:56:58,011 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.1275/0.1281
[2023-08-07 23:56:58,011 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0514/0.0531
[2023-08-07 23:56:58,011 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.3943/0.4493
[2023-08-07 23:56:58,012 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-07 23:56:58,012 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0971/0.2517
[2023-08-07 23:56:58,012 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-07 23:56:58,012 INFO misc.py line 152 22900] Currently Best mIoU: 0.2356
[2023-08-07 23:56:58,012 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-07 23:57:03,518 INFO misc.py line 115 22900] Train: [34/100][1/156] Data 1.261 (1.261) Batch 4.719 (4.719) Remain 13:42:01 loss: 0.4051 Lr: 0.04002
[2023-08-07 23:57:06,506 INFO misc.py line 115 22900] Train: [34/100][2/156] Data 0.001 (0.001) Batch 2.988 (2.988) Remain 08:40:28 loss: 0.3591 Lr: 0.04001
[2023-08-07 23:57:09,895 INFO misc.py line 115 22900] Train: [34/100][3/156] Data 0.001 (0.001) Batch 3.389 (3.389) Remain 09:50:12 loss: 0.2084 Lr: 0.04001
[2023-08-07 23:57:13,392 INFO misc.py line 115 22900] Train: [34/100][4/156] Data 0.001 (0.001) Batch 3.497 (3.497) Remain 10:08:57 loss: 0.3086 Lr: 0.04001
[2023-08-07 23:57:17,554 INFO misc.py line 115 22900] Train: [34/100][5/156] Data 0.001 (0.001) Batch 4.161 (3.829) Remain 11:06:43 loss: 0.6893 Lr: 0.04000
[2023-08-07 23:57:20,143 INFO misc.py line 115 22900] Train: [34/100][6/156] Data 0.001 (0.001) Batch 2.589 (3.416) Remain 09:54:41 loss: 0.3627 Lr: 0.04000
[2023-08-07 23:57:23,019 INFO misc.py line 115 22900] Train: [34/100][7/156] Data 0.001 (0.001) Batch 2.876 (3.281) Remain 09:31:08 loss: 0.2831 Lr: 0.03999
[2023-08-07 23:57:26,493 INFO misc.py line 115 22900] Train: [34/100][8/156] Data 0.001 (0.001) Batch 3.474 (3.319) Remain 09:37:48 loss: 0.2087 Lr: 0.03999
[2023-08-07 23:57:30,488 INFO misc.py line 115 22900] Train: [34/100][9/156] Data 0.001 (0.001) Batch 3.996 (3.432) Remain 09:57:22 loss: 0.6376 Lr: 0.03998
[2023-08-07 23:57:33,920 INFO misc.py line 115 22900] Train: [34/100][10/156] Data 0.001 (0.001) Batch 3.431 (3.432) Remain 09:57:17 loss: 0.2868 Lr: 0.03998
[2023-08-07 23:57:37,945 INFO misc.py line 115 22900] Train: [34/100][11/156] Data 0.001 (0.001) Batch 4.026 (3.506) Remain 10:10:08 loss: 0.3081 Lr: 0.03998
[2023-08-07 23:57:42,330 INFO misc.py line 115 22900] Train: [34/100][12/156] Data 0.001 (0.001) Batch 4.384 (3.604) Remain 10:27:03 loss: 0.5415 Lr: 0.03997
[2023-08-07 23:57:45,551 INFO misc.py line 115 22900] Train: [34/100][13/156] Data 0.001 (0.001) Batch 3.221 (3.566) Remain 10:20:21 loss: 0.2182 Lr: 0.03997
[2023-08-07 23:57:49,592 INFO misc.py line 115 22900] Train: [34/100][14/156] Data 0.001 (0.001) Batch 4.041 (3.609) Remain 10:27:48 loss: 0.6747 Lr: 0.03996
[2023-08-07 23:57:53,916 INFO misc.py line 115 22900] Train: [34/100][15/156] Data 0.001 (0.001) Batch 4.324 (3.668) Remain 10:38:07 loss: 0.4284 Lr: 0.03996
[2023-08-07 23:57:56,637 INFO misc.py line 115 22900] Train: [34/100][16/156] Data 0.001 (0.001) Batch 2.721 (3.596) Remain 10:25:22 loss: 0.1646 Lr: 0.03996
[2023-08-07 23:57:59,808 INFO misc.py line 115 22900] Train: [34/100][17/156] Data 0.001 (0.001) Batch 3.171 (3.565) Remain 10:20:03 loss: 0.4588 Lr: 0.03995
[2023-08-07 23:58:02,971 INFO misc.py line 115 22900] Train: [34/100][18/156] Data 0.001 (0.001) Batch 3.163 (3.538) Remain 10:15:19 loss: 0.2758 Lr: 0.03995
[2023-08-07 23:58:05,841 INFO misc.py line 115 22900] Train: [34/100][19/156] Data 0.001 (0.001) Batch 2.870 (3.497) Remain 10:08:00 loss: 0.2290 Lr: 0.03994
[2023-08-07 23:58:09,356 INFO misc.py line 115 22900] Train: [34/100][20/156] Data 0.001 (0.001) Batch 3.515 (3.498) Remain 10:08:07 loss: 0.3788 Lr: 0.03994
[2023-08-07 23:58:12,701 INFO misc.py line 115 22900] Train: [34/100][21/156] Data 0.001 (0.001) Batch 3.345 (3.489) Remain 10:06:35 loss: 0.2624 Lr: 0.03993
[2023-08-07 23:58:17,196 INFO misc.py line 115 22900] Train: [34/100][22/156] Data 0.001 (0.001) Batch 4.495 (3.542) Remain 10:15:44 loss: 0.4147 Lr: 0.03993
[2023-08-07 23:58:20,796 INFO misc.py line 115 22900] Train: [34/100][23/156] Data 0.001 (0.001) Batch 3.600 (3.545) Remain 10:16:11 loss: 0.4975 Lr: 0.03993
[2023-08-07 23:58:23,628 INFO misc.py line 115 22900] Train: [34/100][24/156] Data 0.001 (0.001) Batch 2.832 (3.511) Remain 10:10:13 loss: 0.3799 Lr: 0.03992
[2023-08-07 23:58:26,976 INFO misc.py line 115 22900] Train: [34/100][25/156] Data 0.001 (0.001) Batch 3.348 (3.504) Remain 10:08:52 loss: 0.5135 Lr: 0.03992
[2023-08-07 23:58:30,569 INFO misc.py line 115 22900] Train: [34/100][26/156] Data 0.001 (0.001) Batch 3.593 (3.508) Remain 10:09:29 loss: 0.3570 Lr: 0.03991
[2023-08-07 23:58:35,120 INFO misc.py line 115 22900] Train: [34/100][27/156] Data 0.001 (0.001) Batch 4.551 (3.551) Remain 10:16:59 loss: 0.4698 Lr: 0.03991
[2023-08-07 23:58:38,903 INFO misc.py line 115 22900] Train: [34/100][28/156] Data 0.001 (0.001) Batch 3.782 (3.560) Remain 10:18:32 loss: 0.1973 Lr: 0.03990
[2023-08-07 23:58:42,163 INFO misc.py line 115 22900] Train: [34/100][29/156] Data 0.001 (0.001) Batch 3.260 (3.549) Remain 10:16:28 loss: 0.6420 Lr: 0.03990
[2023-08-07 23:58:45,000 INFO misc.py line 115 22900] Train: [34/100][30/156] Data 0.001 (0.001) Batch 2.838 (3.522) Remain 10:11:50 loss: 0.2114 Lr: 0.03990
[2023-08-07 23:58:48,998 INFO misc.py line 115 22900] Train: [34/100][31/156] Data 0.001 (0.001) Batch 3.997 (3.539) Remain 10:14:43 loss: 0.4044 Lr: 0.03989
[2023-08-07 23:58:52,403 INFO misc.py line 115 22900] Train: [34/100][32/156] Data 0.001 (0.001) Batch 3.406 (3.535) Remain 10:13:52 loss: 0.3820 Lr: 0.03989
[2023-08-07 23:58:55,201 INFO misc.py line 115 22900] Train: [34/100][33/156] Data 0.001 (0.001) Batch 2.798 (3.510) Remain 10:09:32 loss: 0.3191 Lr: 0.03988
[2023-08-07 23:58:58,444 INFO misc.py line 115 22900] Train: [34/100][34/156] Data 0.001 (0.001) Batch 3.243 (3.502) Remain 10:07:59 loss: 0.3340 Lr: 0.03988
[2023-08-07 23:59:02,963 INFO misc.py line 115 22900] Train: [34/100][35/156] Data 0.001 (0.001) Batch 4.519 (3.533) Remain 10:13:27 loss: 0.5095 Lr: 0.03987
[2023-08-07 23:59:06,444 INFO misc.py line 115 22900] Train: [34/100][36/156] Data 0.001 (0.001) Batch 3.481 (3.532) Remain 10:13:06 loss: 0.3633 Lr: 0.03987
[2023-08-07 23:59:09,061 INFO misc.py line 115 22900] Train: [34/100][37/156] Data 0.001 (0.001) Batch 2.617 (3.505) Remain 10:08:23 loss: 0.1733 Lr: 0.03987
[2023-08-07 23:59:12,619 INFO misc.py line 115 22900] Train: [34/100][38/156] Data 0.001 (0.001) Batch 3.559 (3.506) Remain 10:08:35 loss: 0.5594 Lr: 0.03986
[2023-08-07 23:59:15,585 INFO misc.py line 115 22900] Train: [34/100][39/156] Data 0.001 (0.001) Batch 2.965 (3.491) Remain 10:05:55 loss: 0.2265 Lr: 0.03986
[2023-08-07 23:59:19,987 INFO misc.py line 115 22900] Train: [34/100][40/156] Data 0.001 (0.001) Batch 4.402 (3.516) Remain 10:10:08 loss: 0.5480 Lr: 0.03985
[2023-08-07 23:59:22,671 INFO misc.py line 115 22900] Train: [34/100][41/156] Data 0.001 (0.001) Batch 2.684 (3.494) Remain 10:06:16 loss: 0.3840 Lr: 0.03985
[2023-08-07 23:59:26,284 INFO misc.py line 115 22900] Train: [34/100][42/156] Data 0.001 (0.001) Batch 3.613 (3.497) Remain 10:06:45 loss: 0.3812 Lr: 0.03984
[2023-08-07 23:59:29,505 INFO misc.py line 115 22900] Train: [34/100][43/156] Data 0.001 (0.001) Batch 3.222 (3.490) Remain 10:05:30 loss: 0.5291 Lr: 0.03984
[2023-08-07 23:59:32,982 INFO misc.py line 115 22900] Train: [34/100][44/156] Data 0.001 (0.001) Batch 3.476 (3.490) Remain 10:05:22 loss: 0.3531 Lr: 0.03984
[2023-08-07 23:59:36,112 INFO misc.py line 115 22900] Train: [34/100][45/156] Data 0.001 (0.001) Batch 3.130 (3.481) Remain 10:03:50 loss: 0.3800 Lr: 0.03983
[2023-08-07 23:59:39,189 INFO misc.py line 115 22900] Train: [34/100][46/156] Data 0.001 (0.001) Batch 3.077 (3.472) Remain 10:02:09 loss: 0.4502 Lr: 0.03983
[2023-08-07 23:59:43,653 INFO misc.py line 115 22900] Train: [34/100][47/156] Data 0.001 (0.001) Batch 4.464 (3.494) Remain 10:06:00 loss: 0.7169 Lr: 0.03982
[2023-08-07 23:59:46,484 INFO misc.py line 115 22900] Train: [34/100][48/156] Data 0.001 (0.001) Batch 2.831 (3.480) Remain 10:03:23 loss: 0.3813 Lr: 0.03982
[2023-08-07 23:59:50,170 INFO misc.py line 115 22900] Train: [34/100][49/156] Data 0.001 (0.001) Batch 3.685 (3.484) Remain 10:04:06 loss: 0.4580 Lr: 0.03981
[2023-08-07 23:59:53,710 INFO misc.py line 115 22900] Train: [34/100][50/156] Data 0.002 (0.001) Batch 3.540 (3.485) Remain 10:04:15 loss: 0.3012 Lr: 0.03981
[2023-08-07 23:59:57,702 INFO misc.py line 115 22900] Train: [34/100][51/156] Data 0.001 (0.001) Batch 3.992 (3.496) Remain 10:06:01 loss: 0.2902 Lr: 0.03981
[2023-08-08 00:00:00,918 INFO misc.py line 115 22900] Train: [34/100][52/156] Data 0.001 (0.001) Batch 3.216 (3.490) Remain 10:04:58 loss: 0.3739 Lr: 0.03980
[2023-08-08 00:00:03,757 INFO misc.py line 115 22900] Train: [34/100][53/156] Data 0.001 (0.001) Batch 2.838 (3.477) Remain 10:02:39 loss: 0.3478 Lr: 0.03980
[2023-08-08 00:00:06,755 INFO misc.py line 115 22900] Train: [34/100][54/156] Data 0.001 (0.001) Batch 2.999 (3.468) Remain 10:00:58 loss: 0.2036 Lr: 0.03979
[2023-08-08 00:00:10,954 INFO misc.py line 115 22900] Train: [34/100][55/156] Data 0.001 (0.001) Batch 4.199 (3.482) Remain 10:03:21 loss: 0.5941 Lr: 0.03979
[2023-08-08 00:00:14,136 INFO misc.py line 115 22900] Train: [34/100][56/156] Data 0.001 (0.001) Batch 3.181 (3.476) Remain 10:02:18 loss: 0.3931 Lr: 0.03978
[2023-08-08 00:00:18,131 INFO misc.py line 115 22900] Train: [34/100][57/156] Data 0.001 (0.001) Batch 3.996 (3.486) Remain 10:03:55 loss: 0.4188 Lr: 0.03978
[2023-08-08 00:00:21,167 INFO misc.py line 115 22900] Train: [34/100][58/156] Data 0.001 (0.001) Batch 3.036 (3.478) Remain 10:02:26 loss: 0.3542 Lr: 0.03978
[2023-08-08 00:00:24,777 INFO misc.py line 115 22900] Train: [34/100][59/156] Data 0.001 (0.001) Batch 3.610 (3.480) Remain 10:02:47 loss: 0.2941 Lr: 0.03977
[2023-08-08 00:00:28,156 INFO misc.py line 115 22900] Train: [34/100][60/156] Data 0.001 (0.001) Batch 3.380 (3.478) Remain 10:02:26 loss: 0.2123 Lr: 0.03977
[2023-08-08 00:00:30,740 INFO misc.py line 115 22900] Train: [34/100][61/156] Data 0.001 (0.001) Batch 2.584 (3.463) Remain 09:59:42 loss: 0.2485 Lr: 0.03976
[2023-08-08 00:00:34,551 INFO misc.py line 115 22900] Train: [34/100][62/156] Data 0.001 (0.001) Batch 3.811 (3.469) Remain 10:00:40 loss: 0.4468 Lr: 0.03976
[2023-08-08 00:00:37,291 INFO misc.py line 115 22900] Train: [34/100][63/156] Data 0.001 (0.001) Batch 2.740 (3.457) Remain 09:58:30 loss: 0.1965 Lr: 0.03975
[2023-08-08 00:00:39,925 INFO misc.py line 115 22900] Train: [34/100][64/156] Data 0.001 (0.001) Batch 2.634 (3.443) Remain 09:56:06 loss: 0.2669 Lr: 0.03975
[2023-08-08 00:00:44,267 INFO misc.py line 115 22900] Train: [34/100][65/156] Data 0.001 (0.001) Batch 4.343 (3.458) Remain 09:58:34 loss: 0.7141 Lr: 0.03975
[2023-08-08 00:00:47,674 INFO misc.py line 115 22900] Train: [34/100][66/156] Data 0.001 (0.001) Batch 3.407 (3.457) Remain 09:58:22 loss: 0.3052 Lr: 0.03974
[2023-08-08 00:00:51,409 INFO misc.py line 115 22900] Train: [34/100][67/156] Data 0.001 (0.001) Batch 3.735 (3.461) Remain 09:59:04 loss: 0.2253 Lr: 0.03974
[2023-08-08 00:00:55,459 INFO misc.py line 115 22900] Train: [34/100][68/156] Data 0.001 (0.001) Batch 4.050 (3.470) Remain 10:00:34 loss: 0.5645 Lr: 0.03973
[2023-08-08 00:00:58,429 INFO misc.py line 115 22900] Train: [34/100][69/156] Data 0.001 (0.001) Batch 2.970 (3.463) Remain 09:59:12 loss: 0.3791 Lr: 0.03973
[2023-08-08 00:01:02,011 INFO misc.py line 115 22900] Train: [34/100][70/156] Data 0.001 (0.001) Batch 3.582 (3.464) Remain 09:59:27 loss: 0.2943 Lr: 0.03972
[2023-08-08 00:01:05,286 INFO misc.py line 115 22900] Train: [34/100][71/156] Data 0.001 (0.001) Batch 3.276 (3.462) Remain 09:58:55 loss: 0.1883 Lr: 0.03972
[2023-08-08 00:01:09,168 INFO misc.py line 115 22900] Train: [34/100][72/156] Data 0.001 (0.001) Batch 3.882 (3.468) Remain 09:59:54 loss: 0.4661 Lr: 0.03972
[2023-08-08 00:01:13,355 INFO misc.py line 115 22900] Train: [34/100][73/156] Data 0.001 (0.001) Batch 4.186 (3.478) Remain 10:01:38 loss: 0.4291 Lr: 0.03971
[2023-08-08 00:01:17,449 INFO misc.py line 115 22900] Train: [34/100][74/156] Data 0.001 (0.001) Batch 4.094 (3.487) Remain 10:03:04 loss: 0.3709 Lr: 0.03971
[2023-08-08 00:01:21,129 INFO misc.py line 115 22900] Train: [34/100][75/156] Data 0.001 (0.001) Batch 3.680 (3.489) Remain 10:03:29 loss: 0.3711 Lr: 0.03970
[2023-08-08 00:01:25,416 INFO misc.py line 115 22900] Train: [34/100][76/156] Data 0.001 (0.001) Batch 4.287 (3.500) Remain 10:05:18 loss: 0.6011 Lr: 0.03970
[2023-08-08 00:01:28,942 INFO misc.py line 115 22900] Train: [34/100][77/156] Data 0.001 (0.001) Batch 3.526 (3.501) Remain 10:05:19 loss: 0.3888 Lr: 0.03969
[2023-08-08 00:01:32,114 INFO misc.py line 115 22900] Train: [34/100][78/156] Data 0.001 (0.001) Batch 3.172 (3.496) Remain 10:04:30 loss: 0.2706 Lr: 0.03969
[2023-08-08 00:01:35,408 INFO misc.py line 115 22900] Train: [34/100][79/156] Data 0.001 (0.001) Batch 3.294 (3.494) Remain 10:03:58 loss: 0.2409 Lr: 0.03969
[2023-08-08 00:01:37,979 INFO misc.py line 115 22900] Train: [34/100][80/156] Data 0.001 (0.001) Batch 2.571 (3.482) Remain 10:01:51 loss: 0.2314 Lr: 0.03968
[2023-08-08 00:01:41,989 INFO misc.py line 115 22900] Train: [34/100][81/156] Data 0.001 (0.001) Batch 4.010 (3.488) Remain 10:02:57 loss: 0.6001 Lr: 0.03968
[2023-08-08 00:01:46,058 INFO misc.py line 115 22900] Train: [34/100][82/156] Data 0.001 (0.001) Batch 4.069 (3.496) Remain 10:04:10 loss: 0.3662 Lr: 0.03967
[2023-08-08 00:01:49,838 INFO misc.py line 115 22900] Train: [34/100][83/156] Data 0.001 (0.001) Batch 3.780 (3.499) Remain 10:04:44 loss: 0.3614 Lr: 0.03967
[2023-08-08 00:01:52,059 INFO misc.py line 115 22900] Train: [34/100][84/156] Data 0.001 (0.001) Batch 2.220 (3.483) Remain 10:01:56 loss: 0.1473 Lr: 0.03966
[2023-08-08 00:01:55,332 INFO misc.py line 115 22900] Train: [34/100][85/156] Data 0.001 (0.001) Batch 3.273 (3.481) Remain 10:01:26 loss: 0.3799 Lr: 0.03966
[2023-08-08 00:02:00,079 INFO misc.py line 115 22900] Train: [34/100][86/156] Data 0.001 (0.001) Batch 4.747 (3.496) Remain 10:04:01 loss: 0.4015 Lr: 0.03966
[2023-08-08 00:02:03,529 INFO misc.py line 115 22900] Train: [34/100][87/156] Data 0.001 (0.001) Batch 3.450 (3.496) Remain 10:03:52 loss: 0.1934 Lr: 0.03965
[2023-08-08 00:02:06,476 INFO misc.py line 115 22900] Train: [34/100][88/156] Data 0.001 (0.001) Batch 2.947 (3.489) Remain 10:02:41 loss: 0.1525 Lr: 0.03965
[2023-08-08 00:02:09,984 INFO misc.py line 115 22900] Train: [34/100][89/156] Data 0.001 (0.001) Batch 3.508 (3.489) Remain 10:02:40 loss: 0.1642 Lr: 0.03964
[2023-08-08 00:02:12,019 INFO misc.py line 115 22900] Train: [34/100][90/156] Data 0.001 (0.001) Batch 2.035 (3.473) Remain 09:59:43 loss: 0.2583 Lr: 0.03964
[2023-08-08 00:02:16,123 INFO misc.py line 115 22900] Train: [34/100][91/156] Data 0.001 (0.001) Batch 4.104 (3.480) Remain 10:00:54 loss: 0.5070 Lr: 0.03963
[2023-08-08 00:02:18,820 INFO misc.py line 115 22900] Train: [34/100][92/156] Data 0.001 (0.001) Batch 2.697 (3.471) Remain 09:59:20 loss: 0.3191 Lr: 0.03963
[2023-08-08 00:02:22,381 INFO misc.py line 115 22900] Train: [34/100][93/156] Data 0.001 (0.001) Batch 3.561 (3.472) Remain 09:59:27 loss: 0.6356 Lr: 0.03963
[2023-08-08 00:02:26,390 INFO misc.py line 115 22900] Train: [34/100][94/156] Data 0.001 (0.001) Batch 4.009 (3.478) Remain 10:00:24 loss: 0.1688 Lr: 0.03962
[2023-08-08 00:02:30,007 INFO misc.py line 115 22900] Train: [34/100][95/156] Data 0.001 (0.001) Batch 3.618 (3.479) Remain 10:00:36 loss: 0.2705 Lr: 0.03962
[2023-08-08 00:02:33,766 INFO misc.py line 115 22900] Train: [34/100][96/156] Data 0.001 (0.001) Batch 3.758 (3.482) Remain 10:01:04 loss: 0.5423 Lr: 0.03961
[2023-08-08 00:02:37,806 INFO misc.py line 115 22900] Train: [34/100][97/156] Data 0.001 (0.001) Batch 4.040 (3.488) Remain 10:02:02 loss: 0.3573 Lr: 0.03961
[2023-08-08 00:02:40,337 INFO misc.py line 115 22900] Train: [34/100][98/156] Data 0.001 (0.001) Batch 2.531 (3.478) Remain 10:00:14 loss: 0.2965 Lr: 0.03960
[2023-08-08 00:02:43,414 INFO misc.py line 115 22900] Train: [34/100][99/156] Data 0.002 (0.001) Batch 3.077 (3.474) Remain 09:59:27 loss: 0.3887 Lr: 0.03960
[2023-08-08 00:02:47,428 INFO misc.py line 115 22900] Train: [34/100][100/156] Data 0.001 (0.001) Batch 4.014 (3.480) Remain 10:00:22 loss: 0.3821 Lr: 0.03960
[2023-08-08 00:02:50,364 INFO misc.py line 115 22900] Train: [34/100][101/156] Data 0.001 (0.001) Batch 2.936 (3.474) Remain 09:59:21 loss: 0.2904 Lr: 0.03959
[2023-08-08 00:02:53,944 INFO misc.py line 115 22900] Train: [34/100][102/156] Data 0.001 (0.001) Batch 3.579 (3.475) Remain 09:59:28 loss: 0.2647 Lr: 0.03959
[2023-08-08 00:02:58,444 INFO misc.py line 115 22900] Train: [34/100][103/156] Data 0.001 (0.001) Batch 4.500 (3.485) Remain 10:01:11 loss: 0.6495 Lr: 0.03958
[2023-08-08 00:03:02,479 INFO misc.py line 115 22900] Train: [34/100][104/156] Data 0.001 (0.001) Batch 4.035 (3.491) Remain 10:02:04 loss: 0.4950 Lr: 0.03958
[2023-08-08 00:03:06,022 INFO misc.py line 115 22900] Train: [34/100][105/156] Data 0.001 (0.001) Batch 3.543 (3.491) Remain 10:02:05 loss: 0.3536 Lr: 0.03957
[2023-08-08 00:03:09,638 INFO misc.py line 115 22900] Train: [34/100][106/156] Data 0.001 (0.001) Batch 3.616 (3.493) Remain 10:02:14 loss: 0.3367 Lr: 0.03957
[2023-08-08 00:03:11,991 INFO misc.py line 115 22900] Train: [34/100][107/156] Data 0.001 (0.001) Batch 2.353 (3.482) Remain 10:00:18 loss: 0.2063 Lr: 0.03957
[2023-08-08 00:03:15,838 INFO misc.py line 115 22900] Train: [34/100][108/156] Data 0.001 (0.001) Batch 3.846 (3.485) Remain 10:00:50 loss: 0.4216 Lr: 0.03956
[2023-08-08 00:03:19,867 INFO misc.py line 115 22900] Train: [34/100][109/156] Data 0.001 (0.001) Batch 4.029 (3.490) Remain 10:01:40 loss: 0.2844 Lr: 0.03956
[2023-08-08 00:03:23,247 INFO misc.py line 115 22900] Train: [34/100][110/156] Data 0.001 (0.001) Batch 3.380 (3.489) Remain 10:01:25 loss: 0.3665 Lr: 0.03955
[2023-08-08 00:03:26,642 INFO misc.py line 115 22900] Train: [34/100][111/156] Data 0.001 (0.001) Batch 3.395 (3.488) Remain 10:01:13 loss: 0.4404 Lr: 0.03955
[2023-08-08 00:03:28,719 INFO misc.py line 115 22900] Train: [34/100][112/156] Data 0.001 (0.001) Batch 2.077 (3.475) Remain 09:58:56 loss: 0.2382 Lr: 0.03954
[2023-08-08 00:03:31,945 INFO misc.py line 115 22900] Train: [34/100][113/156] Data 0.001 (0.001) Batch 3.226 (3.473) Remain 09:58:29 loss: 0.3640 Lr: 0.03954
[2023-08-08 00:03:35,311 INFO misc.py line 115 22900] Train: [34/100][114/156] Data 0.001 (0.001) Batch 3.366 (3.472) Remain 09:58:15 loss: 0.3616 Lr: 0.03954
[2023-08-08 00:03:37,702 INFO misc.py line 115 22900] Train: [34/100][115/156] Data 0.001 (0.001) Batch 2.391 (3.463) Remain 09:56:32 loss: 0.2844 Lr: 0.03953
[2023-08-08 00:03:41,419 INFO misc.py line 115 22900] Train: [34/100][116/156] Data 0.001 (0.001) Batch 3.717 (3.465) Remain 09:56:52 loss: 0.3628 Lr: 0.03953
[2023-08-08 00:03:45,771 INFO misc.py line 115 22900] Train: [34/100][117/156] Data 0.001 (0.001) Batch 4.352 (3.473) Remain 09:58:09 loss: 0.5456 Lr: 0.03952
[2023-08-08 00:03:49,140 INFO misc.py line 115 22900] Train: [34/100][118/156] Data 0.001 (0.001) Batch 3.369 (3.472) Remain 09:57:56 loss: 0.2709 Lr: 0.03952
[2023-08-08 00:03:53,150 INFO misc.py line 115 22900] Train: [34/100][119/156] Data 0.001 (0.001) Batch 4.010 (3.476) Remain 09:58:40 loss: 0.7008 Lr: 0.03951
[2023-08-08 00:03:57,147 INFO misc.py line 115 22900] Train: [34/100][120/156] Data 0.001 (0.001) Batch 3.997 (3.481) Remain 09:59:23 loss: 0.6255 Lr: 0.03951
[2023-08-08 00:04:00,517 INFO misc.py line 115 22900] Train: [34/100][121/156] Data 0.001 (0.001) Batch 3.370 (3.480) Remain 09:59:10 loss: 0.4053 Lr: 0.03951
[2023-08-08 00:04:04,552 INFO misc.py line 115 22900] Train: [34/100][122/156] Data 0.001 (0.001) Batch 4.036 (3.485) Remain 09:59:55 loss: 0.4336 Lr: 0.03950
[2023-08-08 00:04:08,302 INFO misc.py line 115 22900] Train: [34/100][123/156] Data 0.001 (0.001) Batch 3.750 (3.487) Remain 10:00:14 loss: 0.4075 Lr: 0.03950
[2023-08-08 00:04:11,926 INFO misc.py line 115 22900] Train: [34/100][124/156] Data 0.001 (0.001) Batch 3.624 (3.488) Remain 10:00:22 loss: 0.3144 Lr: 0.03949
[2023-08-08 00:04:14,302 INFO misc.py line 115 22900] Train: [34/100][125/156] Data 0.001 (0.001) Batch 2.376 (3.479) Remain 09:58:44 loss: 0.1903 Lr: 0.03949
[2023-08-08 00:04:17,452 INFO misc.py line 115 22900] Train: [34/100][126/156] Data 0.001 (0.001) Batch 3.150 (3.476) Remain 09:58:13 loss: 0.3309 Lr: 0.03948
[2023-08-08 00:04:20,735 INFO misc.py line 115 22900] Train: [34/100][127/156] Data 0.001 (0.001) Batch 3.283 (3.475) Remain 09:57:54 loss: 0.3100 Lr: 0.03948
[2023-08-08 00:04:24,663 INFO misc.py line 115 22900] Train: [34/100][128/156] Data 0.001 (0.001) Batch 3.928 (3.478) Remain 09:58:28 loss: 0.3609 Lr: 0.03948
[2023-08-08 00:04:28,190 INFO misc.py line 115 22900] Train: [34/100][129/156] Data 0.001 (0.001) Batch 3.527 (3.479) Remain 09:58:28 loss: 0.4337 Lr: 0.03947
[2023-08-08 00:04:32,106 INFO misc.py line 115 22900] Train: [34/100][130/156] Data 0.001 (0.001) Batch 3.916 (3.482) Remain 09:59:00 loss: 0.3891 Lr: 0.03947
[2023-08-08 00:04:35,364 INFO misc.py line 115 22900] Train: [34/100][131/156] Data 0.001 (0.001) Batch 3.258 (3.480) Remain 09:58:39 loss: 0.2042 Lr: 0.03946
[2023-08-08 00:04:39,840 INFO misc.py line 115 22900] Train: [34/100][132/156] Data 0.001 (0.001) Batch 4.476 (3.488) Remain 09:59:55 loss: 0.5513 Lr: 0.03946
[2023-08-08 00:04:43,000 INFO misc.py line 115 22900] Train: [34/100][133/156] Data 0.001 (0.001) Batch 3.160 (3.485) Remain 09:59:26 loss: 0.4910 Lr: 0.03945
[2023-08-08 00:04:46,298 INFO misc.py line 115 22900] Train: [34/100][134/156] Data 0.001 (0.001) Batch 3.299 (3.484) Remain 09:59:07 loss: 0.3755 Lr: 0.03945
[2023-08-08 00:04:48,726 INFO misc.py line 115 22900] Train: [34/100][135/156] Data 0.001 (0.001) Batch 2.427 (3.476) Remain 09:57:41 loss: 0.3674 Lr: 0.03944
[2023-08-08 00:04:51,634 INFO misc.py line 115 22900] Train: [34/100][136/156] Data 0.001 (0.001) Batch 2.908 (3.472) Remain 09:56:54 loss: 0.3888 Lr: 0.03944
[2023-08-08 00:04:54,578 INFO misc.py line 115 22900] Train: [34/100][137/156] Data 0.001 (0.001) Batch 2.944 (3.468) Remain 09:56:10 loss: 0.2938 Lr: 0.03944
[2023-08-08 00:04:58,425 INFO misc.py line 115 22900] Train: [34/100][138/156] Data 0.001 (0.001) Batch 3.847 (3.471) Remain 09:56:35 loss: 0.2175 Lr: 0.03943
[2023-08-08 00:05:01,596 INFO misc.py line 115 22900] Train: [34/100][139/156] Data 0.001 (0.001) Batch 3.171 (3.468) Remain 09:56:09 loss: 0.3164 Lr: 0.03943
[2023-08-08 00:05:04,211 INFO misc.py line 115 22900] Train: [34/100][140/156] Data 0.001 (0.001) Batch 2.615 (3.462) Remain 09:55:01 loss: 0.3398 Lr: 0.03942
[2023-08-08 00:05:07,677 INFO misc.py line 115 22900] Train: [34/100][141/156] Data 0.001 (0.001) Batch 3.466 (3.462) Remain 09:54:58 loss: 0.5118 Lr: 0.03942
[2023-08-08 00:05:10,543 INFO misc.py line 115 22900] Train: [34/100][142/156] Data 0.001 (0.001) Batch 2.865 (3.458) Remain 09:54:10 loss: 0.2951 Lr: 0.03941
[2023-08-08 00:05:13,776 INFO misc.py line 115 22900] Train: [34/100][143/156] Data 0.001 (0.001) Batch 3.233 (3.456) Remain 09:53:50 loss: 0.3963 Lr: 0.03941
[2023-08-08 00:05:16,765 INFO misc.py line 115 22900] Train: [34/100][144/156] Data 0.001 (0.001) Batch 2.989 (3.453) Remain 09:53:13 loss: 0.3797 Lr: 0.03941
[2023-08-08 00:05:20,478 INFO misc.py line 115 22900] Train: [34/100][145/156] Data 0.001 (0.001) Batch 3.713 (3.455) Remain 09:53:28 loss: 0.4245 Lr: 0.03940
[2023-08-08 00:05:24,186 INFO misc.py line 115 22900] Train: [34/100][146/156] Data 0.001 (0.001) Batch 3.708 (3.457) Remain 09:53:43 loss: 0.2428 Lr: 0.03940
[2023-08-08 00:05:27,675 INFO misc.py line 115 22900] Train: [34/100][147/156] Data 0.001 (0.001) Batch 3.489 (3.457) Remain 09:53:42 loss: 0.2420 Lr: 0.03939
[2023-08-08 00:05:31,321 INFO misc.py line 115 22900] Train: [34/100][148/156] Data 0.001 (0.001) Batch 3.646 (3.458) Remain 09:53:52 loss: 0.4568 Lr: 0.03939
[2023-08-08 00:05:34,248 INFO misc.py line 115 22900] Train: [34/100][149/156] Data 0.001 (0.001) Batch 2.927 (3.454) Remain 09:53:11 loss: 0.2054 Lr: 0.03938
[2023-08-08 00:05:36,981 INFO misc.py line 115 22900] Train: [34/100][150/156] Data 0.001 (0.001) Batch 2.734 (3.450) Remain 09:52:17 loss: 0.2303 Lr: 0.03938
[2023-08-08 00:05:41,139 INFO misc.py line 115 22900] Train: [34/100][151/156] Data 0.001 (0.001) Batch 4.158 (3.454) Remain 09:53:03 loss: 0.7331 Lr: 0.03938
[2023-08-08 00:05:44,822 INFO misc.py line 115 22900] Train: [34/100][152/156] Data 0.001 (0.001) Batch 3.683 (3.456) Remain 09:53:15 loss: 0.4185 Lr: 0.03937
[2023-08-08 00:05:48,397 INFO misc.py line 115 22900] Train: [34/100][153/156] Data 0.001 (0.001) Batch 3.575 (3.457) Remain 09:53:20 loss: 0.5846 Lr: 0.03937
[2023-08-08 00:05:52,432 INFO misc.py line 115 22900] Train: [34/100][154/156] Data 0.001 (0.001) Batch 4.035 (3.461) Remain 09:53:56 loss: 0.3957 Lr: 0.03936
[2023-08-08 00:05:56,863 INFO misc.py line 115 22900] Train: [34/100][155/156] Data 0.001 (0.001) Batch 4.431 (3.467) Remain 09:54:58 loss: 0.3565 Lr: 0.03936
[2023-08-08 00:05:58,897 INFO misc.py line 115 22900] Train: [34/100][156/156] Data 0.001 (0.001) Batch 2.033 (3.458) Remain 09:53:18 loss: 0.1934 Lr: 0.03935
[2023-08-08 00:05:58,897 INFO misc.py line 129 22900] Train result: loss: 0.3728 
[2023-08-08 00:05:58,897 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 00:06:01,024 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.0992 
[2023-08-08 00:06:01,892 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.5487 
[2023-08-08 00:06:03,555 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.8625 
[2023-08-08 00:06:05,078 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.6427 
[2023-08-08 00:06:06,922 INFO evaluator.py line 122 22900] Test: [5/24] Loss 2.0603 
[2023-08-08 00:06:08,584 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.8109 
[2023-08-08 00:06:10,726 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.8273 
[2023-08-08 00:06:12,530 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.2119 
[2023-08-08 00:06:13,814 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.7889 
[2023-08-08 00:06:15,943 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3868 
[2023-08-08 00:06:16,468 INFO evaluator.py line 122 22900] Test: [11/24] Loss 2.0606 
[2023-08-08 00:06:18,000 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.8136 
[2023-08-08 00:06:20,712 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.1903 
[2023-08-08 00:06:22,392 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.7957 
[2023-08-08 00:06:24,414 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.4755 
[2023-08-08 00:06:27,125 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.2956 
[2023-08-08 00:06:29,831 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.4570 
[2023-08-08 00:06:31,679 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.5616 
[2023-08-08 00:06:32,430 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.1602 
[2023-08-08 00:06:33,314 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.8353 
[2023-08-08 00:06:35,575 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.4926 
[2023-08-08 00:06:37,540 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.6747 
[2023-08-08 00:06:39,387 INFO evaluator.py line 122 22900] Test: [23/24] Loss 3.0655 
[2023-08-08 00:06:41,322 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.8554 
[2023-08-08 00:06:41,370 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2197/0.3179/0.6651.
[2023-08-08 00:06:41,370 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6170/0.9012
[2023-08-08 00:06:41,370 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9440/0.9898
[2023-08-08 00:06:41,371 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1557/0.4446
[2023-08-08 00:06:41,371 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.0501/0.0552
[2023-08-08 00:06:41,371 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6178/0.7072
[2023-08-08 00:06:41,371 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.2031/0.2371
[2023-08-08 00:06:41,371 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5507/0.7093
[2023-08-08 00:06:41,371 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1142/0.1286
[2023-08-08 00:06:41,371 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1218/0.5824
[2023-08-08 00:06:41,371 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0209/0.0211
[2023-08-08 00:06:41,371 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 00:06:41,371 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0018/0.0019
[2023-08-08 00:06:41,371 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.2528/0.4635
[2023-08-08 00:06:41,371 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0295/0.0325
[2023-08-08 00:06:41,371 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0431/0.0516
[2023-08-08 00:06:41,371 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.2058/0.2222
[2023-08-08 00:06:41,371 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.2122/0.2364
[2023-08-08 00:06:41,371 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.1799/0.4586
[2023-08-08 00:06:41,371 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 00:06:41,371 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0729/0.1158
[2023-08-08 00:06:41,371 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 00:06:41,371 INFO misc.py line 152 22900] Currently Best mIoU: 0.2356
[2023-08-08 00:06:41,371 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 00:06:47,827 INFO misc.py line 115 22900] Train: [35/100][1/156] Data 1.907 (1.907) Batch 5.678 (5.678) Remain 16:14:10 loss: 0.3666 Lr: 0.03935
[2023-08-08 00:06:50,531 INFO misc.py line 115 22900] Train: [35/100][2/156] Data 0.001 (0.001) Batch 2.704 (2.704) Remain 07:43:52 loss: 0.1715 Lr: 0.03935
[2023-08-08 00:06:53,865 INFO misc.py line 115 22900] Train: [35/100][3/156] Data 0.001 (0.001) Batch 3.334 (3.334) Remain 09:31:56 loss: 0.5087 Lr: 0.03934
[2023-08-08 00:06:58,344 INFO misc.py line 115 22900] Train: [35/100][4/156] Data 0.001 (0.001) Batch 4.479 (4.479) Remain 12:48:17 loss: 0.5526 Lr: 0.03934
[2023-08-08 00:07:02,383 INFO misc.py line 115 22900] Train: [35/100][5/156] Data 0.001 (0.001) Batch 4.039 (4.259) Remain 12:10:28 loss: 0.4482 Lr: 0.03933
[2023-08-08 00:07:05,937 INFO misc.py line 115 22900] Train: [35/100][6/156] Data 0.001 (0.001) Batch 3.554 (4.024) Remain 11:30:05 loss: 0.2376 Lr: 0.03933
[2023-08-08 00:07:09,697 INFO misc.py line 115 22900] Train: [35/100][7/156] Data 0.001 (0.001) Batch 3.760 (3.958) Remain 11:18:44 loss: 0.3544 Lr: 0.03932
[2023-08-08 00:07:12,984 INFO misc.py line 115 22900] Train: [35/100][8/156] Data 0.001 (0.001) Batch 3.286 (3.824) Remain 10:55:37 loss: 0.6022 Lr: 0.03932
[2023-08-08 00:07:15,591 INFO misc.py line 115 22900] Train: [35/100][9/156] Data 0.001 (0.001) Batch 2.607 (3.621) Remain 10:20:48 loss: 0.1834 Lr: 0.03931
[2023-08-08 00:07:19,014 INFO misc.py line 115 22900] Train: [35/100][10/156] Data 0.001 (0.001) Batch 3.423 (3.593) Remain 10:15:54 loss: 0.3499 Lr: 0.03931
[2023-08-08 00:07:21,848 INFO misc.py line 115 22900] Train: [35/100][11/156] Data 0.001 (0.001) Batch 2.834 (3.498) Remain 09:59:35 loss: 0.2334 Lr: 0.03931
[2023-08-08 00:07:24,992 INFO misc.py line 115 22900] Train: [35/100][12/156] Data 0.001 (0.001) Batch 3.144 (3.459) Remain 09:52:47 loss: 0.6502 Lr: 0.03930
[2023-08-08 00:07:27,816 INFO misc.py line 115 22900] Train: [35/100][13/156] Data 0.001 (0.001) Batch 2.824 (3.395) Remain 09:41:52 loss: 0.4170 Lr: 0.03930
[2023-08-08 00:07:31,347 INFO misc.py line 115 22900] Train: [35/100][14/156] Data 0.001 (0.001) Batch 3.531 (3.407) Remain 09:43:55 loss: 0.2616 Lr: 0.03929
[2023-08-08 00:07:35,472 INFO misc.py line 115 22900] Train: [35/100][15/156] Data 0.001 (0.001) Batch 4.124 (3.467) Remain 09:54:06 loss: 0.3242 Lr: 0.03929
[2023-08-08 00:07:38,971 INFO misc.py line 115 22900] Train: [35/100][16/156] Data 0.001 (0.001) Batch 3.500 (3.470) Remain 09:54:28 loss: 0.3389 Lr: 0.03928
[2023-08-08 00:07:41,145 INFO misc.py line 115 22900] Train: [35/100][17/156] Data 0.001 (0.001) Batch 2.173 (3.377) Remain 09:38:33 loss: 0.1348 Lr: 0.03928
[2023-08-08 00:07:45,019 INFO misc.py line 115 22900] Train: [35/100][18/156] Data 0.001 (0.001) Batch 3.874 (3.410) Remain 09:44:10 loss: 0.2851 Lr: 0.03928
[2023-08-08 00:07:48,711 INFO misc.py line 115 22900] Train: [35/100][19/156] Data 0.001 (0.001) Batch 3.692 (3.428) Remain 09:47:08 loss: 0.4868 Lr: 0.03927
[2023-08-08 00:07:51,904 INFO misc.py line 115 22900] Train: [35/100][20/156] Data 0.001 (0.001) Batch 3.193 (3.414) Remain 09:44:42 loss: 0.2003 Lr: 0.03927
[2023-08-08 00:07:55,683 INFO misc.py line 115 22900] Train: [35/100][21/156] Data 0.001 (0.001) Batch 3.778 (3.434) Remain 09:48:07 loss: 0.3726 Lr: 0.03926
[2023-08-08 00:07:59,555 INFO misc.py line 115 22900] Train: [35/100][22/156] Data 0.001 (0.001) Batch 3.873 (3.457) Remain 09:52:01 loss: 0.5235 Lr: 0.03926
[2023-08-08 00:08:02,821 INFO misc.py line 115 22900] Train: [35/100][23/156] Data 0.001 (0.001) Batch 3.266 (3.448) Remain 09:50:19 loss: 0.4310 Lr: 0.03925
[2023-08-08 00:08:05,356 INFO misc.py line 115 22900] Train: [35/100][24/156] Data 0.001 (0.001) Batch 2.535 (3.404) Remain 09:42:49 loss: 0.1516 Lr: 0.03925
[2023-08-08 00:08:09,036 INFO misc.py line 115 22900] Train: [35/100][25/156] Data 0.001 (0.001) Batch 3.680 (3.417) Remain 09:44:54 loss: 0.2482 Lr: 0.03925
[2023-08-08 00:08:12,775 INFO misc.py line 115 22900] Train: [35/100][26/156] Data 0.001 (0.001) Batch 3.739 (3.431) Remain 09:47:14 loss: 0.3799 Lr: 0.03924
[2023-08-08 00:08:15,270 INFO misc.py line 115 22900] Train: [35/100][27/156] Data 0.001 (0.001) Batch 2.495 (3.392) Remain 09:40:31 loss: 0.1656 Lr: 0.03924
[2023-08-08 00:08:19,454 INFO misc.py line 115 22900] Train: [35/100][28/156] Data 0.001 (0.001) Batch 4.184 (3.424) Remain 09:45:53 loss: 0.3572 Lr: 0.03923
[2023-08-08 00:08:21,966 INFO misc.py line 115 22900] Train: [35/100][29/156] Data 0.001 (0.001) Batch 2.512 (3.388) Remain 09:39:49 loss: 0.2159 Lr: 0.03923
[2023-08-08 00:08:24,987 INFO misc.py line 115 22900] Train: [35/100][30/156] Data 0.001 (0.001) Batch 3.020 (3.375) Remain 09:37:26 loss: 0.3290 Lr: 0.03922
[2023-08-08 00:08:28,273 INFO misc.py line 115 22900] Train: [35/100][31/156] Data 0.001 (0.001) Batch 3.286 (3.372) Remain 09:36:50 loss: 0.4834 Lr: 0.03922
[2023-08-08 00:08:31,011 INFO misc.py line 115 22900] Train: [35/100][32/156] Data 0.001 (0.001) Batch 2.738 (3.350) Remain 09:33:03 loss: 0.4267 Lr: 0.03921
[2023-08-08 00:08:34,404 INFO misc.py line 115 22900] Train: [35/100][33/156] Data 0.001 (0.001) Batch 3.393 (3.351) Remain 09:33:14 loss: 0.2839 Lr: 0.03921
[2023-08-08 00:08:37,604 INFO misc.py line 115 22900] Train: [35/100][34/156] Data 0.001 (0.001) Batch 3.200 (3.346) Remain 09:32:20 loss: 0.2975 Lr: 0.03921
[2023-08-08 00:08:41,198 INFO misc.py line 115 22900] Train: [35/100][35/156] Data 0.001 (0.001) Batch 3.594 (3.354) Remain 09:33:36 loss: 0.3052 Lr: 0.03920
[2023-08-08 00:08:44,361 INFO misc.py line 115 22900] Train: [35/100][36/156] Data 0.001 (0.001) Batch 3.163 (3.348) Remain 09:32:33 loss: 0.1908 Lr: 0.03920
[2023-08-08 00:08:48,903 INFO misc.py line 115 22900] Train: [35/100][37/156] Data 0.001 (0.001) Batch 4.542 (3.383) Remain 09:38:30 loss: 0.4643 Lr: 0.03919
[2023-08-08 00:08:52,910 INFO misc.py line 115 22900] Train: [35/100][38/156] Data 0.001 (0.001) Batch 4.007 (3.401) Remain 09:41:30 loss: 0.3160 Lr: 0.03919
[2023-08-08 00:08:55,755 INFO misc.py line 115 22900] Train: [35/100][39/156] Data 0.001 (0.001) Batch 2.845 (3.386) Remain 09:38:48 loss: 0.2420 Lr: 0.03918
[2023-08-08 00:08:59,009 INFO misc.py line 115 22900] Train: [35/100][40/156] Data 0.001 (0.001) Batch 3.254 (3.382) Remain 09:38:08 loss: 0.3755 Lr: 0.03918
[2023-08-08 00:09:01,725 INFO misc.py line 115 22900] Train: [35/100][41/156] Data 0.001 (0.001) Batch 2.717 (3.365) Remain 09:35:05 loss: 0.2497 Lr: 0.03918
[2023-08-08 00:09:05,276 INFO misc.py line 115 22900] Train: [35/100][42/156] Data 0.001 (0.001) Batch 3.551 (3.370) Remain 09:35:50 loss: 0.5499 Lr: 0.03917
[2023-08-08 00:09:08,245 INFO misc.py line 115 22900] Train: [35/100][43/156] Data 0.001 (0.001) Batch 2.968 (3.359) Remain 09:34:04 loss: 0.2536 Lr: 0.03917
[2023-08-08 00:09:11,389 INFO misc.py line 115 22900] Train: [35/100][44/156] Data 0.001 (0.001) Batch 3.144 (3.354) Remain 09:33:07 loss: 0.1259 Lr: 0.03916
[2023-08-08 00:09:15,438 INFO misc.py line 115 22900] Train: [35/100][45/156] Data 0.001 (0.001) Batch 4.049 (3.371) Remain 09:35:53 loss: 0.3090 Lr: 0.03916
[2023-08-08 00:09:19,933 INFO misc.py line 115 22900] Train: [35/100][46/156] Data 0.001 (0.001) Batch 4.495 (3.397) Remain 09:40:18 loss: 0.6678 Lr: 0.03915
[2023-08-08 00:09:23,610 INFO misc.py line 115 22900] Train: [35/100][47/156] Data 0.001 (0.001) Batch 3.677 (3.403) Remain 09:41:20 loss: 0.3434 Lr: 0.03915
[2023-08-08 00:09:26,735 INFO misc.py line 115 22900] Train: [35/100][48/156] Data 0.001 (0.001) Batch 3.125 (3.397) Remain 09:40:13 loss: 0.2562 Lr: 0.03914
[2023-08-08 00:09:30,576 INFO misc.py line 115 22900] Train: [35/100][49/156] Data 0.001 (0.001) Batch 3.841 (3.407) Remain 09:41:49 loss: 0.3002 Lr: 0.03914
[2023-08-08 00:09:34,754 INFO misc.py line 115 22900] Train: [35/100][50/156] Data 0.001 (0.001) Batch 4.178 (3.423) Remain 09:44:33 loss: 0.6321 Lr: 0.03914
[2023-08-08 00:09:38,047 INFO misc.py line 115 22900] Train: [35/100][51/156] Data 0.001 (0.001) Batch 3.293 (3.420) Remain 09:44:02 loss: 0.3043 Lr: 0.03913
[2023-08-08 00:09:42,078 INFO misc.py line 115 22900] Train: [35/100][52/156] Data 0.001 (0.001) Batch 4.031 (3.433) Remain 09:46:06 loss: 0.5216 Lr: 0.03913
[2023-08-08 00:09:45,449 INFO misc.py line 115 22900] Train: [35/100][53/156] Data 0.001 (0.001) Batch 3.372 (3.432) Remain 09:45:50 loss: 0.1969 Lr: 0.03912
[2023-08-08 00:09:49,408 INFO misc.py line 115 22900] Train: [35/100][54/156] Data 0.001 (0.001) Batch 3.959 (3.442) Remain 09:47:33 loss: 0.5549 Lr: 0.03912
[2023-08-08 00:09:52,996 INFO misc.py line 115 22900] Train: [35/100][55/156] Data 0.001 (0.001) Batch 3.588 (3.445) Remain 09:47:58 loss: 0.1951 Lr: 0.03911
[2023-08-08 00:09:55,282 INFO misc.py line 115 22900] Train: [35/100][56/156] Data 0.001 (0.001) Batch 2.286 (3.423) Remain 09:44:11 loss: 0.1981 Lr: 0.03911
[2023-08-08 00:09:58,590 INFO misc.py line 115 22900] Train: [35/100][57/156] Data 0.001 (0.001) Batch 3.308 (3.421) Remain 09:43:45 loss: 0.3347 Lr: 0.03911
[2023-08-08 00:10:02,718 INFO misc.py line 115 22900] Train: [35/100][58/156] Data 0.001 (0.001) Batch 4.128 (3.434) Remain 09:45:54 loss: 0.2968 Lr: 0.03910
[2023-08-08 00:10:06,495 INFO misc.py line 115 22900] Train: [35/100][59/156] Data 0.001 (0.001) Batch 3.777 (3.440) Remain 09:46:53 loss: 0.3973 Lr: 0.03910
[2023-08-08 00:10:09,845 INFO misc.py line 115 22900] Train: [35/100][60/156] Data 0.001 (0.001) Batch 3.350 (3.438) Remain 09:46:33 loss: 0.2618 Lr: 0.03909
[2023-08-08 00:10:13,149 INFO misc.py line 115 22900] Train: [35/100][61/156] Data 0.001 (0.001) Batch 3.304 (3.436) Remain 09:46:06 loss: 0.5772 Lr: 0.03909
[2023-08-08 00:10:17,138 INFO misc.py line 115 22900] Train: [35/100][62/156] Data 0.001 (0.001) Batch 3.989 (3.445) Remain 09:47:39 loss: 0.3635 Lr: 0.03908
[2023-08-08 00:10:21,028 INFO misc.py line 115 22900] Train: [35/100][63/156] Data 0.001 (0.001) Batch 3.889 (3.453) Remain 09:48:51 loss: 0.4357 Lr: 0.03908
[2023-08-08 00:10:25,038 INFO misc.py line 115 22900] Train: [35/100][64/156] Data 0.001 (0.001) Batch 4.011 (3.462) Remain 09:50:21 loss: 0.3439 Lr: 0.03907
[2023-08-08 00:10:28,778 INFO misc.py line 115 22900] Train: [35/100][65/156] Data 0.001 (0.001) Batch 3.740 (3.466) Remain 09:51:04 loss: 0.3135 Lr: 0.03907
[2023-08-08 00:10:31,787 INFO misc.py line 115 22900] Train: [35/100][66/156] Data 0.001 (0.001) Batch 3.009 (3.459) Remain 09:49:46 loss: 0.1988 Lr: 0.03907
[2023-08-08 00:10:35,037 INFO misc.py line 115 22900] Train: [35/100][67/156] Data 0.001 (0.001) Batch 3.250 (3.456) Remain 09:49:09 loss: 0.2057 Lr: 0.03906
[2023-08-08 00:10:37,905 INFO misc.py line 115 22900] Train: [35/100][68/156] Data 0.001 (0.001) Batch 2.868 (3.447) Remain 09:47:33 loss: 0.4037 Lr: 0.03906
[2023-08-08 00:10:40,977 INFO misc.py line 115 22900] Train: [35/100][69/156] Data 0.001 (0.001) Batch 3.072 (3.441) Remain 09:46:31 loss: 0.3663 Lr: 0.03905
[2023-08-08 00:10:43,801 INFO misc.py line 115 22900] Train: [35/100][70/156] Data 0.001 (0.001) Batch 2.825 (3.432) Remain 09:44:54 loss: 0.2544 Lr: 0.03905
[2023-08-08 00:10:47,419 INFO misc.py line 115 22900] Train: [35/100][71/156] Data 0.001 (0.001) Batch 3.618 (3.435) Remain 09:45:18 loss: 0.2769 Lr: 0.03904
[2023-08-08 00:10:50,332 INFO misc.py line 115 22900] Train: [35/100][72/156] Data 0.001 (0.001) Batch 2.913 (3.427) Remain 09:43:58 loss: 0.1631 Lr: 0.03904
[2023-08-08 00:10:53,643 INFO misc.py line 115 22900] Train: [35/100][73/156] Data 0.001 (0.001) Batch 3.310 (3.425) Remain 09:43:37 loss: 0.2254 Lr: 0.03904
[2023-08-08 00:10:57,408 INFO misc.py line 115 22900] Train: [35/100][74/156] Data 0.001 (0.001) Batch 3.765 (3.430) Remain 09:44:23 loss: 0.3212 Lr: 0.03903
[2023-08-08 00:11:00,845 INFO misc.py line 115 22900] Train: [35/100][75/156] Data 0.001 (0.001) Batch 3.438 (3.430) Remain 09:44:20 loss: 0.3772 Lr: 0.03903
[2023-08-08 00:11:03,564 INFO misc.py line 115 22900] Train: [35/100][76/156] Data 0.001 (0.001) Batch 2.719 (3.421) Remain 09:42:37 loss: 0.6921 Lr: 0.03902
[2023-08-08 00:11:07,016 INFO misc.py line 115 22900] Train: [35/100][77/156] Data 0.001 (0.001) Batch 3.451 (3.421) Remain 09:42:38 loss: 0.4661 Lr: 0.03902
[2023-08-08 00:11:10,672 INFO misc.py line 115 22900] Train: [35/100][78/156] Data 0.001 (0.001) Batch 3.657 (3.424) Remain 09:43:07 loss: 0.4275 Lr: 0.03901
[2023-08-08 00:11:14,626 INFO misc.py line 115 22900] Train: [35/100][79/156] Data 0.001 (0.001) Batch 3.954 (3.431) Remain 09:44:15 loss: 0.9750 Lr: 0.03901
[2023-08-08 00:11:19,523 INFO misc.py line 115 22900] Train: [35/100][80/156] Data 0.001 (0.001) Batch 4.897 (3.450) Remain 09:47:26 loss: 0.7608 Lr: 0.03900
[2023-08-08 00:11:23,257 INFO misc.py line 115 22900] Train: [35/100][81/156] Data 0.001 (0.001) Batch 3.734 (3.454) Remain 09:47:59 loss: 0.4295 Lr: 0.03900
[2023-08-08 00:11:27,317 INFO misc.py line 115 22900] Train: [35/100][82/156] Data 0.001 (0.001) Batch 4.060 (3.461) Remain 09:49:14 loss: 0.3977 Lr: 0.03900
[2023-08-08 00:11:29,069 INFO misc.py line 115 22900] Train: [35/100][83/156] Data 0.001 (0.001) Batch 1.752 (3.440) Remain 09:45:33 loss: 0.1853 Lr: 0.03899
[2023-08-08 00:11:32,097 INFO misc.py line 115 22900] Train: [35/100][84/156] Data 0.001 (0.001) Batch 3.027 (3.435) Remain 09:44:37 loss: 0.3873 Lr: 0.03899
[2023-08-08 00:11:36,122 INFO misc.py line 115 22900] Train: [35/100][85/156] Data 0.001 (0.001) Batch 4.025 (3.442) Remain 09:45:47 loss: 0.5655 Lr: 0.03898
[2023-08-08 00:11:40,427 INFO misc.py line 115 22900] Train: [35/100][86/156] Data 0.001 (0.001) Batch 4.305 (3.453) Remain 09:47:30 loss: 0.4182 Lr: 0.03898
[2023-08-08 00:11:44,147 INFO misc.py line 115 22900] Train: [35/100][87/156] Data 0.001 (0.001) Batch 3.720 (3.456) Remain 09:47:59 loss: 0.4208 Lr: 0.03897
[2023-08-08 00:11:48,177 INFO misc.py line 115 22900] Train: [35/100][88/156] Data 0.001 (0.001) Batch 4.031 (3.462) Remain 09:49:05 loss: 0.2987 Lr: 0.03897
[2023-08-08 00:11:51,691 INFO misc.py line 115 22900] Train: [35/100][89/156] Data 0.001 (0.001) Batch 3.514 (3.463) Remain 09:49:07 loss: 0.3481 Lr: 0.03897
[2023-08-08 00:11:54,817 INFO misc.py line 115 22900] Train: [35/100][90/156] Data 0.001 (0.001) Batch 3.126 (3.459) Remain 09:48:24 loss: 0.3472 Lr: 0.03896
[2023-08-08 00:11:57,688 INFO misc.py line 115 22900] Train: [35/100][91/156] Data 0.001 (0.001) Batch 2.871 (3.453) Remain 09:47:13 loss: 0.2925 Lr: 0.03896
[2023-08-08 00:12:01,639 INFO misc.py line 115 22900] Train: [35/100][92/156] Data 0.001 (0.001) Batch 3.951 (3.458) Remain 09:48:06 loss: 0.6649 Lr: 0.03895
[2023-08-08 00:12:03,521 INFO misc.py line 115 22900] Train: [35/100][93/156] Data 0.001 (0.001) Batch 1.882 (3.441) Remain 09:45:04 loss: 0.3984 Lr: 0.03895
[2023-08-08 00:12:07,047 INFO misc.py line 115 22900] Train: [35/100][94/156] Data 0.001 (0.001) Batch 3.526 (3.442) Remain 09:45:10 loss: 0.3421 Lr: 0.03894
[2023-08-08 00:12:10,726 INFO misc.py line 115 22900] Train: [35/100][95/156] Data 0.001 (0.001) Batch 3.679 (3.444) Remain 09:45:33 loss: 0.2530 Lr: 0.03894
[2023-08-08 00:12:14,560 INFO misc.py line 115 22900] Train: [35/100][96/156] Data 0.001 (0.001) Batch 3.833 (3.448) Remain 09:46:12 loss: 0.5971 Lr: 0.03893
[2023-08-08 00:12:17,813 INFO misc.py line 115 22900] Train: [35/100][97/156] Data 0.001 (0.001) Batch 3.253 (3.446) Remain 09:45:48 loss: 0.3251 Lr: 0.03893
[2023-08-08 00:12:20,761 INFO misc.py line 115 22900] Train: [35/100][98/156] Data 0.001 (0.001) Batch 2.948 (3.441) Remain 09:44:51 loss: 0.1395 Lr: 0.03893
[2023-08-08 00:12:24,760 INFO misc.py line 115 22900] Train: [35/100][99/156] Data 0.001 (0.001) Batch 3.999 (3.447) Remain 09:45:47 loss: 0.3169 Lr: 0.03892
[2023-08-08 00:12:27,154 INFO misc.py line 115 22900] Train: [35/100][100/156] Data 0.001 (0.001) Batch 2.394 (3.436) Remain 09:43:53 loss: 0.2021 Lr: 0.03892
[2023-08-08 00:12:31,221 INFO misc.py line 115 22900] Train: [35/100][101/156] Data 0.001 (0.001) Batch 4.066 (3.442) Remain 09:44:55 loss: 0.4264 Lr: 0.03891
[2023-08-08 00:12:35,228 INFO misc.py line 115 22900] Train: [35/100][102/156] Data 0.001 (0.001) Batch 4.007 (3.448) Remain 09:45:50 loss: 0.5202 Lr: 0.03891
[2023-08-08 00:12:38,460 INFO misc.py line 115 22900] Train: [35/100][103/156] Data 0.001 (0.001) Batch 3.232 (3.446) Remain 09:45:24 loss: 0.3358 Lr: 0.03890
[2023-08-08 00:12:42,389 INFO misc.py line 115 22900] Train: [35/100][104/156] Data 0.001 (0.001) Batch 3.929 (3.451) Remain 09:46:09 loss: 0.4874 Lr: 0.03890
[2023-08-08 00:12:45,988 INFO misc.py line 115 22900] Train: [35/100][105/156] Data 0.001 (0.001) Batch 3.599 (3.452) Remain 09:46:21 loss: 0.2775 Lr: 0.03889
[2023-08-08 00:12:50,522 INFO misc.py line 115 22900] Train: [35/100][106/156] Data 0.001 (0.001) Batch 4.533 (3.463) Remain 09:48:04 loss: 0.4524 Lr: 0.03889
[2023-08-08 00:12:54,508 INFO misc.py line 115 22900] Train: [35/100][107/156] Data 0.001 (0.001) Batch 3.986 (3.468) Remain 09:48:52 loss: 0.3237 Lr: 0.03889
[2023-08-08 00:12:57,213 INFO misc.py line 115 22900] Train: [35/100][108/156] Data 0.001 (0.001) Batch 2.705 (3.460) Remain 09:47:35 loss: 0.1400 Lr: 0.03888
[2023-08-08 00:13:00,978 INFO misc.py line 115 22900] Train: [35/100][109/156] Data 0.001 (0.001) Batch 3.766 (3.463) Remain 09:48:00 loss: 0.3980 Lr: 0.03888
[2023-08-08 00:13:04,458 INFO misc.py line 115 22900] Train: [35/100][110/156] Data 0.001 (0.001) Batch 3.479 (3.463) Remain 09:47:59 loss: 0.2995 Lr: 0.03887
[2023-08-08 00:13:07,929 INFO misc.py line 115 22900] Train: [35/100][111/156] Data 0.001 (0.001) Batch 3.471 (3.464) Remain 09:47:56 loss: 0.6741 Lr: 0.03887
[2023-08-08 00:13:10,970 INFO misc.py line 115 22900] Train: [35/100][112/156] Data 0.001 (0.001) Batch 3.042 (3.460) Remain 09:47:13 loss: 0.3302 Lr: 0.03886
[2023-08-08 00:13:14,742 INFO misc.py line 115 22900] Train: [35/100][113/156] Data 0.001 (0.001) Batch 3.772 (3.463) Remain 09:47:38 loss: 0.2293 Lr: 0.03886
[2023-08-08 00:13:18,729 INFO misc.py line 115 22900] Train: [35/100][114/156] Data 0.001 (0.001) Batch 3.987 (3.467) Remain 09:48:23 loss: 0.2774 Lr: 0.03886
[2023-08-08 00:13:22,946 INFO misc.py line 115 22900] Train: [35/100][115/156] Data 0.001 (0.001) Batch 4.217 (3.474) Remain 09:49:28 loss: 0.5293 Lr: 0.03885
[2023-08-08 00:13:25,873 INFO misc.py line 115 22900] Train: [35/100][116/156] Data 0.001 (0.001) Batch 2.928 (3.469) Remain 09:48:35 loss: 0.2327 Lr: 0.03885
[2023-08-08 00:13:30,154 INFO misc.py line 115 22900] Train: [35/100][117/156] Data 0.001 (0.001) Batch 4.280 (3.476) Remain 09:49:44 loss: 0.5792 Lr: 0.03884
[2023-08-08 00:13:33,914 INFO misc.py line 115 22900] Train: [35/100][118/156] Data 0.001 (0.001) Batch 3.760 (3.479) Remain 09:50:06 loss: 0.3681 Lr: 0.03884
[2023-08-08 00:13:37,950 INFO misc.py line 115 22900] Train: [35/100][119/156] Data 0.001 (0.001) Batch 4.036 (3.483) Remain 09:50:51 loss: 0.4940 Lr: 0.03883
[2023-08-08 00:13:41,197 INFO misc.py line 115 22900] Train: [35/100][120/156] Data 0.001 (0.001) Batch 3.247 (3.481) Remain 09:50:27 loss: 0.2725 Lr: 0.03883
[2023-08-08 00:13:45,270 INFO misc.py line 115 22900] Train: [35/100][121/156] Data 0.001 (0.001) Batch 4.073 (3.486) Remain 09:51:14 loss: 0.3512 Lr: 0.03882
[2023-08-08 00:13:49,015 INFO misc.py line 115 22900] Train: [35/100][122/156] Data 0.001 (0.001) Batch 3.745 (3.489) Remain 09:51:33 loss: 0.4033 Lr: 0.03882
[2023-08-08 00:13:52,471 INFO misc.py line 115 22900] Train: [35/100][123/156] Data 0.001 (0.001) Batch 3.456 (3.488) Remain 09:51:27 loss: 0.4051 Lr: 0.03882
[2023-08-08 00:13:56,026 INFO misc.py line 115 22900] Train: [35/100][124/156] Data 0.001 (0.001) Batch 3.555 (3.489) Remain 09:51:29 loss: 0.3309 Lr: 0.03881
[2023-08-08 00:13:59,826 INFO misc.py line 115 22900] Train: [35/100][125/156] Data 0.001 (0.001) Batch 3.800 (3.491) Remain 09:51:51 loss: 0.3391 Lr: 0.03881
[2023-08-08 00:14:03,171 INFO misc.py line 115 22900] Train: [35/100][126/156] Data 0.001 (0.001) Batch 3.345 (3.490) Remain 09:51:36 loss: 0.3526 Lr: 0.03880
[2023-08-08 00:14:05,888 INFO misc.py line 115 22900] Train: [35/100][127/156] Data 0.001 (0.001) Batch 2.717 (3.484) Remain 09:50:29 loss: 0.2409 Lr: 0.03880
[2023-08-08 00:14:09,205 INFO misc.py line 115 22900] Train: [35/100][128/156] Data 0.001 (0.001) Batch 3.317 (3.483) Remain 09:50:12 loss: 0.2504 Lr: 0.03879
[2023-08-08 00:14:11,679 INFO misc.py line 115 22900] Train: [35/100][129/156] Data 0.001 (0.001) Batch 2.474 (3.475) Remain 09:48:47 loss: 0.2537 Lr: 0.03879
[2023-08-08 00:14:14,987 INFO misc.py line 115 22900] Train: [35/100][130/156] Data 0.001 (0.001) Batch 3.308 (3.473) Remain 09:48:30 loss: 0.3259 Lr: 0.03878
[2023-08-08 00:14:18,823 INFO misc.py line 115 22900] Train: [35/100][131/156] Data 0.001 (0.001) Batch 3.836 (3.476) Remain 09:48:55 loss: 0.2890 Lr: 0.03878
[2023-08-08 00:14:22,738 INFO misc.py line 115 22900] Train: [35/100][132/156] Data 0.001 (0.001) Batch 3.915 (3.480) Remain 09:49:26 loss: 0.4951 Lr: 0.03878
[2023-08-08 00:14:26,526 INFO misc.py line 115 22900] Train: [35/100][133/156] Data 0.001 (0.001) Batch 3.789 (3.482) Remain 09:49:47 loss: 0.5485 Lr: 0.03877
[2023-08-08 00:14:29,944 INFO misc.py line 115 22900] Train: [35/100][134/156] Data 0.001 (0.001) Batch 3.418 (3.482) Remain 09:49:39 loss: 0.3196 Lr: 0.03877
[2023-08-08 00:14:32,578 INFO misc.py line 115 22900] Train: [35/100][135/156] Data 0.001 (0.001) Batch 2.633 (3.475) Remain 09:48:30 loss: 0.3582 Lr: 0.03876
[2023-08-08 00:14:36,188 INFO misc.py line 115 22900] Train: [35/100][136/156] Data 0.001 (0.001) Batch 3.611 (3.476) Remain 09:48:37 loss: 0.3698 Lr: 0.03876
[2023-08-08 00:14:40,214 INFO misc.py line 115 22900] Train: [35/100][137/156] Data 0.001 (0.001) Batch 4.025 (3.480) Remain 09:49:15 loss: 0.2533 Lr: 0.03875
[2023-08-08 00:14:43,348 INFO misc.py line 115 22900] Train: [35/100][138/156] Data 0.001 (0.001) Batch 3.135 (3.478) Remain 09:48:45 loss: 0.2490 Lr: 0.03875
[2023-08-08 00:14:46,857 INFO misc.py line 115 22900] Train: [35/100][139/156] Data 0.001 (0.001) Batch 3.509 (3.478) Remain 09:48:44 loss: 0.3966 Lr: 0.03874
[2023-08-08 00:14:50,135 INFO misc.py line 115 22900] Train: [35/100][140/156] Data 0.001 (0.001) Batch 3.278 (3.476) Remain 09:48:26 loss: 0.3539 Lr: 0.03874
[2023-08-08 00:14:54,164 INFO misc.py line 115 22900] Train: [35/100][141/156] Data 0.001 (0.001) Batch 4.029 (3.480) Remain 09:49:03 loss: 0.3907 Lr: 0.03874
[2023-08-08 00:14:57,662 INFO misc.py line 115 22900] Train: [35/100][142/156] Data 0.001 (0.001) Batch 3.498 (3.481) Remain 09:49:01 loss: 0.2752 Lr: 0.03873
[2023-08-08 00:15:01,963 INFO misc.py line 115 22900] Train: [35/100][143/156] Data 0.001 (0.001) Batch 4.301 (3.486) Remain 09:49:57 loss: 0.6141 Lr: 0.03873
[2023-08-08 00:15:05,476 INFO misc.py line 115 22900] Train: [35/100][144/156] Data 0.001 (0.001) Batch 3.512 (3.487) Remain 09:49:55 loss: 0.3582 Lr: 0.03872
[2023-08-08 00:15:07,653 INFO misc.py line 115 22900] Train: [35/100][145/156] Data 0.001 (0.001) Batch 2.178 (3.477) Remain 09:48:18 loss: 0.3084 Lr: 0.03872
[2023-08-08 00:15:09,798 INFO misc.py line 115 22900] Train: [35/100][146/156] Data 0.001 (0.001) Batch 2.144 (3.468) Remain 09:46:40 loss: 0.3185 Lr: 0.03871
[2023-08-08 00:15:13,428 INFO misc.py line 115 22900] Train: [35/100][147/156] Data 0.001 (0.001) Batch 3.630 (3.469) Remain 09:46:48 loss: 0.7003 Lr: 0.03871
[2023-08-08 00:15:17,225 INFO misc.py line 115 22900] Train: [35/100][148/156] Data 0.001 (0.001) Batch 3.797 (3.471) Remain 09:47:08 loss: 0.6371 Lr: 0.03870
[2023-08-08 00:15:21,211 INFO misc.py line 115 22900] Train: [35/100][149/156] Data 0.001 (0.001) Batch 3.986 (3.475) Remain 09:47:40 loss: 0.4132 Lr: 0.03870
[2023-08-08 00:15:24,531 INFO misc.py line 115 22900] Train: [35/100][150/156] Data 0.001 (0.001) Batch 3.320 (3.474) Remain 09:47:26 loss: 0.4194 Lr: 0.03870
[2023-08-08 00:15:27,641 INFO misc.py line 115 22900] Train: [35/100][151/156] Data 0.001 (0.001) Batch 3.110 (3.471) Remain 09:46:57 loss: 0.4196 Lr: 0.03869
[2023-08-08 00:15:29,627 INFO misc.py line 115 22900] Train: [35/100][152/156] Data 0.001 (0.001) Batch 1.986 (3.461) Remain 09:45:13 loss: 0.3702 Lr: 0.03869
[2023-08-08 00:15:31,897 INFO misc.py line 115 22900] Train: [35/100][153/156] Data 0.001 (0.001) Batch 2.271 (3.454) Remain 09:43:49 loss: 0.1501 Lr: 0.03868
[2023-08-08 00:15:35,644 INFO misc.py line 115 22900] Train: [35/100][154/156] Data 0.001 (0.001) Batch 3.747 (3.455) Remain 09:44:05 loss: 0.3902 Lr: 0.03868
[2023-08-08 00:15:39,567 INFO misc.py line 115 22900] Train: [35/100][155/156] Data 0.001 (0.001) Batch 3.923 (3.459) Remain 09:44:33 loss: 0.5953 Lr: 0.03867
[2023-08-08 00:15:43,584 INFO misc.py line 115 22900] Train: [35/100][156/156] Data 0.001 (0.001) Batch 4.017 (3.462) Remain 09:45:06 loss: 0.3846 Lr: 0.03867
[2023-08-08 00:15:43,584 INFO misc.py line 129 22900] Train result: loss: 0.3678 
[2023-08-08 00:15:43,585 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 00:15:45,678 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.8968 
[2023-08-08 00:15:46,547 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.4803 
[2023-08-08 00:15:48,212 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.6395 
[2023-08-08 00:15:49,734 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.2044 
[2023-08-08 00:15:51,579 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.7140 
[2023-08-08 00:15:53,243 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.5767 
[2023-08-08 00:15:55,382 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.1925 
[2023-08-08 00:15:57,186 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.8914 
[2023-08-08 00:15:58,471 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.3644 
[2023-08-08 00:16:00,601 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3579 
[2023-08-08 00:16:01,127 INFO evaluator.py line 122 22900] Test: [11/24] Loss 0.7400 
[2023-08-08 00:16:02,659 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.6424 
[2023-08-08 00:16:05,368 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.1175 
[2023-08-08 00:16:07,047 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.6695 
[2023-08-08 00:16:09,068 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.3504 
[2023-08-08 00:16:11,776 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.1490 
[2023-08-08 00:16:14,483 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.3647 
[2023-08-08 00:16:16,328 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.2858 
[2023-08-08 00:16:17,076 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.0431 
[2023-08-08 00:16:17,961 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.7591 
[2023-08-08 00:16:20,223 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.2935 
[2023-08-08 00:16:22,188 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.6793 
[2023-08-08 00:16:24,036 INFO evaluator.py line 122 22900] Test: [23/24] Loss 3.0720 
[2023-08-08 00:16:25,971 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.6007 
[2023-08-08 00:16:26,021 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2254/0.3240/0.6877.
[2023-08-08 00:16:26,021 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6343/0.9568
[2023-08-08 00:16:26,021 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9525/0.9855
[2023-08-08 00:16:26,021 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1612/0.4294
[2023-08-08 00:16:26,021 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1608/0.2591
[2023-08-08 00:16:26,021 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.5680/0.6459
[2023-08-08 00:16:26,021 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.4028/0.6545
[2023-08-08 00:16:26,021 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5325/0.7691
[2023-08-08 00:16:26,021 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1523/0.1693
[2023-08-08 00:16:26,021 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1290/0.2826
[2023-08-08 00:16:26,021 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0439/0.0442
[2023-08-08 00:16:26,021 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0332/0.2863
[2023-08-08 00:16:26,021 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0118/0.0124
[2023-08-08 00:16:26,021 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1829/0.2772
[2023-08-08 00:16:26,021 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0657/0.0725
[2023-08-08 00:16:26,021 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0247/0.0252
[2023-08-08 00:16:26,021 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0084/0.0084
[2023-08-08 00:16:26,021 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0386/0.0401
[2023-08-08 00:16:26,021 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.3374/0.4493
[2023-08-08 00:16:26,021 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 00:16:26,021 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0689/0.1128
[2023-08-08 00:16:26,022 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 00:16:26,022 INFO misc.py line 152 22900] Currently Best mIoU: 0.2356
[2023-08-08 00:16:26,022 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 00:16:31,091 INFO misc.py line 115 22900] Train: [36/100][1/156] Data 1.186 (1.186) Batch 4.297 (4.297) Remain 12:06:02 loss: 0.3310 Lr: 0.03866
[2023-08-08 00:16:34,423 INFO misc.py line 115 22900] Train: [36/100][2/156] Data 0.001 (0.001) Batch 3.332 (3.332) Remain 09:22:57 loss: 0.3287 Lr: 0.03866
[2023-08-08 00:16:37,310 INFO misc.py line 115 22900] Train: [36/100][3/156] Data 0.001 (0.001) Batch 2.887 (2.887) Remain 08:07:43 loss: 0.3274 Lr: 0.03866
[2023-08-08 00:16:41,194 INFO misc.py line 115 22900] Train: [36/100][4/156] Data 0.001 (0.001) Batch 3.884 (3.884) Remain 10:56:08 loss: 0.3038 Lr: 0.03865
[2023-08-08 00:16:45,247 INFO misc.py line 115 22900] Train: [36/100][5/156] Data 0.001 (0.001) Batch 4.053 (3.968) Remain 11:10:20 loss: 0.3757 Lr: 0.03865
[2023-08-08 00:16:48,220 INFO misc.py line 115 22900] Train: [36/100][6/156] Data 0.001 (0.001) Batch 2.973 (3.637) Remain 10:14:14 loss: 0.1458 Lr: 0.03864
[2023-08-08 00:16:51,705 INFO misc.py line 115 22900] Train: [36/100][7/156] Data 0.001 (0.001) Batch 3.485 (3.599) Remain 10:07:46 loss: 0.2241 Lr: 0.03864
[2023-08-08 00:16:54,512 INFO misc.py line 115 22900] Train: [36/100][8/156] Data 0.001 (0.001) Batch 2.807 (3.441) Remain 09:40:59 loss: 0.2933 Lr: 0.03863
[2023-08-08 00:16:57,881 INFO misc.py line 115 22900] Train: [36/100][9/156] Data 0.001 (0.001) Batch 3.368 (3.429) Remain 09:38:54 loss: 0.5211 Lr: 0.03863
[2023-08-08 00:17:01,456 INFO misc.py line 115 22900] Train: [36/100][10/156] Data 0.001 (0.001) Batch 3.575 (3.449) Remain 09:42:22 loss: 0.4412 Lr: 0.03862
[2023-08-08 00:17:04,651 INFO misc.py line 115 22900] Train: [36/100][11/156] Data 0.001 (0.001) Batch 3.195 (3.418) Remain 09:36:57 loss: 0.4844 Lr: 0.03862
[2023-08-08 00:17:08,690 INFO misc.py line 115 22900] Train: [36/100][12/156] Data 0.001 (0.001) Batch 4.039 (3.487) Remain 09:48:33 loss: 0.5440 Lr: 0.03862
[2023-08-08 00:17:12,711 INFO misc.py line 115 22900] Train: [36/100][13/156] Data 0.001 (0.001) Batch 4.021 (3.540) Remain 09:57:31 loss: 0.3110 Lr: 0.03861
[2023-08-08 00:17:16,748 INFO misc.py line 115 22900] Train: [36/100][14/156] Data 0.001 (0.001) Batch 4.037 (3.585) Remain 10:05:05 loss: 0.4947 Lr: 0.03861
[2023-08-08 00:17:21,495 INFO misc.py line 115 22900] Train: [36/100][15/156] Data 0.001 (0.001) Batch 4.747 (3.682) Remain 10:21:21 loss: 0.9288 Lr: 0.03860
[2023-08-08 00:17:23,635 INFO misc.py line 115 22900] Train: [36/100][16/156] Data 0.001 (0.001) Batch 2.140 (3.564) Remain 10:01:16 loss: 0.2068 Lr: 0.03860
[2023-08-08 00:17:27,705 INFO misc.py line 115 22900] Train: [36/100][17/156] Data 0.001 (0.001) Batch 4.069 (3.600) Remain 10:07:19 loss: 0.5737 Lr: 0.03859
[2023-08-08 00:17:31,143 INFO misc.py line 115 22900] Train: [36/100][18/156] Data 0.001 (0.001) Batch 3.438 (3.589) Remain 10:05:26 loss: 0.4179 Lr: 0.03859
[2023-08-08 00:17:33,938 INFO misc.py line 115 22900] Train: [36/100][19/156] Data 0.001 (0.001) Batch 2.795 (3.539) Remain 09:57:00 loss: 0.2462 Lr: 0.03858
[2023-08-08 00:17:37,081 INFO misc.py line 115 22900] Train: [36/100][20/156] Data 0.001 (0.001) Batch 3.143 (3.516) Remain 09:53:01 loss: 0.6318 Lr: 0.03858
[2023-08-08 00:17:40,321 INFO misc.py line 115 22900] Train: [36/100][21/156] Data 0.001 (0.001) Batch 3.239 (3.501) Remain 09:50:22 loss: 0.3276 Lr: 0.03858
[2023-08-08 00:17:43,961 INFO misc.py line 115 22900] Train: [36/100][22/156] Data 0.001 (0.001) Batch 3.640 (3.508) Remain 09:51:33 loss: 0.3515 Lr: 0.03857
[2023-08-08 00:17:47,470 INFO misc.py line 115 22900] Train: [36/100][23/156] Data 0.001 (0.001) Batch 3.510 (3.508) Remain 09:51:30 loss: 0.2605 Lr: 0.03857
[2023-08-08 00:17:51,489 INFO misc.py line 115 22900] Train: [36/100][24/156] Data 0.001 (0.001) Batch 4.018 (3.532) Remain 09:55:33 loss: 0.3738 Lr: 0.03856
[2023-08-08 00:17:55,156 INFO misc.py line 115 22900] Train: [36/100][25/156] Data 0.001 (0.001) Batch 3.668 (3.538) Remain 09:56:31 loss: 0.6124 Lr: 0.03856
[2023-08-08 00:17:59,030 INFO misc.py line 115 22900] Train: [36/100][26/156] Data 0.001 (0.001) Batch 3.874 (3.553) Remain 09:58:55 loss: 0.3909 Lr: 0.03855
[2023-08-08 00:18:02,499 INFO misc.py line 115 22900] Train: [36/100][27/156] Data 0.001 (0.001) Batch 3.468 (3.550) Remain 09:58:16 loss: 0.5030 Lr: 0.03855
[2023-08-08 00:18:06,470 INFO misc.py line 115 22900] Train: [36/100][28/156] Data 0.001 (0.001) Batch 3.971 (3.566) Remain 10:01:03 loss: 0.3070 Lr: 0.03854
[2023-08-08 00:18:09,908 INFO misc.py line 115 22900] Train: [36/100][29/156] Data 0.001 (0.001) Batch 3.439 (3.561) Remain 10:00:10 loss: 0.4095 Lr: 0.03854
[2023-08-08 00:18:13,652 INFO misc.py line 115 22900] Train: [36/100][30/156] Data 0.001 (0.001) Batch 3.744 (3.568) Remain 10:01:15 loss: 0.3824 Lr: 0.03854
[2023-08-08 00:18:18,085 INFO misc.py line 115 22900] Train: [36/100][31/156] Data 0.001 (0.001) Batch 4.433 (3.599) Remain 10:06:23 loss: 0.8827 Lr: 0.03853
[2023-08-08 00:18:21,625 INFO misc.py line 115 22900] Train: [36/100][32/156] Data 0.001 (0.001) Batch 3.539 (3.597) Remain 10:05:59 loss: 0.3306 Lr: 0.03853
[2023-08-08 00:18:24,965 INFO misc.py line 115 22900] Train: [36/100][33/156] Data 0.001 (0.001) Batch 3.340 (3.588) Remain 10:04:28 loss: 0.4224 Lr: 0.03852
[2023-08-08 00:18:29,140 INFO misc.py line 115 22900] Train: [36/100][34/156] Data 0.001 (0.001) Batch 4.175 (3.607) Remain 10:07:36 loss: 0.5644 Lr: 0.03852
[2023-08-08 00:18:33,558 INFO misc.py line 115 22900] Train: [36/100][35/156] Data 0.001 (0.001) Batch 4.418 (3.633) Remain 10:11:49 loss: 0.3935 Lr: 0.03851
[2023-08-08 00:18:37,077 INFO misc.py line 115 22900] Train: [36/100][36/156] Data 0.001 (0.001) Batch 3.519 (3.629) Remain 10:11:10 loss: 0.3499 Lr: 0.03851
[2023-08-08 00:18:40,570 INFO misc.py line 115 22900] Train: [36/100][37/156] Data 0.001 (0.001) Batch 3.493 (3.625) Remain 10:10:26 loss: 0.2554 Lr: 0.03850
[2023-08-08 00:18:44,564 INFO misc.py line 115 22900] Train: [36/100][38/156] Data 0.001 (0.001) Batch 3.994 (3.636) Remain 10:12:09 loss: 0.3525 Lr: 0.03850
[2023-08-08 00:18:47,794 INFO misc.py line 115 22900] Train: [36/100][39/156] Data 0.001 (0.001) Batch 3.230 (3.625) Remain 10:10:11 loss: 0.4761 Lr: 0.03850
[2023-08-08 00:18:51,820 INFO misc.py line 115 22900] Train: [36/100][40/156] Data 0.001 (0.001) Batch 4.026 (3.635) Remain 10:11:57 loss: 0.6574 Lr: 0.03849
[2023-08-08 00:18:54,418 INFO misc.py line 115 22900] Train: [36/100][41/156] Data 0.001 (0.001) Batch 2.598 (3.608) Remain 10:07:18 loss: 0.4075 Lr: 0.03849
[2023-08-08 00:18:57,667 INFO misc.py line 115 22900] Train: [36/100][42/156] Data 0.001 (0.001) Batch 3.249 (3.599) Remain 10:05:41 loss: 0.4116 Lr: 0.03848
[2023-08-08 00:19:01,730 INFO misc.py line 115 22900] Train: [36/100][43/156] Data 0.001 (0.001) Batch 4.063 (3.611) Remain 10:07:35 loss: 0.2964 Lr: 0.03848
[2023-08-08 00:19:04,496 INFO misc.py line 115 22900] Train: [36/100][44/156] Data 0.001 (0.001) Batch 2.766 (3.590) Remain 10:04:03 loss: 0.2801 Lr: 0.03847
[2023-08-08 00:19:08,333 INFO misc.py line 115 22900] Train: [36/100][45/156] Data 0.001 (0.001) Batch 3.838 (3.596) Remain 10:04:59 loss: 0.4194 Lr: 0.03847
[2023-08-08 00:19:11,226 INFO misc.py line 115 22900] Train: [36/100][46/156] Data 0.001 (0.001) Batch 2.893 (3.579) Remain 10:02:10 loss: 0.0853 Lr: 0.03846
[2023-08-08 00:19:15,022 INFO misc.py line 115 22900] Train: [36/100][47/156] Data 0.001 (0.001) Batch 3.796 (3.584) Remain 10:02:56 loss: 0.3955 Lr: 0.03846
[2023-08-08 00:19:17,980 INFO misc.py line 115 22900] Train: [36/100][48/156] Data 0.001 (0.001) Batch 2.959 (3.570) Remain 10:00:32 loss: 0.4244 Lr: 0.03846
[2023-08-08 00:19:21,443 INFO misc.py line 115 22900] Train: [36/100][49/156] Data 0.001 (0.001) Batch 3.463 (3.568) Remain 10:00:05 loss: 0.4458 Lr: 0.03845
[2023-08-08 00:19:24,816 INFO misc.py line 115 22900] Train: [36/100][50/156] Data 0.001 (0.001) Batch 3.373 (3.564) Remain 09:59:20 loss: 0.3567 Lr: 0.03845
[2023-08-08 00:19:28,149 INFO misc.py line 115 22900] Train: [36/100][51/156] Data 0.001 (0.001) Batch 3.333 (3.559) Remain 09:58:28 loss: 0.2789 Lr: 0.03844
[2023-08-08 00:19:31,377 INFO misc.py line 115 22900] Train: [36/100][52/156] Data 0.001 (0.001) Batch 3.228 (3.552) Remain 09:57:16 loss: 0.2543 Lr: 0.03844
[2023-08-08 00:19:35,411 INFO misc.py line 115 22900] Train: [36/100][53/156] Data 0.001 (0.001) Batch 4.034 (3.562) Remain 09:58:50 loss: 0.2808 Lr: 0.03843
[2023-08-08 00:19:38,212 INFO misc.py line 115 22900] Train: [36/100][54/156] Data 0.001 (0.001) Batch 2.801 (3.547) Remain 09:56:16 loss: 0.2705 Lr: 0.03843
[2023-08-08 00:19:41,541 INFO misc.py line 115 22900] Train: [36/100][55/156] Data 0.001 (0.001) Batch 3.329 (3.543) Remain 09:55:30 loss: 0.3219 Lr: 0.03842
[2023-08-08 00:19:43,899 INFO misc.py line 115 22900] Train: [36/100][56/156] Data 0.001 (0.001) Batch 2.358 (3.521) Remain 09:51:41 loss: 0.1744 Lr: 0.03842
[2023-08-08 00:19:46,586 INFO misc.py line 115 22900] Train: [36/100][57/156] Data 0.001 (0.001) Batch 2.687 (3.505) Remain 09:49:02 loss: 0.2505 Lr: 0.03842
[2023-08-08 00:19:49,361 INFO misc.py line 115 22900] Train: [36/100][58/156] Data 0.001 (0.001) Batch 2.775 (3.492) Remain 09:46:44 loss: 0.4229 Lr: 0.03841
[2023-08-08 00:19:53,478 INFO misc.py line 115 22900] Train: [36/100][59/156] Data 0.001 (0.001) Batch 4.117 (3.503) Remain 09:48:33 loss: 0.4654 Lr: 0.03841
[2023-08-08 00:19:56,143 INFO misc.py line 115 22900] Train: [36/100][60/156] Data 0.001 (0.001) Batch 2.665 (3.488) Remain 09:46:02 loss: 0.2797 Lr: 0.03840
[2023-08-08 00:19:59,668 INFO misc.py line 115 22900] Train: [36/100][61/156] Data 0.001 (0.001) Batch 3.525 (3.489) Remain 09:46:05 loss: 0.4338 Lr: 0.03840
[2023-08-08 00:20:03,096 INFO misc.py line 115 22900] Train: [36/100][62/156] Data 0.001 (0.001) Batch 3.427 (3.488) Remain 09:45:51 loss: 0.4788 Lr: 0.03839
[2023-08-08 00:20:06,610 INFO misc.py line 115 22900] Train: [36/100][63/156] Data 0.001 (0.001) Batch 3.514 (3.488) Remain 09:45:52 loss: 0.5499 Lr: 0.03839
[2023-08-08 00:20:10,675 INFO misc.py line 115 22900] Train: [36/100][64/156] Data 0.001 (0.001) Batch 4.065 (3.498) Remain 09:47:23 loss: 0.5482 Lr: 0.03838
[2023-08-08 00:20:14,753 INFO misc.py line 115 22900] Train: [36/100][65/156] Data 0.001 (0.001) Batch 4.078 (3.507) Remain 09:48:54 loss: 0.5558 Lr: 0.03838
[2023-08-08 00:20:17,930 INFO misc.py line 115 22900] Train: [36/100][66/156] Data 0.001 (0.001) Batch 3.178 (3.502) Remain 09:47:58 loss: 0.3688 Lr: 0.03838
[2023-08-08 00:20:22,500 INFO misc.py line 115 22900] Train: [36/100][67/156] Data 0.001 (0.001) Batch 4.570 (3.519) Remain 09:50:42 loss: 0.6362 Lr: 0.03837
[2023-08-08 00:20:25,580 INFO misc.py line 115 22900] Train: [36/100][68/156] Data 0.001 (0.001) Batch 3.080 (3.512) Remain 09:49:31 loss: 0.4355 Lr: 0.03837
[2023-08-08 00:20:28,889 INFO misc.py line 115 22900] Train: [36/100][69/156] Data 0.001 (0.001) Batch 3.309 (3.509) Remain 09:48:56 loss: 0.2707 Lr: 0.03836
[2023-08-08 00:20:32,508 INFO misc.py line 115 22900] Train: [36/100][70/156] Data 0.001 (0.001) Batch 3.619 (3.510) Remain 09:49:09 loss: 0.4216 Lr: 0.03836
[2023-08-08 00:20:35,906 INFO misc.py line 115 22900] Train: [36/100][71/156] Data 0.001 (0.001) Batch 3.398 (3.509) Remain 09:48:49 loss: 0.2504 Lr: 0.03835
[2023-08-08 00:20:39,660 INFO misc.py line 115 22900] Train: [36/100][72/156] Data 0.001 (0.001) Batch 3.754 (3.512) Remain 09:49:22 loss: 0.5050 Lr: 0.03835
[2023-08-08 00:20:42,119 INFO misc.py line 115 22900] Train: [36/100][73/156] Data 0.001 (0.001) Batch 2.459 (3.497) Remain 09:46:47 loss: 0.4475 Lr: 0.03834
[2023-08-08 00:20:45,888 INFO misc.py line 115 22900] Train: [36/100][74/156] Data 0.001 (0.001) Batch 3.769 (3.501) Remain 09:47:22 loss: 0.3390 Lr: 0.03834
[2023-08-08 00:20:49,357 INFO misc.py line 115 22900] Train: [36/100][75/156] Data 0.001 (0.001) Batch 3.469 (3.501) Remain 09:47:14 loss: 0.2859 Lr: 0.03833
[2023-08-08 00:20:52,586 INFO misc.py line 115 22900] Train: [36/100][76/156] Data 0.001 (0.001) Batch 3.230 (3.497) Remain 09:46:33 loss: 0.2718 Lr: 0.03833
[2023-08-08 00:20:55,954 INFO misc.py line 115 22900] Train: [36/100][77/156] Data 0.001 (0.001) Batch 3.368 (3.495) Remain 09:46:12 loss: 0.6351 Lr: 0.03833
[2023-08-08 00:20:58,940 INFO misc.py line 115 22900] Train: [36/100][78/156] Data 0.001 (0.001) Batch 2.986 (3.488) Remain 09:45:00 loss: 0.2651 Lr: 0.03832
[2023-08-08 00:21:03,117 INFO misc.py line 115 22900] Train: [36/100][79/156] Data 0.001 (0.001) Batch 4.176 (3.497) Remain 09:46:27 loss: 0.4655 Lr: 0.03832
[2023-08-08 00:21:04,912 INFO misc.py line 115 22900] Train: [36/100][80/156] Data 0.001 (0.001) Batch 1.795 (3.475) Remain 09:42:42 loss: 0.1604 Lr: 0.03831
[2023-08-08 00:21:08,576 INFO misc.py line 115 22900] Train: [36/100][81/156] Data 0.001 (0.001) Batch 3.665 (3.478) Remain 09:43:02 loss: 0.2344 Lr: 0.03831
[2023-08-08 00:21:12,684 INFO misc.py line 115 22900] Train: [36/100][82/156] Data 0.001 (0.001) Batch 4.107 (3.486) Remain 09:44:19 loss: 0.4820 Lr: 0.03830
[2023-08-08 00:21:16,241 INFO misc.py line 115 22900] Train: [36/100][83/156] Data 0.001 (0.001) Batch 3.557 (3.487) Remain 09:44:25 loss: 0.4032 Lr: 0.03830
[2023-08-08 00:21:19,450 INFO misc.py line 115 22900] Train: [36/100][84/156] Data 0.001 (0.001) Batch 3.209 (3.483) Remain 09:43:47 loss: 0.2653 Lr: 0.03829
[2023-08-08 00:21:22,352 INFO misc.py line 115 22900] Train: [36/100][85/156] Data 0.001 (0.001) Batch 2.902 (3.476) Remain 09:42:32 loss: 0.1720 Lr: 0.03829
[2023-08-08 00:21:26,332 INFO misc.py line 115 22900] Train: [36/100][86/156] Data 0.001 (0.001) Batch 3.979 (3.482) Remain 09:43:29 loss: 0.3706 Lr: 0.03829
[2023-08-08 00:21:30,331 INFO misc.py line 115 22900] Train: [36/100][87/156] Data 0.001 (0.001) Batch 4.000 (3.488) Remain 09:44:28 loss: 0.5703 Lr: 0.03828
[2023-08-08 00:21:34,040 INFO misc.py line 115 22900] Train: [36/100][88/156] Data 0.001 (0.001) Batch 3.708 (3.491) Remain 09:44:50 loss: 0.3938 Lr: 0.03828
[2023-08-08 00:21:37,225 INFO misc.py line 115 22900] Train: [36/100][89/156] Data 0.001 (0.001) Batch 3.186 (3.487) Remain 09:44:11 loss: 0.2907 Lr: 0.03827
[2023-08-08 00:21:40,894 INFO misc.py line 115 22900] Train: [36/100][90/156] Data 0.001 (0.001) Batch 3.669 (3.489) Remain 09:44:29 loss: 0.3869 Lr: 0.03827
[2023-08-08 00:21:43,766 INFO misc.py line 115 22900] Train: [36/100][91/156] Data 0.001 (0.001) Batch 2.872 (3.482) Remain 09:43:15 loss: 0.3048 Lr: 0.03826
[2023-08-08 00:21:46,956 INFO misc.py line 115 22900] Train: [36/100][92/156] Data 0.001 (0.001) Batch 3.189 (3.479) Remain 09:42:38 loss: 0.3404 Lr: 0.03826
[2023-08-08 00:21:50,505 INFO misc.py line 115 22900] Train: [36/100][93/156] Data 0.001 (0.001) Batch 3.549 (3.480) Remain 09:42:43 loss: 0.3672 Lr: 0.03825
[2023-08-08 00:21:54,661 INFO misc.py line 115 22900] Train: [36/100][94/156] Data 0.001 (0.001) Batch 4.156 (3.487) Remain 09:43:54 loss: 0.5101 Lr: 0.03825
[2023-08-08 00:21:57,599 INFO misc.py line 115 22900] Train: [36/100][95/156] Data 0.001 (0.001) Batch 2.937 (3.481) Remain 09:42:50 loss: 0.0920 Lr: 0.03825
[2023-08-08 00:22:01,120 INFO misc.py line 115 22900] Train: [36/100][96/156] Data 0.001 (0.001) Batch 3.522 (3.482) Remain 09:42:51 loss: 0.2833 Lr: 0.03824
[2023-08-08 00:22:04,758 INFO misc.py line 115 22900] Train: [36/100][97/156] Data 0.001 (0.001) Batch 3.637 (3.483) Remain 09:43:04 loss: 0.3141 Lr: 0.03824
[2023-08-08 00:22:08,860 INFO misc.py line 115 22900] Train: [36/100][98/156] Data 0.001 (0.001) Batch 4.102 (3.490) Remain 09:44:06 loss: 0.4092 Lr: 0.03823
[2023-08-08 00:22:11,004 INFO misc.py line 115 22900] Train: [36/100][99/156] Data 0.001 (0.001) Batch 2.144 (3.476) Remain 09:41:42 loss: 0.0935 Lr: 0.03823
[2023-08-08 00:22:14,010 INFO misc.py line 115 22900] Train: [36/100][100/156] Data 0.001 (0.001) Batch 3.007 (3.471) Remain 09:40:50 loss: 0.2266 Lr: 0.03822
[2023-08-08 00:22:17,343 INFO misc.py line 115 22900] Train: [36/100][101/156] Data 0.001 (0.001) Batch 3.333 (3.470) Remain 09:40:32 loss: 0.3824 Lr: 0.03822
[2023-08-08 00:22:21,104 INFO misc.py line 115 22900] Train: [36/100][102/156] Data 0.001 (0.001) Batch 3.761 (3.473) Remain 09:40:58 loss: 0.1617 Lr: 0.03821
[2023-08-08 00:22:23,631 INFO misc.py line 115 22900] Train: [36/100][103/156] Data 0.001 (0.001) Batch 2.526 (3.463) Remain 09:39:20 loss: 0.2014 Lr: 0.03821
[2023-08-08 00:22:27,502 INFO misc.py line 115 22900] Train: [36/100][104/156] Data 0.001 (0.001) Batch 3.871 (3.467) Remain 09:39:57 loss: 0.4534 Lr: 0.03820
[2023-08-08 00:22:31,833 INFO misc.py line 115 22900] Train: [36/100][105/156] Data 0.001 (0.001) Batch 4.331 (3.476) Remain 09:41:18 loss: 0.4082 Lr: 0.03820
[2023-08-08 00:22:35,309 INFO misc.py line 115 22900] Train: [36/100][106/156] Data 0.001 (0.001) Batch 3.476 (3.476) Remain 09:41:15 loss: 0.3131 Lr: 0.03820
[2023-08-08 00:22:37,929 INFO misc.py line 115 22900] Train: [36/100][107/156] Data 0.001 (0.001) Batch 2.620 (3.467) Remain 09:39:49 loss: 0.2485 Lr: 0.03819
[2023-08-08 00:22:41,001 INFO misc.py line 115 22900] Train: [36/100][108/156] Data 0.001 (0.001) Batch 3.072 (3.464) Remain 09:39:08 loss: 0.2520 Lr: 0.03819
[2023-08-08 00:22:45,001 INFO misc.py line 115 22900] Train: [36/100][109/156] Data 0.001 (0.001) Batch 4.000 (3.469) Remain 09:39:55 loss: 0.5224 Lr: 0.03818
[2023-08-08 00:22:47,115 INFO misc.py line 115 22900] Train: [36/100][110/156] Data 0.001 (0.001) Batch 2.114 (3.456) Remain 09:37:44 loss: 0.3194 Lr: 0.03818
[2023-08-08 00:22:51,438 INFO misc.py line 115 22900] Train: [36/100][111/156] Data 0.001 (0.001) Batch 4.322 (3.464) Remain 09:39:01 loss: 0.5023 Lr: 0.03817
[2023-08-08 00:22:54,668 INFO misc.py line 115 22900] Train: [36/100][112/156] Data 0.001 (0.001) Batch 3.231 (3.462) Remain 09:38:36 loss: 0.2662 Lr: 0.03817
[2023-08-08 00:22:58,030 INFO misc.py line 115 22900] Train: [36/100][113/156] Data 0.001 (0.001) Batch 3.362 (3.461) Remain 09:38:24 loss: 0.4330 Lr: 0.03816
[2023-08-08 00:23:01,343 INFO misc.py line 115 22900] Train: [36/100][114/156] Data 0.001 (0.001) Batch 3.313 (3.460) Remain 09:38:07 loss: 0.3308 Lr: 0.03816
[2023-08-08 00:23:05,422 INFO misc.py line 115 22900] Train: [36/100][115/156] Data 0.001 (0.001) Batch 4.078 (3.465) Remain 09:38:59 loss: 0.4102 Lr: 0.03816
[2023-08-08 00:23:08,791 INFO misc.py line 115 22900] Train: [36/100][116/156] Data 0.001 (0.001) Batch 3.369 (3.464) Remain 09:38:47 loss: 0.2671 Lr: 0.03815
[2023-08-08 00:23:11,352 INFO misc.py line 115 22900] Train: [36/100][117/156] Data 0.001 (0.001) Batch 2.561 (3.457) Remain 09:37:24 loss: 0.3030 Lr: 0.03815
[2023-08-08 00:23:15,373 INFO misc.py line 115 22900] Train: [36/100][118/156] Data 0.001 (0.001) Batch 4.022 (3.461) Remain 09:38:10 loss: 0.3442 Lr: 0.03814
[2023-08-08 00:23:19,293 INFO misc.py line 115 22900] Train: [36/100][119/156] Data 0.001 (0.001) Batch 3.920 (3.465) Remain 09:38:46 loss: 0.3886 Lr: 0.03814
[2023-08-08 00:23:21,911 INFO misc.py line 115 22900] Train: [36/100][120/156] Data 0.001 (0.001) Batch 2.618 (3.458) Remain 09:37:30 loss: 0.1586 Lr: 0.03813
[2023-08-08 00:23:25,090 INFO misc.py line 115 22900] Train: [36/100][121/156] Data 0.001 (0.001) Batch 3.179 (3.456) Remain 09:37:03 loss: 0.2846 Lr: 0.03813
[2023-08-08 00:23:28,465 INFO misc.py line 115 22900] Train: [36/100][122/156] Data 0.001 (0.001) Batch 3.375 (3.455) Remain 09:36:53 loss: 0.4037 Lr: 0.03812
[2023-08-08 00:23:31,389 INFO misc.py line 115 22900] Train: [36/100][123/156] Data 0.001 (0.001) Batch 2.924 (3.451) Remain 09:36:05 loss: 0.1655 Lr: 0.03812
[2023-08-08 00:23:35,124 INFO misc.py line 115 22900] Train: [36/100][124/156] Data 0.001 (0.001) Batch 3.734 (3.453) Remain 09:36:25 loss: 0.4895 Lr: 0.03811
[2023-08-08 00:23:38,062 INFO misc.py line 115 22900] Train: [36/100][125/156] Data 0.001 (0.001) Batch 2.939 (3.449) Remain 09:35:39 loss: 0.1325 Lr: 0.03811
[2023-08-08 00:23:41,628 INFO misc.py line 115 22900] Train: [36/100][126/156] Data 0.001 (0.001) Batch 3.566 (3.450) Remain 09:35:45 loss: 0.2254 Lr: 0.03811
[2023-08-08 00:23:44,544 INFO misc.py line 115 22900] Train: [36/100][127/156] Data 0.001 (0.001) Batch 2.916 (3.445) Remain 09:34:59 loss: 0.1552 Lr: 0.03810
[2023-08-08 00:23:47,170 INFO misc.py line 115 22900] Train: [36/100][128/156] Data 0.001 (0.001) Batch 2.627 (3.439) Remain 09:33:50 loss: 0.2827 Lr: 0.03810
[2023-08-08 00:23:50,547 INFO misc.py line 115 22900] Train: [36/100][129/156] Data 0.001 (0.001) Batch 3.377 (3.438) Remain 09:33:41 loss: 0.4198 Lr: 0.03809
[2023-08-08 00:23:54,465 INFO misc.py line 115 22900] Train: [36/100][130/156] Data 0.001 (0.001) Batch 3.918 (3.442) Remain 09:34:16 loss: 0.3933 Lr: 0.03809
[2023-08-08 00:23:58,482 INFO misc.py line 115 22900] Train: [36/100][131/156] Data 0.001 (0.001) Batch 4.018 (3.447) Remain 09:34:57 loss: 0.2187 Lr: 0.03808
[2023-08-08 00:24:01,744 INFO misc.py line 115 22900] Train: [36/100][132/156] Data 0.001 (0.001) Batch 3.261 (3.445) Remain 09:34:39 loss: 0.2668 Lr: 0.03808
[2023-08-08 00:24:04,877 INFO misc.py line 115 22900] Train: [36/100][133/156] Data 0.001 (0.001) Batch 3.134 (3.443) Remain 09:34:12 loss: 0.2012 Lr: 0.03807
[2023-08-08 00:24:07,970 INFO misc.py line 115 22900] Train: [36/100][134/156] Data 0.001 (0.001) Batch 3.093 (3.440) Remain 09:33:42 loss: 0.2026 Lr: 0.03807
[2023-08-08 00:24:11,461 INFO misc.py line 115 22900] Train: [36/100][135/156] Data 0.001 (0.001) Batch 3.491 (3.441) Remain 09:33:42 loss: 0.3584 Lr: 0.03806
[2023-08-08 00:24:14,215 INFO misc.py line 115 22900] Train: [36/100][136/156] Data 0.001 (0.001) Batch 2.754 (3.435) Remain 09:32:47 loss: 0.1559 Lr: 0.03806
[2023-08-08 00:24:17,833 INFO misc.py line 115 22900] Train: [36/100][137/156] Data 0.001 (0.001) Batch 3.618 (3.437) Remain 09:32:57 loss: 0.5423 Lr: 0.03806
[2023-08-08 00:24:20,444 INFO misc.py line 115 22900] Train: [36/100][138/156] Data 0.001 (0.001) Batch 2.611 (3.431) Remain 09:31:53 loss: 0.2469 Lr: 0.03805
[2023-08-08 00:24:23,381 INFO misc.py line 115 22900] Train: [36/100][139/156] Data 0.001 (0.001) Batch 2.937 (3.427) Remain 09:31:13 loss: 0.2076 Lr: 0.03805
[2023-08-08 00:24:26,079 INFO misc.py line 115 22900] Train: [36/100][140/156] Data 0.001 (0.001) Batch 2.698 (3.422) Remain 09:30:16 loss: 0.2795 Lr: 0.03804
[2023-08-08 00:24:29,399 INFO misc.py line 115 22900] Train: [36/100][141/156] Data 0.001 (0.001) Batch 3.321 (3.421) Remain 09:30:05 loss: 0.4634 Lr: 0.03804
[2023-08-08 00:24:33,705 INFO misc.py line 115 22900] Train: [36/100][142/156] Data 0.001 (0.001) Batch 4.306 (3.427) Remain 09:31:06 loss: 0.3279 Lr: 0.03803
[2023-08-08 00:24:37,321 INFO misc.py line 115 22900] Train: [36/100][143/156] Data 0.001 (0.001) Batch 3.616 (3.429) Remain 09:31:16 loss: 0.3580 Lr: 0.03803
[2023-08-08 00:24:40,885 INFO misc.py line 115 22900] Train: [36/100][144/156] Data 0.001 (0.001) Batch 3.564 (3.430) Remain 09:31:22 loss: 0.3783 Lr: 0.03802
[2023-08-08 00:24:45,465 INFO misc.py line 115 22900] Train: [36/100][145/156] Data 0.001 (0.001) Batch 4.580 (3.438) Remain 09:32:39 loss: 0.5510 Lr: 0.03802
[2023-08-08 00:24:48,176 INFO misc.py line 115 22900] Train: [36/100][146/156] Data 0.001 (0.001) Batch 2.711 (3.433) Remain 09:31:45 loss: 0.2861 Lr: 0.03802
[2023-08-08 00:24:52,178 INFO misc.py line 115 22900] Train: [36/100][147/156] Data 0.001 (0.001) Batch 4.001 (3.437) Remain 09:32:21 loss: 0.4098 Lr: 0.03801
[2023-08-08 00:24:56,322 INFO misc.py line 115 22900] Train: [36/100][148/156] Data 0.001 (0.001) Batch 4.145 (3.441) Remain 09:33:07 loss: 0.2470 Lr: 0.03801
[2023-08-08 00:24:58,865 INFO misc.py line 115 22900] Train: [36/100][149/156] Data 0.001 (0.001) Batch 2.543 (3.435) Remain 09:32:02 loss: 0.3258 Lr: 0.03800
[2023-08-08 00:25:02,926 INFO misc.py line 115 22900] Train: [36/100][150/156] Data 0.001 (0.001) Batch 4.061 (3.440) Remain 09:32:41 loss: 0.3895 Lr: 0.03800
[2023-08-08 00:25:06,016 INFO misc.py line 115 22900] Train: [36/100][151/156] Data 0.001 (0.001) Batch 3.091 (3.437) Remain 09:32:14 loss: 0.3659 Lr: 0.03799
[2023-08-08 00:25:10,730 INFO misc.py line 115 22900] Train: [36/100][152/156] Data 0.001 (0.001) Batch 4.713 (3.446) Remain 09:33:36 loss: 0.6471 Lr: 0.03799
[2023-08-08 00:25:13,814 INFO misc.py line 115 22900] Train: [36/100][153/156] Data 0.001 (0.001) Batch 3.085 (3.443) Remain 09:33:08 loss: 0.3304 Lr: 0.03798
[2023-08-08 00:25:17,923 INFO misc.py line 115 22900] Train: [36/100][154/156] Data 0.001 (0.001) Batch 4.109 (3.448) Remain 09:33:49 loss: 0.5394 Lr: 0.03798
[2023-08-08 00:25:20,319 INFO misc.py line 115 22900] Train: [36/100][155/156] Data 0.001 (0.001) Batch 2.396 (3.441) Remain 09:32:36 loss: 0.3190 Lr: 0.03797
[2023-08-08 00:25:24,327 INFO misc.py line 115 22900] Train: [36/100][156/156] Data 0.001 (0.001) Batch 4.008 (3.445) Remain 09:33:10 loss: 0.3792 Lr: 0.03797
[2023-08-08 00:25:24,328 INFO misc.py line 129 22900] Train result: loss: 0.3635 
[2023-08-08 00:25:24,328 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 00:25:26,425 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.9327 
[2023-08-08 00:25:27,293 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.5304 
[2023-08-08 00:25:28,955 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.7350 
[2023-08-08 00:25:30,478 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.4158 
[2023-08-08 00:25:32,323 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.9143 
[2023-08-08 00:25:33,989 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.6176 
[2023-08-08 00:25:36,127 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.9447 
[2023-08-08 00:25:37,929 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.9615 
[2023-08-08 00:25:39,213 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.5800 
[2023-08-08 00:25:41,342 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3878 
[2023-08-08 00:25:41,867 INFO evaluator.py line 122 22900] Test: [11/24] Loss 0.9379 
[2023-08-08 00:25:43,401 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.7445 
[2023-08-08 00:25:46,114 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.1342 
[2023-08-08 00:25:47,792 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.6812 
[2023-08-08 00:25:49,813 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.4219 
[2023-08-08 00:25:52,521 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.1274 
[2023-08-08 00:25:55,229 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.3890 
[2023-08-08 00:25:57,078 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.6112 
[2023-08-08 00:25:57,826 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.1125 
[2023-08-08 00:25:58,712 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.7208 
[2023-08-08 00:26:00,972 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3134 
[2023-08-08 00:26:02,938 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.7564 
[2023-08-08 00:26:04,785 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.5901 
[2023-08-08 00:26:06,718 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.6916 
[2023-08-08 00:26:06,767 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2402/0.3231/0.6908.
[2023-08-08 00:26:06,768 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6536/0.9593
[2023-08-08 00:26:06,768 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9358/0.9915
[2023-08-08 00:26:06,768 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1843/0.4117
[2023-08-08 00:26:06,768 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1724/0.2656
[2023-08-08 00:26:06,768 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6206/0.7291
[2023-08-08 00:26:06,768 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.3655/0.5450
[2023-08-08 00:26:06,768 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5586/0.6505
[2023-08-08 00:26:06,768 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0688/0.0767
[2023-08-08 00:26:06,768 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1371/0.3688
[2023-08-08 00:26:06,768 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.1025/0.1038
[2023-08-08 00:26:06,768 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 00:26:06,768 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0048/0.0048
[2023-08-08 00:26:06,768 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.2071/0.3512
[2023-08-08 00:26:06,768 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0419/0.0451
[2023-08-08 00:26:06,768 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0367/0.0458
[2023-08-08 00:26:06,768 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.1713/0.1714
[2023-08-08 00:26:06,768 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0526/0.0573
[2023-08-08 00:26:06,768 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.3713/0.4059
[2023-08-08 00:26:06,768 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 00:26:06,768 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.1196/0.2776
[2023-08-08 00:26:06,769 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 00:26:06,769 INFO misc.py line 150 22900] Best validation mIoU updated to: 0.2402
[2023-08-08 00:26:06,769 INFO misc.py line 152 22900] Currently Best mIoU: 0.2402
[2023-08-08 00:26:06,769 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 00:26:12,355 INFO misc.py line 115 22900] Train: [37/100][1/156] Data 0.850 (0.850) Batch 4.548 (4.548) Remain 12:36:38 loss: 0.3955 Lr: 0.03797
[2023-08-08 00:26:15,294 INFO misc.py line 115 22900] Train: [37/100][2/156] Data 0.001 (0.001) Batch 2.939 (2.939) Remain 08:08:56 loss: 0.4251 Lr: 0.03796
[2023-08-08 00:26:19,671 INFO misc.py line 115 22900] Train: [37/100][3/156] Data 0.001 (0.001) Batch 4.377 (4.377) Remain 12:08:02 loss: 0.5563 Lr: 0.03796
[2023-08-08 00:26:23,069 INFO misc.py line 115 22900] Train: [37/100][4/156] Data 0.001 (0.001) Batch 3.399 (3.399) Remain 09:25:17 loss: 0.3854 Lr: 0.03795
[2023-08-08 00:26:26,405 INFO misc.py line 115 22900] Train: [37/100][5/156] Data 0.001 (0.001) Batch 3.336 (3.367) Remain 09:19:59 loss: 0.3816 Lr: 0.03795
[2023-08-08 00:26:31,056 INFO misc.py line 115 22900] Train: [37/100][6/156] Data 0.001 (0.001) Batch 4.652 (3.795) Remain 10:31:08 loss: 0.3961 Lr: 0.03794
[2023-08-08 00:26:36,427 INFO misc.py line 115 22900] Train: [37/100][7/156] Data 0.001 (0.001) Batch 5.371 (4.189) Remain 11:36:34 loss: 0.5127 Lr: 0.03794
[2023-08-08 00:26:39,947 INFO misc.py line 115 22900] Train: [37/100][8/156] Data 0.001 (0.001) Batch 3.520 (4.055) Remain 11:14:15 loss: 0.2309 Lr: 0.03793
[2023-08-08 00:26:44,333 INFO misc.py line 115 22900] Train: [37/100][9/156] Data 0.002 (0.001) Batch 4.386 (4.110) Remain 11:23:21 loss: 0.3933 Lr: 0.03793
[2023-08-08 00:26:47,635 INFO misc.py line 115 22900] Train: [37/100][10/156] Data 0.001 (0.001) Batch 3.301 (3.995) Remain 11:04:04 loss: 0.3685 Lr: 0.03792
[2023-08-08 00:26:51,836 INFO misc.py line 115 22900] Train: [37/100][11/156] Data 0.001 (0.001) Batch 4.201 (4.021) Remain 11:08:17 loss: 0.5010 Lr: 0.03792
[2023-08-08 00:26:55,422 INFO misc.py line 115 22900] Train: [37/100][12/156] Data 0.001 (0.001) Batch 3.587 (3.972) Remain 11:00:13 loss: 0.3216 Lr: 0.03792
[2023-08-08 00:26:58,173 INFO misc.py line 115 22900] Train: [37/100][13/156] Data 0.001 (0.001) Batch 2.750 (3.850) Remain 10:39:50 loss: 0.3596 Lr: 0.03791
[2023-08-08 00:27:02,178 INFO misc.py line 115 22900] Train: [37/100][14/156] Data 0.001 (0.001) Batch 4.006 (3.864) Remain 10:42:07 loss: 0.6567 Lr: 0.03791
[2023-08-08 00:27:06,254 INFO misc.py line 115 22900] Train: [37/100][15/156] Data 0.001 (0.001) Batch 4.076 (3.882) Remain 10:44:59 loss: 0.7949 Lr: 0.03790
[2023-08-08 00:27:09,522 INFO misc.py line 115 22900] Train: [37/100][16/156] Data 0.001 (0.001) Batch 3.267 (3.835) Remain 10:37:04 loss: 0.4239 Lr: 0.03790
[2023-08-08 00:27:13,507 INFO misc.py line 115 22900] Train: [37/100][17/156] Data 0.001 (0.001) Batch 3.986 (3.845) Remain 10:38:47 loss: 0.4465 Lr: 0.03789
[2023-08-08 00:27:16,887 INFO misc.py line 115 22900] Train: [37/100][18/156] Data 0.001 (0.001) Batch 3.380 (3.814) Remain 10:33:34 loss: 0.4859 Lr: 0.03789
[2023-08-08 00:27:20,021 INFO misc.py line 115 22900] Train: [37/100][19/156] Data 0.001 (0.001) Batch 3.134 (3.772) Remain 10:26:26 loss: 0.1537 Lr: 0.03788
[2023-08-08 00:27:22,547 INFO misc.py line 115 22900] Train: [37/100][20/156] Data 0.001 (0.001) Batch 2.526 (3.699) Remain 10:14:12 loss: 0.2567 Lr: 0.03788
[2023-08-08 00:27:25,582 INFO misc.py line 115 22900] Train: [37/100][21/156] Data 0.001 (0.001) Batch 3.035 (3.662) Remain 10:08:01 loss: 0.2775 Lr: 0.03787
[2023-08-08 00:27:27,771 INFO misc.py line 115 22900] Train: [37/100][22/156] Data 0.001 (0.001) Batch 2.189 (3.584) Remain 09:55:06 loss: 0.2513 Lr: 0.03787
[2023-08-08 00:27:31,750 INFO misc.py line 115 22900] Train: [37/100][23/156] Data 0.001 (0.001) Batch 3.979 (3.604) Remain 09:58:19 loss: 0.4454 Lr: 0.03787
[2023-08-08 00:27:35,004 INFO misc.py line 115 22900] Train: [37/100][24/156] Data 0.001 (0.001) Batch 3.254 (3.587) Remain 09:55:29 loss: 0.3709 Lr: 0.03786
[2023-08-08 00:27:37,226 INFO misc.py line 115 22900] Train: [37/100][25/156] Data 0.001 (0.001) Batch 2.222 (3.525) Remain 09:45:07 loss: 0.1514 Lr: 0.03786
[2023-08-08 00:27:40,026 INFO misc.py line 115 22900] Train: [37/100][26/156] Data 0.001 (0.001) Batch 2.800 (3.494) Remain 09:39:50 loss: 0.2872 Lr: 0.03785
[2023-08-08 00:27:43,222 INFO misc.py line 115 22900] Train: [37/100][27/156] Data 0.001 (0.001) Batch 3.196 (3.481) Remain 09:37:43 loss: 0.1346 Lr: 0.03785
[2023-08-08 00:27:47,404 INFO misc.py line 115 22900] Train: [37/100][28/156] Data 0.001 (0.001) Batch 4.182 (3.509) Remain 09:42:18 loss: 0.5102 Lr: 0.03784
[2023-08-08 00:27:51,102 INFO misc.py line 115 22900] Train: [37/100][29/156] Data 0.001 (0.001) Batch 3.698 (3.517) Remain 09:43:27 loss: 0.3569 Lr: 0.03784
[2023-08-08 00:27:54,817 INFO misc.py line 115 22900] Train: [37/100][30/156] Data 0.001 (0.001) Batch 3.715 (3.524) Remain 09:44:37 loss: 0.3032 Lr: 0.03783
[2023-08-08 00:27:58,160 INFO misc.py line 115 22900] Train: [37/100][31/156] Data 0.001 (0.001) Batch 3.343 (3.517) Remain 09:43:29 loss: 0.2893 Lr: 0.03783
[2023-08-08 00:28:02,203 INFO misc.py line 115 22900] Train: [37/100][32/156] Data 0.001 (0.001) Batch 4.043 (3.536) Remain 09:46:26 loss: 0.3966 Lr: 0.03782
[2023-08-08 00:28:04,453 INFO misc.py line 115 22900] Train: [37/100][33/156] Data 0.001 (0.001) Batch 2.250 (3.493) Remain 09:39:16 loss: 0.1568 Lr: 0.03782
[2023-08-08 00:28:07,491 INFO misc.py line 115 22900] Train: [37/100][34/156] Data 0.001 (0.001) Batch 3.037 (3.478) Remain 09:36:46 loss: 0.2919 Lr: 0.03782
[2023-08-08 00:28:10,722 INFO misc.py line 115 22900] Train: [37/100][35/156] Data 0.001 (0.001) Batch 3.231 (3.470) Remain 09:35:26 loss: 0.1707 Lr: 0.03781
[2023-08-08 00:28:14,670 INFO misc.py line 115 22900] Train: [37/100][36/156] Data 0.001 (0.001) Batch 3.948 (3.485) Remain 09:37:47 loss: 0.4418 Lr: 0.03781
[2023-08-08 00:28:17,171 INFO misc.py line 115 22900] Train: [37/100][37/156] Data 0.001 (0.001) Batch 2.500 (3.456) Remain 09:32:55 loss: 0.2144 Lr: 0.03780
[2023-08-08 00:28:21,240 INFO misc.py line 115 22900] Train: [37/100][38/156] Data 0.001 (0.001) Batch 4.069 (3.473) Remain 09:35:46 loss: 0.4029 Lr: 0.03780
[2023-08-08 00:28:25,656 INFO misc.py line 115 22900] Train: [37/100][39/156] Data 0.001 (0.001) Batch 4.416 (3.500) Remain 09:40:03 loss: 0.4335 Lr: 0.03779
[2023-08-08 00:28:28,645 INFO misc.py line 115 22900] Train: [37/100][40/156] Data 0.001 (0.001) Batch 2.989 (3.486) Remain 09:37:42 loss: 0.1889 Lr: 0.03779
[2023-08-08 00:28:31,858 INFO misc.py line 115 22900] Train: [37/100][41/156] Data 0.001 (0.001) Batch 3.213 (3.479) Remain 09:36:27 loss: 0.1722 Lr: 0.03778
[2023-08-08 00:28:34,529 INFO misc.py line 115 22900] Train: [37/100][42/156] Data 0.001 (0.001) Batch 2.671 (3.458) Remain 09:32:58 loss: 0.2245 Lr: 0.03778
[2023-08-08 00:28:36,331 INFO misc.py line 115 22900] Train: [37/100][43/156] Data 0.001 (0.001) Batch 1.802 (3.417) Remain 09:26:03 loss: 0.3115 Lr: 0.03777
[2023-08-08 00:28:40,898 INFO misc.py line 115 22900] Train: [37/100][44/156] Data 0.001 (0.001) Batch 4.566 (3.445) Remain 09:30:38 loss: 0.4510 Lr: 0.03777
[2023-08-08 00:28:44,834 INFO misc.py line 115 22900] Train: [37/100][45/156] Data 0.001 (0.001) Batch 3.937 (3.456) Remain 09:32:31 loss: 0.2923 Lr: 0.03777
[2023-08-08 00:28:47,987 INFO misc.py line 115 22900] Train: [37/100][46/156] Data 0.001 (0.001) Batch 3.153 (3.449) Remain 09:31:18 loss: 0.3389 Lr: 0.03776
[2023-08-08 00:28:50,664 INFO misc.py line 115 22900] Train: [37/100][47/156] Data 0.001 (0.001) Batch 2.677 (3.432) Remain 09:28:20 loss: 0.1752 Lr: 0.03776
[2023-08-08 00:28:54,709 INFO misc.py line 115 22900] Train: [37/100][48/156] Data 0.001 (0.001) Batch 4.045 (3.445) Remain 09:30:32 loss: 0.4244 Lr: 0.03775
[2023-08-08 00:28:58,163 INFO misc.py line 115 22900] Train: [37/100][49/156] Data 0.001 (0.001) Batch 3.454 (3.445) Remain 09:30:30 loss: 0.2992 Lr: 0.03775
[2023-08-08 00:29:01,242 INFO misc.py line 115 22900] Train: [37/100][50/156] Data 0.001 (0.001) Batch 3.079 (3.438) Remain 09:29:09 loss: 0.3083 Lr: 0.03774
[2023-08-08 00:29:04,716 INFO misc.py line 115 22900] Train: [37/100][51/156] Data 0.001 (0.001) Batch 3.474 (3.438) Remain 09:29:14 loss: 0.1988 Lr: 0.03774
[2023-08-08 00:29:08,240 INFO misc.py line 115 22900] Train: [37/100][52/156] Data 0.001 (0.001) Batch 3.524 (3.440) Remain 09:29:27 loss: 0.4066 Lr: 0.03773
[2023-08-08 00:29:10,988 INFO misc.py line 115 22900] Train: [37/100][53/156] Data 0.001 (0.001) Batch 2.748 (3.426) Remain 09:27:07 loss: 0.2925 Lr: 0.03773
[2023-08-08 00:29:15,017 INFO misc.py line 115 22900] Train: [37/100][54/156] Data 0.001 (0.001) Batch 4.029 (3.438) Remain 09:29:00 loss: 0.8151 Lr: 0.03772
[2023-08-08 00:29:17,575 INFO misc.py line 115 22900] Train: [37/100][55/156] Data 0.001 (0.001) Batch 2.558 (3.421) Remain 09:26:09 loss: 0.1875 Lr: 0.03772
[2023-08-08 00:29:20,410 INFO misc.py line 115 22900] Train: [37/100][56/156] Data 0.001 (0.001) Batch 2.835 (3.410) Remain 09:24:16 loss: 0.2520 Lr: 0.03772
[2023-08-08 00:29:23,737 INFO misc.py line 115 22900] Train: [37/100][57/156] Data 0.001 (0.001) Batch 3.327 (3.409) Remain 09:23:57 loss: 0.4078 Lr: 0.03771
[2023-08-08 00:29:27,773 INFO misc.py line 115 22900] Train: [37/100][58/156] Data 0.001 (0.001) Batch 4.036 (3.420) Remain 09:25:47 loss: 0.4730 Lr: 0.03771
[2023-08-08 00:29:30,703 INFO misc.py line 115 22900] Train: [37/100][59/156] Data 0.001 (0.001) Batch 2.929 (3.411) Remain 09:24:16 loss: 0.1649 Lr: 0.03770
[2023-08-08 00:29:34,214 INFO misc.py line 115 22900] Train: [37/100][60/156] Data 0.001 (0.001) Batch 3.511 (3.413) Remain 09:24:30 loss: 0.2635 Lr: 0.03770
[2023-08-08 00:29:37,722 INFO misc.py line 115 22900] Train: [37/100][61/156] Data 0.001 (0.001) Batch 3.508 (3.415) Remain 09:24:43 loss: 0.3003 Lr: 0.03769
[2023-08-08 00:29:41,150 INFO misc.py line 115 22900] Train: [37/100][62/156] Data 0.001 (0.001) Batch 3.428 (3.415) Remain 09:24:42 loss: 0.3558 Lr: 0.03769
[2023-08-08 00:29:44,586 INFO misc.py line 115 22900] Train: [37/100][63/156] Data 0.001 (0.001) Batch 3.437 (3.415) Remain 09:24:42 loss: 0.3064 Lr: 0.03768
[2023-08-08 00:29:48,532 INFO misc.py line 115 22900] Train: [37/100][64/156] Data 0.001 (0.001) Batch 3.946 (3.424) Remain 09:26:05 loss: 0.3250 Lr: 0.03768
[2023-08-08 00:29:52,331 INFO misc.py line 115 22900] Train: [37/100][65/156] Data 0.001 (0.001) Batch 3.798 (3.430) Remain 09:27:02 loss: 0.3901 Lr: 0.03767
[2023-08-08 00:29:56,504 INFO misc.py line 115 22900] Train: [37/100][66/156] Data 0.001 (0.001) Batch 4.173 (3.442) Remain 09:28:55 loss: 0.2290 Lr: 0.03767
[2023-08-08 00:30:00,568 INFO misc.py line 115 22900] Train: [37/100][67/156] Data 0.001 (0.001) Batch 4.063 (3.452) Remain 09:30:28 loss: 0.8664 Lr: 0.03766
[2023-08-08 00:30:03,734 INFO misc.py line 115 22900] Train: [37/100][68/156] Data 0.001 (0.001) Batch 3.167 (3.447) Remain 09:29:41 loss: 0.1505 Lr: 0.03766
[2023-08-08 00:30:07,810 INFO misc.py line 115 22900] Train: [37/100][69/156] Data 0.001 (0.001) Batch 4.076 (3.457) Remain 09:31:12 loss: 0.4468 Lr: 0.03766
[2023-08-08 00:30:10,938 INFO misc.py line 115 22900] Train: [37/100][70/156] Data 0.001 (0.001) Batch 3.129 (3.452) Remain 09:30:20 loss: 0.1804 Lr: 0.03765
[2023-08-08 00:30:16,057 INFO misc.py line 115 22900] Train: [37/100][71/156] Data 0.001 (0.001) Batch 5.119 (3.476) Remain 09:34:20 loss: 0.7778 Lr: 0.03765
[2023-08-08 00:30:20,070 INFO misc.py line 115 22900] Train: [37/100][72/156] Data 0.001 (0.001) Batch 4.012 (3.484) Remain 09:35:33 loss: 0.2286 Lr: 0.03764
[2023-08-08 00:30:23,374 INFO misc.py line 115 22900] Train: [37/100][73/156] Data 0.001 (0.001) Batch 3.304 (3.481) Remain 09:35:04 loss: 0.1420 Lr: 0.03764
[2023-08-08 00:30:25,810 INFO misc.py line 115 22900] Train: [37/100][74/156] Data 0.001 (0.001) Batch 2.437 (3.467) Remain 09:32:35 loss: 0.2276 Lr: 0.03763
[2023-08-08 00:30:29,519 INFO misc.py line 115 22900] Train: [37/100][75/156] Data 0.001 (0.001) Batch 3.709 (3.470) Remain 09:33:05 loss: 0.3669 Lr: 0.03763
[2023-08-08 00:30:33,588 INFO misc.py line 115 22900] Train: [37/100][76/156] Data 0.001 (0.001) Batch 4.068 (3.478) Remain 09:34:23 loss: 0.5858 Lr: 0.03762
[2023-08-08 00:30:36,633 INFO misc.py line 115 22900] Train: [37/100][77/156] Data 0.001 (0.001) Batch 3.045 (3.472) Remain 09:33:21 loss: 0.4798 Lr: 0.03762
[2023-08-08 00:30:38,520 INFO misc.py line 115 22900] Train: [37/100][78/156] Data 0.001 (0.001) Batch 1.888 (3.451) Remain 09:29:48 loss: 0.1533 Lr: 0.03761
[2023-08-08 00:30:41,916 INFO misc.py line 115 22900] Train: [37/100][79/156] Data 0.001 (0.001) Batch 3.396 (3.451) Remain 09:29:38 loss: 0.1291 Lr: 0.03761
[2023-08-08 00:30:45,307 INFO misc.py line 115 22900] Train: [37/100][80/156] Data 0.001 (0.001) Batch 3.391 (3.450) Remain 09:29:27 loss: 0.5675 Lr: 0.03761
[2023-08-08 00:30:49,134 INFO misc.py line 115 22900] Train: [37/100][81/156] Data 0.001 (0.001) Batch 3.827 (3.455) Remain 09:30:11 loss: 0.4239 Lr: 0.03760
[2023-08-08 00:30:52,728 INFO misc.py line 115 22900] Train: [37/100][82/156] Data 0.001 (0.001) Batch 3.594 (3.456) Remain 09:30:25 loss: 0.3906 Lr: 0.03760
[2023-08-08 00:30:56,327 INFO misc.py line 115 22900] Train: [37/100][83/156] Data 0.001 (0.001) Batch 3.599 (3.458) Remain 09:30:39 loss: 0.2789 Lr: 0.03759
[2023-08-08 00:31:00,304 INFO misc.py line 115 22900] Train: [37/100][84/156] Data 0.001 (0.001) Batch 3.977 (3.465) Remain 09:31:39 loss: 0.3899 Lr: 0.03759
[2023-08-08 00:31:04,382 INFO misc.py line 115 22900] Train: [37/100][85/156] Data 0.001 (0.001) Batch 4.078 (3.472) Remain 09:32:50 loss: 0.3469 Lr: 0.03758
[2023-08-08 00:31:08,449 INFO misc.py line 115 22900] Train: [37/100][86/156] Data 0.001 (0.001) Batch 4.066 (3.479) Remain 09:33:57 loss: 0.2989 Lr: 0.03758
[2023-08-08 00:31:12,305 INFO misc.py line 115 22900] Train: [37/100][87/156] Data 0.001 (0.001) Batch 3.856 (3.484) Remain 09:34:38 loss: 0.2664 Lr: 0.03757
[2023-08-08 00:31:16,366 INFO misc.py line 115 22900] Train: [37/100][88/156] Data 0.001 (0.001) Batch 4.061 (3.491) Remain 09:35:42 loss: 0.3077 Lr: 0.03757
[2023-08-08 00:31:19,823 INFO misc.py line 115 22900] Train: [37/100][89/156] Data 0.001 (0.001) Batch 3.456 (3.490) Remain 09:35:34 loss: 0.3954 Lr: 0.03756
[2023-08-08 00:31:22,029 INFO misc.py line 115 22900] Train: [37/100][90/156] Data 0.001 (0.001) Batch 2.207 (3.475) Remain 09:33:05 loss: 0.1973 Lr: 0.03756
[2023-08-08 00:31:23,899 INFO misc.py line 115 22900] Train: [37/100][91/156] Data 0.001 (0.001) Batch 1.870 (3.457) Remain 09:30:01 loss: 0.1708 Lr: 0.03756
[2023-08-08 00:31:27,159 INFO misc.py line 115 22900] Train: [37/100][92/156] Data 0.001 (0.001) Batch 3.260 (3.455) Remain 09:29:36 loss: 0.1833 Lr: 0.03755
[2023-08-08 00:31:31,267 INFO misc.py line 115 22900] Train: [37/100][93/156] Data 0.001 (0.001) Batch 4.109 (3.462) Remain 09:30:44 loss: 0.3742 Lr: 0.03755
[2023-08-08 00:31:34,756 INFO misc.py line 115 22900] Train: [37/100][94/156] Data 0.001 (0.001) Batch 3.488 (3.462) Remain 09:30:43 loss: 0.4473 Lr: 0.03754
[2023-08-08 00:31:37,887 INFO misc.py line 115 22900] Train: [37/100][95/156] Data 0.001 (0.001) Batch 3.131 (3.459) Remain 09:30:04 loss: 0.1961 Lr: 0.03754
[2023-08-08 00:31:41,031 INFO misc.py line 115 22900] Train: [37/100][96/156] Data 0.001 (0.001) Batch 3.144 (3.455) Remain 09:29:27 loss: 0.2896 Lr: 0.03753
[2023-08-08 00:31:44,930 INFO misc.py line 115 22900] Train: [37/100][97/156] Data 0.001 (0.001) Batch 3.899 (3.460) Remain 09:30:11 loss: 0.2805 Lr: 0.03753
[2023-08-08 00:31:48,623 INFO misc.py line 115 22900] Train: [37/100][98/156] Data 0.001 (0.001) Batch 3.694 (3.463) Remain 09:30:31 loss: 0.4449 Lr: 0.03752
[2023-08-08 00:31:51,741 INFO misc.py line 115 22900] Train: [37/100][99/156] Data 0.001 (0.001) Batch 3.118 (3.459) Remain 09:29:52 loss: 0.2980 Lr: 0.03752
[2023-08-08 00:31:55,901 INFO misc.py line 115 22900] Train: [37/100][100/156] Data 0.001 (0.001) Batch 4.160 (3.466) Remain 09:31:00 loss: 0.3357 Lr: 0.03751
[2023-08-08 00:31:59,930 INFO misc.py line 115 22900] Train: [37/100][101/156] Data 0.001 (0.001) Batch 4.029 (3.472) Remain 09:31:54 loss: 0.7850 Lr: 0.03751
[2023-08-08 00:32:03,737 INFO misc.py line 115 22900] Train: [37/100][102/156] Data 0.001 (0.001) Batch 3.808 (3.475) Remain 09:32:24 loss: 0.5452 Lr: 0.03750
[2023-08-08 00:32:07,539 INFO misc.py line 115 22900] Train: [37/100][103/156] Data 0.001 (0.001) Batch 3.801 (3.479) Remain 09:32:52 loss: 0.3163 Lr: 0.03750
[2023-08-08 00:32:11,126 INFO misc.py line 115 22900] Train: [37/100][104/156] Data 0.001 (0.001) Batch 3.588 (3.480) Remain 09:33:00 loss: 0.2487 Lr: 0.03750
[2023-08-08 00:32:15,062 INFO misc.py line 115 22900] Train: [37/100][105/156] Data 0.001 (0.001) Batch 3.936 (3.484) Remain 09:33:40 loss: 0.2864 Lr: 0.03749
[2023-08-08 00:32:19,090 INFO misc.py line 115 22900] Train: [37/100][106/156] Data 0.001 (0.001) Batch 4.028 (3.490) Remain 09:34:29 loss: 0.4814 Lr: 0.03749
[2023-08-08 00:32:22,909 INFO misc.py line 115 22900] Train: [37/100][107/156] Data 0.001 (0.001) Batch 3.819 (3.493) Remain 09:34:57 loss: 0.2703 Lr: 0.03748
[2023-08-08 00:32:26,859 INFO misc.py line 115 22900] Train: [37/100][108/156] Data 0.001 (0.001) Batch 3.950 (3.497) Remain 09:35:36 loss: 0.5794 Lr: 0.03748
[2023-08-08 00:32:30,005 INFO misc.py line 115 22900] Train: [37/100][109/156] Data 0.001 (0.001) Batch 3.146 (3.494) Remain 09:35:00 loss: 0.2633 Lr: 0.03747
[2023-08-08 00:32:33,689 INFO misc.py line 115 22900] Train: [37/100][110/156] Data 0.001 (0.001) Batch 3.684 (3.495) Remain 09:35:14 loss: 0.5133 Lr: 0.03747
[2023-08-08 00:32:37,432 INFO misc.py line 115 22900] Train: [37/100][111/156] Data 0.001 (0.001) Batch 3.743 (3.498) Remain 09:35:33 loss: 0.4586 Lr: 0.03746
[2023-08-08 00:32:41,545 INFO misc.py line 115 22900] Train: [37/100][112/156] Data 0.001 (0.001) Batch 4.113 (3.503) Remain 09:36:25 loss: 0.3925 Lr: 0.03746
[2023-08-08 00:32:43,260 INFO misc.py line 115 22900] Train: [37/100][113/156] Data 0.001 (0.001) Batch 1.715 (3.487) Remain 09:33:41 loss: 0.2100 Lr: 0.03745
[2023-08-08 00:32:46,478 INFO misc.py line 115 22900] Train: [37/100][114/156] Data 0.001 (0.001) Batch 3.218 (3.485) Remain 09:33:14 loss: 0.4346 Lr: 0.03745
[2023-08-08 00:32:50,622 INFO misc.py line 115 22900] Train: [37/100][115/156] Data 0.001 (0.001) Batch 4.144 (3.491) Remain 09:34:09 loss: 0.3783 Lr: 0.03745
[2023-08-08 00:32:53,873 INFO misc.py line 115 22900] Train: [37/100][116/156] Data 0.001 (0.001) Batch 3.251 (3.489) Remain 09:33:44 loss: 0.4993 Lr: 0.03744
[2023-08-08 00:32:57,406 INFO misc.py line 115 22900] Train: [37/100][117/156] Data 0.001 (0.001) Batch 3.533 (3.489) Remain 09:33:45 loss: 0.3329 Lr: 0.03744
[2023-08-08 00:33:00,954 INFO misc.py line 115 22900] Train: [37/100][118/156] Data 0.001 (0.001) Batch 3.547 (3.489) Remain 09:33:46 loss: 0.4544 Lr: 0.03743
[2023-08-08 00:33:05,006 INFO misc.py line 115 22900] Train: [37/100][119/156] Data 0.001 (0.001) Batch 4.052 (3.494) Remain 09:34:30 loss: 0.4460 Lr: 0.03743
[2023-08-08 00:33:07,700 INFO misc.py line 115 22900] Train: [37/100][120/156] Data 0.001 (0.001) Batch 2.694 (3.487) Remain 09:33:20 loss: 0.3510 Lr: 0.03742
[2023-08-08 00:33:11,281 INFO misc.py line 115 22900] Train: [37/100][121/156] Data 0.001 (0.001) Batch 3.580 (3.488) Remain 09:33:24 loss: 0.2262 Lr: 0.03742
[2023-08-08 00:33:14,876 INFO misc.py line 115 22900] Train: [37/100][122/156] Data 0.001 (0.001) Batch 3.595 (3.489) Remain 09:33:29 loss: 0.5560 Lr: 0.03741
[2023-08-08 00:33:18,866 INFO misc.py line 115 22900] Train: [37/100][123/156] Data 0.001 (0.001) Batch 3.990 (3.493) Remain 09:34:07 loss: 0.2770 Lr: 0.03741
[2023-08-08 00:33:21,580 INFO misc.py line 115 22900] Train: [37/100][124/156] Data 0.001 (0.001) Batch 2.714 (3.487) Remain 09:33:00 loss: 0.3124 Lr: 0.03740
[2023-08-08 00:33:25,324 INFO misc.py line 115 22900] Train: [37/100][125/156] Data 0.001 (0.001) Batch 3.744 (3.489) Remain 09:33:17 loss: 0.2937 Lr: 0.03740
[2023-08-08 00:33:28,174 INFO misc.py line 115 22900] Train: [37/100][126/156] Data 0.001 (0.001) Batch 2.850 (3.484) Remain 09:32:22 loss: 0.2033 Lr: 0.03739
[2023-08-08 00:33:30,966 INFO misc.py line 115 22900] Train: [37/100][127/156] Data 0.001 (0.001) Batch 2.793 (3.478) Remain 09:31:24 loss: 0.2264 Lr: 0.03739
[2023-08-08 00:33:33,644 INFO misc.py line 115 22900] Train: [37/100][128/156] Data 0.001 (0.001) Batch 2.678 (3.472) Remain 09:30:17 loss: 0.3647 Lr: 0.03739
[2023-08-08 00:33:37,933 INFO misc.py line 115 22900] Train: [37/100][129/156] Data 0.001 (0.001) Batch 4.288 (3.478) Remain 09:31:18 loss: 0.4964 Lr: 0.03738
[2023-08-08 00:33:41,966 INFO misc.py line 115 22900] Train: [37/100][130/156] Data 0.001 (0.001) Batch 4.033 (3.483) Remain 09:31:57 loss: 0.3665 Lr: 0.03738
[2023-08-08 00:33:44,547 INFO misc.py line 115 22900] Train: [37/100][131/156] Data 0.001 (0.001) Batch 2.581 (3.476) Remain 09:30:45 loss: 0.3359 Lr: 0.03737
[2023-08-08 00:33:48,200 INFO misc.py line 115 22900] Train: [37/100][132/156] Data 0.001 (0.001) Batch 3.653 (3.477) Remain 09:30:55 loss: 0.3146 Lr: 0.03737
[2023-08-08 00:33:51,121 INFO misc.py line 115 22900] Train: [37/100][133/156] Data 0.001 (0.001) Batch 2.921 (3.473) Remain 09:30:09 loss: 0.4092 Lr: 0.03736
[2023-08-08 00:33:54,771 INFO misc.py line 115 22900] Train: [37/100][134/156] Data 0.001 (0.001) Batch 3.650 (3.474) Remain 09:30:19 loss: 0.3047 Lr: 0.03736
[2023-08-08 00:33:58,756 INFO misc.py line 115 22900] Train: [37/100][135/156] Data 0.001 (0.001) Batch 3.985 (3.478) Remain 09:30:54 loss: 0.3580 Lr: 0.03735
[2023-08-08 00:34:02,094 INFO misc.py line 115 22900] Train: [37/100][136/156] Data 0.001 (0.001) Batch 3.338 (3.477) Remain 09:30:40 loss: 0.3228 Lr: 0.03735
[2023-08-08 00:34:05,431 INFO misc.py line 115 22900] Train: [37/100][137/156] Data 0.001 (0.001) Batch 3.337 (3.476) Remain 09:30:26 loss: 0.2418 Lr: 0.03734
[2023-08-08 00:34:09,242 INFO misc.py line 115 22900] Train: [37/100][138/156] Data 0.001 (0.001) Batch 3.811 (3.478) Remain 09:30:47 loss: 0.4338 Lr: 0.03734
[2023-08-08 00:34:11,992 INFO misc.py line 115 22900] Train: [37/100][139/156] Data 0.001 (0.001) Batch 2.750 (3.473) Remain 09:29:51 loss: 0.3676 Lr: 0.03733
[2023-08-08 00:34:14,051 INFO misc.py line 115 22900] Train: [37/100][140/156] Data 0.001 (0.001) Batch 2.059 (3.463) Remain 09:28:06 loss: 0.2934 Lr: 0.03733
[2023-08-08 00:34:16,261 INFO misc.py line 115 22900] Train: [37/100][141/156] Data 0.001 (0.001) Batch 2.210 (3.454) Remain 09:26:33 loss: 0.1251 Lr: 0.03733
[2023-08-08 00:34:19,519 INFO misc.py line 115 22900] Train: [37/100][142/156] Data 0.001 (0.001) Batch 3.258 (3.452) Remain 09:26:16 loss: 0.3846 Lr: 0.03732
[2023-08-08 00:34:22,786 INFO misc.py line 115 22900] Train: [37/100][143/156] Data 0.001 (0.001) Batch 3.267 (3.451) Remain 09:25:59 loss: 0.3381 Lr: 0.03732
[2023-08-08 00:34:25,778 INFO misc.py line 115 22900] Train: [37/100][144/156] Data 0.001 (0.001) Batch 2.992 (3.448) Remain 09:25:24 loss: 0.2184 Lr: 0.03731
[2023-08-08 00:34:29,768 INFO misc.py line 115 22900] Train: [37/100][145/156] Data 0.001 (0.001) Batch 3.990 (3.451) Remain 09:25:58 loss: 0.2554 Lr: 0.03731
[2023-08-08 00:34:33,087 INFO misc.py line 115 22900] Train: [37/100][146/156] Data 0.001 (0.001) Batch 3.318 (3.450) Remain 09:25:45 loss: 0.3577 Lr: 0.03730
[2023-08-08 00:34:36,649 INFO misc.py line 115 22900] Train: [37/100][147/156] Data 0.001 (0.001) Batch 3.563 (3.451) Remain 09:25:49 loss: 0.3695 Lr: 0.03730
[2023-08-08 00:34:39,693 INFO misc.py line 115 22900] Train: [37/100][148/156] Data 0.001 (0.001) Batch 3.044 (3.448) Remain 09:25:18 loss: 0.3095 Lr: 0.03729
[2023-08-08 00:34:43,790 INFO misc.py line 115 22900] Train: [37/100][149/156] Data 0.001 (0.001) Batch 4.096 (3.453) Remain 09:25:58 loss: 0.5213 Lr: 0.03729
[2023-08-08 00:34:47,893 INFO misc.py line 115 22900] Train: [37/100][150/156] Data 0.001 (0.001) Batch 4.104 (3.457) Remain 09:26:39 loss: 0.4395 Lr: 0.03728
[2023-08-08 00:34:51,270 INFO misc.py line 115 22900] Train: [37/100][151/156] Data 0.001 (0.001) Batch 3.376 (3.457) Remain 09:26:30 loss: 0.3513 Lr: 0.03728
[2023-08-08 00:34:55,074 INFO misc.py line 115 22900] Train: [37/100][152/156] Data 0.001 (0.001) Batch 3.805 (3.459) Remain 09:26:49 loss: 0.4059 Lr: 0.03727
[2023-08-08 00:34:58,045 INFO misc.py line 115 22900] Train: [37/100][153/156] Data 0.001 (0.001) Batch 2.971 (3.456) Remain 09:26:14 loss: 0.3292 Lr: 0.03727
[2023-08-08 00:35:02,101 INFO misc.py line 115 22900] Train: [37/100][154/156] Data 0.001 (0.001) Batch 4.056 (3.460) Remain 09:26:49 loss: 0.3509 Lr: 0.03727
[2023-08-08 00:35:06,503 INFO misc.py line 115 22900] Train: [37/100][155/156] Data 0.001 (0.001) Batch 4.402 (3.466) Remain 09:27:47 loss: 0.4121 Lr: 0.03726
[2023-08-08 00:35:10,648 INFO misc.py line 115 22900] Train: [37/100][156/156] Data 0.001 (0.001) Batch 4.145 (3.470) Remain 09:28:27 loss: 0.4297 Lr: 0.03726
[2023-08-08 00:35:10,648 INFO misc.py line 129 22900] Train result: loss: 0.3529 
[2023-08-08 00:35:10,649 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 00:35:12,733 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.8078 
[2023-08-08 00:35:13,601 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.4492 
[2023-08-08 00:35:15,266 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.5888 
[2023-08-08 00:35:16,786 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.3824 
[2023-08-08 00:35:18,632 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.8285 
[2023-08-08 00:35:20,291 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.6665 
[2023-08-08 00:35:22,432 INFO evaluator.py line 122 22900] Test: [7/24] Loss 5.1795 
[2023-08-08 00:35:24,235 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.8877 
[2023-08-08 00:35:25,518 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.5359 
[2023-08-08 00:35:27,647 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.2731 
[2023-08-08 00:35:28,173 INFO evaluator.py line 122 22900] Test: [11/24] Loss 0.7318 
[2023-08-08 00:35:29,706 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.6487 
[2023-08-08 00:35:32,418 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.1397 
[2023-08-08 00:35:34,097 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.7053 
[2023-08-08 00:35:36,119 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.3834 
[2023-08-08 00:35:38,829 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.1465 
[2023-08-08 00:35:41,535 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.3428 
[2023-08-08 00:35:43,383 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.5376 
[2023-08-08 00:35:44,131 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.0406 
[2023-08-08 00:35:45,015 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.7534 
[2023-08-08 00:35:47,275 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3902 
[2023-08-08 00:35:49,240 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.6476 
[2023-08-08 00:35:51,086 INFO evaluator.py line 122 22900] Test: [23/24] Loss 3.4027 
[2023-08-08 00:35:53,019 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.4182 
[2023-08-08 00:35:53,066 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2562/0.3898/0.6971.
[2023-08-08 00:35:53,066 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6406/0.9412
[2023-08-08 00:35:53,066 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9508/0.9882
[2023-08-08 00:35:53,066 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1770/0.4567
[2023-08-08 00:35:53,066 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1651/0.2810
[2023-08-08 00:35:53,066 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6213/0.7073
[2023-08-08 00:35:53,066 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.3986/0.6579
[2023-08-08 00:35:53,066 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5672/0.6526
[2023-08-08 00:35:53,066 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1706/0.1943
[2023-08-08 00:35:53,066 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1645/0.4022
[2023-08-08 00:35:53,066 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0755/0.0761
[2023-08-08 00:35:53,066 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0432/0.7219
[2023-08-08 00:35:53,067 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0660/0.0753
[2023-08-08 00:35:53,067 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.2789/0.5466
[2023-08-08 00:35:53,067 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.1235/0.1405
[2023-08-08 00:35:53,067 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0034/0.0037
[2023-08-08 00:35:53,067 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0122/0.0122
[2023-08-08 00:35:53,067 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.2015/0.2397
[2023-08-08 00:35:53,067 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.3867/0.5703
[2023-08-08 00:35:53,067 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 00:35:53,067 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0779/0.1292
[2023-08-08 00:35:53,067 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 00:35:53,067 INFO misc.py line 150 22900] Best validation mIoU updated to: 0.2562
[2023-08-08 00:35:53,067 INFO misc.py line 152 22900] Currently Best mIoU: 0.2562
[2023-08-08 00:35:53,067 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 00:35:57,715 INFO misc.py line 115 22900] Train: [38/100][1/156] Data 0.825 (0.825) Batch 3.601 (3.601) Remain 09:49:45 loss: 0.4468 Lr: 0.03725
[2023-08-08 00:36:02,149 INFO misc.py line 115 22900] Train: [38/100][2/156] Data 0.001 (0.001) Batch 4.434 (4.434) Remain 12:06:08 loss: 0.5006 Lr: 0.03725
[2023-08-08 00:36:05,975 INFO misc.py line 115 22900] Train: [38/100][3/156] Data 0.001 (0.001) Batch 3.825 (3.825) Remain 10:26:24 loss: 0.2864 Lr: 0.03724
[2023-08-08 00:36:10,004 INFO misc.py line 115 22900] Train: [38/100][4/156] Data 0.001 (0.001) Batch 4.029 (4.029) Remain 10:59:43 loss: 0.2337 Lr: 0.03724
[2023-08-08 00:36:14,091 INFO misc.py line 115 22900] Train: [38/100][5/156] Data 0.001 (0.001) Batch 4.087 (4.058) Remain 11:04:24 loss: 0.1782 Lr: 0.03723
[2023-08-08 00:36:17,091 INFO misc.py line 115 22900] Train: [38/100][6/156] Data 0.001 (0.001) Batch 3.000 (3.705) Remain 10:06:34 loss: 0.5030 Lr: 0.03723
[2023-08-08 00:36:20,372 INFO misc.py line 115 22900] Train: [38/100][7/156] Data 0.001 (0.001) Batch 3.281 (3.599) Remain 09:49:08 loss: 0.2144 Lr: 0.03722
[2023-08-08 00:36:23,433 INFO misc.py line 115 22900] Train: [38/100][8/156] Data 0.001 (0.001) Batch 3.062 (3.492) Remain 09:31:29 loss: 0.3360 Lr: 0.03722
[2023-08-08 00:36:25,620 INFO misc.py line 115 22900] Train: [38/100][9/156] Data 0.001 (0.001) Batch 2.186 (3.274) Remain 08:55:49 loss: 0.1895 Lr: 0.03721
[2023-08-08 00:36:28,361 INFO misc.py line 115 22900] Train: [38/100][10/156] Data 0.001 (0.001) Batch 2.741 (3.198) Remain 08:43:18 loss: 0.1532 Lr: 0.03721
[2023-08-08 00:36:32,165 INFO misc.py line 115 22900] Train: [38/100][11/156] Data 0.001 (0.001) Batch 3.804 (3.274) Remain 08:55:38 loss: 0.3825 Lr: 0.03721
[2023-08-08 00:36:36,201 INFO misc.py line 115 22900] Train: [38/100][12/156] Data 0.001 (0.001) Batch 4.037 (3.359) Remain 09:09:27 loss: 0.2208 Lr: 0.03720
[2023-08-08 00:36:40,584 INFO misc.py line 115 22900] Train: [38/100][13/156] Data 0.001 (0.001) Batch 4.383 (3.461) Remain 09:26:09 loss: 0.3935 Lr: 0.03720
[2023-08-08 00:36:43,939 INFO misc.py line 115 22900] Train: [38/100][14/156] Data 0.001 (0.001) Batch 3.355 (3.451) Remain 09:24:31 loss: 0.4670 Lr: 0.03719
[2023-08-08 00:36:48,298 INFO misc.py line 115 22900] Train: [38/100][15/156] Data 0.001 (0.001) Batch 4.359 (3.527) Remain 09:36:49 loss: 0.3348 Lr: 0.03719
[2023-08-08 00:36:52,131 INFO misc.py line 115 22900] Train: [38/100][16/156] Data 0.001 (0.001) Batch 3.834 (3.551) Remain 09:40:37 loss: 0.1708 Lr: 0.03718
[2023-08-08 00:36:55,592 INFO misc.py line 115 22900] Train: [38/100][17/156] Data 0.001 (0.001) Batch 3.461 (3.544) Remain 09:39:31 loss: 0.2122 Lr: 0.03718
[2023-08-08 00:36:58,154 INFO misc.py line 115 22900] Train: [38/100][18/156] Data 0.001 (0.001) Batch 2.561 (3.479) Remain 09:28:45 loss: 0.3516 Lr: 0.03717
[2023-08-08 00:37:01,308 INFO misc.py line 115 22900] Train: [38/100][19/156] Data 0.001 (0.001) Batch 3.154 (3.458) Remain 09:25:22 loss: 0.2929 Lr: 0.03717
[2023-08-08 00:37:04,277 INFO misc.py line 115 22900] Train: [38/100][20/156] Data 0.001 (0.001) Batch 2.969 (3.430) Remain 09:20:37 loss: 0.1978 Lr: 0.03716
[2023-08-08 00:37:07,212 INFO misc.py line 115 22900] Train: [38/100][21/156] Data 0.001 (0.001) Batch 2.935 (3.402) Remain 09:16:04 loss: 0.3013 Lr: 0.03716
[2023-08-08 00:37:10,481 INFO misc.py line 115 22900] Train: [38/100][22/156] Data 0.001 (0.001) Batch 3.269 (3.395) Remain 09:14:52 loss: 0.4638 Lr: 0.03715
[2023-08-08 00:37:14,399 INFO misc.py line 115 22900] Train: [38/100][23/156] Data 0.001 (0.001) Batch 3.918 (3.421) Remain 09:19:04 loss: 0.3903 Lr: 0.03715
[2023-08-08 00:37:18,378 INFO misc.py line 115 22900] Train: [38/100][24/156] Data 0.001 (0.001) Batch 3.979 (3.448) Remain 09:23:21 loss: 0.3947 Lr: 0.03715
[2023-08-08 00:37:21,838 INFO misc.py line 115 22900] Train: [38/100][25/156] Data 0.001 (0.001) Batch 3.460 (3.448) Remain 09:23:23 loss: 0.3471 Lr: 0.03714
[2023-08-08 00:37:25,230 INFO misc.py line 115 22900] Train: [38/100][26/156] Data 0.001 (0.001) Batch 3.392 (3.446) Remain 09:22:56 loss: 0.2380 Lr: 0.03714
[2023-08-08 00:37:29,279 INFO misc.py line 115 22900] Train: [38/100][27/156] Data 0.001 (0.001) Batch 4.049 (3.471) Remain 09:26:59 loss: 0.5843 Lr: 0.03713
[2023-08-08 00:37:33,262 INFO misc.py line 115 22900] Train: [38/100][28/156] Data 0.001 (0.001) Batch 3.983 (3.492) Remain 09:30:16 loss: 0.2053 Lr: 0.03713
[2023-08-08 00:37:36,201 INFO misc.py line 115 22900] Train: [38/100][29/156] Data 0.001 (0.001) Batch 2.939 (3.470) Remain 09:26:44 loss: 0.3618 Lr: 0.03712
[2023-08-08 00:37:40,108 INFO misc.py line 115 22900] Train: [38/100][30/156] Data 0.001 (0.001) Batch 3.908 (3.486) Remain 09:29:20 loss: 0.3733 Lr: 0.03712
[2023-08-08 00:37:42,545 INFO misc.py line 115 22900] Train: [38/100][31/156] Data 0.001 (0.001) Batch 2.437 (3.449) Remain 09:23:09 loss: 0.1895 Lr: 0.03711
[2023-08-08 00:37:46,598 INFO misc.py line 115 22900] Train: [38/100][32/156] Data 0.001 (0.001) Batch 4.052 (3.470) Remain 09:26:29 loss: 0.2979 Lr: 0.03711
[2023-08-08 00:37:51,063 INFO misc.py line 115 22900] Train: [38/100][33/156] Data 0.001 (0.001) Batch 4.465 (3.503) Remain 09:31:51 loss: 0.3771 Lr: 0.03710
[2023-08-08 00:37:55,081 INFO misc.py line 115 22900] Train: [38/100][34/156] Data 0.001 (0.001) Batch 4.018 (3.520) Remain 09:34:30 loss: 0.2845 Lr: 0.03710
[2023-08-08 00:37:59,513 INFO misc.py line 115 22900] Train: [38/100][35/156] Data 0.001 (0.001) Batch 4.432 (3.548) Remain 09:39:06 loss: 0.3785 Lr: 0.03709
[2023-08-08 00:38:03,032 INFO misc.py line 115 22900] Train: [38/100][36/156] Data 0.001 (0.001) Batch 3.520 (3.547) Remain 09:38:54 loss: 0.1906 Lr: 0.03709
[2023-08-08 00:38:06,187 INFO misc.py line 115 22900] Train: [38/100][37/156] Data 0.001 (0.001) Batch 3.155 (3.536) Remain 09:36:57 loss: 0.2831 Lr: 0.03708
[2023-08-08 00:38:09,456 INFO misc.py line 115 22900] Train: [38/100][38/156] Data 0.001 (0.001) Batch 3.268 (3.528) Remain 09:35:39 loss: 0.3421 Lr: 0.03708
[2023-08-08 00:38:12,052 INFO misc.py line 115 22900] Train: [38/100][39/156] Data 0.001 (0.001) Batch 2.597 (3.502) Remain 09:31:22 loss: 0.1748 Lr: 0.03708
[2023-08-08 00:38:15,080 INFO misc.py line 115 22900] Train: [38/100][40/156] Data 0.001 (0.001) Batch 3.028 (3.489) Remain 09:29:13 loss: 0.3986 Lr: 0.03707
[2023-08-08 00:38:19,230 INFO misc.py line 115 22900] Train: [38/100][41/156] Data 0.001 (0.001) Batch 4.150 (3.507) Remain 09:32:00 loss: 0.2290 Lr: 0.03707
[2023-08-08 00:38:21,820 INFO misc.py line 115 22900] Train: [38/100][42/156] Data 0.001 (0.001) Batch 2.589 (3.483) Remain 09:28:06 loss: 0.1815 Lr: 0.03706
[2023-08-08 00:38:25,901 INFO misc.py line 115 22900] Train: [38/100][43/156] Data 0.001 (0.001) Batch 4.081 (3.498) Remain 09:30:29 loss: 0.3305 Lr: 0.03706
[2023-08-08 00:38:29,886 INFO misc.py line 115 22900] Train: [38/100][44/156] Data 0.001 (0.001) Batch 3.985 (3.510) Remain 09:32:22 loss: 0.4677 Lr: 0.03705
[2023-08-08 00:38:32,407 INFO misc.py line 115 22900] Train: [38/100][45/156] Data 0.001 (0.001) Batch 2.521 (3.486) Remain 09:28:28 loss: 0.2161 Lr: 0.03705
[2023-08-08 00:38:36,007 INFO misc.py line 115 22900] Train: [38/100][46/156] Data 0.001 (0.001) Batch 3.599 (3.489) Remain 09:28:50 loss: 0.4058 Lr: 0.03704
[2023-08-08 00:38:39,220 INFO misc.py line 115 22900] Train: [38/100][47/156] Data 0.001 (0.001) Batch 3.214 (3.483) Remain 09:27:45 loss: 0.2575 Lr: 0.03704
[2023-08-08 00:38:43,313 INFO misc.py line 115 22900] Train: [38/100][48/156] Data 0.001 (0.001) Batch 4.092 (3.496) Remain 09:29:54 loss: 0.4403 Lr: 0.03703
[2023-08-08 00:38:46,867 INFO misc.py line 115 22900] Train: [38/100][49/156] Data 0.001 (0.001) Batch 3.555 (3.498) Remain 09:30:03 loss: 0.4344 Lr: 0.03703
[2023-08-08 00:38:49,718 INFO misc.py line 115 22900] Train: [38/100][50/156] Data 0.001 (0.001) Batch 2.851 (3.484) Remain 09:27:45 loss: 0.3516 Lr: 0.03702
[2023-08-08 00:38:53,016 INFO misc.py line 115 22900] Train: [38/100][51/156] Data 0.001 (0.001) Batch 3.298 (3.480) Remain 09:27:04 loss: 0.2816 Lr: 0.03702
[2023-08-08 00:38:56,805 INFO misc.py line 115 22900] Train: [38/100][52/156] Data 0.001 (0.001) Batch 3.788 (3.486) Remain 09:28:02 loss: 0.5842 Lr: 0.03702
[2023-08-08 00:39:00,776 INFO misc.py line 115 22900] Train: [38/100][53/156] Data 0.001 (0.001) Batch 3.971 (3.496) Remain 09:29:33 loss: 0.4295 Lr: 0.03701
[2023-08-08 00:39:04,242 INFO misc.py line 115 22900] Train: [38/100][54/156] Data 0.001 (0.001) Batch 3.467 (3.495) Remain 09:29:24 loss: 0.4116 Lr: 0.03701
[2023-08-08 00:39:07,497 INFO misc.py line 115 22900] Train: [38/100][55/156] Data 0.001 (0.001) Batch 3.255 (3.491) Remain 09:28:35 loss: 0.3688 Lr: 0.03700
[2023-08-08 00:39:11,509 INFO misc.py line 115 22900] Train: [38/100][56/156] Data 0.001 (0.001) Batch 4.011 (3.501) Remain 09:30:08 loss: 0.5018 Lr: 0.03700
[2023-08-08 00:39:15,364 INFO misc.py line 115 22900] Train: [38/100][57/156] Data 0.001 (0.001) Batch 3.855 (3.507) Remain 09:31:08 loss: 0.3141 Lr: 0.03699
[2023-08-08 00:39:17,989 INFO misc.py line 115 22900] Train: [38/100][58/156] Data 0.001 (0.001) Batch 2.626 (3.491) Remain 09:28:28 loss: 0.2584 Lr: 0.03699
[2023-08-08 00:39:21,204 INFO misc.py line 115 22900] Train: [38/100][59/156] Data 0.001 (0.001) Batch 3.214 (3.486) Remain 09:27:36 loss: 0.2634 Lr: 0.03698
[2023-08-08 00:39:24,640 INFO misc.py line 115 22900] Train: [38/100][60/156] Data 0.001 (0.001) Batch 3.436 (3.485) Remain 09:27:24 loss: 0.1809 Lr: 0.03698
[2023-08-08 00:39:28,516 INFO misc.py line 115 22900] Train: [38/100][61/156] Data 0.001 (0.001) Batch 3.876 (3.492) Remain 09:28:27 loss: 0.2476 Lr: 0.03697
[2023-08-08 00:39:31,088 INFO misc.py line 115 22900] Train: [38/100][62/156] Data 0.001 (0.001) Batch 2.572 (3.476) Remain 09:25:51 loss: 0.1842 Lr: 0.03697
[2023-08-08 00:39:35,138 INFO misc.py line 115 22900] Train: [38/100][63/156] Data 0.001 (0.001) Batch 4.050 (3.486) Remain 09:27:21 loss: 0.3005 Lr: 0.03696
[2023-08-08 00:39:38,742 INFO misc.py line 115 22900] Train: [38/100][64/156] Data 0.001 (0.001) Batch 3.604 (3.488) Remain 09:27:36 loss: 0.2940 Lr: 0.03696
[2023-08-08 00:39:43,555 INFO misc.py line 115 22900] Train: [38/100][65/156] Data 0.001 (0.001) Batch 4.813 (3.509) Remain 09:31:01 loss: 0.5703 Lr: 0.03695
[2023-08-08 00:39:47,454 INFO misc.py line 115 22900] Train: [38/100][66/156] Data 0.001 (0.001) Batch 3.899 (3.516) Remain 09:31:58 loss: 0.4429 Lr: 0.03695
[2023-08-08 00:39:51,508 INFO misc.py line 115 22900] Train: [38/100][67/156] Data 0.001 (0.001) Batch 4.054 (3.524) Remain 09:33:17 loss: 0.3977 Lr: 0.03695
[2023-08-08 00:39:53,623 INFO misc.py line 115 22900] Train: [38/100][68/156] Data 0.001 (0.001) Batch 2.115 (3.502) Remain 09:29:42 loss: 0.3658 Lr: 0.03694
[2023-08-08 00:39:56,994 INFO misc.py line 115 22900] Train: [38/100][69/156] Data 0.001 (0.001) Batch 3.371 (3.500) Remain 09:29:19 loss: 0.5067 Lr: 0.03694
[2023-08-08 00:40:00,820 INFO misc.py line 115 22900] Train: [38/100][70/156] Data 0.001 (0.001) Batch 3.826 (3.505) Remain 09:30:03 loss: 0.4516 Lr: 0.03693
[2023-08-08 00:40:04,954 INFO misc.py line 115 22900] Train: [38/100][71/156] Data 0.001 (0.001) Batch 4.134 (3.514) Remain 09:31:29 loss: 0.5347 Lr: 0.03693
[2023-08-08 00:40:08,322 INFO misc.py line 115 22900] Train: [38/100][72/156] Data 0.001 (0.001) Batch 3.368 (3.512) Remain 09:31:05 loss: 0.4819 Lr: 0.03692
[2023-08-08 00:40:11,500 INFO misc.py line 115 22900] Train: [38/100][73/156] Data 0.001 (0.001) Batch 3.178 (3.508) Remain 09:30:15 loss: 0.2169 Lr: 0.03692
[2023-08-08 00:40:15,723 INFO misc.py line 115 22900] Train: [38/100][74/156] Data 0.001 (0.001) Batch 4.223 (3.518) Remain 09:31:50 loss: 0.2478 Lr: 0.03691
[2023-08-08 00:40:18,732 INFO misc.py line 115 22900] Train: [38/100][75/156] Data 0.001 (0.001) Batch 3.010 (3.511) Remain 09:30:38 loss: 0.2376 Lr: 0.03691
[2023-08-08 00:40:22,797 INFO misc.py line 115 22900] Train: [38/100][76/156] Data 0.001 (0.001) Batch 4.065 (3.518) Remain 09:31:48 loss: 0.4135 Lr: 0.03690
[2023-08-08 00:40:26,059 INFO misc.py line 115 22900] Train: [38/100][77/156] Data 0.001 (0.001) Batch 3.262 (3.515) Remain 09:31:11 loss: 0.2793 Lr: 0.03690
[2023-08-08 00:40:29,529 INFO misc.py line 115 22900] Train: [38/100][78/156] Data 0.001 (0.001) Batch 3.469 (3.514) Remain 09:31:01 loss: 0.3199 Lr: 0.03689
[2023-08-08 00:40:32,620 INFO misc.py line 115 22900] Train: [38/100][79/156] Data 0.001 (0.001) Batch 3.091 (3.508) Remain 09:30:04 loss: 0.3262 Lr: 0.03689
[2023-08-08 00:40:35,917 INFO misc.py line 115 22900] Train: [38/100][80/156] Data 0.001 (0.001) Batch 3.297 (3.506) Remain 09:29:33 loss: 0.2687 Lr: 0.03688
[2023-08-08 00:40:38,430 INFO misc.py line 115 22900] Train: [38/100][81/156] Data 0.001 (0.001) Batch 2.513 (3.493) Remain 09:27:26 loss: 0.1428 Lr: 0.03688
[2023-08-08 00:40:41,962 INFO misc.py line 115 22900] Train: [38/100][82/156] Data 0.001 (0.001) Batch 3.532 (3.494) Remain 09:27:27 loss: 0.2374 Lr: 0.03688
[2023-08-08 00:40:46,001 INFO misc.py line 115 22900] Train: [38/100][83/156] Data 0.001 (0.001) Batch 4.039 (3.500) Remain 09:28:30 loss: 0.4621 Lr: 0.03687
[2023-08-08 00:40:49,046 INFO misc.py line 115 22900] Train: [38/100][84/156] Data 0.001 (0.001) Batch 3.045 (3.495) Remain 09:27:32 loss: 0.1688 Lr: 0.03687
[2023-08-08 00:40:53,143 INFO misc.py line 115 22900] Train: [38/100][85/156] Data 0.001 (0.001) Batch 4.097 (3.502) Remain 09:28:40 loss: 0.7678 Lr: 0.03686
[2023-08-08 00:40:55,998 INFO misc.py line 115 22900] Train: [38/100][86/156] Data 0.001 (0.001) Batch 2.855 (3.494) Remain 09:27:21 loss: 0.3464 Lr: 0.03686
[2023-08-08 00:40:59,901 INFO misc.py line 115 22900] Train: [38/100][87/156] Data 0.001 (0.001) Batch 3.903 (3.499) Remain 09:28:04 loss: 0.4045 Lr: 0.03685
[2023-08-08 00:41:03,730 INFO misc.py line 115 22900] Train: [38/100][88/156] Data 0.001 (0.001) Batch 3.829 (3.503) Remain 09:28:39 loss: 0.2012 Lr: 0.03685
[2023-08-08 00:41:07,004 INFO misc.py line 115 22900] Train: [38/100][89/156] Data 0.001 (0.001) Batch 3.275 (3.500) Remain 09:28:09 loss: 0.2778 Lr: 0.03684
[2023-08-08 00:41:09,508 INFO misc.py line 115 22900] Train: [38/100][90/156] Data 0.001 (0.001) Batch 2.504 (3.489) Remain 09:26:14 loss: 0.2809 Lr: 0.03684
[2023-08-08 00:41:12,913 INFO misc.py line 115 22900] Train: [38/100][91/156] Data 0.001 (0.001) Batch 3.405 (3.488) Remain 09:26:02 loss: 0.3984 Lr: 0.03683
[2023-08-08 00:41:15,528 INFO misc.py line 115 22900] Train: [38/100][92/156] Data 0.001 (0.001) Batch 2.615 (3.478) Remain 09:24:23 loss: 0.1536 Lr: 0.03683
[2023-08-08 00:41:19,012 INFO misc.py line 115 22900] Train: [38/100][93/156] Data 0.001 (0.001) Batch 3.485 (3.478) Remain 09:24:20 loss: 0.4180 Lr: 0.03682
[2023-08-08 00:41:22,297 INFO misc.py line 115 22900] Train: [38/100][94/156] Data 0.001 (0.001) Batch 3.284 (3.476) Remain 09:23:56 loss: 0.5953 Lr: 0.03682
[2023-08-08 00:41:25,541 INFO misc.py line 115 22900] Train: [38/100][95/156] Data 0.001 (0.001) Batch 3.244 (3.474) Remain 09:23:28 loss: 0.2498 Lr: 0.03681
[2023-08-08 00:41:29,316 INFO misc.py line 115 22900] Train: [38/100][96/156] Data 0.001 (0.001) Batch 3.775 (3.477) Remain 09:23:56 loss: 0.2312 Lr: 0.03681
[2023-08-08 00:41:32,975 INFO misc.py line 115 22900] Train: [38/100][97/156] Data 0.001 (0.001) Batch 3.659 (3.479) Remain 09:24:11 loss: 0.3687 Lr: 0.03681
[2023-08-08 00:41:36,377 INFO misc.py line 115 22900] Train: [38/100][98/156] Data 0.001 (0.001) Batch 3.402 (3.478) Remain 09:24:00 loss: 0.3651 Lr: 0.03680
[2023-08-08 00:41:39,591 INFO misc.py line 115 22900] Train: [38/100][99/156] Data 0.001 (0.001) Batch 3.214 (3.475) Remain 09:23:29 loss: 0.4010 Lr: 0.03680
[2023-08-08 00:41:42,575 INFO misc.py line 115 22900] Train: [38/100][100/156] Data 0.001 (0.001) Batch 2.983 (3.470) Remain 09:22:37 loss: 0.2476 Lr: 0.03679
[2023-08-08 00:41:46,848 INFO misc.py line 115 22900] Train: [38/100][101/156] Data 0.001 (0.001) Batch 4.274 (3.478) Remain 09:23:53 loss: 0.2872 Lr: 0.03679
[2023-08-08 00:41:50,902 INFO misc.py line 115 22900] Train: [38/100][102/156] Data 0.001 (0.001) Batch 4.053 (3.484) Remain 09:24:46 loss: 0.2631 Lr: 0.03678
[2023-08-08 00:41:54,891 INFO misc.py line 115 22900] Train: [38/100][103/156] Data 0.001 (0.001) Batch 3.990 (3.489) Remain 09:25:32 loss: 0.3733 Lr: 0.03678
[2023-08-08 00:41:58,955 INFO misc.py line 115 22900] Train: [38/100][104/156] Data 0.001 (0.001) Batch 4.063 (3.495) Remain 09:26:23 loss: 0.4063 Lr: 0.03677
[2023-08-08 00:42:02,998 INFO misc.py line 115 22900] Train: [38/100][105/156] Data 0.001 (0.001) Batch 4.043 (3.500) Remain 09:27:12 loss: 0.5045 Lr: 0.03677
[2023-08-08 00:42:07,118 INFO misc.py line 115 22900] Train: [38/100][106/156] Data 0.001 (0.001) Batch 4.121 (3.506) Remain 09:28:07 loss: 0.4089 Lr: 0.03676
[2023-08-08 00:42:10,676 INFO misc.py line 115 22900] Train: [38/100][107/156] Data 0.001 (0.001) Batch 3.557 (3.507) Remain 09:28:09 loss: 0.4871 Lr: 0.03676
[2023-08-08 00:42:13,294 INFO misc.py line 115 22900] Train: [38/100][108/156] Data 0.001 (0.001) Batch 2.619 (3.498) Remain 09:26:43 loss: 0.1169 Lr: 0.03675
[2023-08-08 00:42:17,859 INFO misc.py line 115 22900] Train: [38/100][109/156] Data 0.001 (0.001) Batch 4.564 (3.508) Remain 09:28:17 loss: 0.5324 Lr: 0.03675
[2023-08-08 00:42:21,054 INFO misc.py line 115 22900] Train: [38/100][110/156] Data 0.001 (0.001) Batch 3.195 (3.505) Remain 09:27:45 loss: 0.3487 Lr: 0.03674
[2023-08-08 00:42:24,810 INFO misc.py line 115 22900] Train: [38/100][111/156] Data 0.001 (0.001) Batch 3.756 (3.508) Remain 09:28:04 loss: 0.2911 Lr: 0.03674
[2023-08-08 00:42:27,703 INFO misc.py line 115 22900] Train: [38/100][112/156] Data 0.001 (0.001) Batch 2.893 (3.502) Remain 09:27:06 loss: 0.3194 Lr: 0.03674
[2023-08-08 00:42:30,816 INFO misc.py line 115 22900] Train: [38/100][113/156] Data 0.001 (0.001) Batch 3.113 (3.499) Remain 09:26:28 loss: 0.2932 Lr: 0.03673
[2023-08-08 00:42:34,243 INFO misc.py line 115 22900] Train: [38/100][114/156] Data 0.001 (0.001) Batch 3.427 (3.498) Remain 09:26:18 loss: 0.3630 Lr: 0.03673
[2023-08-08 00:42:37,828 INFO misc.py line 115 22900] Train: [38/100][115/156] Data 0.001 (0.001) Batch 3.586 (3.499) Remain 09:26:22 loss: 0.2149 Lr: 0.03672
[2023-08-08 00:42:41,901 INFO misc.py line 115 22900] Train: [38/100][116/156] Data 0.001 (0.001) Batch 4.072 (3.504) Remain 09:27:08 loss: 0.6920 Lr: 0.03672
[2023-08-08 00:42:44,253 INFO misc.py line 115 22900] Train: [38/100][117/156] Data 0.001 (0.001) Batch 2.353 (3.494) Remain 09:25:27 loss: 0.2512 Lr: 0.03671
[2023-08-08 00:42:47,443 INFO misc.py line 115 22900] Train: [38/100][118/156] Data 0.001 (0.001) Batch 3.190 (3.491) Remain 09:24:57 loss: 0.2832 Lr: 0.03671
[2023-08-08 00:42:50,099 INFO misc.py line 115 22900] Train: [38/100][119/156] Data 0.001 (0.001) Batch 2.656 (3.484) Remain 09:23:44 loss: 0.3938 Lr: 0.03670
[2023-08-08 00:42:53,973 INFO misc.py line 115 22900] Train: [38/100][120/156] Data 0.001 (0.001) Batch 3.874 (3.487) Remain 09:24:13 loss: 0.3735 Lr: 0.03670
[2023-08-08 00:42:57,238 INFO misc.py line 115 22900] Train: [38/100][121/156] Data 0.001 (0.001) Batch 3.265 (3.485) Remain 09:23:51 loss: 0.3345 Lr: 0.03669
[2023-08-08 00:43:00,054 INFO misc.py line 115 22900] Train: [38/100][122/156] Data 0.001 (0.001) Batch 2.816 (3.480) Remain 09:22:53 loss: 0.1691 Lr: 0.03669
[2023-08-08 00:43:02,546 INFO misc.py line 115 22900] Train: [38/100][123/156] Data 0.001 (0.001) Batch 2.493 (3.471) Remain 09:21:30 loss: 0.3190 Lr: 0.03668
[2023-08-08 00:43:05,837 INFO misc.py line 115 22900] Train: [38/100][124/156] Data 0.001 (0.001) Batch 3.291 (3.470) Remain 09:21:12 loss: 0.5686 Lr: 0.03668
[2023-08-08 00:43:08,567 INFO misc.py line 115 22900] Train: [38/100][125/156] Data 0.001 (0.001) Batch 2.730 (3.464) Remain 09:20:09 loss: 0.1870 Lr: 0.03667
[2023-08-08 00:43:12,023 INFO misc.py line 115 22900] Train: [38/100][126/156] Data 0.001 (0.001) Batch 3.456 (3.464) Remain 09:20:05 loss: 0.2822 Lr: 0.03667
[2023-08-08 00:43:15,157 INFO misc.py line 115 22900] Train: [38/100][127/156] Data 0.001 (0.001) Batch 3.133 (3.461) Remain 09:19:36 loss: 0.3020 Lr: 0.03667
[2023-08-08 00:43:18,865 INFO misc.py line 115 22900] Train: [38/100][128/156] Data 0.001 (0.001) Batch 3.709 (3.463) Remain 09:19:52 loss: 0.2720 Lr: 0.03666
[2023-08-08 00:43:23,079 INFO misc.py line 115 22900] Train: [38/100][129/156] Data 0.001 (0.001) Batch 4.213 (3.469) Remain 09:20:46 loss: 0.3451 Lr: 0.03666
[2023-08-08 00:43:26,663 INFO misc.py line 115 22900] Train: [38/100][130/156] Data 0.001 (0.001) Batch 3.584 (3.470) Remain 09:20:51 loss: 0.3489 Lr: 0.03665
[2023-08-08 00:43:29,254 INFO misc.py line 115 22900] Train: [38/100][131/156] Data 0.001 (0.001) Batch 2.592 (3.463) Remain 09:19:41 loss: 0.2957 Lr: 0.03665
[2023-08-08 00:43:33,159 INFO misc.py line 115 22900] Train: [38/100][132/156] Data 0.001 (0.001) Batch 3.905 (3.467) Remain 09:20:11 loss: 0.3615 Lr: 0.03664
[2023-08-08 00:43:35,763 INFO misc.py line 115 22900] Train: [38/100][133/156] Data 0.001 (0.001) Batch 2.603 (3.460) Remain 09:19:03 loss: 0.1722 Lr: 0.03664
[2023-08-08 00:43:39,694 INFO misc.py line 115 22900] Train: [38/100][134/156] Data 0.001 (0.001) Batch 3.932 (3.464) Remain 09:19:35 loss: 0.3561 Lr: 0.03663
[2023-08-08 00:43:42,611 INFO misc.py line 115 22900] Train: [38/100][135/156] Data 0.001 (0.001) Batch 2.917 (3.459) Remain 09:18:51 loss: 0.2765 Lr: 0.03663
[2023-08-08 00:43:46,664 INFO misc.py line 115 22900] Train: [38/100][136/156] Data 0.001 (0.001) Batch 4.053 (3.464) Remain 09:19:31 loss: 0.4626 Lr: 0.03662
[2023-08-08 00:43:50,302 INFO misc.py line 115 22900] Train: [38/100][137/156] Data 0.001 (0.001) Batch 3.637 (3.465) Remain 09:19:40 loss: 0.2709 Lr: 0.03662
[2023-08-08 00:43:53,941 INFO misc.py line 115 22900] Train: [38/100][138/156] Data 0.001 (0.001) Batch 3.640 (3.466) Remain 09:19:49 loss: 0.3126 Lr: 0.03661
[2023-08-08 00:43:57,617 INFO misc.py line 115 22900] Train: [38/100][139/156] Data 0.001 (0.001) Batch 3.675 (3.468) Remain 09:20:01 loss: 0.3885 Lr: 0.03661
[2023-08-08 00:44:01,365 INFO misc.py line 115 22900] Train: [38/100][140/156] Data 0.001 (0.001) Batch 3.748 (3.470) Remain 09:20:17 loss: 0.3699 Lr: 0.03660
[2023-08-08 00:44:04,521 INFO misc.py line 115 22900] Train: [38/100][141/156] Data 0.001 (0.001) Batch 3.156 (3.468) Remain 09:19:51 loss: 0.3075 Lr: 0.03660
[2023-08-08 00:44:08,931 INFO misc.py line 115 22900] Train: [38/100][142/156] Data 0.001 (0.001) Batch 4.410 (3.475) Remain 09:20:54 loss: 0.5269 Lr: 0.03659
[2023-08-08 00:44:12,766 INFO misc.py line 115 22900] Train: [38/100][143/156] Data 0.001 (0.001) Batch 3.835 (3.477) Remain 09:21:15 loss: 0.4355 Lr: 0.03659
[2023-08-08 00:44:16,293 INFO misc.py line 115 22900] Train: [38/100][144/156] Data 0.001 (0.001) Batch 3.527 (3.477) Remain 09:21:15 loss: 0.3610 Lr: 0.03659
[2023-08-08 00:44:19,435 INFO misc.py line 115 22900] Train: [38/100][145/156] Data 0.001 (0.001) Batch 3.141 (3.475) Remain 09:20:49 loss: 0.2908 Lr: 0.03658
[2023-08-08 00:44:22,934 INFO misc.py line 115 22900] Train: [38/100][146/156] Data 0.001 (0.001) Batch 3.499 (3.475) Remain 09:20:47 loss: 0.2795 Lr: 0.03658
[2023-08-08 00:44:26,982 INFO misc.py line 115 22900] Train: [38/100][147/156] Data 0.001 (0.001) Batch 4.048 (3.479) Remain 09:21:22 loss: 0.4894 Lr: 0.03657
[2023-08-08 00:44:30,291 INFO misc.py line 115 22900] Train: [38/100][148/156] Data 0.001 (0.001) Batch 3.309 (3.478) Remain 09:21:07 loss: 0.2930 Lr: 0.03657
[2023-08-08 00:44:33,621 INFO misc.py line 115 22900] Train: [38/100][149/156] Data 0.001 (0.001) Batch 3.330 (3.477) Remain 09:20:54 loss: 0.3590 Lr: 0.03656
[2023-08-08 00:44:36,788 INFO misc.py line 115 22900] Train: [38/100][150/156] Data 0.001 (0.001) Batch 3.167 (3.475) Remain 09:20:30 loss: 0.2973 Lr: 0.03656
[2023-08-08 00:44:40,024 INFO misc.py line 115 22900] Train: [38/100][151/156] Data 0.001 (0.001) Batch 3.235 (3.473) Remain 09:20:11 loss: 0.4267 Lr: 0.03655
[2023-08-08 00:44:44,199 INFO misc.py line 115 22900] Train: [38/100][152/156] Data 0.001 (0.001) Batch 4.176 (3.478) Remain 09:20:53 loss: 0.4115 Lr: 0.03655
[2023-08-08 00:44:47,744 INFO misc.py line 115 22900] Train: [38/100][153/156] Data 0.001 (0.001) Batch 3.544 (3.478) Remain 09:20:54 loss: 0.3360 Lr: 0.03654
[2023-08-08 00:44:50,557 INFO misc.py line 115 22900] Train: [38/100][154/156] Data 0.001 (0.001) Batch 2.814 (3.474) Remain 09:20:08 loss: 0.1310 Lr: 0.03654
[2023-08-08 00:44:53,726 INFO misc.py line 115 22900] Train: [38/100][155/156] Data 0.001 (0.001) Batch 3.169 (3.472) Remain 09:19:45 loss: 0.1646 Lr: 0.03653
[2023-08-08 00:44:57,616 INFO misc.py line 115 22900] Train: [38/100][156/156] Data 0.001 (0.001) Batch 3.889 (3.475) Remain 09:20:08 loss: 0.4382 Lr: 0.03653
[2023-08-08 00:44:57,616 INFO misc.py line 129 22900] Train result: loss: 0.3366 
[2023-08-08 00:44:57,616 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 00:44:59,711 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.8486 
[2023-08-08 00:45:00,581 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.4872 
[2023-08-08 00:45:02,243 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.7209 
[2023-08-08 00:45:03,764 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.5296 
[2023-08-08 00:45:05,612 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.8812 
[2023-08-08 00:45:07,276 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.5370 
[2023-08-08 00:45:09,414 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.9377 
[2023-08-08 00:45:11,223 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.9012 
[2023-08-08 00:45:12,508 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.5908 
[2023-08-08 00:45:14,638 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3949 
[2023-08-08 00:45:15,164 INFO evaluator.py line 122 22900] Test: [11/24] Loss 0.8254 
[2023-08-08 00:45:16,697 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.7144 
[2023-08-08 00:45:19,408 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.1187 
[2023-08-08 00:45:21,090 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.8400 
[2023-08-08 00:45:23,113 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.4210 
[2023-08-08 00:45:25,822 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.1382 
[2023-08-08 00:45:28,527 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.4672 
[2023-08-08 00:45:30,373 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.5881 
[2023-08-08 00:45:31,122 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.0364 
[2023-08-08 00:45:32,007 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.6919 
[2023-08-08 00:45:34,269 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3664 
[2023-08-08 00:45:36,236 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.7720 
[2023-08-08 00:45:38,080 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.7651 
[2023-08-08 00:45:40,018 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.5009 
[2023-08-08 00:45:40,065 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2415/0.3303/0.6946.
[2023-08-08 00:45:40,065 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6496/0.9565
[2023-08-08 00:45:40,065 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9493/0.9871
[2023-08-08 00:45:40,065 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1697/0.5068
[2023-08-08 00:45:40,065 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1180/0.2037
[2023-08-08 00:45:40,065 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6631/0.7601
[2023-08-08 00:45:40,065 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.4416/0.5993
[2023-08-08 00:45:40,065 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5370/0.6897
[2023-08-08 00:45:40,065 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1085/0.1164
[2023-08-08 00:45:40,065 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1377/0.3428
[2023-08-08 00:45:40,065 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0581/0.0586
[2023-08-08 00:45:40,065 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0094/0.0177
[2023-08-08 00:45:40,065 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0000/0.0000
[2023-08-08 00:45:40,065 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1566/0.2127
[2023-08-08 00:45:40,065 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0755/0.0820
[2023-08-08 00:45:40,065 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0460/0.0575
[2023-08-08 00:45:40,065 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.1692/0.1696
[2023-08-08 00:45:40,065 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0553/0.0651
[2023-08-08 00:45:40,065 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.3882/0.5841
[2023-08-08 00:45:40,065 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 00:45:40,065 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0971/0.1959
[2023-08-08 00:45:40,065 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 00:45:40,066 INFO misc.py line 152 22900] Currently Best mIoU: 0.2562
[2023-08-08 00:45:40,066 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 00:45:45,863 INFO misc.py line 115 22900] Train: [39/100][1/156] Data 1.335 (1.335) Batch 4.996 (4.996) Remain 13:25:20 loss: 0.1904 Lr: 0.03652
[2023-08-08 00:45:49,486 INFO misc.py line 115 22900] Train: [39/100][2/156] Data 0.001 (0.001) Batch 3.622 (3.622) Remain 09:43:48 loss: 0.3242 Lr: 0.03652
[2023-08-08 00:45:52,110 INFO misc.py line 115 22900] Train: [39/100][3/156] Data 0.001 (0.001) Batch 2.624 (2.624) Remain 07:02:54 loss: 0.1998 Lr: 0.03651
[2023-08-08 00:45:55,753 INFO misc.py line 115 22900] Train: [39/100][4/156] Data 0.001 (0.001) Batch 3.643 (3.643) Remain 09:47:03 loss: 0.2650 Lr: 0.03651
[2023-08-08 00:45:59,041 INFO misc.py line 115 22900] Train: [39/100][5/156] Data 0.001 (0.001) Batch 3.287 (3.465) Remain 09:18:19 loss: 0.2078 Lr: 0.03651
[2023-08-08 00:46:01,158 INFO misc.py line 115 22900] Train: [39/100][6/156] Data 0.001 (0.001) Batch 2.117 (3.016) Remain 08:05:51 loss: 0.1817 Lr: 0.03650
[2023-08-08 00:46:05,279 INFO misc.py line 115 22900] Train: [39/100][7/156] Data 0.001 (0.001) Batch 4.121 (3.292) Remain 08:50:19 loss: 0.3804 Lr: 0.03650
[2023-08-08 00:46:09,029 INFO misc.py line 115 22900] Train: [39/100][8/156] Data 0.001 (0.001) Batch 3.751 (3.384) Remain 09:05:02 loss: 0.3739 Lr: 0.03649
[2023-08-08 00:46:12,325 INFO misc.py line 115 22900] Train: [39/100][9/156] Data 0.001 (0.001) Batch 3.296 (3.369) Remain 09:02:36 loss: 0.1857 Lr: 0.03649
[2023-08-08 00:46:14,335 INFO misc.py line 115 22900] Train: [39/100][10/156] Data 0.001 (0.001) Batch 2.009 (3.175) Remain 08:31:16 loss: 0.2686 Lr: 0.03648
[2023-08-08 00:46:18,852 INFO misc.py line 115 22900] Train: [39/100][11/156] Data 0.001 (0.001) Batch 4.518 (3.343) Remain 08:58:14 loss: 0.4028 Lr: 0.03648
[2023-08-08 00:46:21,157 INFO misc.py line 115 22900] Train: [39/100][12/156] Data 0.001 (0.001) Batch 2.305 (3.227) Remain 08:39:37 loss: 0.1773 Lr: 0.03647
[2023-08-08 00:46:25,235 INFO misc.py line 115 22900] Train: [39/100][13/156] Data 0.001 (0.001) Batch 4.078 (3.313) Remain 08:53:15 loss: 0.3351 Lr: 0.03647
[2023-08-08 00:46:28,913 INFO misc.py line 115 22900] Train: [39/100][14/156] Data 0.001 (0.001) Batch 3.678 (3.346) Remain 08:58:32 loss: 0.2327 Lr: 0.03646
[2023-08-08 00:46:32,402 INFO misc.py line 115 22900] Train: [39/100][15/156] Data 0.001 (0.001) Batch 3.489 (3.358) Remain 09:00:24 loss: 0.4243 Lr: 0.03646
[2023-08-08 00:46:36,395 INFO misc.py line 115 22900] Train: [39/100][16/156] Data 0.001 (0.001) Batch 3.993 (3.407) Remain 09:08:13 loss: 0.3526 Lr: 0.03645
[2023-08-08 00:46:40,319 INFO misc.py line 115 22900] Train: [39/100][17/156] Data 0.001 (0.001) Batch 3.923 (3.443) Remain 09:14:06 loss: 0.5539 Lr: 0.03645
[2023-08-08 00:46:42,911 INFO misc.py line 115 22900] Train: [39/100][18/156] Data 0.001 (0.001) Batch 2.592 (3.387) Remain 09:04:55 loss: 0.1661 Lr: 0.03644
[2023-08-08 00:46:46,230 INFO misc.py line 115 22900] Train: [39/100][19/156] Data 0.001 (0.001) Batch 3.319 (3.383) Remain 09:04:11 loss: 0.2816 Lr: 0.03644
[2023-08-08 00:46:49,775 INFO misc.py line 115 22900] Train: [39/100][20/156] Data 0.001 (0.001) Batch 3.545 (3.392) Remain 09:05:40 loss: 0.3587 Lr: 0.03643
[2023-08-08 00:46:52,990 INFO misc.py line 115 22900] Train: [39/100][21/156] Data 0.001 (0.001) Batch 3.215 (3.382) Remain 09:04:02 loss: 0.3069 Lr: 0.03643
[2023-08-08 00:46:56,186 INFO misc.py line 115 22900] Train: [39/100][22/156] Data 0.001 (0.001) Batch 3.196 (3.372) Remain 09:02:24 loss: 0.3884 Lr: 0.03643
[2023-08-08 00:47:00,028 INFO misc.py line 115 22900] Train: [39/100][23/156] Data 0.001 (0.001) Batch 3.842 (3.396) Remain 09:06:06 loss: 0.2193 Lr: 0.03642
[2023-08-08 00:47:03,374 INFO misc.py line 115 22900] Train: [39/100][24/156] Data 0.001 (0.001) Batch 3.346 (3.394) Remain 09:05:40 loss: 0.1938 Lr: 0.03642
[2023-08-08 00:47:05,490 INFO misc.py line 115 22900] Train: [39/100][25/156] Data 0.001 (0.001) Batch 2.117 (3.335) Remain 08:56:17 loss: 0.1813 Lr: 0.03641
[2023-08-08 00:47:09,452 INFO misc.py line 115 22900] Train: [39/100][26/156] Data 0.001 (0.001) Batch 3.961 (3.363) Remain 09:00:36 loss: 0.4205 Lr: 0.03641
[2023-08-08 00:47:13,064 INFO misc.py line 115 22900] Train: [39/100][27/156] Data 0.001 (0.001) Batch 3.613 (3.373) Remain 09:02:13 loss: 0.2031 Lr: 0.03640
[2023-08-08 00:47:17,105 INFO misc.py line 115 22900] Train: [39/100][28/156] Data 0.001 (0.001) Batch 4.040 (3.400) Remain 09:06:27 loss: 0.3681 Lr: 0.03640
[2023-08-08 00:47:20,296 INFO misc.py line 115 22900] Train: [39/100][29/156] Data 0.001 (0.001) Batch 3.192 (3.392) Remain 09:05:07 loss: 0.2379 Lr: 0.03639
[2023-08-08 00:47:23,621 INFO misc.py line 115 22900] Train: [39/100][30/156] Data 0.001 (0.001) Batch 3.325 (3.389) Remain 09:04:39 loss: 0.4652 Lr: 0.03639
[2023-08-08 00:47:28,379 INFO misc.py line 115 22900] Train: [39/100][31/156] Data 0.001 (0.001) Batch 4.757 (3.438) Remain 09:12:27 loss: 0.5598 Lr: 0.03638
[2023-08-08 00:47:32,581 INFO misc.py line 115 22900] Train: [39/100][32/156] Data 0.001 (0.001) Batch 4.202 (3.465) Remain 09:16:38 loss: 0.4919 Lr: 0.03638
[2023-08-08 00:47:36,637 INFO misc.py line 115 22900] Train: [39/100][33/156] Data 0.001 (0.001) Batch 4.055 (3.484) Remain 09:19:44 loss: 0.2745 Lr: 0.03637
[2023-08-08 00:47:40,253 INFO misc.py line 115 22900] Train: [39/100][34/156] Data 0.001 (0.001) Batch 3.617 (3.488) Remain 09:20:22 loss: 0.3824 Lr: 0.03637
[2023-08-08 00:47:43,242 INFO misc.py line 115 22900] Train: [39/100][35/156] Data 0.001 (0.001) Batch 2.989 (3.473) Remain 09:17:48 loss: 0.1599 Lr: 0.03636
[2023-08-08 00:47:47,387 INFO misc.py line 115 22900] Train: [39/100][36/156] Data 0.001 (0.001) Batch 4.146 (3.493) Remain 09:21:01 loss: 0.2520 Lr: 0.03636
[2023-08-08 00:47:50,170 INFO misc.py line 115 22900] Train: [39/100][37/156] Data 0.001 (0.001) Batch 2.782 (3.472) Remain 09:17:36 loss: 0.3188 Lr: 0.03635
[2023-08-08 00:47:54,391 INFO misc.py line 115 22900] Train: [39/100][38/156] Data 0.001 (0.001) Batch 4.221 (3.494) Remain 09:20:58 loss: 0.3773 Lr: 0.03635
[2023-08-08 00:47:57,309 INFO misc.py line 115 22900] Train: [39/100][39/156] Data 0.001 (0.001) Batch 2.918 (3.478) Remain 09:18:21 loss: 0.1329 Lr: 0.03635
[2023-08-08 00:48:00,261 INFO misc.py line 115 22900] Train: [39/100][40/156] Data 0.001 (0.001) Batch 2.953 (3.464) Remain 09:16:00 loss: 0.1126 Lr: 0.03634
[2023-08-08 00:48:04,124 INFO misc.py line 115 22900] Train: [39/100][41/156] Data 0.001 (0.001) Batch 3.863 (3.474) Remain 09:17:38 loss: 0.3988 Lr: 0.03634
[2023-08-08 00:48:06,800 INFO misc.py line 115 22900] Train: [39/100][42/156] Data 0.001 (0.001) Batch 2.676 (3.454) Remain 09:14:18 loss: 0.1774 Lr: 0.03633
[2023-08-08 00:48:10,264 INFO misc.py line 115 22900] Train: [39/100][43/156] Data 0.001 (0.001) Batch 3.464 (3.454) Remain 09:14:17 loss: 0.3101 Lr: 0.03633
[2023-08-08 00:48:13,165 INFO misc.py line 115 22900] Train: [39/100][44/156] Data 0.001 (0.001) Batch 2.901 (3.440) Remain 09:12:03 loss: 0.1618 Lr: 0.03632
[2023-08-08 00:48:17,720 INFO misc.py line 115 22900] Train: [39/100][45/156] Data 0.001 (0.001) Batch 4.555 (3.467) Remain 09:16:15 loss: 0.3345 Lr: 0.03632
[2023-08-08 00:48:20,525 INFO misc.py line 115 22900] Train: [39/100][46/156] Data 0.001 (0.001) Batch 2.805 (3.452) Remain 09:13:44 loss: 0.4176 Lr: 0.03631
[2023-08-08 00:48:24,292 INFO misc.py line 115 22900] Train: [39/100][47/156] Data 0.001 (0.001) Batch 3.767 (3.459) Remain 09:14:49 loss: 0.1976 Lr: 0.03631
[2023-08-08 00:48:27,011 INFO misc.py line 115 22900] Train: [39/100][48/156] Data 0.001 (0.001) Batch 2.719 (3.442) Remain 09:12:08 loss: 0.3485 Lr: 0.03630
[2023-08-08 00:48:31,615 INFO misc.py line 115 22900] Train: [39/100][49/156] Data 0.001 (0.001) Batch 4.604 (3.468) Remain 09:16:07 loss: 0.4825 Lr: 0.03630
[2023-08-08 00:48:34,771 INFO misc.py line 115 22900] Train: [39/100][50/156] Data 0.001 (0.001) Batch 3.156 (3.461) Remain 09:15:00 loss: 0.1204 Lr: 0.03629
[2023-08-08 00:48:38,829 INFO misc.py line 115 22900] Train: [39/100][51/156] Data 0.001 (0.001) Batch 4.058 (3.473) Remain 09:16:56 loss: 0.4606 Lr: 0.03629
[2023-08-08 00:48:42,316 INFO misc.py line 115 22900] Train: [39/100][52/156] Data 0.001 (0.001) Batch 3.487 (3.474) Remain 09:16:55 loss: 0.2749 Lr: 0.03628
[2023-08-08 00:48:45,887 INFO misc.py line 115 22900] Train: [39/100][53/156] Data 0.001 (0.001) Batch 3.572 (3.476) Remain 09:17:11 loss: 0.3235 Lr: 0.03628
[2023-08-08 00:48:49,920 INFO misc.py line 115 22900] Train: [39/100][54/156] Data 0.001 (0.001) Batch 4.033 (3.486) Remain 09:18:52 loss: 0.4736 Lr: 0.03627
[2023-08-08 00:48:53,973 INFO misc.py line 115 22900] Train: [39/100][55/156] Data 0.001 (0.001) Batch 4.052 (3.497) Remain 09:20:34 loss: 0.5214 Lr: 0.03627
[2023-08-08 00:48:56,935 INFO misc.py line 115 22900] Train: [39/100][56/156] Data 0.001 (0.001) Batch 2.963 (3.487) Remain 09:18:53 loss: 0.3600 Lr: 0.03626
[2023-08-08 00:49:01,249 INFO misc.py line 115 22900] Train: [39/100][57/156] Data 0.001 (0.001) Batch 4.314 (3.503) Remain 09:21:17 loss: 0.4911 Lr: 0.03626
[2023-08-08 00:49:05,776 INFO misc.py line 115 22900] Train: [39/100][58/156] Data 0.001 (0.001) Batch 4.527 (3.521) Remain 09:24:12 loss: 0.4155 Lr: 0.03626
[2023-08-08 00:49:08,919 INFO misc.py line 115 22900] Train: [39/100][59/156] Data 0.001 (0.001) Batch 3.143 (3.514) Remain 09:23:04 loss: 0.3706 Lr: 0.03625
[2023-08-08 00:49:13,030 INFO misc.py line 115 22900] Train: [39/100][60/156] Data 0.001 (0.001) Batch 4.111 (3.525) Remain 09:24:41 loss: 0.4115 Lr: 0.03625
[2023-08-08 00:49:16,120 INFO misc.py line 115 22900] Train: [39/100][61/156] Data 0.001 (0.001) Batch 3.090 (3.517) Remain 09:23:25 loss: 0.3418 Lr: 0.03624
[2023-08-08 00:49:18,955 INFO misc.py line 115 22900] Train: [39/100][62/156] Data 0.001 (0.001) Batch 2.835 (3.506) Remain 09:21:31 loss: 0.1629 Lr: 0.03624
[2023-08-08 00:49:22,490 INFO misc.py line 115 22900] Train: [39/100][63/156] Data 0.001 (0.001) Batch 3.535 (3.506) Remain 09:21:32 loss: 0.3844 Lr: 0.03623
[2023-08-08 00:49:25,552 INFO misc.py line 115 22900] Train: [39/100][64/156] Data 0.001 (0.001) Batch 3.062 (3.499) Remain 09:20:18 loss: 0.2972 Lr: 0.03623
[2023-08-08 00:49:29,716 INFO misc.py line 115 22900] Train: [39/100][65/156] Data 0.001 (0.001) Batch 4.164 (3.510) Remain 09:21:58 loss: 0.3313 Lr: 0.03622
[2023-08-08 00:49:33,040 INFO misc.py line 115 22900] Train: [39/100][66/156] Data 0.001 (0.001) Batch 3.324 (3.507) Remain 09:21:26 loss: 0.1856 Lr: 0.03622
[2023-08-08 00:49:36,403 INFO misc.py line 115 22900] Train: [39/100][67/156] Data 0.001 (0.001) Batch 3.363 (3.505) Remain 09:21:01 loss: 0.5215 Lr: 0.03621
[2023-08-08 00:49:39,462 INFO misc.py line 115 22900] Train: [39/100][68/156] Data 0.001 (0.001) Batch 3.060 (3.498) Remain 09:19:52 loss: 0.2061 Lr: 0.03621
[2023-08-08 00:49:43,101 INFO misc.py line 115 22900] Train: [39/100][69/156] Data 0.001 (0.001) Batch 3.639 (3.500) Remain 09:20:09 loss: 0.2671 Lr: 0.03620
[2023-08-08 00:49:46,458 INFO misc.py line 115 22900] Train: [39/100][70/156] Data 0.001 (0.001) Batch 3.357 (3.498) Remain 09:19:45 loss: 0.4056 Lr: 0.03620
[2023-08-08 00:49:49,745 INFO misc.py line 115 22900] Train: [39/100][71/156] Data 0.001 (0.001) Batch 3.286 (3.495) Remain 09:19:11 loss: 0.2047 Lr: 0.03619
[2023-08-08 00:49:53,803 INFO misc.py line 115 22900] Train: [39/100][72/156] Data 0.002 (0.001) Batch 4.059 (3.503) Remain 09:20:26 loss: 0.3752 Lr: 0.03619
[2023-08-08 00:49:57,245 INFO misc.py line 115 22900] Train: [39/100][73/156] Data 0.001 (0.001) Batch 3.442 (3.502) Remain 09:20:15 loss: 0.3254 Lr: 0.03618
[2023-08-08 00:50:01,283 INFO misc.py line 115 22900] Train: [39/100][74/156] Data 0.001 (0.001) Batch 4.038 (3.509) Remain 09:21:23 loss: 0.2546 Lr: 0.03618
[2023-08-08 00:50:05,048 INFO misc.py line 115 22900] Train: [39/100][75/156] Data 0.001 (0.001) Batch 3.765 (3.513) Remain 09:21:54 loss: 0.4710 Lr: 0.03617
[2023-08-08 00:50:07,920 INFO misc.py line 115 22900] Train: [39/100][76/156] Data 0.001 (0.001) Batch 2.872 (3.504) Remain 09:20:26 loss: 0.1746 Lr: 0.03617
[2023-08-08 00:50:11,408 INFO misc.py line 115 22900] Train: [39/100][77/156] Data 0.001 (0.001) Batch 3.488 (3.504) Remain 09:20:21 loss: 0.5085 Lr: 0.03617
[2023-08-08 00:50:14,031 INFO misc.py line 115 22900] Train: [39/100][78/156] Data 0.001 (0.001) Batch 2.623 (3.492) Remain 09:18:24 loss: 0.3344 Lr: 0.03616
[2023-08-08 00:50:17,157 INFO misc.py line 115 22900] Train: [39/100][79/156] Data 0.001 (0.001) Batch 3.126 (3.487) Remain 09:17:35 loss: 0.3958 Lr: 0.03616
[2023-08-08 00:50:20,685 INFO misc.py line 115 22900] Train: [39/100][80/156] Data 0.001 (0.001) Batch 3.527 (3.488) Remain 09:17:36 loss: 0.1605 Lr: 0.03615
[2023-08-08 00:50:24,028 INFO misc.py line 115 22900] Train: [39/100][81/156] Data 0.001 (0.001) Batch 3.343 (3.486) Remain 09:17:15 loss: 0.3442 Lr: 0.03615
[2023-08-08 00:50:26,709 INFO misc.py line 115 22900] Train: [39/100][82/156] Data 0.001 (0.001) Batch 2.682 (3.476) Remain 09:15:34 loss: 0.1739 Lr: 0.03614
[2023-08-08 00:50:29,985 INFO misc.py line 115 22900] Train: [39/100][83/156] Data 0.001 (0.001) Batch 3.275 (3.473) Remain 09:15:06 loss: 0.2721 Lr: 0.03614
[2023-08-08 00:50:32,504 INFO misc.py line 115 22900] Train: [39/100][84/156] Data 0.001 (0.001) Batch 2.519 (3.462) Remain 09:13:10 loss: 0.1870 Lr: 0.03613
[2023-08-08 00:50:35,910 INFO misc.py line 115 22900] Train: [39/100][85/156] Data 0.001 (0.001) Batch 3.406 (3.461) Remain 09:13:00 loss: 0.2504 Lr: 0.03613
[2023-08-08 00:50:39,423 INFO misc.py line 115 22900] Train: [39/100][86/156] Data 0.001 (0.001) Batch 3.513 (3.462) Remain 09:13:02 loss: 0.2897 Lr: 0.03612
[2023-08-08 00:50:43,404 INFO misc.py line 115 22900] Train: [39/100][87/156] Data 0.001 (0.001) Batch 3.981 (3.468) Remain 09:13:58 loss: 0.2662 Lr: 0.03612
[2023-08-08 00:50:47,003 INFO misc.py line 115 22900] Train: [39/100][88/156] Data 0.001 (0.001) Batch 3.599 (3.469) Remain 09:14:10 loss: 0.3888 Lr: 0.03611
[2023-08-08 00:50:50,408 INFO misc.py line 115 22900] Train: [39/100][89/156] Data 0.001 (0.001) Batch 3.405 (3.469) Remain 09:13:59 loss: 0.2368 Lr: 0.03611
[2023-08-08 00:50:54,322 INFO misc.py line 115 22900] Train: [39/100][90/156] Data 0.001 (0.001) Batch 3.914 (3.474) Remain 09:14:44 loss: 0.4453 Lr: 0.03611
[2023-08-08 00:50:57,056 INFO misc.py line 115 22900] Train: [39/100][91/156] Data 0.001 (0.001) Batch 2.734 (3.465) Remain 09:13:20 loss: 0.1600 Lr: 0.03610
[2023-08-08 00:51:00,185 INFO misc.py line 115 22900] Train: [39/100][92/156] Data 0.001 (0.001) Batch 3.130 (3.462) Remain 09:12:41 loss: 0.2863 Lr: 0.03610
[2023-08-08 00:51:03,646 INFO misc.py line 115 22900] Train: [39/100][93/156] Data 0.001 (0.001) Batch 3.461 (3.462) Remain 09:12:37 loss: 0.3153 Lr: 0.03609
[2023-08-08 00:51:07,361 INFO misc.py line 115 22900] Train: [39/100][94/156] Data 0.001 (0.001) Batch 3.715 (3.464) Remain 09:13:01 loss: 0.4458 Lr: 0.03609
[2023-08-08 00:51:12,154 INFO misc.py line 115 22900] Train: [39/100][95/156] Data 0.001 (0.001) Batch 4.792 (3.479) Remain 09:15:15 loss: 0.3801 Lr: 0.03608
[2023-08-08 00:51:16,232 INFO misc.py line 115 22900] Train: [39/100][96/156] Data 0.001 (0.001) Batch 4.078 (3.485) Remain 09:16:14 loss: 0.2473 Lr: 0.03608
[2023-08-08 00:51:19,875 INFO misc.py line 115 22900] Train: [39/100][97/156] Data 0.001 (0.001) Batch 3.643 (3.487) Remain 09:16:26 loss: 0.4822 Lr: 0.03608
[2023-08-08 00:51:23,559 INFO misc.py line 115 22900] Train: [39/100][98/156] Data 0.001 (0.001) Batch 3.685 (3.489) Remain 09:16:43 loss: 0.2078 Lr: 0.03607
[2023-08-08 00:51:27,604 INFO misc.py line 115 22900] Train: [39/100][99/156] Data 0.001 (0.001) Batch 4.044 (3.495) Remain 09:17:35 loss: 0.4897 Lr: 0.03607
[2023-08-08 00:51:31,608 INFO misc.py line 115 22900] Train: [39/100][100/156] Data 0.001 (0.001) Batch 4.005 (3.500) Remain 09:18:21 loss: 0.2967 Lr: 0.03606
[2023-08-08 00:51:35,108 INFO misc.py line 115 22900] Train: [39/100][101/156] Data 0.001 (0.001) Batch 3.500 (3.500) Remain 09:18:18 loss: 0.4149 Lr: 0.03606
[2023-08-08 00:51:38,131 INFO misc.py line 115 22900] Train: [39/100][102/156] Data 0.001 (0.001) Batch 3.022 (3.495) Remain 09:17:28 loss: 0.2566 Lr: 0.03605
[2023-08-08 00:51:42,193 INFO misc.py line 115 22900] Train: [39/100][103/156] Data 0.001 (0.001) Batch 4.062 (3.501) Remain 09:18:19 loss: 0.2937 Lr: 0.03605
[2023-08-08 00:51:45,318 INFO misc.py line 115 22900] Train: [39/100][104/156] Data 0.001 (0.001) Batch 3.125 (3.497) Remain 09:17:40 loss: 0.2259 Lr: 0.03604
[2023-08-08 00:51:48,376 INFO misc.py line 115 22900] Train: [39/100][105/156] Data 0.001 (0.001) Batch 3.058 (3.493) Remain 09:16:55 loss: 0.1843 Lr: 0.03604
[2023-08-08 00:51:51,524 INFO misc.py line 115 22900] Train: [39/100][106/156] Data 0.001 (0.001) Batch 3.147 (3.489) Remain 09:16:20 loss: 0.2453 Lr: 0.03603
[2023-08-08 00:51:55,489 INFO misc.py line 115 22900] Train: [39/100][107/156] Data 0.001 (0.001) Batch 3.966 (3.494) Remain 09:17:00 loss: 0.4481 Lr: 0.03603
[2023-08-08 00:51:59,624 INFO misc.py line 115 22900] Train: [39/100][108/156] Data 0.001 (0.001) Batch 4.134 (3.500) Remain 09:17:55 loss: 0.4591 Lr: 0.03602
[2023-08-08 00:52:03,682 INFO misc.py line 115 22900] Train: [39/100][109/156] Data 0.001 (0.001) Batch 4.058 (3.505) Remain 09:18:42 loss: 0.4157 Lr: 0.03602
[2023-08-08 00:52:06,329 INFO misc.py line 115 22900] Train: [39/100][110/156] Data 0.001 (0.001) Batch 2.648 (3.497) Remain 09:17:21 loss: 0.1003 Lr: 0.03601
[2023-08-08 00:52:09,692 INFO misc.py line 115 22900] Train: [39/100][111/156] Data 0.001 (0.001) Batch 3.363 (3.496) Remain 09:17:06 loss: 0.2394 Lr: 0.03601
[2023-08-08 00:52:13,330 INFO misc.py line 115 22900] Train: [39/100][112/156] Data 0.001 (0.001) Batch 3.637 (3.497) Remain 09:17:15 loss: 0.3493 Lr: 0.03600
[2023-08-08 00:52:17,575 INFO misc.py line 115 22900] Train: [39/100][113/156] Data 0.001 (0.001) Batch 4.245 (3.504) Remain 09:18:16 loss: 0.5450 Lr: 0.03600
[2023-08-08 00:52:20,872 INFO misc.py line 115 22900] Train: [39/100][114/156] Data 0.001 (0.001) Batch 3.297 (3.502) Remain 09:17:55 loss: 0.4413 Lr: 0.03599
[2023-08-08 00:52:24,234 INFO misc.py line 115 22900] Train: [39/100][115/156] Data 0.001 (0.001) Batch 3.362 (3.501) Remain 09:17:40 loss: 0.3977 Lr: 0.03599
[2023-08-08 00:52:28,706 INFO misc.py line 115 22900] Train: [39/100][116/156] Data 0.001 (0.001) Batch 4.472 (3.510) Remain 09:18:58 loss: 0.7671 Lr: 0.03598
[2023-08-08 00:52:32,799 INFO misc.py line 115 22900] Train: [39/100][117/156] Data 0.001 (0.001) Batch 4.093 (3.515) Remain 09:19:44 loss: 0.2596 Lr: 0.03598
[2023-08-08 00:52:35,095 INFO misc.py line 115 22900] Train: [39/100][118/156] Data 0.001 (0.001) Batch 2.296 (3.504) Remain 09:17:59 loss: 0.2391 Lr: 0.03598
[2023-08-08 00:52:37,699 INFO misc.py line 115 22900] Train: [39/100][119/156] Data 0.001 (0.001) Batch 2.604 (3.496) Remain 09:16:41 loss: 0.2173 Lr: 0.03597
[2023-08-08 00:52:39,773 INFO misc.py line 115 22900] Train: [39/100][120/156] Data 0.001 (0.001) Batch 2.074 (3.484) Remain 09:14:42 loss: 0.1713 Lr: 0.03597
[2023-08-08 00:52:43,570 INFO misc.py line 115 22900] Train: [39/100][121/156] Data 0.001 (0.001) Batch 3.797 (3.487) Remain 09:15:03 loss: 0.3112 Lr: 0.03596
[2023-08-08 00:52:46,851 INFO misc.py line 115 22900] Train: [39/100][122/156] Data 0.001 (0.001) Batch 3.281 (3.485) Remain 09:14:43 loss: 0.4227 Lr: 0.03596
[2023-08-08 00:52:49,777 INFO misc.py line 115 22900] Train: [39/100][123/156] Data 0.001 (0.001) Batch 2.925 (3.481) Remain 09:13:55 loss: 0.3566 Lr: 0.03595
[2023-08-08 00:52:53,315 INFO misc.py line 115 22900] Train: [39/100][124/156] Data 0.001 (0.001) Batch 3.539 (3.481) Remain 09:13:56 loss: 0.2155 Lr: 0.03595
[2023-08-08 00:52:56,589 INFO misc.py line 115 22900] Train: [39/100][125/156] Data 0.001 (0.001) Batch 3.274 (3.479) Remain 09:13:37 loss: 0.3856 Lr: 0.03594
[2023-08-08 00:53:00,451 INFO misc.py line 115 22900] Train: [39/100][126/156] Data 0.001 (0.001) Batch 3.861 (3.482) Remain 09:14:03 loss: 0.4132 Lr: 0.03594
[2023-08-08 00:53:03,499 INFO misc.py line 115 22900] Train: [39/100][127/156] Data 0.001 (0.001) Batch 3.048 (3.479) Remain 09:13:26 loss: 0.2245 Lr: 0.03593
[2023-08-08 00:53:06,700 INFO misc.py line 115 22900] Train: [39/100][128/156] Data 0.001 (0.001) Batch 3.201 (3.477) Remain 09:13:01 loss: 0.1670 Lr: 0.03593
[2023-08-08 00:53:10,749 INFO misc.py line 115 22900] Train: [39/100][129/156] Data 0.001 (0.001) Batch 4.049 (3.481) Remain 09:13:41 loss: 0.5780 Lr: 0.03592
[2023-08-08 00:53:14,266 INFO misc.py line 115 22900] Train: [39/100][130/156] Data 0.001 (0.001) Batch 3.517 (3.482) Remain 09:13:40 loss: 0.2783 Lr: 0.03592
[2023-08-08 00:53:17,332 INFO misc.py line 115 22900] Train: [39/100][131/156] Data 0.001 (0.001) Batch 3.066 (3.478) Remain 09:13:06 loss: 0.2829 Lr: 0.03591
[2023-08-08 00:53:20,510 INFO misc.py line 115 22900] Train: [39/100][132/156] Data 0.001 (0.001) Batch 3.178 (3.476) Remain 09:12:40 loss: 0.1250 Lr: 0.03591
[2023-08-08 00:53:23,829 INFO misc.py line 115 22900] Train: [39/100][133/156] Data 0.001 (0.001) Batch 3.320 (3.475) Remain 09:12:25 loss: 0.2069 Lr: 0.03590
[2023-08-08 00:53:26,801 INFO misc.py line 115 22900] Train: [39/100][134/156] Data 0.001 (0.001) Batch 2.972 (3.471) Remain 09:11:45 loss: 0.2821 Lr: 0.03590
[2023-08-08 00:53:29,656 INFO misc.py line 115 22900] Train: [39/100][135/156] Data 0.001 (0.001) Batch 2.855 (3.466) Remain 09:10:57 loss: 0.3742 Lr: 0.03589
[2023-08-08 00:53:33,358 INFO misc.py line 115 22900] Train: [39/100][136/156] Data 0.001 (0.001) Batch 3.702 (3.468) Remain 09:11:11 loss: 0.5227 Lr: 0.03589
[2023-08-08 00:53:35,885 INFO misc.py line 115 22900] Train: [39/100][137/156] Data 0.001 (0.001) Batch 2.526 (3.461) Remain 09:10:00 loss: 0.2298 Lr: 0.03588
[2023-08-08 00:53:39,354 INFO misc.py line 115 22900] Train: [39/100][138/156] Data 0.001 (0.001) Batch 3.469 (3.461) Remain 09:09:57 loss: 0.2829 Lr: 0.03588
[2023-08-08 00:53:41,283 INFO misc.py line 115 22900] Train: [39/100][139/156] Data 0.001 (0.001) Batch 1.929 (3.450) Remain 09:08:06 loss: 0.4487 Lr: 0.03588
[2023-08-08 00:53:44,866 INFO misc.py line 115 22900] Train: [39/100][140/156] Data 0.001 (0.001) Batch 3.583 (3.451) Remain 09:08:12 loss: 0.1913 Lr: 0.03587
[2023-08-08 00:53:48,687 INFO misc.py line 115 22900] Train: [39/100][141/156] Data 0.001 (0.001) Batch 3.822 (3.453) Remain 09:08:34 loss: 0.3260 Lr: 0.03587
[2023-08-08 00:53:51,951 INFO misc.py line 115 22900] Train: [39/100][142/156] Data 0.001 (0.001) Batch 3.264 (3.452) Remain 09:08:18 loss: 0.2850 Lr: 0.03586
[2023-08-08 00:53:55,350 INFO misc.py line 115 22900] Train: [39/100][143/156] Data 0.001 (0.001) Batch 3.399 (3.452) Remain 09:08:11 loss: 0.3359 Lr: 0.03586
[2023-08-08 00:53:58,346 INFO misc.py line 115 22900] Train: [39/100][144/156] Data 0.001 (0.001) Batch 2.996 (3.448) Remain 09:07:37 loss: 0.3154 Lr: 0.03585
[2023-08-08 00:54:01,478 INFO misc.py line 115 22900] Train: [39/100][145/156] Data 0.001 (0.001) Batch 3.132 (3.446) Remain 09:07:12 loss: 0.2882 Lr: 0.03585
[2023-08-08 00:54:05,335 INFO misc.py line 115 22900] Train: [39/100][146/156] Data 0.001 (0.001) Batch 3.856 (3.449) Remain 09:07:36 loss: 0.4170 Lr: 0.03584
[2023-08-08 00:54:08,773 INFO misc.py line 115 22900] Train: [39/100][147/156] Data 0.001 (0.001) Batch 3.439 (3.449) Remain 09:07:32 loss: 0.2193 Lr: 0.03584
[2023-08-08 00:54:12,749 INFO misc.py line 115 22900] Train: [39/100][148/156] Data 0.001 (0.001) Batch 3.976 (3.453) Remain 09:08:03 loss: 0.6568 Lr: 0.03583
[2023-08-08 00:54:17,048 INFO misc.py line 115 22900] Train: [39/100][149/156] Data 0.001 (0.001) Batch 4.298 (3.458) Remain 09:08:55 loss: 0.3252 Lr: 0.03583
[2023-08-08 00:54:20,238 INFO misc.py line 115 22900] Train: [39/100][150/156] Data 0.001 (0.001) Batch 3.191 (3.457) Remain 09:08:34 loss: 0.4597 Lr: 0.03582
[2023-08-08 00:54:24,047 INFO misc.py line 115 22900] Train: [39/100][151/156] Data 0.001 (0.001) Batch 3.809 (3.459) Remain 09:08:53 loss: 0.3368 Lr: 0.03582
[2023-08-08 00:54:27,325 INFO misc.py line 115 22900] Train: [39/100][152/156] Data 0.001 (0.001) Batch 3.278 (3.458) Remain 09:08:38 loss: 0.3660 Lr: 0.03581
[2023-08-08 00:54:30,021 INFO misc.py line 115 22900] Train: [39/100][153/156] Data 0.001 (0.001) Batch 2.697 (3.453) Remain 09:07:46 loss: 0.1823 Lr: 0.03581
[2023-08-08 00:54:34,583 INFO misc.py line 115 22900] Train: [39/100][154/156] Data 0.001 (0.001) Batch 4.562 (3.460) Remain 09:08:53 loss: 0.6931 Lr: 0.03580
[2023-08-08 00:54:38,226 INFO misc.py line 115 22900] Train: [39/100][155/156] Data 0.001 (0.001) Batch 3.642 (3.461) Remain 09:09:01 loss: 0.3553 Lr: 0.03580
[2023-08-08 00:54:41,459 INFO misc.py line 115 22900] Train: [39/100][156/156] Data 0.001 (0.001) Batch 3.233 (3.460) Remain 09:08:43 loss: 0.1999 Lr: 0.03579
[2023-08-08 00:54:41,459 INFO misc.py line 129 22900] Train result: loss: 0.3216 
[2023-08-08 00:54:41,459 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 00:54:43,585 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.9363 
[2023-08-08 00:54:44,452 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.6790 
[2023-08-08 00:54:46,118 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.8409 
[2023-08-08 00:54:47,643 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.2064 
[2023-08-08 00:54:49,489 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.8703 
[2023-08-08 00:54:51,152 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.6584 
[2023-08-08 00:54:53,290 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.2695 
[2023-08-08 00:54:55,096 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.0092 
[2023-08-08 00:54:56,378 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.4776 
[2023-08-08 00:54:58,509 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.5676 
[2023-08-08 00:54:59,034 INFO evaluator.py line 122 22900] Test: [11/24] Loss 0.9841 
[2023-08-08 00:55:00,568 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.8395 
[2023-08-08 00:55:03,281 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.1170 
[2023-08-08 00:55:04,963 INFO evaluator.py line 122 22900] Test: [14/24] Loss 1.0107 
[2023-08-08 00:55:06,986 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.3187 
[2023-08-08 00:55:09,697 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.2379 
[2023-08-08 00:55:12,405 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.4748 
[2023-08-08 00:55:14,253 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.3075 
[2023-08-08 00:55:15,002 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.1320 
[2023-08-08 00:55:15,888 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.8558 
[2023-08-08 00:55:18,149 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.4289 
[2023-08-08 00:55:20,116 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.5712 
[2023-08-08 00:55:21,966 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.9674 
[2023-08-08 00:55:23,902 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.5562 
[2023-08-08 00:55:23,950 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2163/0.2914/0.6841.
[2023-08-08 00:55:23,950 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6136/0.9802
[2023-08-08 00:55:23,950 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9439/0.9893
[2023-08-08 00:55:23,950 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1509/0.2859
[2023-08-08 00:55:23,950 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.0888/0.1627
[2023-08-08 00:55:23,950 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6296/0.7824
[2023-08-08 00:55:23,950 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.2220/0.2655
[2023-08-08 00:55:23,950 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5163/0.6663
[2023-08-08 00:55:23,950 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1400/0.1475
[2023-08-08 00:55:23,950 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.0631/0.1283
[2023-08-08 00:55:23,950 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0266/0.0268
[2023-08-08 00:55:23,950 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 00:55:23,950 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0019/0.0019
[2023-08-08 00:55:23,950 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.2271/0.3463
[2023-08-08 00:55:23,950 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0956/0.1145
[2023-08-08 00:55:23,951 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0328/0.0342
[2023-08-08 00:55:23,951 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0004/0.0004
[2023-08-08 00:55:23,951 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.1712/0.2190
[2023-08-08 00:55:23,951 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.3029/0.4751
[2023-08-08 00:55:23,951 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 00:55:23,951 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0991/0.2025
[2023-08-08 00:55:23,951 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 00:55:23,951 INFO misc.py line 152 22900] Currently Best mIoU: 0.2562
[2023-08-08 00:55:23,951 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 00:55:29,597 INFO misc.py line 115 22900] Train: [40/100][1/156] Data 1.127 (1.127) Batch 4.860 (4.860) Remain 12:50:44 loss: 0.2882 Lr: 0.03579
[2023-08-08 00:55:33,055 INFO misc.py line 115 22900] Train: [40/100][2/156] Data 0.001 (0.001) Batch 3.458 (3.458) Remain 09:08:22 loss: 0.3696 Lr: 0.03578
[2023-08-08 00:55:37,184 INFO misc.py line 115 22900] Train: [40/100][3/156] Data 0.001 (0.001) Batch 4.129 (4.129) Remain 10:54:36 loss: 0.3489 Lr: 0.03578
[2023-08-08 00:55:39,531 INFO misc.py line 115 22900] Train: [40/100][4/156] Data 0.001 (0.001) Batch 2.347 (2.347) Remain 06:12:01 loss: 0.1893 Lr: 0.03577
[2023-08-08 00:55:44,075 INFO misc.py line 115 22900] Train: [40/100][5/156] Data 0.001 (0.001) Batch 4.544 (3.445) Remain 09:06:09 loss: 0.4029 Lr: 0.03577
[2023-08-08 00:55:47,183 INFO misc.py line 115 22900] Train: [40/100][6/156] Data 0.001 (0.001) Batch 3.108 (3.333) Remain 08:48:15 loss: 0.2384 Lr: 0.03577
[2023-08-08 00:55:50,112 INFO misc.py line 115 22900] Train: [40/100][7/156] Data 0.001 (0.001) Batch 2.929 (3.232) Remain 08:32:13 loss: 0.2197 Lr: 0.03576
[2023-08-08 00:55:54,096 INFO misc.py line 115 22900] Train: [40/100][8/156] Data 0.001 (0.001) Batch 3.984 (3.382) Remain 08:56:00 loss: 0.3026 Lr: 0.03576
[2023-08-08 00:55:57,935 INFO misc.py line 115 22900] Train: [40/100][9/156] Data 0.001 (0.001) Batch 3.839 (3.458) Remain 09:07:59 loss: 0.2147 Lr: 0.03575
[2023-08-08 00:56:01,554 INFO misc.py line 115 22900] Train: [40/100][10/156] Data 0.001 (0.001) Batch 3.619 (3.481) Remain 09:11:34 loss: 0.4226 Lr: 0.03575
[2023-08-08 00:56:04,449 INFO misc.py line 115 22900] Train: [40/100][11/156] Data 0.001 (0.001) Batch 2.895 (3.408) Remain 08:59:53 loss: 0.2558 Lr: 0.03574
[2023-08-08 00:56:07,769 INFO misc.py line 115 22900] Train: [40/100][12/156] Data 0.001 (0.001) Batch 3.321 (3.398) Remain 08:58:18 loss: 0.2549 Lr: 0.03574
[2023-08-08 00:56:11,551 INFO misc.py line 115 22900] Train: [40/100][13/156] Data 0.001 (0.001) Batch 3.782 (3.437) Remain 09:04:19 loss: 0.4111 Lr: 0.03573
[2023-08-08 00:56:15,012 INFO misc.py line 115 22900] Train: [40/100][14/156] Data 0.001 (0.001) Batch 3.461 (3.439) Remain 09:04:36 loss: 0.2028 Lr: 0.03573
[2023-08-08 00:56:18,757 INFO misc.py line 115 22900] Train: [40/100][15/156] Data 0.001 (0.001) Batch 3.745 (3.464) Remain 09:08:35 loss: 0.4238 Lr: 0.03572
[2023-08-08 00:56:22,889 INFO misc.py line 115 22900] Train: [40/100][16/156] Data 0.001 (0.001) Batch 4.132 (3.516) Remain 09:16:40 loss: 0.4328 Lr: 0.03572
[2023-08-08 00:56:26,205 INFO misc.py line 115 22900] Train: [40/100][17/156] Data 0.001 (0.001) Batch 3.315 (3.501) Remain 09:14:20 loss: 0.2515 Lr: 0.03571
[2023-08-08 00:56:30,452 INFO misc.py line 115 22900] Train: [40/100][18/156] Data 0.001 (0.001) Batch 4.247 (3.551) Remain 09:22:09 loss: 0.4259 Lr: 0.03571
[2023-08-08 00:56:34,042 INFO misc.py line 115 22900] Train: [40/100][19/156] Data 0.001 (0.001) Batch 3.590 (3.554) Remain 09:22:28 loss: 0.2768 Lr: 0.03570
[2023-08-08 00:56:37,562 INFO misc.py line 115 22900] Train: [40/100][20/156] Data 0.001 (0.001) Batch 3.520 (3.552) Remain 09:22:06 loss: 0.4184 Lr: 0.03570
[2023-08-08 00:56:41,333 INFO misc.py line 115 22900] Train: [40/100][21/156] Data 0.001 (0.001) Batch 3.771 (3.564) Remain 09:23:58 loss: 0.4432 Lr: 0.03569
[2023-08-08 00:56:44,487 INFO misc.py line 115 22900] Train: [40/100][22/156] Data 0.001 (0.001) Batch 3.155 (3.542) Remain 09:20:30 loss: 0.4483 Lr: 0.03569
[2023-08-08 00:56:48,156 INFO misc.py line 115 22900] Train: [40/100][23/156] Data 0.001 (0.001) Batch 3.668 (3.549) Remain 09:21:26 loss: 0.3004 Lr: 0.03568
[2023-08-08 00:56:50,658 INFO misc.py line 115 22900] Train: [40/100][24/156] Data 0.001 (0.001) Batch 2.502 (3.499) Remain 09:13:30 loss: 0.1472 Lr: 0.03568
[2023-08-08 00:56:53,310 INFO misc.py line 115 22900] Train: [40/100][25/156] Data 0.001 (0.001) Batch 2.652 (3.460) Remain 09:07:21 loss: 0.3491 Lr: 0.03567
[2023-08-08 00:56:57,823 INFO misc.py line 115 22900] Train: [40/100][26/156] Data 0.001 (0.001) Batch 4.513 (3.506) Remain 09:14:32 loss: 0.3640 Lr: 0.03567
[2023-08-08 00:57:00,401 INFO misc.py line 115 22900] Train: [40/100][27/156] Data 0.001 (0.001) Batch 2.578 (3.467) Remain 09:08:21 loss: 0.1023 Lr: 0.03566
[2023-08-08 00:57:04,473 INFO misc.py line 115 22900] Train: [40/100][28/156] Data 0.001 (0.001) Batch 4.072 (3.492) Remain 09:12:07 loss: 0.1859 Lr: 0.03566
[2023-08-08 00:57:08,483 INFO misc.py line 115 22900] Train: [40/100][29/156] Data 0.001 (0.001) Batch 4.010 (3.511) Remain 09:15:13 loss: 0.2320 Lr: 0.03566
[2023-08-08 00:57:11,698 INFO misc.py line 115 22900] Train: [40/100][30/156] Data 0.001 (0.001) Batch 3.215 (3.501) Remain 09:13:25 loss: 0.2431 Lr: 0.03565
[2023-08-08 00:57:15,161 INFO misc.py line 115 22900] Train: [40/100][31/156] Data 0.001 (0.001) Batch 3.463 (3.499) Remain 09:13:09 loss: 0.2419 Lr: 0.03565
[2023-08-08 00:57:18,177 INFO misc.py line 115 22900] Train: [40/100][32/156] Data 0.001 (0.001) Batch 3.016 (3.483) Remain 09:10:28 loss: 0.1317 Lr: 0.03564
[2023-08-08 00:57:20,773 INFO misc.py line 115 22900] Train: [40/100][33/156] Data 0.001 (0.001) Batch 2.596 (3.453) Remain 09:05:44 loss: 0.2548 Lr: 0.03564
[2023-08-08 00:57:24,303 INFO misc.py line 115 22900] Train: [40/100][34/156] Data 0.001 (0.001) Batch 3.530 (3.455) Remain 09:06:04 loss: 0.2841 Lr: 0.03563
[2023-08-08 00:57:27,621 INFO misc.py line 115 22900] Train: [40/100][35/156] Data 0.001 (0.001) Batch 3.318 (3.451) Remain 09:05:20 loss: 0.2619 Lr: 0.03563
[2023-08-08 00:57:31,931 INFO misc.py line 115 22900] Train: [40/100][36/156] Data 0.001 (0.001) Batch 4.311 (3.477) Remain 09:09:23 loss: 0.3764 Lr: 0.03562
[2023-08-08 00:57:34,992 INFO misc.py line 115 22900] Train: [40/100][37/156] Data 0.001 (0.001) Batch 3.061 (3.465) Remain 09:07:24 loss: 0.2810 Lr: 0.03562
[2023-08-08 00:57:38,842 INFO misc.py line 115 22900] Train: [40/100][38/156] Data 0.001 (0.001) Batch 3.849 (3.476) Remain 09:09:04 loss: 0.4235 Lr: 0.03561
[2023-08-08 00:57:43,050 INFO misc.py line 115 22900] Train: [40/100][39/156] Data 0.001 (0.001) Batch 4.209 (3.496) Remain 09:12:14 loss: 0.4902 Lr: 0.03561
[2023-08-08 00:57:46,798 INFO misc.py line 115 22900] Train: [40/100][40/156] Data 0.001 (0.001) Batch 3.748 (3.503) Remain 09:13:15 loss: 0.2390 Lr: 0.03560
[2023-08-08 00:57:50,733 INFO misc.py line 115 22900] Train: [40/100][41/156] Data 0.001 (0.001) Batch 3.935 (3.514) Remain 09:14:59 loss: 0.3193 Lr: 0.03560
[2023-08-08 00:57:54,531 INFO misc.py line 115 22900] Train: [40/100][42/156] Data 0.001 (0.001) Batch 3.797 (3.522) Remain 09:16:04 loss: 0.1959 Lr: 0.03559
[2023-08-08 00:57:57,409 INFO misc.py line 115 22900] Train: [40/100][43/156] Data 0.001 (0.001) Batch 2.878 (3.506) Remain 09:13:28 loss: 0.3310 Lr: 0.03559
[2023-08-08 00:58:00,097 INFO misc.py line 115 22900] Train: [40/100][44/156] Data 0.001 (0.001) Batch 2.688 (3.486) Remain 09:10:16 loss: 0.2064 Lr: 0.03558
[2023-08-08 00:58:03,506 INFO misc.py line 115 22900] Train: [40/100][45/156] Data 0.001 (0.001) Batch 3.409 (3.484) Remain 09:09:55 loss: 0.2327 Lr: 0.03558
[2023-08-08 00:58:07,601 INFO misc.py line 115 22900] Train: [40/100][46/156] Data 0.001 (0.001) Batch 4.095 (3.498) Remain 09:12:06 loss: 0.4663 Lr: 0.03557
[2023-08-08 00:58:10,605 INFO misc.py line 115 22900] Train: [40/100][47/156] Data 0.001 (0.001) Batch 3.004 (3.487) Remain 09:10:16 loss: 0.1991 Lr: 0.03557
[2023-08-08 00:58:14,301 INFO misc.py line 115 22900] Train: [40/100][48/156] Data 0.001 (0.001) Batch 3.696 (3.491) Remain 09:10:57 loss: 0.3303 Lr: 0.03556
[2023-08-08 00:58:17,445 INFO misc.py line 115 22900] Train: [40/100][49/156] Data 0.001 (0.001) Batch 3.144 (3.484) Remain 09:09:42 loss: 0.2560 Lr: 0.03556
[2023-08-08 00:58:20,295 INFO misc.py line 115 22900] Train: [40/100][50/156] Data 0.001 (0.001) Batch 2.850 (3.470) Remain 09:07:31 loss: 0.1886 Lr: 0.03555
[2023-08-08 00:58:24,373 INFO misc.py line 115 22900] Train: [40/100][51/156] Data 0.001 (0.001) Batch 4.078 (3.483) Remain 09:09:27 loss: 0.3690 Lr: 0.03555
[2023-08-08 00:58:28,014 INFO misc.py line 115 22900] Train: [40/100][52/156] Data 0.001 (0.001) Batch 3.641 (3.486) Remain 09:09:54 loss: 0.3992 Lr: 0.03554
[2023-08-08 00:58:32,023 INFO misc.py line 115 22900] Train: [40/100][53/156] Data 0.001 (0.001) Batch 4.009 (3.497) Remain 09:11:30 loss: 0.3627 Lr: 0.03554
[2023-08-08 00:58:35,457 INFO misc.py line 115 22900] Train: [40/100][54/156] Data 0.001 (0.001) Batch 3.433 (3.496) Remain 09:11:14 loss: 0.4362 Lr: 0.03554
[2023-08-08 00:58:39,459 INFO misc.py line 115 22900] Train: [40/100][55/156] Data 0.001 (0.001) Batch 4.003 (3.505) Remain 09:12:43 loss: 0.4678 Lr: 0.03553
[2023-08-08 00:58:42,827 INFO misc.py line 115 22900] Train: [40/100][56/156] Data 0.001 (0.001) Batch 3.368 (3.503) Remain 09:12:15 loss: 0.2577 Lr: 0.03553
[2023-08-08 00:58:46,463 INFO misc.py line 115 22900] Train: [40/100][57/156] Data 0.001 (0.001) Batch 3.635 (3.505) Remain 09:12:35 loss: 0.2588 Lr: 0.03552
[2023-08-08 00:58:48,900 INFO misc.py line 115 22900] Train: [40/100][58/156] Data 0.001 (0.001) Batch 2.437 (3.486) Remain 09:09:28 loss: 0.3667 Lr: 0.03552
[2023-08-08 00:58:52,372 INFO misc.py line 115 22900] Train: [40/100][59/156] Data 0.001 (0.001) Batch 3.472 (3.485) Remain 09:09:22 loss: 0.3728 Lr: 0.03551
[2023-08-08 00:58:55,931 INFO misc.py line 115 22900] Train: [40/100][60/156] Data 0.001 (0.001) Batch 3.559 (3.487) Remain 09:09:31 loss: 0.2964 Lr: 0.03551
[2023-08-08 00:58:58,979 INFO misc.py line 115 22900] Train: [40/100][61/156] Data 0.001 (0.001) Batch 3.049 (3.479) Remain 09:08:16 loss: 0.1744 Lr: 0.03550
[2023-08-08 00:59:01,716 INFO misc.py line 115 22900] Train: [40/100][62/156] Data 0.001 (0.001) Batch 2.737 (3.467) Remain 09:06:13 loss: 0.2207 Lr: 0.03550
[2023-08-08 00:59:05,283 INFO misc.py line 115 22900] Train: [40/100][63/156] Data 0.001 (0.001) Batch 3.567 (3.468) Remain 09:06:25 loss: 0.3163 Lr: 0.03549
[2023-08-08 00:59:08,968 INFO misc.py line 115 22900] Train: [40/100][64/156] Data 0.001 (0.001) Batch 3.685 (3.472) Remain 09:06:56 loss: 0.4888 Lr: 0.03549
[2023-08-08 00:59:12,333 INFO misc.py line 115 22900] Train: [40/100][65/156] Data 0.001 (0.001) Batch 3.365 (3.470) Remain 09:06:36 loss: 0.3654 Lr: 0.03548
[2023-08-08 00:59:16,409 INFO misc.py line 115 22900] Train: [40/100][66/156] Data 0.001 (0.001) Batch 4.076 (3.480) Remain 09:08:03 loss: 0.5684 Lr: 0.03548
[2023-08-08 00:59:20,425 INFO misc.py line 115 22900] Train: [40/100][67/156] Data 0.001 (0.001) Batch 4.016 (3.488) Remain 09:09:19 loss: 0.4242 Lr: 0.03547
[2023-08-08 00:59:23,701 INFO misc.py line 115 22900] Train: [40/100][68/156] Data 0.001 (0.001) Batch 3.277 (3.485) Remain 09:08:45 loss: 0.2963 Lr: 0.03547
[2023-08-08 00:59:27,998 INFO misc.py line 115 22900] Train: [40/100][69/156] Data 0.001 (0.001) Batch 4.297 (3.497) Remain 09:10:37 loss: 0.4572 Lr: 0.03546
[2023-08-08 00:59:31,735 INFO misc.py line 115 22900] Train: [40/100][70/156] Data 0.001 (0.001) Batch 3.737 (3.501) Remain 09:11:08 loss: 0.3780 Lr: 0.03546
[2023-08-08 00:59:34,929 INFO misc.py line 115 22900] Train: [40/100][71/156] Data 0.001 (0.001) Batch 3.195 (3.496) Remain 09:10:22 loss: 0.4344 Lr: 0.03545
[2023-08-08 00:59:37,845 INFO misc.py line 115 22900] Train: [40/100][72/156] Data 0.001 (0.001) Batch 2.916 (3.488) Remain 09:08:59 loss: 0.2740 Lr: 0.03545
[2023-08-08 00:59:41,203 INFO misc.py line 115 22900] Train: [40/100][73/156] Data 0.001 (0.001) Batch 3.357 (3.486) Remain 09:08:38 loss: 0.3670 Lr: 0.03544
[2023-08-08 00:59:44,550 INFO misc.py line 115 22900] Train: [40/100][74/156] Data 0.001 (0.001) Batch 3.348 (3.484) Remain 09:08:16 loss: 0.1527 Lr: 0.03544
[2023-08-08 00:59:48,546 INFO misc.py line 115 22900] Train: [40/100][75/156] Data 0.001 (0.001) Batch 3.995 (3.491) Remain 09:09:19 loss: 0.4746 Lr: 0.03543
[2023-08-08 00:59:52,143 INFO misc.py line 115 22900] Train: [40/100][76/156] Data 0.001 (0.001) Batch 3.597 (3.493) Remain 09:09:30 loss: 0.3469 Lr: 0.03543
[2023-08-08 00:59:54,771 INFO misc.py line 115 22900] Train: [40/100][77/156] Data 0.001 (0.001) Batch 2.628 (3.481) Remain 09:07:36 loss: 1.0294 Lr: 0.03542
[2023-08-08 00:59:58,244 INFO misc.py line 115 22900] Train: [40/100][78/156] Data 0.001 (0.001) Batch 3.473 (3.481) Remain 09:07:31 loss: 0.4383 Lr: 0.03542
[2023-08-08 01:00:02,054 INFO misc.py line 115 22900] Train: [40/100][79/156] Data 0.001 (0.001) Batch 3.810 (3.485) Remain 09:08:09 loss: 0.3547 Lr: 0.03541
[2023-08-08 01:00:06,154 INFO misc.py line 115 22900] Train: [40/100][80/156] Data 0.001 (0.001) Batch 4.100 (3.493) Remain 09:09:21 loss: 0.5023 Lr: 0.03541
[2023-08-08 01:00:09,203 INFO misc.py line 115 22900] Train: [40/100][81/156] Data 0.001 (0.001) Batch 3.050 (3.487) Remain 09:08:23 loss: 0.4701 Lr: 0.03541
[2023-08-08 01:00:13,107 INFO misc.py line 115 22900] Train: [40/100][82/156] Data 0.001 (0.001) Batch 3.904 (3.493) Remain 09:09:10 loss: 0.3288 Lr: 0.03540
[2023-08-08 01:00:17,776 INFO misc.py line 115 22900] Train: [40/100][83/156] Data 0.001 (0.001) Batch 4.669 (3.507) Remain 09:11:25 loss: 0.4734 Lr: 0.03540
[2023-08-08 01:00:21,942 INFO misc.py line 115 22900] Train: [40/100][84/156] Data 0.001 (0.001) Batch 4.166 (3.516) Remain 09:12:38 loss: 0.4404 Lr: 0.03539
[2023-08-08 01:00:25,383 INFO misc.py line 115 22900] Train: [40/100][85/156] Data 0.001 (0.001) Batch 3.441 (3.515) Remain 09:12:26 loss: 0.4061 Lr: 0.03539
[2023-08-08 01:00:27,557 INFO misc.py line 115 22900] Train: [40/100][86/156] Data 0.001 (0.001) Batch 2.175 (3.498) Remain 09:09:50 loss: 0.2076 Lr: 0.03538
[2023-08-08 01:00:30,751 INFO misc.py line 115 22900] Train: [40/100][87/156] Data 0.001 (0.001) Batch 3.193 (3.495) Remain 09:09:12 loss: 0.3539 Lr: 0.03538
[2023-08-08 01:00:34,371 INFO misc.py line 115 22900] Train: [40/100][88/156] Data 0.001 (0.001) Batch 3.620 (3.496) Remain 09:09:23 loss: 0.4291 Lr: 0.03537
[2023-08-08 01:00:36,586 INFO misc.py line 115 22900] Train: [40/100][89/156] Data 0.001 (0.001) Batch 2.215 (3.481) Remain 09:06:59 loss: 0.1167 Lr: 0.03537
[2023-08-08 01:00:39,881 INFO misc.py line 115 22900] Train: [40/100][90/156] Data 0.001 (0.001) Batch 3.295 (3.479) Remain 09:06:35 loss: 0.4451 Lr: 0.03536
[2023-08-08 01:00:43,482 INFO misc.py line 115 22900] Train: [40/100][91/156] Data 0.001 (0.001) Batch 3.601 (3.481) Remain 09:06:45 loss: 0.3685 Lr: 0.03536
[2023-08-08 01:00:47,052 INFO misc.py line 115 22900] Train: [40/100][92/156] Data 0.001 (0.001) Batch 3.570 (3.482) Remain 09:06:51 loss: 0.3555 Lr: 0.03535
[2023-08-08 01:00:49,477 INFO misc.py line 115 22900] Train: [40/100][93/156] Data 0.001 (0.001) Batch 2.425 (3.470) Remain 09:04:57 loss: 0.2777 Lr: 0.03535
[2023-08-08 01:00:53,580 INFO misc.py line 115 22900] Train: [40/100][94/156] Data 0.001 (0.001) Batch 4.104 (3.477) Remain 09:05:59 loss: 0.3226 Lr: 0.03534
[2023-08-08 01:00:58,030 INFO misc.py line 115 22900] Train: [40/100][95/156] Data 0.001 (0.001) Batch 4.450 (3.487) Remain 09:07:35 loss: 0.5043 Lr: 0.03534
[2023-08-08 01:01:00,947 INFO misc.py line 115 22900] Train: [40/100][96/156] Data 0.001 (0.001) Batch 2.917 (3.481) Remain 09:06:34 loss: 0.2667 Lr: 0.03533
[2023-08-08 01:01:05,047 INFO misc.py line 115 22900] Train: [40/100][97/156] Data 0.001 (0.001) Batch 4.099 (3.488) Remain 09:07:32 loss: 0.3351 Lr: 0.03533
[2023-08-08 01:01:09,132 INFO misc.py line 115 22900] Train: [40/100][98/156] Data 0.001 (0.001) Batch 4.085 (3.494) Remain 09:08:28 loss: 0.5595 Lr: 0.03532
[2023-08-08 01:01:12,438 INFO misc.py line 115 22900] Train: [40/100][99/156] Data 0.001 (0.001) Batch 3.306 (3.492) Remain 09:08:06 loss: 0.3769 Lr: 0.03532
[2023-08-08 01:01:16,146 INFO misc.py line 115 22900] Train: [40/100][100/156] Data 0.001 (0.001) Batch 3.709 (3.494) Remain 09:08:23 loss: 0.3647 Lr: 0.03531
[2023-08-08 01:01:18,879 INFO misc.py line 115 22900] Train: [40/100][101/156] Data 0.001 (0.001) Batch 2.733 (3.487) Remain 09:07:07 loss: 0.3884 Lr: 0.03531
[2023-08-08 01:01:22,635 INFO misc.py line 115 22900] Train: [40/100][102/156] Data 0.001 (0.001) Batch 3.755 (3.489) Remain 09:07:29 loss: 0.4766 Lr: 0.03530
[2023-08-08 01:01:25,999 INFO misc.py line 115 22900] Train: [40/100][103/156] Data 0.001 (0.001) Batch 3.365 (3.488) Remain 09:07:13 loss: 0.2948 Lr: 0.03530
[2023-08-08 01:01:28,580 INFO misc.py line 115 22900] Train: [40/100][104/156] Data 0.001 (0.001) Batch 2.580 (3.479) Remain 09:05:45 loss: 0.2101 Lr: 0.03529
[2023-08-08 01:01:32,262 INFO misc.py line 115 22900] Train: [40/100][105/156] Data 0.001 (0.001) Batch 3.682 (3.481) Remain 09:06:01 loss: 0.3382 Lr: 0.03529
[2023-08-08 01:01:36,220 INFO misc.py line 115 22900] Train: [40/100][106/156] Data 0.001 (0.001) Batch 3.958 (3.486) Remain 09:06:41 loss: 0.6017 Lr: 0.03528
[2023-08-08 01:01:38,876 INFO misc.py line 115 22900] Train: [40/100][107/156] Data 0.001 (0.001) Batch 2.656 (3.478) Remain 09:05:22 loss: 0.1932 Lr: 0.03528
[2023-08-08 01:01:42,173 INFO misc.py line 115 22900] Train: [40/100][108/156] Data 0.001 (0.001) Batch 3.297 (3.476) Remain 09:05:02 loss: 0.3881 Lr: 0.03527
[2023-08-08 01:01:44,533 INFO misc.py line 115 22900] Train: [40/100][109/156] Data 0.001 (0.001) Batch 2.360 (3.466) Remain 09:03:20 loss: 0.2777 Lr: 0.03527
[2023-08-08 01:01:48,289 INFO misc.py line 115 22900] Train: [40/100][110/156] Data 0.001 (0.001) Batch 3.757 (3.468) Remain 09:03:42 loss: 0.3103 Lr: 0.03527
[2023-08-08 01:01:51,818 INFO misc.py line 115 22900] Train: [40/100][111/156] Data 0.001 (0.001) Batch 3.529 (3.469) Remain 09:03:44 loss: 0.2413 Lr: 0.03526
[2023-08-08 01:01:55,813 INFO misc.py line 115 22900] Train: [40/100][112/156] Data 0.001 (0.001) Batch 3.995 (3.474) Remain 09:04:26 loss: 0.5258 Lr: 0.03526
[2023-08-08 01:01:59,651 INFO misc.py line 115 22900] Train: [40/100][113/156] Data 0.001 (0.001) Batch 3.838 (3.477) Remain 09:04:53 loss: 0.2111 Lr: 0.03525
[2023-08-08 01:02:03,463 INFO misc.py line 115 22900] Train: [40/100][114/156] Data 0.001 (0.001) Batch 3.813 (3.480) Remain 09:05:18 loss: 0.4931 Lr: 0.03525
[2023-08-08 01:02:06,331 INFO misc.py line 115 22900] Train: [40/100][115/156] Data 0.001 (0.001) Batch 2.868 (3.475) Remain 09:04:24 loss: 0.1686 Lr: 0.03524
[2023-08-08 01:02:10,306 INFO misc.py line 115 22900] Train: [40/100][116/156] Data 0.001 (0.001) Batch 3.975 (3.479) Remain 09:05:02 loss: 0.5724 Lr: 0.03524
[2023-08-08 01:02:12,475 INFO misc.py line 115 22900] Train: [40/100][117/156] Data 0.001 (0.001) Batch 2.169 (3.467) Remain 09:03:10 loss: 0.1264 Lr: 0.03523
[2023-08-08 01:02:16,138 INFO misc.py line 115 22900] Train: [40/100][118/156] Data 0.001 (0.001) Batch 3.663 (3.469) Remain 09:03:23 loss: 0.1820 Lr: 0.03523
[2023-08-08 01:02:19,906 INFO misc.py line 115 22900] Train: [40/100][119/156] Data 0.001 (0.001) Batch 3.768 (3.472) Remain 09:03:43 loss: 0.3040 Lr: 0.03522
[2023-08-08 01:02:23,356 INFO misc.py line 115 22900] Train: [40/100][120/156] Data 0.001 (0.001) Batch 3.450 (3.472) Remain 09:03:38 loss: 0.1618 Lr: 0.03522
[2023-08-08 01:02:26,452 INFO misc.py line 115 22900] Train: [40/100][121/156] Data 0.001 (0.001) Batch 3.096 (3.468) Remain 09:03:05 loss: 0.2941 Lr: 0.03521
[2023-08-08 01:02:29,795 INFO misc.py line 115 22900] Train: [40/100][122/156] Data 0.001 (0.001) Batch 3.343 (3.467) Remain 09:02:51 loss: 0.4147 Lr: 0.03521
[2023-08-08 01:02:33,407 INFO misc.py line 115 22900] Train: [40/100][123/156] Data 0.001 (0.001) Batch 3.612 (3.469) Remain 09:02:59 loss: 0.3195 Lr: 0.03520
[2023-08-08 01:02:37,850 INFO misc.py line 115 22900] Train: [40/100][124/156] Data 0.001 (0.001) Batch 4.443 (3.477) Remain 09:04:12 loss: 0.4318 Lr: 0.03520
[2023-08-08 01:02:42,599 INFO misc.py line 115 22900] Train: [40/100][125/156] Data 0.001 (0.001) Batch 4.749 (3.487) Remain 09:05:46 loss: 0.7608 Lr: 0.03519
[2023-08-08 01:02:46,064 INFO misc.py line 115 22900] Train: [40/100][126/156] Data 0.001 (0.001) Batch 3.465 (3.487) Remain 09:05:41 loss: 0.2618 Lr: 0.03519
[2023-08-08 01:02:49,058 INFO misc.py line 115 22900] Train: [40/100][127/156] Data 0.001 (0.001) Batch 2.994 (3.483) Remain 09:05:00 loss: 0.2879 Lr: 0.03518
[2023-08-08 01:02:52,366 INFO misc.py line 115 22900] Train: [40/100][128/156] Data 0.001 (0.001) Batch 3.308 (3.481) Remain 09:04:43 loss: 0.2482 Lr: 0.03518
[2023-08-08 01:02:56,092 INFO misc.py line 115 22900] Train: [40/100][129/156] Data 0.001 (0.001) Batch 3.725 (3.483) Remain 09:04:58 loss: 0.4186 Lr: 0.03517
[2023-08-08 01:02:59,504 INFO misc.py line 115 22900] Train: [40/100][130/156] Data 0.001 (0.001) Batch 3.413 (3.483) Remain 09:04:49 loss: 0.2902 Lr: 0.03517
[2023-08-08 01:03:03,726 INFO misc.py line 115 22900] Train: [40/100][131/156] Data 0.001 (0.001) Batch 4.221 (3.489) Remain 09:05:40 loss: 0.4208 Lr: 0.03516
[2023-08-08 01:03:06,558 INFO misc.py line 115 22900] Train: [40/100][132/156] Data 0.001 (0.001) Batch 2.832 (3.484) Remain 09:04:49 loss: 0.1522 Lr: 0.03516
[2023-08-08 01:03:09,914 INFO misc.py line 115 22900] Train: [40/100][133/156] Data 0.001 (0.001) Batch 3.357 (3.483) Remain 09:04:36 loss: 0.2313 Lr: 0.03515
[2023-08-08 01:03:13,086 INFO misc.py line 115 22900] Train: [40/100][134/156] Data 0.001 (0.001) Batch 3.172 (3.480) Remain 09:04:10 loss: 0.3545 Lr: 0.03515
[2023-08-08 01:03:16,278 INFO misc.py line 115 22900] Train: [40/100][135/156] Data 0.001 (0.001) Batch 3.192 (3.478) Remain 09:03:46 loss: 0.1424 Lr: 0.03514
[2023-08-08 01:03:19,939 INFO misc.py line 115 22900] Train: [40/100][136/156] Data 0.001 (0.001) Batch 3.661 (3.479) Remain 09:03:56 loss: 0.4166 Lr: 0.03514
[2023-08-08 01:03:23,378 INFO misc.py line 115 22900] Train: [40/100][137/156] Data 0.001 (0.001) Batch 3.439 (3.479) Remain 09:03:50 loss: 0.3915 Lr: 0.03513
[2023-08-08 01:03:25,854 INFO misc.py line 115 22900] Train: [40/100][138/156] Data 0.001 (0.001) Batch 2.476 (3.472) Remain 09:02:36 loss: 0.2206 Lr: 0.03513
[2023-08-08 01:03:29,274 INFO misc.py line 115 22900] Train: [40/100][139/156] Data 0.001 (0.001) Batch 3.420 (3.471) Remain 09:02:29 loss: 0.4333 Lr: 0.03512
[2023-08-08 01:03:32,891 INFO misc.py line 115 22900] Train: [40/100][140/156] Data 0.001 (0.001) Batch 3.617 (3.472) Remain 09:02:36 loss: 0.2437 Lr: 0.03512
[2023-08-08 01:03:35,844 INFO misc.py line 115 22900] Train: [40/100][141/156] Data 0.001 (0.001) Batch 2.954 (3.469) Remain 09:01:57 loss: 0.2287 Lr: 0.03512
[2023-08-08 01:03:39,046 INFO misc.py line 115 22900] Train: [40/100][142/156] Data 0.001 (0.001) Batch 3.202 (3.467) Remain 09:01:36 loss: 0.3619 Lr: 0.03511
[2023-08-08 01:03:42,338 INFO misc.py line 115 22900] Train: [40/100][143/156] Data 0.001 (0.001) Batch 3.291 (3.465) Remain 09:01:21 loss: 0.2875 Lr: 0.03511
[2023-08-08 01:03:45,452 INFO misc.py line 115 22900] Train: [40/100][144/156] Data 0.001 (0.001) Batch 3.114 (3.463) Remain 09:00:54 loss: 0.2930 Lr: 0.03510
[2023-08-08 01:03:49,538 INFO misc.py line 115 22900] Train: [40/100][145/156] Data 0.001 (0.001) Batch 4.086 (3.467) Remain 09:01:31 loss: 0.5746 Lr: 0.03510
[2023-08-08 01:03:52,373 INFO misc.py line 115 22900] Train: [40/100][146/156] Data 0.001 (0.001) Batch 2.835 (3.463) Remain 09:00:46 loss: 0.1890 Lr: 0.03509
[2023-08-08 01:03:55,368 INFO misc.py line 115 22900] Train: [40/100][147/156] Data 0.001 (0.001) Batch 2.995 (3.460) Remain 09:00:13 loss: 0.2620 Lr: 0.03509
[2023-08-08 01:03:59,175 INFO misc.py line 115 22900] Train: [40/100][148/156] Data 0.001 (0.001) Batch 3.806 (3.462) Remain 09:00:32 loss: 0.3071 Lr: 0.03508
[2023-08-08 01:04:03,289 INFO misc.py line 115 22900] Train: [40/100][149/156] Data 0.001 (0.001) Batch 4.114 (3.466) Remain 09:01:10 loss: 0.3420 Lr: 0.03508
[2023-08-08 01:04:06,498 INFO misc.py line 115 22900] Train: [40/100][150/156] Data 0.001 (0.001) Batch 3.209 (3.465) Remain 09:00:50 loss: 0.1648 Lr: 0.03507
[2023-08-08 01:04:09,565 INFO misc.py line 115 22900] Train: [40/100][151/156] Data 0.001 (0.001) Batch 3.067 (3.462) Remain 09:00:21 loss: 0.2712 Lr: 0.03507
[2023-08-08 01:04:13,428 INFO misc.py line 115 22900] Train: [40/100][152/156] Data 0.001 (0.001) Batch 3.863 (3.465) Remain 09:00:43 loss: 0.2117 Lr: 0.03506
[2023-08-08 01:04:17,297 INFO misc.py line 115 22900] Train: [40/100][153/156] Data 0.001 (0.001) Batch 3.869 (3.467) Remain 09:01:05 loss: 0.3800 Lr: 0.03506
[2023-08-08 01:04:20,875 INFO misc.py line 115 22900] Train: [40/100][154/156] Data 0.001 (0.001) Batch 3.578 (3.468) Remain 09:01:08 loss: 0.3967 Lr: 0.03505
[2023-08-08 01:04:24,077 INFO misc.py line 115 22900] Train: [40/100][155/156] Data 0.001 (0.001) Batch 3.203 (3.466) Remain 09:00:48 loss: 0.2589 Lr: 0.03505
[2023-08-08 01:04:27,407 INFO misc.py line 115 22900] Train: [40/100][156/156] Data 0.001 (0.001) Batch 3.330 (3.466) Remain 09:00:37 loss: 0.2383 Lr: 0.03504
[2023-08-08 01:04:27,407 INFO misc.py line 129 22900] Train result: loss: 0.3327 
[2023-08-08 01:04:27,407 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 01:04:29,502 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.3192 
[2023-08-08 01:04:30,372 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.4778 
[2023-08-08 01:04:32,036 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.8585 
[2023-08-08 01:04:33,558 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.5335 
[2023-08-08 01:04:35,403 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.6461 
[2023-08-08 01:04:37,068 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.8432 
[2023-08-08 01:04:39,207 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.3763 
[2023-08-08 01:04:41,012 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.4205 
[2023-08-08 01:04:42,297 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.5996 
[2023-08-08 01:04:44,428 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.5693 
[2023-08-08 01:04:44,953 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.0455 
[2023-08-08 01:04:46,486 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.8982 
[2023-08-08 01:04:49,192 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.2773 
[2023-08-08 01:04:50,868 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.8848 
[2023-08-08 01:04:52,893 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.4027 
[2023-08-08 01:04:55,603 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.1161 
[2023-08-08 01:04:58,309 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.5055 
[2023-08-08 01:05:00,155 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.6334 
[2023-08-08 01:05:00,903 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.3170 
[2023-08-08 01:05:01,788 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.7556 
[2023-08-08 01:05:04,051 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.4451 
[2023-08-08 01:05:06,015 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.8095 
[2023-08-08 01:05:07,860 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.7770 
[2023-08-08 01:05:09,797 INFO evaluator.py line 122 22900] Test: [24/24] Loss 2.0705 
[2023-08-08 01:05:09,846 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2118/0.2977/0.6563.
[2023-08-08 01:05:09,846 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6259/0.9606
[2023-08-08 01:05:09,847 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9466/0.9904
[2023-08-08 01:05:09,847 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1450/0.2743
[2023-08-08 01:05:09,847 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1230/0.2552
[2023-08-08 01:05:09,847 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.4557/0.4725
[2023-08-08 01:05:09,847 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.2980/0.5368
[2023-08-08 01:05:09,847 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.4912/0.6418
[2023-08-08 01:05:09,847 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0490/0.0522
[2023-08-08 01:05:09,847 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1344/0.4147
[2023-08-08 01:05:09,847 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0906/0.0920
[2023-08-08 01:05:09,847 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 01:05:09,847 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0259/0.0277
[2023-08-08 01:05:09,847 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1629/0.2722
[2023-08-08 01:05:09,847 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0397/0.0443
[2023-08-08 01:05:09,847 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0275/0.0369
[2023-08-08 01:05:09,847 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-08 01:05:09,847 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.1187/0.1596
[2023-08-08 01:05:09,847 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.4086/0.4664
[2023-08-08 01:05:09,847 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 01:05:09,847 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0941/0.2570
[2023-08-08 01:05:09,847 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 01:05:09,847 INFO misc.py line 152 22900] Currently Best mIoU: 0.2562
[2023-08-08 01:05:09,848 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 01:05:13,304 INFO misc.py line 115 22900] Train: [41/100][1/156] Data 0.559 (0.559) Batch 2.657 (2.657) Remain 06:54:30 loss: 0.1933 Lr: 0.03504
[2023-08-08 01:05:17,370 INFO misc.py line 115 22900] Train: [41/100][2/156] Data 0.003 (0.003) Batch 4.068 (4.068) Remain 10:34:24 loss: 0.2833 Lr: 0.03503
[2023-08-08 01:05:20,791 INFO misc.py line 115 22900] Train: [41/100][3/156] Data 0.001 (0.001) Batch 3.421 (3.421) Remain 08:53:34 loss: 0.3020 Lr: 0.03503
[2023-08-08 01:05:24,854 INFO misc.py line 115 22900] Train: [41/100][4/156] Data 0.001 (0.001) Batch 4.063 (4.063) Remain 10:33:33 loss: 0.1656 Lr: 0.03502
[2023-08-08 01:05:28,712 INFO misc.py line 115 22900] Train: [41/100][5/156] Data 0.001 (0.001) Batch 3.858 (3.961) Remain 10:17:30 loss: 0.2845 Lr: 0.03502
[2023-08-08 01:05:32,801 INFO misc.py line 115 22900] Train: [41/100][6/156] Data 0.001 (0.001) Batch 4.089 (4.003) Remain 10:24:06 loss: 0.3096 Lr: 0.03501
[2023-08-08 01:05:36,246 INFO misc.py line 115 22900] Train: [41/100][7/156] Data 0.001 (0.001) Batch 3.445 (3.864) Remain 10:02:18 loss: 0.5008 Lr: 0.03501
[2023-08-08 01:05:40,812 INFO misc.py line 115 22900] Train: [41/100][8/156] Data 0.001 (0.001) Batch 4.566 (4.004) Remain 10:24:07 loss: 0.4409 Lr: 0.03500
[2023-08-08 01:05:44,329 INFO misc.py line 115 22900] Train: [41/100][9/156] Data 0.001 (0.001) Batch 3.518 (3.923) Remain 10:11:24 loss: 0.4169 Lr: 0.03500
[2023-08-08 01:05:47,730 INFO misc.py line 115 22900] Train: [41/100][10/156] Data 0.001 (0.001) Batch 3.400 (3.848) Remain 09:59:42 loss: 0.2927 Lr: 0.03499
[2023-08-08 01:05:51,696 INFO misc.py line 115 22900] Train: [41/100][11/156] Data 0.001 (0.001) Batch 3.966 (3.863) Remain 10:01:56 loss: 0.4028 Lr: 0.03499
[2023-08-08 01:05:55,199 INFO misc.py line 115 22900] Train: [41/100][12/156] Data 0.001 (0.001) Batch 3.503 (3.823) Remain 09:55:38 loss: 0.3769 Lr: 0.03498
[2023-08-08 01:05:58,856 INFO misc.py line 115 22900] Train: [41/100][13/156] Data 0.001 (0.001) Batch 3.657 (3.807) Remain 09:52:59 loss: 0.2783 Lr: 0.03498
[2023-08-08 01:06:01,578 INFO misc.py line 115 22900] Train: [41/100][14/156] Data 0.001 (0.001) Batch 2.722 (3.708) Remain 09:37:34 loss: 0.2556 Lr: 0.03497
[2023-08-08 01:06:05,364 INFO misc.py line 115 22900] Train: [41/100][15/156] Data 0.001 (0.001) Batch 3.786 (3.714) Remain 09:38:31 loss: 0.3245 Lr: 0.03497
[2023-08-08 01:06:08,743 INFO misc.py line 115 22900] Train: [41/100][16/156] Data 0.001 (0.001) Batch 3.378 (3.689) Remain 09:34:26 loss: 0.3039 Lr: 0.03496
[2023-08-08 01:06:12,317 INFO misc.py line 115 22900] Train: [41/100][17/156] Data 0.001 (0.001) Batch 3.574 (3.680) Remain 09:33:06 loss: 0.3622 Lr: 0.03496
[2023-08-08 01:06:16,258 INFO misc.py line 115 22900] Train: [41/100][18/156] Data 0.001 (0.001) Batch 3.941 (3.698) Remain 09:35:44 loss: 0.3574 Lr: 0.03496
[2023-08-08 01:06:20,793 INFO misc.py line 115 22900] Train: [41/100][19/156] Data 0.001 (0.001) Batch 4.535 (3.750) Remain 09:43:49 loss: 0.4447 Lr: 0.03495
[2023-08-08 01:06:22,987 INFO misc.py line 115 22900] Train: [41/100][20/156] Data 0.001 (0.001) Batch 2.194 (3.659) Remain 09:29:31 loss: 0.1319 Lr: 0.03495
[2023-08-08 01:06:25,402 INFO misc.py line 115 22900] Train: [41/100][21/156] Data 0.001 (0.001) Batch 2.415 (3.590) Remain 09:18:42 loss: 0.1299 Lr: 0.03494
[2023-08-08 01:06:28,707 INFO misc.py line 115 22900] Train: [41/100][22/156] Data 0.001 (0.001) Batch 3.304 (3.575) Remain 09:16:18 loss: 0.2389 Lr: 0.03494
[2023-08-08 01:06:32,345 INFO misc.py line 115 22900] Train: [41/100][23/156] Data 0.001 (0.001) Batch 3.638 (3.578) Remain 09:16:44 loss: 0.3401 Lr: 0.03493
[2023-08-08 01:06:36,030 INFO misc.py line 115 22900] Train: [41/100][24/156] Data 0.001 (0.001) Batch 3.685 (3.583) Remain 09:17:29 loss: 0.1801 Lr: 0.03493
[2023-08-08 01:06:39,707 INFO misc.py line 115 22900] Train: [41/100][25/156] Data 0.001 (0.001) Batch 3.677 (3.587) Remain 09:18:05 loss: 0.3584 Lr: 0.03492
[2023-08-08 01:06:42,836 INFO misc.py line 115 22900] Train: [41/100][26/156] Data 0.001 (0.001) Batch 3.129 (3.567) Remain 09:14:56 loss: 0.3335 Lr: 0.03492
[2023-08-08 01:06:45,114 INFO misc.py line 115 22900] Train: [41/100][27/156] Data 0.001 (0.001) Batch 2.278 (3.513) Remain 09:06:30 loss: 0.2568 Lr: 0.03491
[2023-08-08 01:06:48,420 INFO misc.py line 115 22900] Train: [41/100][28/156] Data 0.001 (0.001) Batch 3.306 (3.505) Remain 09:05:10 loss: 0.2674 Lr: 0.03491
[2023-08-08 01:06:52,164 INFO misc.py line 115 22900] Train: [41/100][29/156] Data 0.001 (0.001) Batch 3.744 (3.514) Remain 09:06:32 loss: 0.2519 Lr: 0.03490
[2023-08-08 01:06:55,637 INFO misc.py line 115 22900] Train: [41/100][30/156] Data 0.001 (0.001) Batch 3.473 (3.513) Remain 09:06:14 loss: 0.5915 Lr: 0.03490
[2023-08-08 01:06:59,274 INFO misc.py line 115 22900] Train: [41/100][31/156] Data 0.001 (0.001) Batch 3.637 (3.517) Remain 09:06:52 loss: 0.3798 Lr: 0.03489
[2023-08-08 01:07:02,811 INFO misc.py line 115 22900] Train: [41/100][32/156] Data 0.001 (0.001) Batch 3.537 (3.518) Remain 09:06:55 loss: 0.2249 Lr: 0.03489
[2023-08-08 01:07:05,693 INFO misc.py line 115 22900] Train: [41/100][33/156] Data 0.001 (0.001) Batch 2.881 (3.497) Remain 09:03:33 loss: 0.1846 Lr: 0.03488
[2023-08-08 01:07:09,353 INFO misc.py line 115 22900] Train: [41/100][34/156] Data 0.001 (0.001) Batch 3.661 (3.502) Remain 09:04:19 loss: 0.5226 Lr: 0.03488
[2023-08-08 01:07:12,125 INFO misc.py line 115 22900] Train: [41/100][35/156] Data 0.001 (0.001) Batch 2.771 (3.479) Remain 09:00:43 loss: 0.2910 Lr: 0.03487
[2023-08-08 01:07:15,710 INFO misc.py line 115 22900] Train: [41/100][36/156] Data 0.001 (0.001) Batch 3.586 (3.482) Remain 09:01:09 loss: 0.3031 Lr: 0.03487
[2023-08-08 01:07:19,802 INFO misc.py line 115 22900] Train: [41/100][37/156] Data 0.001 (0.001) Batch 4.092 (3.500) Remain 09:03:53 loss: 0.3583 Lr: 0.03486
[2023-08-08 01:07:23,095 INFO misc.py line 115 22900] Train: [41/100][38/156] Data 0.001 (0.001) Batch 3.293 (3.494) Remain 09:02:54 loss: 0.3125 Lr: 0.03486
[2023-08-08 01:07:26,533 INFO misc.py line 115 22900] Train: [41/100][39/156] Data 0.001 (0.001) Batch 3.438 (3.493) Remain 09:02:36 loss: 0.3954 Lr: 0.03485
[2023-08-08 01:07:30,730 INFO misc.py line 115 22900] Train: [41/100][40/156] Data 0.001 (0.001) Batch 4.196 (3.512) Remain 09:05:30 loss: 0.5591 Lr: 0.03485
[2023-08-08 01:07:34,150 INFO misc.py line 115 22900] Train: [41/100][41/156] Data 0.001 (0.001) Batch 3.420 (3.509) Remain 09:05:04 loss: 0.3156 Lr: 0.03484
[2023-08-08 01:07:37,304 INFO misc.py line 115 22900] Train: [41/100][42/156] Data 0.001 (0.001) Batch 3.154 (3.500) Remain 09:03:36 loss: 0.3312 Lr: 0.03484
[2023-08-08 01:07:40,156 INFO misc.py line 115 22900] Train: [41/100][43/156] Data 0.001 (0.001) Batch 2.852 (3.484) Remain 09:01:01 loss: 0.2575 Lr: 0.03483
[2023-08-08 01:07:44,303 INFO misc.py line 115 22900] Train: [41/100][44/156] Data 0.001 (0.001) Batch 4.147 (3.500) Remain 09:03:28 loss: 0.1840 Lr: 0.03483
[2023-08-08 01:07:47,646 INFO misc.py line 115 22900] Train: [41/100][45/156] Data 0.001 (0.001) Batch 3.343 (3.497) Remain 09:02:50 loss: 0.4519 Lr: 0.03482
[2023-08-08 01:07:49,713 INFO misc.py line 115 22900] Train: [41/100][46/156] Data 0.001 (0.001) Batch 2.067 (3.463) Remain 08:57:37 loss: 0.2782 Lr: 0.03482
[2023-08-08 01:07:52,612 INFO misc.py line 115 22900] Train: [41/100][47/156] Data 0.001 (0.001) Batch 2.899 (3.450) Remain 08:55:34 loss: 0.1596 Lr: 0.03481
[2023-08-08 01:07:56,668 INFO misc.py line 115 22900] Train: [41/100][48/156] Data 0.001 (0.001) Batch 4.056 (3.464) Remain 08:57:36 loss: 0.3997 Lr: 0.03481
[2023-08-08 01:07:58,515 INFO misc.py line 115 22900] Train: [41/100][49/156] Data 0.001 (0.001) Batch 1.847 (3.429) Remain 08:52:05 loss: 0.1627 Lr: 0.03480
[2023-08-08 01:08:02,355 INFO misc.py line 115 22900] Train: [41/100][50/156] Data 0.001 (0.001) Batch 3.840 (3.438) Remain 08:53:23 loss: 0.2580 Lr: 0.03480
[2023-08-08 01:08:04,536 INFO misc.py line 115 22900] Train: [41/100][51/156] Data 0.001 (0.001) Batch 2.181 (3.411) Remain 08:49:16 loss: 0.1840 Lr: 0.03479
[2023-08-08 01:08:07,682 INFO misc.py line 115 22900] Train: [41/100][52/156] Data 0.001 (0.001) Batch 3.146 (3.406) Remain 08:48:22 loss: 0.2856 Lr: 0.03479
[2023-08-08 01:08:10,756 INFO misc.py line 115 22900] Train: [41/100][53/156] Data 0.001 (0.001) Batch 3.073 (3.399) Remain 08:47:17 loss: 0.3653 Lr: 0.03478
[2023-08-08 01:08:13,690 INFO misc.py line 115 22900] Train: [41/100][54/156] Data 0.001 (0.001) Batch 2.934 (3.390) Remain 08:45:49 loss: 0.3065 Lr: 0.03478
[2023-08-08 01:08:17,051 INFO misc.py line 115 22900] Train: [41/100][55/156] Data 0.001 (0.001) Batch 3.361 (3.390) Remain 08:45:40 loss: 0.2359 Lr: 0.03477
[2023-08-08 01:08:21,064 INFO misc.py line 115 22900] Train: [41/100][56/156] Data 0.001 (0.001) Batch 4.012 (3.401) Remain 08:47:26 loss: 0.3681 Lr: 0.03477
[2023-08-08 01:08:23,477 INFO misc.py line 115 22900] Train: [41/100][57/156] Data 0.001 (0.001) Batch 2.413 (3.383) Remain 08:44:32 loss: 0.2306 Lr: 0.03477
[2023-08-08 01:08:26,851 INFO misc.py line 115 22900] Train: [41/100][58/156] Data 0.001 (0.001) Batch 3.375 (3.383) Remain 08:44:27 loss: 0.3632 Lr: 0.03476
[2023-08-08 01:08:30,908 INFO misc.py line 115 22900] Train: [41/100][59/156] Data 0.001 (0.001) Batch 4.056 (3.395) Remain 08:46:16 loss: 0.3440 Lr: 0.03476
[2023-08-08 01:08:34,240 INFO misc.py line 115 22900] Train: [41/100][60/156] Data 0.001 (0.001) Batch 3.333 (3.394) Remain 08:46:02 loss: 0.2179 Lr: 0.03475
[2023-08-08 01:08:38,296 INFO misc.py line 115 22900] Train: [41/100][61/156] Data 0.001 (0.001) Batch 4.056 (3.405) Remain 08:47:45 loss: 0.4508 Lr: 0.03475
[2023-08-08 01:08:42,388 INFO misc.py line 115 22900] Train: [41/100][62/156] Data 0.001 (0.001) Batch 4.092 (3.417) Remain 08:49:30 loss: 0.3796 Lr: 0.03474
[2023-08-08 01:08:45,365 INFO misc.py line 115 22900] Train: [41/100][63/156] Data 0.001 (0.001) Batch 2.977 (3.410) Remain 08:48:18 loss: 0.3948 Lr: 0.03474
[2023-08-08 01:08:48,382 INFO misc.py line 115 22900] Train: [41/100][64/156] Data 0.001 (0.001) Batch 3.017 (3.403) Remain 08:47:15 loss: 0.2440 Lr: 0.03473
[2023-08-08 01:08:51,382 INFO misc.py line 115 22900] Train: [41/100][65/156] Data 0.001 (0.001) Batch 3.000 (3.397) Remain 08:46:11 loss: 0.1590 Lr: 0.03473
[2023-08-08 01:08:54,287 INFO misc.py line 115 22900] Train: [41/100][66/156] Data 0.001 (0.001) Batch 2.905 (3.389) Remain 08:44:55 loss: 0.1354 Lr: 0.03472
[2023-08-08 01:08:58,609 INFO misc.py line 115 22900] Train: [41/100][67/156] Data 0.001 (0.001) Batch 4.322 (3.403) Remain 08:47:07 loss: 0.3421 Lr: 0.03472
[2023-08-08 01:09:02,326 INFO misc.py line 115 22900] Train: [41/100][68/156] Data 0.001 (0.001) Batch 3.718 (3.408) Remain 08:47:49 loss: 0.3838 Lr: 0.03471
[2023-08-08 01:09:06,444 INFO misc.py line 115 22900] Train: [41/100][69/156] Data 0.001 (0.001) Batch 4.118 (3.419) Remain 08:49:25 loss: 0.6135 Lr: 0.03471
[2023-08-08 01:09:10,119 INFO misc.py line 115 22900] Train: [41/100][70/156] Data 0.001 (0.001) Batch 3.676 (3.423) Remain 08:49:57 loss: 0.2958 Lr: 0.03470
[2023-08-08 01:09:14,205 INFO misc.py line 115 22900] Train: [41/100][71/156] Data 0.001 (0.001) Batch 4.085 (3.433) Remain 08:51:24 loss: 0.2578 Lr: 0.03470
[2023-08-08 01:09:16,691 INFO misc.py line 115 22900] Train: [41/100][72/156] Data 0.001 (0.001) Batch 2.487 (3.419) Remain 08:49:14 loss: 0.0946 Lr: 0.03469
[2023-08-08 01:09:20,414 INFO misc.py line 115 22900] Train: [41/100][73/156] Data 0.001 (0.001) Batch 3.722 (3.423) Remain 08:49:51 loss: 0.4559 Lr: 0.03469
[2023-08-08 01:09:23,188 INFO misc.py line 115 22900] Train: [41/100][74/156] Data 0.001 (0.001) Batch 2.775 (3.414) Remain 08:48:22 loss: 0.3206 Lr: 0.03468
[2023-08-08 01:09:26,456 INFO misc.py line 115 22900] Train: [41/100][75/156] Data 0.001 (0.001) Batch 3.268 (3.412) Remain 08:48:00 loss: 0.7496 Lr: 0.03468
[2023-08-08 01:09:29,206 INFO misc.py line 115 22900] Train: [41/100][76/156] Data 0.001 (0.001) Batch 2.750 (3.403) Remain 08:46:32 loss: 0.2398 Lr: 0.03467
[2023-08-08 01:09:32,978 INFO misc.py line 115 22900] Train: [41/100][77/156] Data 0.001 (0.001) Batch 3.772 (3.408) Remain 08:47:15 loss: 0.3991 Lr: 0.03467
[2023-08-08 01:09:35,566 INFO misc.py line 115 22900] Train: [41/100][78/156] Data 0.001 (0.001) Batch 2.587 (3.397) Remain 08:45:30 loss: 0.1421 Lr: 0.03466
[2023-08-08 01:09:39,584 INFO misc.py line 115 22900] Train: [41/100][79/156] Data 0.001 (0.001) Batch 4.019 (3.405) Remain 08:46:43 loss: 0.2984 Lr: 0.03466
[2023-08-08 01:09:42,485 INFO misc.py line 115 22900] Train: [41/100][80/156] Data 0.001 (0.001) Batch 2.900 (3.399) Remain 08:45:39 loss: 0.2970 Lr: 0.03465
[2023-08-08 01:09:46,571 INFO misc.py line 115 22900] Train: [41/100][81/156] Data 0.001 (0.001) Batch 4.086 (3.407) Remain 08:46:57 loss: 0.3558 Lr: 0.03465
[2023-08-08 01:09:49,523 INFO misc.py line 115 22900] Train: [41/100][82/156] Data 0.001 (0.001) Batch 2.953 (3.402) Remain 08:46:00 loss: 0.5083 Lr: 0.03464
[2023-08-08 01:09:52,313 INFO misc.py line 115 22900] Train: [41/100][83/156] Data 0.001 (0.001) Batch 2.790 (3.394) Remain 08:44:46 loss: 0.3168 Lr: 0.03464
[2023-08-08 01:09:55,629 INFO misc.py line 115 22900] Train: [41/100][84/156] Data 0.001 (0.001) Batch 3.316 (3.393) Remain 08:44:34 loss: 0.2133 Lr: 0.03463
[2023-08-08 01:09:59,343 INFO misc.py line 115 22900] Train: [41/100][85/156] Data 0.001 (0.001) Batch 3.713 (3.397) Remain 08:45:06 loss: 0.3536 Lr: 0.03463
[2023-08-08 01:10:02,608 INFO misc.py line 115 22900] Train: [41/100][86/156] Data 0.001 (0.001) Batch 3.265 (3.395) Remain 08:44:48 loss: 0.1611 Lr: 0.03462
[2023-08-08 01:10:06,714 INFO misc.py line 115 22900] Train: [41/100][87/156] Data 0.001 (0.001) Batch 4.107 (3.404) Remain 08:46:03 loss: 0.3798 Lr: 0.03462
[2023-08-08 01:10:11,966 INFO misc.py line 115 22900] Train: [41/100][88/156] Data 0.001 (0.001) Batch 5.252 (3.426) Remain 08:49:22 loss: 0.5677 Lr: 0.03461
[2023-08-08 01:10:14,866 INFO misc.py line 115 22900] Train: [41/100][89/156] Data 0.001 (0.001) Batch 2.900 (3.419) Remain 08:48:21 loss: 0.2990 Lr: 0.03461
[2023-08-08 01:10:18,088 INFO misc.py line 115 22900] Train: [41/100][90/156] Data 0.001 (0.001) Batch 3.222 (3.417) Remain 08:47:57 loss: 0.3970 Lr: 0.03460
[2023-08-08 01:10:20,639 INFO misc.py line 115 22900] Train: [41/100][91/156] Data 0.001 (0.001) Batch 2.551 (3.407) Remain 08:46:22 loss: 0.2690 Lr: 0.03460
[2023-08-08 01:10:23,948 INFO misc.py line 115 22900] Train: [41/100][92/156] Data 0.001 (0.001) Batch 3.309 (3.406) Remain 08:46:09 loss: 0.2507 Lr: 0.03459
[2023-08-08 01:10:27,281 INFO misc.py line 115 22900] Train: [41/100][93/156] Data 0.001 (0.001) Batch 3.333 (3.405) Remain 08:45:58 loss: 0.3594 Lr: 0.03459
[2023-08-08 01:10:30,078 INFO misc.py line 115 22900] Train: [41/100][94/156] Data 0.001 (0.001) Batch 2.797 (3.399) Remain 08:44:52 loss: 0.1526 Lr: 0.03458
[2023-08-08 01:10:34,387 INFO misc.py line 115 22900] Train: [41/100][95/156] Data 0.001 (0.001) Batch 4.310 (3.409) Remain 08:46:21 loss: 0.3288 Lr: 0.03458
[2023-08-08 01:10:38,502 INFO misc.py line 115 22900] Train: [41/100][96/156] Data 0.001 (0.001) Batch 4.115 (3.416) Remain 08:47:28 loss: 0.2815 Lr: 0.03457
[2023-08-08 01:10:41,438 INFO misc.py line 115 22900] Train: [41/100][97/156] Data 0.001 (0.001) Batch 2.936 (3.411) Remain 08:46:37 loss: 0.1222 Lr: 0.03457
[2023-08-08 01:10:45,425 INFO misc.py line 115 22900] Train: [41/100][98/156] Data 0.001 (0.001) Batch 3.986 (3.417) Remain 08:47:30 loss: 0.3290 Lr: 0.03456
[2023-08-08 01:10:48,750 INFO misc.py line 115 22900] Train: [41/100][99/156] Data 0.001 (0.001) Batch 3.325 (3.416) Remain 08:47:17 loss: 0.2807 Lr: 0.03456
[2023-08-08 01:10:52,348 INFO misc.py line 115 22900] Train: [41/100][100/156] Data 0.001 (0.001) Batch 3.599 (3.418) Remain 08:47:31 loss: 0.2150 Lr: 0.03455
[2023-08-08 01:10:56,778 INFO misc.py line 115 22900] Train: [41/100][101/156] Data 0.001 (0.001) Batch 4.429 (3.428) Remain 08:49:03 loss: 0.2758 Lr: 0.03455
[2023-08-08 01:11:00,629 INFO misc.py line 115 22900] Train: [41/100][102/156] Data 0.001 (0.001) Batch 3.852 (3.433) Remain 08:49:40 loss: 0.2247 Lr: 0.03455
[2023-08-08 01:11:03,909 INFO misc.py line 115 22900] Train: [41/100][103/156] Data 0.001 (0.001) Batch 3.279 (3.431) Remain 08:49:22 loss: 0.3005 Lr: 0.03454
[2023-08-08 01:11:07,790 INFO misc.py line 115 22900] Train: [41/100][104/156] Data 0.001 (0.001) Batch 3.882 (3.436) Remain 08:50:00 loss: 0.4352 Lr: 0.03454
[2023-08-08 01:11:11,666 INFO misc.py line 115 22900] Train: [41/100][105/156] Data 0.001 (0.001) Batch 3.876 (3.440) Remain 08:50:36 loss: 0.6514 Lr: 0.03453
[2023-08-08 01:11:14,806 INFO misc.py line 115 22900] Train: [41/100][106/156] Data 0.001 (0.001) Batch 3.140 (3.437) Remain 08:50:06 loss: 0.1234 Lr: 0.03453
[2023-08-08 01:11:18,873 INFO misc.py line 115 22900] Train: [41/100][107/156] Data 0.001 (0.001) Batch 4.066 (3.443) Remain 08:50:58 loss: 0.6245 Lr: 0.03452
[2023-08-08 01:11:22,927 INFO misc.py line 115 22900] Train: [41/100][108/156] Data 0.001 (0.001) Batch 4.054 (3.449) Remain 08:51:49 loss: 0.3531 Lr: 0.03452
[2023-08-08 01:11:26,294 INFO misc.py line 115 22900] Train: [41/100][109/156] Data 0.001 (0.001) Batch 3.368 (3.448) Remain 08:51:38 loss: 0.3087 Lr: 0.03451
[2023-08-08 01:11:29,681 INFO misc.py line 115 22900] Train: [41/100][110/156] Data 0.001 (0.001) Batch 3.386 (3.448) Remain 08:51:29 loss: 0.4747 Lr: 0.03451
[2023-08-08 01:11:33,399 INFO misc.py line 115 22900] Train: [41/100][111/156] Data 0.001 (0.001) Batch 3.718 (3.450) Remain 08:51:49 loss: 0.2934 Lr: 0.03450
[2023-08-08 01:11:37,109 INFO misc.py line 115 22900] Train: [41/100][112/156] Data 0.001 (0.001) Batch 3.711 (3.452) Remain 08:52:08 loss: 0.2099 Lr: 0.03450
[2023-08-08 01:11:40,946 INFO misc.py line 115 22900] Train: [41/100][113/156] Data 0.001 (0.001) Batch 3.836 (3.456) Remain 08:52:37 loss: 0.3883 Lr: 0.03449
[2023-08-08 01:11:45,016 INFO misc.py line 115 22900] Train: [41/100][114/156] Data 0.001 (0.001) Batch 4.071 (3.461) Remain 08:53:24 loss: 0.2500 Lr: 0.03449
[2023-08-08 01:11:49,086 INFO misc.py line 115 22900] Train: [41/100][115/156] Data 0.001 (0.001) Batch 4.070 (3.467) Remain 08:54:11 loss: 0.5900 Lr: 0.03448
[2023-08-08 01:11:52,698 INFO misc.py line 115 22900] Train: [41/100][116/156] Data 0.001 (0.001) Batch 3.612 (3.468) Remain 08:54:20 loss: 0.2125 Lr: 0.03448
[2023-08-08 01:11:57,231 INFO misc.py line 115 22900] Train: [41/100][117/156] Data 0.001 (0.001) Batch 4.533 (3.478) Remain 08:55:42 loss: 0.3375 Lr: 0.03447
[2023-08-08 01:12:00,968 INFO misc.py line 115 22900] Train: [41/100][118/156] Data 0.001 (0.001) Batch 3.737 (3.480) Remain 08:56:00 loss: 0.3053 Lr: 0.03447
[2023-08-08 01:12:04,277 INFO misc.py line 115 22900] Train: [41/100][119/156] Data 0.001 (0.001) Batch 3.309 (3.478) Remain 08:55:43 loss: 0.2236 Lr: 0.03446
[2023-08-08 01:12:08,355 INFO misc.py line 115 22900] Train: [41/100][120/156] Data 0.001 (0.001) Batch 4.078 (3.483) Remain 08:56:27 loss: 0.2860 Lr: 0.03446
[2023-08-08 01:12:12,021 INFO misc.py line 115 22900] Train: [41/100][121/156] Data 0.001 (0.001) Batch 3.666 (3.485) Remain 08:56:37 loss: 0.3912 Lr: 0.03445
[2023-08-08 01:12:15,555 INFO misc.py line 115 22900] Train: [41/100][122/156] Data 0.001 (0.001) Batch 3.534 (3.485) Remain 08:56:38 loss: 0.4558 Lr: 0.03445
[2023-08-08 01:12:20,480 INFO misc.py line 115 22900] Train: [41/100][123/156] Data 0.001 (0.001) Batch 4.925 (3.497) Remain 08:58:25 loss: 0.7290 Lr: 0.03444
[2023-08-08 01:12:24,361 INFO misc.py line 115 22900] Train: [41/100][124/156] Data 0.001 (0.001) Batch 3.881 (3.501) Remain 08:58:51 loss: 0.3160 Lr: 0.03444
[2023-08-08 01:12:28,379 INFO misc.py line 115 22900] Train: [41/100][125/156] Data 0.001 (0.001) Batch 4.017 (3.505) Remain 08:59:26 loss: 0.3477 Lr: 0.03443
[2023-08-08 01:12:31,319 INFO misc.py line 115 22900] Train: [41/100][126/156] Data 0.001 (0.001) Batch 2.941 (3.500) Remain 08:58:41 loss: 0.1768 Lr: 0.03443
[2023-08-08 01:12:35,380 INFO misc.py line 115 22900] Train: [41/100][127/156] Data 0.001 (0.001) Batch 4.060 (3.505) Remain 08:59:19 loss: 0.3361 Lr: 0.03442
[2023-08-08 01:12:39,284 INFO misc.py line 115 22900] Train: [41/100][128/156] Data 0.001 (0.001) Batch 3.905 (3.508) Remain 08:59:45 loss: 0.2872 Lr: 0.03442
[2023-08-08 01:12:42,582 INFO misc.py line 115 22900] Train: [41/100][129/156] Data 0.001 (0.001) Batch 3.297 (3.506) Remain 08:59:26 loss: 0.2889 Lr: 0.03441
[2023-08-08 01:12:45,954 INFO misc.py line 115 22900] Train: [41/100][130/156] Data 0.001 (0.001) Batch 3.372 (3.505) Remain 08:59:13 loss: 0.2688 Lr: 0.03441
[2023-08-08 01:12:48,534 INFO misc.py line 115 22900] Train: [41/100][131/156] Data 0.001 (0.001) Batch 2.581 (3.498) Remain 08:58:02 loss: 0.1542 Lr: 0.03440
[2023-08-08 01:12:52,609 INFO misc.py line 115 22900] Train: [41/100][132/156] Data 0.001 (0.001) Batch 4.075 (3.502) Remain 08:58:40 loss: 0.1991 Lr: 0.03440
[2023-08-08 01:12:55,857 INFO misc.py line 115 22900] Train: [41/100][133/156] Data 0.001 (0.001) Batch 3.248 (3.501) Remain 08:58:19 loss: 0.4235 Lr: 0.03439
[2023-08-08 01:12:59,233 INFO misc.py line 115 22900] Train: [41/100][134/156] Data 0.001 (0.001) Batch 3.375 (3.500) Remain 08:58:06 loss: 0.2913 Lr: 0.03439
[2023-08-08 01:13:02,653 INFO misc.py line 115 22900] Train: [41/100][135/156] Data 0.001 (0.001) Batch 3.420 (3.499) Remain 08:57:57 loss: 0.4034 Lr: 0.03438
[2023-08-08 01:13:06,535 INFO misc.py line 115 22900] Train: [41/100][136/156] Data 0.001 (0.001) Batch 3.882 (3.502) Remain 08:58:20 loss: 0.3408 Lr: 0.03438
[2023-08-08 01:13:08,354 INFO misc.py line 115 22900] Train: [41/100][137/156] Data 0.001 (0.001) Batch 1.819 (3.489) Remain 08:56:21 loss: 0.2411 Lr: 0.03437
[2023-08-08 01:13:11,189 INFO misc.py line 115 22900] Train: [41/100][138/156] Data 0.001 (0.001) Batch 2.835 (3.484) Remain 08:55:33 loss: 0.3323 Lr: 0.03437
[2023-08-08 01:13:15,147 INFO misc.py line 115 22900] Train: [41/100][139/156] Data 0.001 (0.001) Batch 3.958 (3.488) Remain 08:56:02 loss: 0.3409 Lr: 0.03436
[2023-08-08 01:13:18,629 INFO misc.py line 115 22900] Train: [41/100][140/156] Data 0.001 (0.001) Batch 3.482 (3.488) Remain 08:55:58 loss: 0.2694 Lr: 0.03436
[2023-08-08 01:13:22,090 INFO misc.py line 115 22900] Train: [41/100][141/156] Data 0.001 (0.001) Batch 3.461 (3.488) Remain 08:55:52 loss: 0.3209 Lr: 0.03435
[2023-08-08 01:13:26,155 INFO misc.py line 115 22900] Train: [41/100][142/156] Data 0.001 (0.001) Batch 4.065 (3.492) Remain 08:56:27 loss: 0.3479 Lr: 0.03435
[2023-08-08 01:13:30,070 INFO misc.py line 115 22900] Train: [41/100][143/156] Data 0.001 (0.001) Batch 3.915 (3.495) Remain 08:56:52 loss: 0.3431 Lr: 0.03434
[2023-08-08 01:13:32,652 INFO misc.py line 115 22900] Train: [41/100][144/156] Data 0.001 (0.001) Batch 2.582 (3.488) Remain 08:55:48 loss: 0.4087 Lr: 0.03434
[2023-08-08 01:13:36,279 INFO misc.py line 115 22900] Train: [41/100][145/156] Data 0.001 (0.001) Batch 3.627 (3.489) Remain 08:55:54 loss: 0.3114 Lr: 0.03433
[2023-08-08 01:13:39,896 INFO misc.py line 115 22900] Train: [41/100][146/156] Data 0.001 (0.001) Batch 3.617 (3.490) Remain 08:55:59 loss: 0.2186 Lr: 0.03433
[2023-08-08 01:13:42,548 INFO misc.py line 115 22900] Train: [41/100][147/156] Data 0.001 (0.001) Batch 2.652 (3.484) Remain 08:55:01 loss: 0.1943 Lr: 0.03432
[2023-08-08 01:13:45,605 INFO misc.py line 115 22900] Train: [41/100][148/156] Data 0.001 (0.001) Batch 3.057 (3.481) Remain 08:54:31 loss: 0.1546 Lr: 0.03432
[2023-08-08 01:13:49,740 INFO misc.py line 115 22900] Train: [41/100][149/156] Data 0.001 (0.001) Batch 4.136 (3.486) Remain 08:55:09 loss: 0.4039 Lr: 0.03431
[2023-08-08 01:13:53,090 INFO misc.py line 115 22900] Train: [41/100][150/156] Data 0.001 (0.001) Batch 3.350 (3.485) Remain 08:54:57 loss: 0.3620 Lr: 0.03431
[2023-08-08 01:13:55,738 INFO misc.py line 115 22900] Train: [41/100][151/156] Data 0.001 (0.001) Batch 2.648 (3.479) Remain 08:54:01 loss: 0.1752 Lr: 0.03430
[2023-08-08 01:13:59,340 INFO misc.py line 115 22900] Train: [41/100][152/156] Data 0.001 (0.001) Batch 3.602 (3.480) Remain 08:54:05 loss: 0.4035 Lr: 0.03430
[2023-08-08 01:14:02,977 INFO misc.py line 115 22900] Train: [41/100][153/156] Data 0.001 (0.001) Batch 3.636 (3.481) Remain 08:54:11 loss: 0.2703 Lr: 0.03429
[2023-08-08 01:14:07,087 INFO misc.py line 115 22900] Train: [41/100][154/156] Data 0.001 (0.001) Batch 4.110 (3.485) Remain 08:54:46 loss: 0.2255 Lr: 0.03429
[2023-08-08 01:14:10,681 INFO misc.py line 115 22900] Train: [41/100][155/156] Data 0.001 (0.001) Batch 3.594 (3.486) Remain 08:54:49 loss: 0.3830 Lr: 0.03429
[2023-08-08 01:14:14,218 INFO misc.py line 115 22900] Train: [41/100][156/156] Data 0.001 (0.001) Batch 3.537 (3.486) Remain 08:54:49 loss: 0.4784 Lr: 0.03428
[2023-08-08 01:14:14,218 INFO misc.py line 129 22900] Train result: loss: 0.3196 
[2023-08-08 01:14:14,218 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 01:14:16,337 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.3516 
[2023-08-08 01:14:17,205 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.5000 
[2023-08-08 01:14:18,867 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.6017 
[2023-08-08 01:14:20,389 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.2672 
[2023-08-08 01:14:22,232 INFO evaluator.py line 122 22900] Test: [5/24] Loss 2.2076 
[2023-08-08 01:14:23,896 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.6515 
[2023-08-08 01:14:26,032 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.9572 
[2023-08-08 01:14:27,836 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.3439 
[2023-08-08 01:14:29,119 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.4393 
[2023-08-08 01:14:31,249 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3586 
[2023-08-08 01:14:31,774 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.0198 
[2023-08-08 01:14:33,309 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.7257 
[2023-08-08 01:14:36,021 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.0916 
[2023-08-08 01:14:37,700 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.7031 
[2023-08-08 01:14:39,725 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.2880 
[2023-08-08 01:14:42,433 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.0988 
[2023-08-08 01:14:45,141 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.4961 
[2023-08-08 01:14:46,988 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.7675 
[2023-08-08 01:14:47,735 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.2866 
[2023-08-08 01:14:48,620 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.7204 
[2023-08-08 01:14:50,881 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3132 
[2023-08-08 01:14:52,846 INFO evaluator.py line 122 22900] Test: [22/24] Loss 2.0811 
[2023-08-08 01:14:54,694 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.7508 
[2023-08-08 01:14:56,629 INFO evaluator.py line 122 22900] Test: [24/24] Loss 2.0389 
[2023-08-08 01:14:56,680 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2325/0.3363/0.6928.
[2023-08-08 01:14:56,680 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6319/0.9676
[2023-08-08 01:14:56,680 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9485/0.9913
[2023-08-08 01:14:56,680 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1899/0.4332
[2023-08-08 01:14:56,680 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1074/0.1801
[2023-08-08 01:14:56,680 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6383/0.7491
[2023-08-08 01:14:56,680 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.4159/0.5618
[2023-08-08 01:14:56,681 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5430/0.6569
[2023-08-08 01:14:56,681 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1244/0.1324
[2023-08-08 01:14:56,681 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1326/0.3221
[2023-08-08 01:14:56,681 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0192/0.0192
[2023-08-08 01:14:56,681 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 01:14:56,681 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0165/0.0170
[2023-08-08 01:14:56,681 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.3492/0.4667
[2023-08-08 01:14:56,681 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0105/0.0109
[2023-08-08 01:14:56,681 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0052/0.0060
[2023-08-08 01:14:56,681 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-08 01:14:56,681 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.2065/0.4109
[2023-08-08 01:14:56,681 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.2150/0.5922
[2023-08-08 01:14:56,681 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 01:14:56,681 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0953/0.2079
[2023-08-08 01:14:56,681 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 01:14:56,681 INFO misc.py line 152 22900] Currently Best mIoU: 0.2562
[2023-08-08 01:14:56,681 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 01:15:02,145 INFO misc.py line 115 22900] Train: [42/100][1/156] Data 0.748 (0.748) Batch 4.670 (4.670) Remain 11:56:22 loss: 0.4634 Lr: 0.03428
[2023-08-08 01:15:06,247 INFO misc.py line 115 22900] Train: [42/100][2/156] Data 0.001 (0.001) Batch 4.102 (4.102) Remain 10:29:07 loss: 0.3159 Lr: 0.03427
[2023-08-08 01:15:09,478 INFO misc.py line 115 22900] Train: [42/100][3/156] Data 0.001 (0.001) Batch 3.231 (3.231) Remain 08:15:29 loss: 0.3277 Lr: 0.03427
[2023-08-08 01:15:13,230 INFO misc.py line 115 22900] Train: [42/100][4/156] Data 0.001 (0.001) Batch 3.751 (3.751) Remain 09:35:13 loss: 0.3403 Lr: 0.03426
[2023-08-08 01:15:16,663 INFO misc.py line 115 22900] Train: [42/100][5/156] Data 0.001 (0.001) Batch 3.433 (3.592) Remain 09:10:47 loss: 0.3368 Lr: 0.03426
[2023-08-08 01:15:20,189 INFO misc.py line 115 22900] Train: [42/100][6/156] Data 0.001 (0.001) Batch 3.526 (3.570) Remain 09:07:19 loss: 0.1955 Lr: 0.03425
[2023-08-08 01:15:24,255 INFO misc.py line 115 22900] Train: [42/100][7/156] Data 0.001 (0.001) Batch 4.066 (3.694) Remain 09:26:14 loss: 0.4287 Lr: 0.03425
[2023-08-08 01:15:26,768 INFO misc.py line 115 22900] Train: [42/100][8/156] Data 0.001 (0.001) Batch 2.514 (3.458) Remain 08:50:00 loss: 0.3014 Lr: 0.03424
[2023-08-08 01:15:30,043 INFO misc.py line 115 22900] Train: [42/100][9/156] Data 0.001 (0.001) Batch 3.275 (3.428) Remain 08:45:15 loss: 0.2659 Lr: 0.03424
[2023-08-08 01:15:33,748 INFO misc.py line 115 22900] Train: [42/100][10/156] Data 0.001 (0.001) Batch 3.705 (3.467) Remain 08:51:16 loss: 0.2825 Lr: 0.03423
[2023-08-08 01:15:36,866 INFO misc.py line 115 22900] Train: [42/100][11/156] Data 0.001 (0.001) Batch 3.118 (3.423) Remain 08:44:31 loss: 0.3542 Lr: 0.03423
[2023-08-08 01:15:39,902 INFO misc.py line 115 22900] Train: [42/100][12/156] Data 0.001 (0.001) Batch 3.036 (3.380) Remain 08:37:53 loss: 0.1837 Lr: 0.03422
[2023-08-08 01:15:43,251 INFO misc.py line 115 22900] Train: [42/100][13/156] Data 0.001 (0.001) Batch 3.349 (3.377) Remain 08:37:20 loss: 0.3485 Lr: 0.03422
[2023-08-08 01:15:47,725 INFO misc.py line 115 22900] Train: [42/100][14/156] Data 0.001 (0.001) Batch 4.474 (3.477) Remain 08:52:33 loss: 0.4767 Lr: 0.03421
[2023-08-08 01:15:51,653 INFO misc.py line 115 22900] Train: [42/100][15/156] Data 0.001 (0.001) Batch 3.927 (3.515) Remain 08:58:15 loss: 0.1923 Lr: 0.03421
[2023-08-08 01:15:55,148 INFO misc.py line 115 22900] Train: [42/100][16/156] Data 0.001 (0.001) Batch 3.495 (3.513) Remain 08:57:58 loss: 0.4229 Lr: 0.03420
[2023-08-08 01:15:58,154 INFO misc.py line 115 22900] Train: [42/100][17/156] Data 0.001 (0.001) Batch 3.006 (3.477) Remain 08:52:21 loss: 0.2071 Lr: 0.03420
[2023-08-08 01:16:02,134 INFO misc.py line 115 22900] Train: [42/100][18/156] Data 0.001 (0.001) Batch 3.980 (3.510) Remain 08:57:26 loss: 0.4175 Lr: 0.03419
[2023-08-08 01:16:06,028 INFO misc.py line 115 22900] Train: [42/100][19/156] Data 0.001 (0.001) Batch 3.894 (3.534) Remain 09:01:03 loss: 0.4196 Lr: 0.03419
[2023-08-08 01:16:09,369 INFO misc.py line 115 22900] Train: [42/100][20/156] Data 0.001 (0.001) Batch 3.341 (3.523) Remain 08:59:15 loss: 0.3768 Lr: 0.03418
[2023-08-08 01:16:13,122 INFO misc.py line 115 22900] Train: [42/100][21/156] Data 0.001 (0.001) Batch 3.753 (3.536) Remain 09:01:09 loss: 0.4268 Lr: 0.03418
[2023-08-08 01:16:17,196 INFO misc.py line 115 22900] Train: [42/100][22/156] Data 0.001 (0.001) Batch 4.074 (3.564) Remain 09:05:25 loss: 0.3494 Lr: 0.03417
[2023-08-08 01:16:20,450 INFO misc.py line 115 22900] Train: [42/100][23/156] Data 0.001 (0.001) Batch 3.254 (3.549) Remain 09:02:59 loss: 0.4448 Lr: 0.03417
[2023-08-08 01:16:24,493 INFO misc.py line 115 22900] Train: [42/100][24/156] Data 0.001 (0.001) Batch 4.043 (3.572) Remain 09:06:32 loss: 0.3717 Lr: 0.03416
[2023-08-08 01:16:27,629 INFO misc.py line 115 22900] Train: [42/100][25/156] Data 0.001 (0.001) Batch 3.136 (3.552) Remain 09:03:26 loss: 0.2565 Lr: 0.03416
[2023-08-08 01:16:31,838 INFO misc.py line 115 22900] Train: [42/100][26/156] Data 0.001 (0.001) Batch 4.209 (3.581) Remain 09:07:45 loss: 0.2001 Lr: 0.03415
[2023-08-08 01:16:35,927 INFO misc.py line 115 22900] Train: [42/100][27/156] Data 0.001 (0.001) Batch 4.089 (3.602) Remain 09:10:55 loss: 0.2316 Lr: 0.03415
[2023-08-08 01:16:39,171 INFO misc.py line 115 22900] Train: [42/100][28/156] Data 0.001 (0.001) Batch 3.244 (3.588) Remain 09:08:40 loss: 0.2380 Lr: 0.03414
[2023-08-08 01:16:43,334 INFO misc.py line 115 22900] Train: [42/100][29/156] Data 0.001 (0.001) Batch 4.163 (3.610) Remain 09:12:00 loss: 0.6601 Lr: 0.03414
[2023-08-08 01:16:47,126 INFO misc.py line 115 22900] Train: [42/100][30/156] Data 0.001 (0.001) Batch 3.791 (3.617) Remain 09:12:58 loss: 0.2510 Lr: 0.03413
[2023-08-08 01:16:51,031 INFO misc.py line 115 22900] Train: [42/100][31/156] Data 0.001 (0.001) Batch 3.906 (3.627) Remain 09:14:29 loss: 0.2573 Lr: 0.03413
[2023-08-08 01:16:54,192 INFO misc.py line 115 22900] Train: [42/100][32/156] Data 0.001 (0.001) Batch 3.161 (3.611) Remain 09:11:58 loss: 0.2062 Lr: 0.03412
[2023-08-08 01:16:56,810 INFO misc.py line 115 22900] Train: [42/100][33/156] Data 0.001 (0.001) Batch 2.618 (3.578) Remain 09:06:51 loss: 0.3476 Lr: 0.03412
[2023-08-08 01:16:59,979 INFO misc.py line 115 22900] Train: [42/100][34/156] Data 0.001 (0.001) Batch 3.169 (3.565) Remain 09:04:47 loss: 0.2208 Lr: 0.03411
[2023-08-08 01:17:04,569 INFO misc.py line 115 22900] Train: [42/100][35/156] Data 0.001 (0.001) Batch 4.589 (3.597) Remain 09:09:36 loss: 0.3290 Lr: 0.03411
[2023-08-08 01:17:07,464 INFO misc.py line 115 22900] Train: [42/100][36/156] Data 0.001 (0.001) Batch 2.895 (3.575) Remain 09:06:18 loss: 0.1192 Lr: 0.03410
[2023-08-08 01:17:11,468 INFO misc.py line 115 22900] Train: [42/100][37/156] Data 0.001 (0.001) Batch 4.004 (3.588) Remain 09:08:10 loss: 0.3148 Lr: 0.03410
[2023-08-08 01:17:14,879 INFO misc.py line 115 22900] Train: [42/100][38/156] Data 0.001 (0.001) Batch 3.412 (3.583) Remain 09:07:20 loss: 0.3334 Lr: 0.03409
[2023-08-08 01:17:18,876 INFO misc.py line 115 22900] Train: [42/100][39/156] Data 0.001 (0.001) Batch 3.996 (3.594) Remain 09:09:02 loss: 0.1761 Lr: 0.03409
[2023-08-08 01:17:22,543 INFO misc.py line 115 22900] Train: [42/100][40/156] Data 0.001 (0.001) Batch 3.667 (3.596) Remain 09:09:16 loss: 0.1726 Lr: 0.03408
[2023-08-08 01:17:25,139 INFO misc.py line 115 22900] Train: [42/100][41/156] Data 0.001 (0.001) Batch 2.596 (3.570) Remain 09:05:12 loss: 0.1739 Lr: 0.03408
[2023-08-08 01:17:28,117 INFO misc.py line 115 22900] Train: [42/100][42/156] Data 0.001 (0.001) Batch 2.978 (3.555) Remain 09:02:49 loss: 0.1765 Lr: 0.03407
[2023-08-08 01:17:30,246 INFO misc.py line 115 22900] Train: [42/100][43/156] Data 0.001 (0.001) Batch 2.129 (3.519) Remain 08:57:19 loss: 0.1365 Lr: 0.03407
[2023-08-08 01:17:33,677 INFO misc.py line 115 22900] Train: [42/100][44/156] Data 0.001 (0.001) Batch 3.431 (3.517) Remain 08:56:56 loss: 0.3236 Lr: 0.03406
[2023-08-08 01:17:36,180 INFO misc.py line 115 22900] Train: [42/100][45/156] Data 0.001 (0.001) Batch 2.503 (3.493) Remain 08:53:11 loss: 0.3250 Lr: 0.03406
[2023-08-08 01:17:39,895 INFO misc.py line 115 22900] Train: [42/100][46/156] Data 0.001 (0.001) Batch 3.715 (3.498) Remain 08:53:55 loss: 0.2945 Lr: 0.03405
[2023-08-08 01:17:44,152 INFO misc.py line 115 22900] Train: [42/100][47/156] Data 0.001 (0.001) Batch 4.257 (3.515) Remain 08:56:29 loss: 0.5541 Lr: 0.03405
[2023-08-08 01:17:47,384 INFO misc.py line 115 22900] Train: [42/100][48/156] Data 0.001 (0.001) Batch 3.232 (3.509) Remain 08:55:28 loss: 0.1850 Lr: 0.03404
[2023-08-08 01:17:51,078 INFO misc.py line 115 22900] Train: [42/100][49/156] Data 0.001 (0.001) Batch 3.695 (3.513) Remain 08:56:01 loss: 0.2418 Lr: 0.03404
[2023-08-08 01:17:53,909 INFO misc.py line 115 22900] Train: [42/100][50/156] Data 0.001 (0.001) Batch 2.831 (3.499) Remain 08:53:45 loss: 0.2997 Lr: 0.03403
[2023-08-08 01:17:56,718 INFO misc.py line 115 22900] Train: [42/100][51/156] Data 0.001 (0.001) Batch 2.808 (3.484) Remain 08:51:30 loss: 0.0564 Lr: 0.03403
[2023-08-08 01:17:59,572 INFO misc.py line 115 22900] Train: [42/100][52/156] Data 0.001 (0.001) Batch 2.854 (3.471) Remain 08:49:29 loss: 0.2363 Lr: 0.03402
[2023-08-08 01:18:03,332 INFO misc.py line 115 22900] Train: [42/100][53/156] Data 0.001 (0.001) Batch 3.761 (3.477) Remain 08:50:18 loss: 0.2855 Lr: 0.03402
[2023-08-08 01:18:07,386 INFO misc.py line 115 22900] Train: [42/100][54/156] Data 0.001 (0.001) Batch 4.053 (3.488) Remain 08:51:58 loss: 0.2627 Lr: 0.03401
[2023-08-08 01:18:10,840 INFO misc.py line 115 22900] Train: [42/100][55/156] Data 0.001 (0.001) Batch 3.454 (3.488) Remain 08:51:49 loss: 0.2635 Lr: 0.03401
[2023-08-08 01:18:14,562 INFO misc.py line 115 22900] Train: [42/100][56/156] Data 0.001 (0.001) Batch 3.722 (3.492) Remain 08:52:26 loss: 0.2793 Lr: 0.03400
[2023-08-08 01:18:18,629 INFO misc.py line 115 22900] Train: [42/100][57/156] Data 0.001 (0.001) Batch 4.067 (3.503) Remain 08:54:00 loss: 0.3724 Lr: 0.03400
[2023-08-08 01:18:21,800 INFO misc.py line 115 22900] Train: [42/100][58/156] Data 0.001 (0.001) Batch 3.171 (3.497) Remain 08:53:01 loss: 0.4235 Lr: 0.03399
[2023-08-08 01:18:25,775 INFO misc.py line 115 22900] Train: [42/100][59/156] Data 0.001 (0.001) Batch 3.975 (3.505) Remain 08:54:16 loss: 0.4634 Lr: 0.03399
[2023-08-08 01:18:29,451 INFO misc.py line 115 22900] Train: [42/100][60/156] Data 0.001 (0.001) Batch 3.676 (3.508) Remain 08:54:39 loss: 0.2533 Lr: 0.03398
[2023-08-08 01:18:32,830 INFO misc.py line 115 22900] Train: [42/100][61/156] Data 0.001 (0.001) Batch 3.379 (3.506) Remain 08:54:15 loss: 0.2724 Lr: 0.03398
[2023-08-08 01:18:36,909 INFO misc.py line 115 22900] Train: [42/100][62/156] Data 0.001 (0.001) Batch 4.079 (3.516) Remain 08:55:41 loss: 0.4295 Lr: 0.03397
[2023-08-08 01:18:39,777 INFO misc.py line 115 22900] Train: [42/100][63/156] Data 0.001 (0.001) Batch 2.868 (3.505) Remain 08:53:59 loss: 0.2473 Lr: 0.03397
[2023-08-08 01:18:42,319 INFO misc.py line 115 22900] Train: [42/100][64/156] Data 0.001 (0.001) Batch 2.542 (3.489) Remain 08:51:31 loss: 0.2248 Lr: 0.03396
[2023-08-08 01:18:46,821 INFO misc.py line 115 22900] Train: [42/100][65/156] Data 0.001 (0.001) Batch 4.502 (3.506) Remain 08:53:57 loss: 0.4288 Lr: 0.03396
[2023-08-08 01:18:49,686 INFO misc.py line 115 22900] Train: [42/100][66/156] Data 0.001 (0.001) Batch 2.865 (3.495) Remain 08:52:20 loss: 0.3252 Lr: 0.03395
[2023-08-08 01:18:53,716 INFO misc.py line 115 22900] Train: [42/100][67/156] Data 0.001 (0.001) Batch 4.031 (3.504) Remain 08:53:33 loss: 0.2628 Lr: 0.03395
[2023-08-08 01:18:57,631 INFO misc.py line 115 22900] Train: [42/100][68/156] Data 0.001 (0.001) Batch 3.914 (3.510) Remain 08:54:27 loss: 0.4040 Lr: 0.03394
[2023-08-08 01:19:00,469 INFO misc.py line 115 22900] Train: [42/100][69/156] Data 0.001 (0.001) Batch 2.838 (3.500) Remain 08:52:51 loss: 0.2614 Lr: 0.03394
[2023-08-08 01:19:04,128 INFO misc.py line 115 22900] Train: [42/100][70/156] Data 0.001 (0.001) Batch 3.659 (3.502) Remain 08:53:09 loss: 0.2126 Lr: 0.03393
[2023-08-08 01:19:08,207 INFO misc.py line 115 22900] Train: [42/100][71/156] Data 0.001 (0.001) Batch 4.079 (3.511) Remain 08:54:23 loss: 0.3069 Lr: 0.03393
[2023-08-08 01:19:12,086 INFO misc.py line 115 22900] Train: [42/100][72/156] Data 0.001 (0.001) Batch 3.879 (3.516) Remain 08:55:08 loss: 0.2861 Lr: 0.03392
[2023-08-08 01:19:16,052 INFO misc.py line 115 22900] Train: [42/100][73/156] Data 0.001 (0.001) Batch 3.966 (3.522) Remain 08:56:03 loss: 0.2501 Lr: 0.03392
[2023-08-08 01:19:19,778 INFO misc.py line 115 22900] Train: [42/100][74/156] Data 0.001 (0.001) Batch 3.726 (3.525) Remain 08:56:26 loss: 0.3024 Lr: 0.03391
[2023-08-08 01:19:22,483 INFO misc.py line 115 22900] Train: [42/100][75/156] Data 0.001 (0.001) Batch 2.705 (3.514) Remain 08:54:38 loss: 0.2058 Lr: 0.03391
[2023-08-08 01:19:23,954 INFO misc.py line 115 22900] Train: [42/100][76/156] Data 0.001 (0.001) Batch 1.471 (3.486) Remain 08:50:19 loss: 0.0615 Lr: 0.03390
[2023-08-08 01:19:27,149 INFO misc.py line 115 22900] Train: [42/100][77/156] Data 0.001 (0.001) Batch 3.196 (3.482) Remain 08:49:40 loss: 0.4971 Lr: 0.03390
[2023-08-08 01:19:30,656 INFO misc.py line 115 22900] Train: [42/100][78/156] Data 0.001 (0.001) Batch 3.506 (3.482) Remain 08:49:40 loss: 0.3615 Lr: 0.03390
[2023-08-08 01:19:34,377 INFO misc.py line 115 22900] Train: [42/100][79/156] Data 0.001 (0.001) Batch 3.721 (3.486) Remain 08:50:05 loss: 0.2889 Lr: 0.03389
[2023-08-08 01:19:37,708 INFO misc.py line 115 22900] Train: [42/100][80/156] Data 0.001 (0.001) Batch 3.331 (3.483) Remain 08:49:43 loss: 0.3320 Lr: 0.03389
[2023-08-08 01:19:40,759 INFO misc.py line 115 22900] Train: [42/100][81/156] Data 0.001 (0.001) Batch 3.052 (3.478) Remain 08:48:49 loss: 0.1950 Lr: 0.03388
[2023-08-08 01:19:43,850 INFO misc.py line 115 22900] Train: [42/100][82/156] Data 0.001 (0.001) Batch 3.091 (3.473) Remain 08:48:01 loss: 0.3773 Lr: 0.03388
[2023-08-08 01:19:46,930 INFO misc.py line 115 22900] Train: [42/100][83/156] Data 0.001 (0.001) Batch 3.079 (3.468) Remain 08:47:12 loss: 0.1610 Lr: 0.03387
[2023-08-08 01:19:49,666 INFO misc.py line 115 22900] Train: [42/100][84/156] Data 0.001 (0.001) Batch 2.736 (3.459) Remain 08:45:47 loss: 0.4090 Lr: 0.03387
[2023-08-08 01:19:52,965 INFO misc.py line 115 22900] Train: [42/100][85/156] Data 0.001 (0.001) Batch 3.299 (3.457) Remain 08:45:25 loss: 0.3462 Lr: 0.03386
[2023-08-08 01:19:56,715 INFO misc.py line 115 22900] Train: [42/100][86/156] Data 0.001 (0.001) Batch 3.749 (3.461) Remain 08:45:54 loss: 0.2784 Lr: 0.03386
[2023-08-08 01:20:00,856 INFO misc.py line 115 22900] Train: [42/100][87/156] Data 0.001 (0.001) Batch 4.141 (3.469) Remain 08:47:04 loss: 0.3554 Lr: 0.03385
[2023-08-08 01:20:04,009 INFO misc.py line 115 22900] Train: [42/100][88/156] Data 0.001 (0.001) Batch 3.154 (3.465) Remain 08:46:27 loss: 0.3074 Lr: 0.03385
[2023-08-08 01:20:06,885 INFO misc.py line 115 22900] Train: [42/100][89/156] Data 0.001 (0.001) Batch 2.875 (3.458) Remain 08:45:21 loss: 0.1083 Lr: 0.03384
[2023-08-08 01:20:11,050 INFO misc.py line 115 22900] Train: [42/100][90/156] Data 0.001 (0.001) Batch 4.165 (3.466) Remain 08:46:32 loss: 0.4090 Lr: 0.03384
[2023-08-08 01:20:14,382 INFO misc.py line 115 22900] Train: [42/100][91/156] Data 0.001 (0.001) Batch 3.332 (3.465) Remain 08:46:14 loss: 0.2901 Lr: 0.03383
[2023-08-08 01:20:18,027 INFO misc.py line 115 22900] Train: [42/100][92/156] Data 0.001 (0.001) Batch 3.646 (3.467) Remain 08:46:29 loss: 0.1843 Lr: 0.03383
[2023-08-08 01:20:21,178 INFO misc.py line 115 22900] Train: [42/100][93/156] Data 0.001 (0.001) Batch 3.151 (3.463) Remain 08:45:54 loss: 0.2783 Lr: 0.03382
[2023-08-08 01:20:24,911 INFO misc.py line 115 22900] Train: [42/100][94/156] Data 0.001 (0.001) Batch 3.733 (3.466) Remain 08:46:17 loss: 0.4324 Lr: 0.03382
[2023-08-08 01:20:28,095 INFO misc.py line 115 22900] Train: [42/100][95/156] Data 0.001 (0.001) Batch 3.184 (3.463) Remain 08:45:46 loss: 0.2723 Lr: 0.03381
[2023-08-08 01:20:30,538 INFO misc.py line 115 22900] Train: [42/100][96/156] Data 0.001 (0.001) Batch 2.443 (3.452) Remain 08:44:03 loss: 0.1616 Lr: 0.03381
[2023-08-08 01:20:34,013 INFO misc.py line 115 22900] Train: [42/100][97/156] Data 0.001 (0.001) Batch 3.475 (3.452) Remain 08:44:01 loss: 0.2408 Lr: 0.03380
[2023-08-08 01:20:38,075 INFO misc.py line 115 22900] Train: [42/100][98/156] Data 0.001 (0.001) Batch 4.062 (3.459) Remain 08:44:56 loss: 0.4525 Lr: 0.03380
[2023-08-08 01:20:42,129 INFO misc.py line 115 22900] Train: [42/100][99/156] Data 0.001 (0.001) Batch 4.054 (3.465) Remain 08:45:49 loss: 0.2252 Lr: 0.03379
[2023-08-08 01:20:45,696 INFO misc.py line 115 22900] Train: [42/100][100/156] Data 0.001 (0.001) Batch 3.567 (3.466) Remain 08:45:55 loss: 0.1888 Lr: 0.03379
[2023-08-08 01:20:48,631 INFO misc.py line 115 22900] Train: [42/100][101/156] Data 0.001 (0.001) Batch 2.935 (3.461) Remain 08:45:03 loss: 0.2678 Lr: 0.03378
[2023-08-08 01:20:51,338 INFO misc.py line 115 22900] Train: [42/100][102/156] Data 0.001 (0.001) Batch 2.707 (3.453) Remain 08:43:50 loss: 0.2751 Lr: 0.03378
[2023-08-08 01:20:55,030 INFO misc.py line 115 22900] Train: [42/100][103/156] Data 0.001 (0.001) Batch 3.692 (3.456) Remain 08:44:08 loss: 0.2046 Lr: 0.03377
[2023-08-08 01:20:57,624 INFO misc.py line 115 22900] Train: [42/100][104/156] Data 0.001 (0.001) Batch 2.594 (3.447) Remain 08:42:47 loss: 0.1291 Lr: 0.03377
[2023-08-08 01:21:01,385 INFO misc.py line 115 22900] Train: [42/100][105/156] Data 0.001 (0.001) Batch 3.761 (3.450) Remain 08:43:12 loss: 0.1579 Lr: 0.03376
[2023-08-08 01:21:05,300 INFO misc.py line 115 22900] Train: [42/100][106/156] Data 0.001 (0.001) Batch 3.915 (3.455) Remain 08:43:49 loss: 0.3207 Lr: 0.03376
[2023-08-08 01:21:08,342 INFO misc.py line 115 22900] Train: [42/100][107/156] Data 0.001 (0.001) Batch 3.041 (3.451) Remain 08:43:10 loss: 0.1836 Lr: 0.03375
[2023-08-08 01:21:10,952 INFO misc.py line 115 22900] Train: [42/100][108/156] Data 0.001 (0.001) Batch 2.611 (3.443) Remain 08:41:53 loss: 0.1290 Lr: 0.03375
[2023-08-08 01:21:14,355 INFO misc.py line 115 22900] Train: [42/100][109/156] Data 0.001 (0.001) Batch 3.403 (3.442) Remain 08:41:47 loss: 0.3609 Lr: 0.03374
[2023-08-08 01:21:17,844 INFO misc.py line 115 22900] Train: [42/100][110/156] Data 0.001 (0.001) Batch 3.489 (3.443) Remain 08:41:47 loss: 0.4843 Lr: 0.03374
[2023-08-08 01:21:20,901 INFO misc.py line 115 22900] Train: [42/100][111/156] Data 0.001 (0.001) Batch 3.057 (3.439) Remain 08:41:11 loss: 0.2922 Lr: 0.03373
[2023-08-08 01:21:25,045 INFO misc.py line 115 22900] Train: [42/100][112/156] Data 0.001 (0.001) Batch 4.145 (3.446) Remain 08:42:07 loss: 0.3568 Lr: 0.03373
[2023-08-08 01:21:27,700 INFO misc.py line 115 22900] Train: [42/100][113/156] Data 0.001 (0.001) Batch 2.655 (3.438) Remain 08:40:58 loss: 0.2535 Lr: 0.03372
[2023-08-08 01:21:31,689 INFO misc.py line 115 22900] Train: [42/100][114/156] Data 0.001 (0.001) Batch 3.989 (3.443) Remain 08:41:39 loss: 0.3574 Lr: 0.03372
[2023-08-08 01:21:34,729 INFO misc.py line 115 22900] Train: [42/100][115/156] Data 0.001 (0.001) Batch 3.040 (3.440) Remain 08:41:03 loss: 0.2609 Lr: 0.03371
[2023-08-08 01:21:38,373 INFO misc.py line 115 22900] Train: [42/100][116/156] Data 0.001 (0.001) Batch 3.644 (3.442) Remain 08:41:16 loss: 0.1557 Lr: 0.03371
[2023-08-08 01:21:41,091 INFO misc.py line 115 22900] Train: [42/100][117/156] Data 0.001 (0.001) Batch 2.718 (3.435) Remain 08:40:15 loss: 0.2097 Lr: 0.03370
[2023-08-08 01:21:45,446 INFO misc.py line 115 22900] Train: [42/100][118/156] Data 0.001 (0.001) Batch 4.355 (3.443) Remain 08:41:24 loss: 0.5792 Lr: 0.03370
[2023-08-08 01:21:49,425 INFO misc.py line 115 22900] Train: [42/100][119/156] Data 0.001 (0.001) Batch 3.979 (3.448) Remain 08:42:03 loss: 0.4434 Lr: 0.03369
[2023-08-08 01:21:53,341 INFO misc.py line 115 22900] Train: [42/100][120/156] Data 0.001 (0.001) Batch 3.915 (3.452) Remain 08:42:36 loss: 0.3689 Lr: 0.03369
[2023-08-08 01:21:56,942 INFO misc.py line 115 22900] Train: [42/100][121/156] Data 0.001 (0.001) Batch 3.601 (3.453) Remain 08:42:44 loss: 0.2529 Lr: 0.03368
[2023-08-08 01:22:00,988 INFO misc.py line 115 22900] Train: [42/100][122/156] Data 0.001 (0.001) Batch 4.046 (3.458) Remain 08:43:26 loss: 0.2726 Lr: 0.03368
[2023-08-08 01:22:04,264 INFO misc.py line 115 22900] Train: [42/100][123/156] Data 0.001 (0.001) Batch 3.276 (3.457) Remain 08:43:08 loss: 0.1228 Lr: 0.03367
[2023-08-08 01:22:08,326 INFO misc.py line 115 22900] Train: [42/100][124/156] Data 0.001 (0.001) Batch 4.062 (3.462) Remain 08:43:50 loss: 0.5654 Lr: 0.03367
[2023-08-08 01:22:12,271 INFO misc.py line 115 22900] Train: [42/100][125/156] Data 0.001 (0.001) Batch 3.945 (3.466) Remain 08:44:23 loss: 0.4365 Lr: 0.03366
[2023-08-08 01:22:15,681 INFO misc.py line 115 22900] Train: [42/100][126/156] Data 0.001 (0.001) Batch 3.410 (3.465) Remain 08:44:15 loss: 0.4787 Lr: 0.03366
[2023-08-08 01:22:18,957 INFO misc.py line 115 22900] Train: [42/100][127/156] Data 0.001 (0.001) Batch 3.276 (3.464) Remain 08:43:58 loss: 0.2340 Lr: 0.03365
[2023-08-08 01:22:22,089 INFO misc.py line 115 22900] Train: [42/100][128/156] Data 0.001 (0.001) Batch 3.132 (3.461) Remain 08:43:31 loss: 0.2825 Lr: 0.03365
[2023-08-08 01:22:25,367 INFO misc.py line 115 22900] Train: [42/100][129/156] Data 0.001 (0.001) Batch 3.278 (3.459) Remain 08:43:14 loss: 0.3048 Lr: 0.03364
[2023-08-08 01:22:28,672 INFO misc.py line 115 22900] Train: [42/100][130/156] Data 0.001 (0.001) Batch 3.305 (3.458) Remain 08:42:59 loss: 0.6060 Lr: 0.03364
[2023-08-08 01:22:32,722 INFO misc.py line 115 22900] Train: [42/100][131/156] Data 0.001 (0.001) Batch 4.050 (3.463) Remain 08:43:38 loss: 0.5846 Lr: 0.03363
[2023-08-08 01:22:36,401 INFO misc.py line 115 22900] Train: [42/100][132/156] Data 0.001 (0.001) Batch 3.679 (3.465) Remain 08:43:50 loss: 0.2545 Lr: 0.03363
[2023-08-08 01:22:38,829 INFO misc.py line 115 22900] Train: [42/100][133/156] Data 0.001 (0.001) Batch 2.429 (3.457) Remain 08:42:34 loss: 0.3420 Lr: 0.03362
[2023-08-08 01:22:42,601 INFO misc.py line 115 22900] Train: [42/100][134/156] Data 0.001 (0.001) Batch 3.772 (3.459) Remain 08:42:52 loss: 0.4029 Lr: 0.03362
[2023-08-08 01:22:44,639 INFO misc.py line 115 22900] Train: [42/100][135/156] Data 0.001 (0.001) Batch 2.038 (3.448) Remain 08:41:11 loss: 0.3925 Lr: 0.03361
[2023-08-08 01:22:48,443 INFO misc.py line 115 22900] Train: [42/100][136/156] Data 0.001 (0.001) Batch 3.804 (3.451) Remain 08:41:32 loss: 0.3503 Lr: 0.03361
[2023-08-08 01:22:51,868 INFO misc.py line 115 22900] Train: [42/100][137/156] Data 0.001 (0.001) Batch 3.426 (3.451) Remain 08:41:27 loss: 0.3058 Lr: 0.03360
[2023-08-08 01:22:55,066 INFO misc.py line 115 22900] Train: [42/100][138/156] Data 0.001 (0.001) Batch 3.198 (3.449) Remain 08:41:06 loss: 0.1794 Lr: 0.03360
[2023-08-08 01:22:58,905 INFO misc.py line 115 22900] Train: [42/100][139/156] Data 0.001 (0.001) Batch 3.839 (3.452) Remain 08:41:29 loss: 0.3131 Lr: 0.03359
[2023-08-08 01:23:01,361 INFO misc.py line 115 22900] Train: [42/100][140/156] Data 0.001 (0.001) Batch 2.456 (3.444) Remain 08:40:20 loss: 0.2017 Lr: 0.03359
[2023-08-08 01:23:04,542 INFO misc.py line 115 22900] Train: [42/100][141/156] Data 0.001 (0.001) Batch 3.180 (3.442) Remain 08:39:59 loss: 0.2837 Lr: 0.03358
[2023-08-08 01:23:07,791 INFO misc.py line 115 22900] Train: [42/100][142/156] Data 0.001 (0.001) Batch 3.249 (3.441) Remain 08:39:43 loss: 0.2953 Lr: 0.03358
[2023-08-08 01:23:11,403 INFO misc.py line 115 22900] Train: [42/100][143/156] Data 0.001 (0.001) Batch 3.612 (3.442) Remain 08:39:50 loss: 0.2906 Lr: 0.03357
[2023-08-08 01:23:15,197 INFO misc.py line 115 22900] Train: [42/100][144/156] Data 0.001 (0.001) Batch 3.794 (3.445) Remain 08:40:09 loss: 0.2210 Lr: 0.03357
[2023-08-08 01:23:19,008 INFO misc.py line 115 22900] Train: [42/100][145/156] Data 0.001 (0.001) Batch 3.811 (3.447) Remain 08:40:29 loss: 0.2844 Lr: 0.03356
[2023-08-08 01:23:22,976 INFO misc.py line 115 22900] Train: [42/100][146/156] Data 0.001 (0.001) Batch 3.969 (3.451) Remain 08:40:59 loss: 0.3611 Lr: 0.03356
[2023-08-08 01:23:25,970 INFO misc.py line 115 22900] Train: [42/100][147/156] Data 0.001 (0.001) Batch 2.994 (3.448) Remain 08:40:27 loss: 0.3695 Lr: 0.03355
[2023-08-08 01:23:29,162 INFO misc.py line 115 22900] Train: [42/100][148/156] Data 0.001 (0.001) Batch 3.191 (3.446) Remain 08:40:07 loss: 0.2934 Lr: 0.03355
[2023-08-08 01:23:32,026 INFO misc.py line 115 22900] Train: [42/100][149/156] Data 0.001 (0.001) Batch 2.864 (3.442) Remain 08:39:28 loss: 0.3435 Lr: 0.03354
[2023-08-08 01:23:34,996 INFO misc.py line 115 22900] Train: [42/100][150/156] Data 0.001 (0.001) Batch 2.970 (3.439) Remain 08:38:55 loss: 0.3475 Lr: 0.03354
[2023-08-08 01:23:38,905 INFO misc.py line 115 22900] Train: [42/100][151/156] Data 0.001 (0.001) Batch 3.909 (3.442) Remain 08:39:21 loss: 0.2737 Lr: 0.03353
[2023-08-08 01:23:42,643 INFO misc.py line 115 22900] Train: [42/100][152/156] Data 0.001 (0.001) Batch 3.738 (3.444) Remain 08:39:35 loss: 0.1772 Lr: 0.03353
[2023-08-08 01:23:46,912 INFO misc.py line 115 22900] Train: [42/100][153/156] Data 0.001 (0.001) Batch 4.269 (3.450) Remain 08:40:21 loss: 0.2538 Lr: 0.03352
[2023-08-08 01:23:49,665 INFO misc.py line 115 22900] Train: [42/100][154/156] Data 0.001 (0.001) Batch 2.753 (3.445) Remain 08:39:36 loss: 0.1020 Lr: 0.03352
[2023-08-08 01:23:53,493 INFO misc.py line 115 22900] Train: [42/100][155/156] Data 0.001 (0.001) Batch 3.828 (3.447) Remain 08:39:56 loss: 0.4381 Lr: 0.03351
[2023-08-08 01:23:57,545 INFO misc.py line 115 22900] Train: [42/100][156/156] Data 0.001 (0.001) Batch 4.052 (3.451) Remain 08:40:28 loss: 0.4180 Lr: 0.03351
[2023-08-08 01:23:57,546 INFO misc.py line 129 22900] Train result: loss: 0.3028 
[2023-08-08 01:23:57,546 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 01:23:59,693 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.9356 
[2023-08-08 01:24:00,561 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.4531 
[2023-08-08 01:24:02,224 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.6989 
[2023-08-08 01:24:03,743 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.4384 
[2023-08-08 01:24:05,584 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.9797 
[2023-08-08 01:24:07,246 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.7353 
[2023-08-08 01:24:09,383 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.2059 
[2023-08-08 01:24:11,186 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.8957 
[2023-08-08 01:24:12,469 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.4637 
[2023-08-08 01:24:14,598 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.2839 
[2023-08-08 01:24:15,122 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.3418 
[2023-08-08 01:24:16,656 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.7560 
[2023-08-08 01:24:19,371 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.1727 
[2023-08-08 01:24:21,053 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.8077 
[2023-08-08 01:24:23,074 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.3946 
[2023-08-08 01:24:25,785 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.1988 
[2023-08-08 01:24:28,494 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.6008 
[2023-08-08 01:24:30,341 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.6478 
[2023-08-08 01:24:31,089 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.0550 
[2023-08-08 01:24:31,975 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.5629 
[2023-08-08 01:24:34,236 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.2981 
[2023-08-08 01:24:36,198 INFO evaluator.py line 122 22900] Test: [22/24] Loss 2.1294 
[2023-08-08 01:24:38,044 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.8264 
[2023-08-08 01:24:39,978 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.5082 
[2023-08-08 01:24:40,024 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2362/0.3298/0.6864.
[2023-08-08 01:24:40,024 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6537/0.9409
[2023-08-08 01:24:40,024 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9372/0.9932
[2023-08-08 01:24:40,024 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1779/0.5297
[2023-08-08 01:24:40,024 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1147/0.3398
[2023-08-08 01:24:40,024 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6552/0.7080
[2023-08-08 01:24:40,024 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.4052/0.6019
[2023-08-08 01:24:40,025 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.4763/0.6343
[2023-08-08 01:24:40,025 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1546/0.1821
[2023-08-08 01:24:40,025 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1454/0.3333
[2023-08-08 01:24:40,025 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0301/0.0302
[2023-08-08 01:24:40,025 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 01:24:40,025 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0219/0.0240
[2023-08-08 01:24:40,025 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1208/0.1340
[2023-08-08 01:24:40,025 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0665/0.0703
[2023-08-08 01:24:40,025 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0210/0.0242
[2023-08-08 01:24:40,025 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.1512/0.1587
[2023-08-08 01:24:40,025 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.1219/0.1696
[2023-08-08 01:24:40,025 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.3782/0.5430
[2023-08-08 01:24:40,025 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 01:24:40,025 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0925/0.1779
[2023-08-08 01:24:40,025 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 01:24:40,025 INFO misc.py line 152 22900] Currently Best mIoU: 0.2562
[2023-08-08 01:24:40,025 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 01:24:46,613 INFO misc.py line 115 22900] Train: [43/100][1/156] Data 1.561 (1.561) Batch 5.764 (5.764) Remain 14:29:10 loss: 0.2841 Lr: 0.03350
[2023-08-08 01:24:50,107 INFO misc.py line 115 22900] Train: [43/100][2/156] Data 0.001 (0.001) Batch 3.494 (3.494) Remain 08:46:47 loss: 0.4806 Lr: 0.03350
[2023-08-08 01:24:54,324 INFO misc.py line 115 22900] Train: [43/100][3/156] Data 0.001 (0.001) Batch 4.216 (4.216) Remain 10:35:36 loss: 0.6616 Lr: 0.03349
[2023-08-08 01:24:57,511 INFO misc.py line 115 22900] Train: [43/100][4/156] Data 0.001 (0.001) Batch 3.187 (3.187) Remain 08:00:23 loss: 0.0953 Lr: 0.03349
[2023-08-08 01:25:00,231 INFO misc.py line 115 22900] Train: [43/100][5/156] Data 0.001 (0.001) Batch 2.721 (2.954) Remain 07:25:11 loss: 0.2059 Lr: 0.03348
[2023-08-08 01:25:03,486 INFO misc.py line 115 22900] Train: [43/100][6/156] Data 0.001 (0.001) Batch 3.255 (3.054) Remain 07:40:15 loss: 0.3534 Lr: 0.03348
[2023-08-08 01:25:07,272 INFO misc.py line 115 22900] Train: [43/100][7/156] Data 0.001 (0.001) Batch 3.786 (3.237) Remain 08:07:46 loss: 0.3699 Lr: 0.03347
[2023-08-08 01:25:09,402 INFO misc.py line 115 22900] Train: [43/100][8/156] Data 0.001 (0.001) Batch 2.130 (3.016) Remain 07:34:21 loss: 0.0650 Lr: 0.03347
[2023-08-08 01:25:13,478 INFO misc.py line 115 22900] Train: [43/100][9/156] Data 0.001 (0.001) Batch 4.076 (3.192) Remain 08:00:55 loss: 0.7047 Lr: 0.03346
[2023-08-08 01:25:16,754 INFO misc.py line 115 22900] Train: [43/100][10/156] Data 0.001 (0.001) Batch 3.276 (3.204) Remain 08:02:40 loss: 0.2538 Lr: 0.03346
[2023-08-08 01:25:19,733 INFO misc.py line 115 22900] Train: [43/100][11/156] Data 0.001 (0.001) Batch 2.979 (3.176) Remain 07:58:23 loss: 0.1922 Lr: 0.03345
[2023-08-08 01:25:23,048 INFO misc.py line 115 22900] Train: [43/100][12/156] Data 0.001 (0.001) Batch 3.314 (3.192) Remain 08:00:38 loss: 0.2639 Lr: 0.03345
[2023-08-08 01:25:25,644 INFO misc.py line 115 22900] Train: [43/100][13/156] Data 0.001 (0.001) Batch 2.597 (3.132) Remain 07:51:38 loss: 0.4148 Lr: 0.03344
[2023-08-08 01:25:28,807 INFO misc.py line 115 22900] Train: [43/100][14/156] Data 0.001 (0.001) Batch 3.162 (3.135) Remain 07:51:59 loss: 0.2518 Lr: 0.03344
[2023-08-08 01:25:31,615 INFO misc.py line 115 22900] Train: [43/100][15/156] Data 0.001 (0.001) Batch 2.809 (3.108) Remain 07:47:51 loss: 0.1513 Lr: 0.03343
[2023-08-08 01:25:34,915 INFO misc.py line 115 22900] Train: [43/100][16/156] Data 0.001 (0.001) Batch 3.300 (3.122) Remain 07:50:01 loss: 0.2808 Lr: 0.03343
[2023-08-08 01:25:38,955 INFO misc.py line 115 22900] Train: [43/100][17/156] Data 0.001 (0.001) Batch 4.040 (3.188) Remain 07:59:50 loss: 0.6749 Lr: 0.03342
[2023-08-08 01:25:42,362 INFO misc.py line 115 22900] Train: [43/100][18/156] Data 0.001 (0.001) Batch 3.407 (3.203) Remain 08:01:59 loss: 0.8356 Lr: 0.03342
[2023-08-08 01:25:45,919 INFO misc.py line 115 22900] Train: [43/100][19/156] Data 0.001 (0.001) Batch 3.556 (3.225) Remain 08:05:15 loss: 0.4475 Lr: 0.03341
[2023-08-08 01:25:50,037 INFO misc.py line 115 22900] Train: [43/100][20/156] Data 0.001 (0.001) Batch 4.118 (3.277) Remain 08:13:06 loss: 0.4945 Lr: 0.03341
[2023-08-08 01:25:53,195 INFO misc.py line 115 22900] Train: [43/100][21/156] Data 0.001 (0.001) Batch 3.158 (3.271) Remain 08:12:03 loss: 0.3432 Lr: 0.03340
[2023-08-08 01:25:57,413 INFO misc.py line 115 22900] Train: [43/100][22/156] Data 0.001 (0.001) Batch 4.219 (3.321) Remain 08:19:30 loss: 0.3857 Lr: 0.03340
[2023-08-08 01:26:00,586 INFO misc.py line 115 22900] Train: [43/100][23/156] Data 0.001 (0.001) Batch 3.173 (3.313) Remain 08:18:21 loss: 0.2540 Lr: 0.03339
[2023-08-08 01:26:04,031 INFO misc.py line 115 22900] Train: [43/100][24/156] Data 0.001 (0.001) Batch 3.444 (3.319) Remain 08:19:14 loss: 0.3976 Lr: 0.03339
[2023-08-08 01:26:07,466 INFO misc.py line 115 22900] Train: [43/100][25/156] Data 0.001 (0.001) Batch 3.435 (3.325) Remain 08:19:58 loss: 0.2924 Lr: 0.03338
[2023-08-08 01:26:10,711 INFO misc.py line 115 22900] Train: [43/100][26/156] Data 0.001 (0.001) Batch 3.245 (3.321) Remain 08:19:23 loss: 0.3600 Lr: 0.03338
[2023-08-08 01:26:15,165 INFO misc.py line 115 22900] Train: [43/100][27/156] Data 0.001 (0.001) Batch 4.454 (3.368) Remain 08:26:26 loss: 0.4101 Lr: 0.03337
[2023-08-08 01:26:18,794 INFO misc.py line 115 22900] Train: [43/100][28/156] Data 0.001 (0.001) Batch 3.629 (3.379) Remain 08:27:56 loss: 0.3353 Lr: 0.03337
[2023-08-08 01:26:21,803 INFO misc.py line 115 22900] Train: [43/100][29/156] Data 0.001 (0.001) Batch 3.009 (3.365) Remain 08:25:45 loss: 0.2044 Lr: 0.03336
[2023-08-08 01:26:26,275 INFO misc.py line 115 22900] Train: [43/100][30/156] Data 0.001 (0.001) Batch 4.472 (3.406) Remain 08:31:51 loss: 0.6977 Lr: 0.03336
[2023-08-08 01:26:30,807 INFO misc.py line 115 22900] Train: [43/100][31/156] Data 0.001 (0.001) Batch 4.532 (3.446) Remain 08:37:51 loss: 0.3775 Lr: 0.03335
[2023-08-08 01:26:35,267 INFO misc.py line 115 22900] Train: [43/100][32/156] Data 0.001 (0.001) Batch 4.460 (3.481) Remain 08:43:03 loss: 0.4316 Lr: 0.03335
[2023-08-08 01:26:37,521 INFO misc.py line 115 22900] Train: [43/100][33/156] Data 0.001 (0.001) Batch 2.254 (3.440) Remain 08:36:50 loss: 0.3024 Lr: 0.03334
[2023-08-08 01:26:40,354 INFO misc.py line 115 22900] Train: [43/100][34/156] Data 0.001 (0.001) Batch 2.833 (3.420) Remain 08:33:51 loss: 0.3800 Lr: 0.03334
[2023-08-08 01:26:42,968 INFO misc.py line 115 22900] Train: [43/100][35/156] Data 0.001 (0.001) Batch 2.614 (3.395) Remain 08:30:00 loss: 0.2975 Lr: 0.03333
[2023-08-08 01:26:47,082 INFO misc.py line 115 22900] Train: [43/100][36/156] Data 0.001 (0.001) Batch 4.114 (3.417) Remain 08:33:13 loss: 0.1916 Lr: 0.03333
[2023-08-08 01:26:50,226 INFO misc.py line 115 22900] Train: [43/100][37/156] Data 0.001 (0.001) Batch 3.143 (3.409) Remain 08:31:57 loss: 0.2043 Lr: 0.03332
[2023-08-08 01:26:54,236 INFO misc.py line 115 22900] Train: [43/100][38/156] Data 0.001 (0.001) Batch 4.010 (3.426) Remain 08:34:28 loss: 0.3059 Lr: 0.03332
[2023-08-08 01:26:57,033 INFO misc.py line 115 22900] Train: [43/100][39/156] Data 0.001 (0.001) Batch 2.797 (3.409) Remain 08:31:47 loss: 0.1493 Lr: 0.03331
[2023-08-08 01:26:59,796 INFO misc.py line 115 22900] Train: [43/100][40/156] Data 0.001 (0.001) Batch 2.763 (3.391) Remain 08:29:07 loss: 0.2284 Lr: 0.03331
[2023-08-08 01:27:03,704 INFO misc.py line 115 22900] Train: [43/100][41/156] Data 0.001 (0.001) Batch 3.908 (3.405) Remain 08:31:06 loss: 0.4767 Lr: 0.03330
[2023-08-08 01:27:06,066 INFO misc.py line 115 22900] Train: [43/100][42/156] Data 0.001 (0.001) Batch 2.363 (3.378) Remain 08:27:02 loss: 0.1722 Lr: 0.03330
[2023-08-08 01:27:09,888 INFO misc.py line 115 22900] Train: [43/100][43/156] Data 0.001 (0.001) Batch 3.822 (3.389) Remain 08:28:39 loss: 0.2423 Lr: 0.03329
[2023-08-08 01:27:12,770 INFO misc.py line 115 22900] Train: [43/100][44/156] Data 0.001 (0.001) Batch 2.881 (3.377) Remain 08:26:44 loss: 0.2598 Lr: 0.03329
[2023-08-08 01:27:15,383 INFO misc.py line 115 22900] Train: [43/100][45/156] Data 0.001 (0.001) Batch 2.613 (3.359) Remain 08:23:57 loss: 0.2130 Lr: 0.03328
[2023-08-08 01:27:19,482 INFO misc.py line 115 22900] Train: [43/100][46/156] Data 0.001 (0.001) Batch 4.099 (3.376) Remain 08:26:28 loss: 0.4070 Lr: 0.03328
[2023-08-08 01:27:22,831 INFO misc.py line 115 22900] Train: [43/100][47/156] Data 0.001 (0.001) Batch 3.349 (3.375) Remain 08:26:19 loss: 0.2440 Lr: 0.03327
[2023-08-08 01:27:25,862 INFO misc.py line 115 22900] Train: [43/100][48/156] Data 0.001 (0.001) Batch 3.031 (3.368) Remain 08:25:07 loss: 0.2655 Lr: 0.03327
[2023-08-08 01:27:29,827 INFO misc.py line 115 22900] Train: [43/100][49/156] Data 0.001 (0.001) Batch 3.965 (3.381) Remain 08:27:01 loss: 0.5159 Lr: 0.03326
[2023-08-08 01:27:33,392 INFO misc.py line 115 22900] Train: [43/100][50/156] Data 0.001 (0.001) Batch 3.565 (3.384) Remain 08:27:33 loss: 0.2238 Lr: 0.03326
[2023-08-08 01:27:36,691 INFO misc.py line 115 22900] Train: [43/100][51/156] Data 0.001 (0.001) Batch 3.298 (3.383) Remain 08:27:13 loss: 0.4473 Lr: 0.03325
[2023-08-08 01:27:40,341 INFO misc.py line 115 22900] Train: [43/100][52/156] Data 0.001 (0.001) Batch 3.651 (3.388) Remain 08:27:59 loss: 0.2044 Lr: 0.03325
[2023-08-08 01:27:42,844 INFO misc.py line 115 22900] Train: [43/100][53/156] Data 0.001 (0.001) Batch 2.503 (3.370) Remain 08:25:16 loss: 0.3332 Lr: 0.03324
[2023-08-08 01:27:46,758 INFO misc.py line 115 22900] Train: [43/100][54/156] Data 0.001 (0.001) Batch 3.914 (3.381) Remain 08:26:49 loss: 0.2022 Lr: 0.03324
[2023-08-08 01:27:50,569 INFO misc.py line 115 22900] Train: [43/100][55/156] Data 0.001 (0.001) Batch 3.811 (3.389) Remain 08:28:00 loss: 0.4918 Lr: 0.03323
[2023-08-08 01:27:53,106 INFO misc.py line 115 22900] Train: [43/100][56/156] Data 0.001 (0.001) Batch 2.537 (3.373) Remain 08:25:32 loss: 0.2849 Lr: 0.03323
[2023-08-08 01:27:55,374 INFO misc.py line 115 22900] Train: [43/100][57/156] Data 0.001 (0.001) Batch 2.268 (3.353) Remain 08:22:24 loss: 0.1791 Lr: 0.03322
[2023-08-08 01:27:58,833 INFO misc.py line 115 22900] Train: [43/100][58/156] Data 0.001 (0.001) Batch 3.460 (3.355) Remain 08:22:38 loss: 0.3281 Lr: 0.03322
[2023-08-08 01:28:02,809 INFO misc.py line 115 22900] Train: [43/100][59/156] Data 0.001 (0.001) Batch 3.975 (3.366) Remain 08:24:15 loss: 0.3467 Lr: 0.03321
[2023-08-08 01:28:06,120 INFO misc.py line 115 22900] Train: [43/100][60/156] Data 0.001 (0.001) Batch 3.312 (3.365) Remain 08:24:03 loss: 0.4962 Lr: 0.03321
[2023-08-08 01:28:08,645 INFO misc.py line 115 22900] Train: [43/100][61/156] Data 0.001 (0.001) Batch 2.525 (3.350) Remain 08:21:49 loss: 0.2049 Lr: 0.03320
[2023-08-08 01:28:11,777 INFO misc.py line 115 22900] Train: [43/100][62/156] Data 0.001 (0.001) Batch 3.132 (3.347) Remain 08:21:13 loss: 0.2252 Lr: 0.03320
[2023-08-08 01:28:15,457 INFO misc.py line 115 22900] Train: [43/100][63/156] Data 0.001 (0.001) Batch 3.679 (3.352) Remain 08:21:59 loss: 0.2242 Lr: 0.03319
[2023-08-08 01:28:19,507 INFO misc.py line 115 22900] Train: [43/100][64/156] Data 0.001 (0.001) Batch 4.051 (3.364) Remain 08:23:39 loss: 0.5140 Lr: 0.03319
[2023-08-08 01:28:21,548 INFO misc.py line 115 22900] Train: [43/100][65/156] Data 0.001 (0.001) Batch 2.041 (3.342) Remain 08:20:24 loss: 0.1559 Lr: 0.03318
[2023-08-08 01:28:24,183 INFO misc.py line 115 22900] Train: [43/100][66/156] Data 0.001 (0.001) Batch 2.635 (3.331) Remain 08:18:39 loss: 0.1482 Lr: 0.03318
[2023-08-08 01:28:27,769 INFO misc.py line 115 22900] Train: [43/100][67/156] Data 0.001 (0.001) Batch 3.586 (3.335) Remain 08:19:12 loss: 0.4087 Lr: 0.03317
[2023-08-08 01:28:30,971 INFO misc.py line 115 22900] Train: [43/100][68/156] Data 0.001 (0.001) Batch 3.202 (3.333) Remain 08:18:50 loss: 0.1643 Lr: 0.03317
[2023-08-08 01:28:34,934 INFO misc.py line 115 22900] Train: [43/100][69/156] Data 0.001 (0.001) Batch 3.962 (3.343) Remain 08:20:12 loss: 0.4708 Lr: 0.03316
[2023-08-08 01:28:38,408 INFO misc.py line 115 22900] Train: [43/100][70/156] Data 0.001 (0.001) Batch 3.474 (3.345) Remain 08:20:27 loss: 0.3552 Lr: 0.03316
[2023-08-08 01:28:41,885 INFO misc.py line 115 22900] Train: [43/100][71/156] Data 0.001 (0.001) Batch 3.477 (3.346) Remain 08:20:41 loss: 0.3132 Lr: 0.03315
[2023-08-08 01:28:44,035 INFO misc.py line 115 22900] Train: [43/100][72/156] Data 0.001 (0.001) Batch 2.150 (3.329) Remain 08:18:02 loss: 0.0904 Lr: 0.03315
[2023-08-08 01:28:48,027 INFO misc.py line 115 22900] Train: [43/100][73/156] Data 0.002 (0.001) Batch 3.993 (3.339) Remain 08:19:24 loss: 0.6136 Lr: 0.03314
[2023-08-08 01:28:51,419 INFO misc.py line 115 22900] Train: [43/100][74/156] Data 0.001 (0.001) Batch 3.392 (3.339) Remain 08:19:27 loss: 0.3170 Lr: 0.03314
[2023-08-08 01:28:55,103 INFO misc.py line 115 22900] Train: [43/100][75/156] Data 0.001 (0.001) Batch 3.684 (3.344) Remain 08:20:07 loss: 0.3881 Lr: 0.03313
[2023-08-08 01:28:59,018 INFO misc.py line 115 22900] Train: [43/100][76/156] Data 0.001 (0.001) Batch 3.916 (3.352) Remain 08:21:13 loss: 0.3930 Lr: 0.03313
[2023-08-08 01:29:01,441 INFO misc.py line 115 22900] Train: [43/100][77/156] Data 0.001 (0.001) Batch 2.422 (3.339) Remain 08:19:17 loss: 0.1792 Lr: 0.03312
[2023-08-08 01:29:04,858 INFO misc.py line 115 22900] Train: [43/100][78/156] Data 0.001 (0.001) Batch 3.418 (3.340) Remain 08:19:23 loss: 0.2633 Lr: 0.03312
[2023-08-08 01:29:08,407 INFO misc.py line 115 22900] Train: [43/100][79/156] Data 0.001 (0.001) Batch 3.549 (3.343) Remain 08:19:45 loss: 0.3256 Lr: 0.03311
[2023-08-08 01:29:12,048 INFO misc.py line 115 22900] Train: [43/100][80/156] Data 0.001 (0.001) Batch 3.640 (3.347) Remain 08:20:16 loss: 0.2660 Lr: 0.03311
[2023-08-08 01:29:15,570 INFO misc.py line 115 22900] Train: [43/100][81/156] Data 0.001 (0.001) Batch 3.523 (3.349) Remain 08:20:33 loss: 0.3637 Lr: 0.03310
[2023-08-08 01:29:18,806 INFO misc.py line 115 22900] Train: [43/100][82/156] Data 0.001 (0.001) Batch 3.236 (3.348) Remain 08:20:17 loss: 0.1090 Lr: 0.03310
[2023-08-08 01:29:23,504 INFO misc.py line 115 22900] Train: [43/100][83/156] Data 0.001 (0.001) Batch 4.697 (3.365) Remain 08:22:45 loss: 0.4150 Lr: 0.03309
[2023-08-08 01:29:26,643 INFO misc.py line 115 22900] Train: [43/100][84/156] Data 0.001 (0.001) Batch 3.139 (3.362) Remain 08:22:16 loss: 0.2871 Lr: 0.03309
[2023-08-08 01:29:31,269 INFO misc.py line 115 22900] Train: [43/100][85/156] Data 0.001 (0.001) Batch 4.626 (3.377) Remain 08:24:31 loss: 0.5223 Lr: 0.03308
[2023-08-08 01:29:35,365 INFO misc.py line 115 22900] Train: [43/100][86/156] Data 0.001 (0.001) Batch 4.096 (3.386) Remain 08:25:45 loss: 0.2161 Lr: 0.03308
[2023-08-08 01:29:38,524 INFO misc.py line 115 22900] Train: [43/100][87/156] Data 0.001 (0.001) Batch 3.159 (3.383) Remain 08:25:18 loss: 0.3664 Lr: 0.03307
[2023-08-08 01:29:41,686 INFO misc.py line 115 22900] Train: [43/100][88/156] Data 0.001 (0.001) Batch 3.161 (3.381) Remain 08:24:51 loss: 0.3175 Lr: 0.03307
[2023-08-08 01:29:45,807 INFO misc.py line 115 22900] Train: [43/100][89/156] Data 0.001 (0.001) Batch 4.122 (3.389) Remain 08:26:05 loss: 0.1374 Lr: 0.03306
[2023-08-08 01:29:50,131 INFO misc.py line 115 22900] Train: [43/100][90/156] Data 0.001 (0.001) Batch 4.324 (3.400) Remain 08:27:37 loss: 0.2296 Lr: 0.03306
[2023-08-08 01:29:53,175 INFO misc.py line 115 22900] Train: [43/100][91/156] Data 0.001 (0.001) Batch 3.043 (3.396) Remain 08:26:58 loss: 0.1718 Lr: 0.03305
[2023-08-08 01:29:56,493 INFO misc.py line 115 22900] Train: [43/100][92/156] Data 0.001 (0.001) Batch 3.318 (3.395) Remain 08:26:47 loss: 0.2384 Lr: 0.03305
[2023-08-08 01:30:00,496 INFO misc.py line 115 22900] Train: [43/100][93/156] Data 0.001 (0.001) Batch 4.004 (3.402) Remain 08:27:44 loss: 0.3682 Lr: 0.03304
[2023-08-08 01:30:03,817 INFO misc.py line 115 22900] Train: [43/100][94/156] Data 0.001 (0.001) Batch 3.321 (3.401) Remain 08:27:32 loss: 0.3966 Lr: 0.03304
[2023-08-08 01:30:07,350 INFO misc.py line 115 22900] Train: [43/100][95/156] Data 0.001 (0.001) Batch 3.533 (3.402) Remain 08:27:42 loss: 0.3180 Lr: 0.03303
[2023-08-08 01:30:10,869 INFO misc.py line 115 22900] Train: [43/100][96/156] Data 0.001 (0.001) Batch 3.519 (3.404) Remain 08:27:49 loss: 0.1853 Lr: 0.03303
[2023-08-08 01:30:14,404 INFO misc.py line 115 22900] Train: [43/100][97/156] Data 0.001 (0.001) Batch 3.535 (3.405) Remain 08:27:59 loss: 0.2697 Lr: 0.03302
[2023-08-08 01:30:17,847 INFO misc.py line 115 22900] Train: [43/100][98/156] Data 0.001 (0.001) Batch 3.443 (3.406) Remain 08:27:59 loss: 0.2595 Lr: 0.03302
[2023-08-08 01:30:20,331 INFO misc.py line 115 22900] Train: [43/100][99/156] Data 0.001 (0.001) Batch 2.485 (3.396) Remain 08:26:30 loss: 0.1727 Lr: 0.03301
[2023-08-08 01:30:23,726 INFO misc.py line 115 22900] Train: [43/100][100/156] Data 0.001 (0.001) Batch 3.395 (3.396) Remain 08:26:26 loss: 0.3049 Lr: 0.03301
[2023-08-08 01:30:26,421 INFO misc.py line 115 22900] Train: [43/100][101/156] Data 0.001 (0.001) Batch 2.695 (3.389) Remain 08:25:19 loss: 0.2040 Lr: 0.03300
[2023-08-08 01:30:29,369 INFO misc.py line 115 22900] Train: [43/100][102/156] Data 0.001 (0.001) Batch 2.949 (3.384) Remain 08:24:35 loss: 0.1056 Lr: 0.03300
[2023-08-08 01:30:33,525 INFO misc.py line 115 22900] Train: [43/100][103/156] Data 0.001 (0.001) Batch 4.156 (3.392) Remain 08:25:41 loss: 0.3706 Lr: 0.03299
[2023-08-08 01:30:35,280 INFO misc.py line 115 22900] Train: [43/100][104/156] Data 0.001 (0.001) Batch 1.754 (3.376) Remain 08:23:13 loss: 0.1525 Lr: 0.03299
[2023-08-08 01:30:38,428 INFO misc.py line 115 22900] Train: [43/100][105/156] Data 0.001 (0.001) Batch 3.149 (3.374) Remain 08:22:49 loss: 0.2256 Lr: 0.03298
[2023-08-08 01:30:42,448 INFO misc.py line 115 22900] Train: [43/100][106/156] Data 0.001 (0.001) Batch 4.019 (3.380) Remain 08:23:42 loss: 0.3167 Lr: 0.03298
[2023-08-08 01:30:46,099 INFO misc.py line 115 22900] Train: [43/100][107/156] Data 0.001 (0.001) Batch 3.651 (3.382) Remain 08:24:02 loss: 0.4187 Lr: 0.03297
[2023-08-08 01:30:49,246 INFO misc.py line 115 22900] Train: [43/100][108/156] Data 0.001 (0.001) Batch 3.147 (3.380) Remain 08:23:39 loss: 0.3317 Lr: 0.03297
[2023-08-08 01:30:51,133 INFO misc.py line 115 22900] Train: [43/100][109/156] Data 0.001 (0.001) Batch 1.888 (3.366) Remain 08:21:29 loss: 0.1382 Lr: 0.03296
[2023-08-08 01:30:54,422 INFO misc.py line 115 22900] Train: [43/100][110/156] Data 0.001 (0.001) Batch 3.289 (3.365) Remain 08:21:19 loss: 0.1529 Lr: 0.03296
[2023-08-08 01:30:57,980 INFO misc.py line 115 22900] Train: [43/100][111/156] Data 0.001 (0.001) Batch 3.558 (3.367) Remain 08:21:32 loss: 0.3134 Lr: 0.03295
[2023-08-08 01:31:01,604 INFO misc.py line 115 22900] Train: [43/100][112/156] Data 0.001 (0.001) Batch 3.624 (3.370) Remain 08:21:50 loss: 0.3738 Lr: 0.03295
[2023-08-08 01:31:04,995 INFO misc.py line 115 22900] Train: [43/100][113/156] Data 0.001 (0.001) Batch 3.390 (3.370) Remain 08:21:48 loss: 0.2641 Lr: 0.03294
[2023-08-08 01:31:07,998 INFO misc.py line 115 22900] Train: [43/100][114/156] Data 0.001 (0.001) Batch 3.004 (3.366) Remain 08:21:15 loss: 0.3070 Lr: 0.03294
[2023-08-08 01:31:11,109 INFO misc.py line 115 22900] Train: [43/100][115/156] Data 0.001 (0.001) Batch 3.111 (3.364) Remain 08:20:51 loss: 0.2496 Lr: 0.03293
[2023-08-08 01:31:14,772 INFO misc.py line 115 22900] Train: [43/100][116/156] Data 0.001 (0.001) Batch 3.663 (3.367) Remain 08:21:12 loss: 0.4904 Lr: 0.03293
[2023-08-08 01:31:18,553 INFO misc.py line 115 22900] Train: [43/100][117/156] Data 0.001 (0.001) Batch 3.781 (3.370) Remain 08:21:41 loss: 0.2808 Lr: 0.03292
[2023-08-08 01:31:22,127 INFO misc.py line 115 22900] Train: [43/100][118/156] Data 0.001 (0.001) Batch 3.574 (3.372) Remain 08:21:53 loss: 0.3268 Lr: 0.03292
[2023-08-08 01:31:25,406 INFO misc.py line 115 22900] Train: [43/100][119/156] Data 0.001 (0.001) Batch 3.279 (3.371) Remain 08:21:43 loss: 0.3662 Lr: 0.03291
[2023-08-08 01:31:29,493 INFO misc.py line 115 22900] Train: [43/100][120/156] Data 0.001 (0.001) Batch 4.087 (3.378) Remain 08:22:34 loss: 0.3248 Lr: 0.03291
[2023-08-08 01:31:32,389 INFO misc.py line 115 22900] Train: [43/100][121/156] Data 0.001 (0.001) Batch 2.896 (3.373) Remain 08:21:54 loss: 0.1704 Lr: 0.03290
[2023-08-08 01:31:36,345 INFO misc.py line 115 22900] Train: [43/100][122/156] Data 0.001 (0.001) Batch 3.956 (3.378) Remain 08:22:34 loss: 0.3179 Lr: 0.03290
[2023-08-08 01:31:39,762 INFO misc.py line 115 22900] Train: [43/100][123/156] Data 0.001 (0.001) Batch 3.417 (3.379) Remain 08:22:34 loss: 0.3403 Lr: 0.03289
[2023-08-08 01:31:42,339 INFO misc.py line 115 22900] Train: [43/100][124/156] Data 0.001 (0.001) Batch 2.577 (3.372) Remain 08:21:31 loss: 0.1627 Lr: 0.03289
[2023-08-08 01:31:46,000 INFO misc.py line 115 22900] Train: [43/100][125/156] Data 0.001 (0.001) Batch 3.661 (3.374) Remain 08:21:49 loss: 0.3501 Lr: 0.03288
[2023-08-08 01:31:49,483 INFO misc.py line 115 22900] Train: [43/100][126/156] Data 0.001 (0.001) Batch 3.483 (3.375) Remain 08:21:54 loss: 0.5297 Lr: 0.03288
[2023-08-08 01:31:52,707 INFO misc.py line 115 22900] Train: [43/100][127/156] Data 0.001 (0.001) Batch 3.224 (3.374) Remain 08:21:39 loss: 0.2423 Lr: 0.03287
[2023-08-08 01:31:56,364 INFO misc.py line 115 22900] Train: [43/100][128/156] Data 0.001 (0.001) Batch 3.656 (3.376) Remain 08:21:56 loss: 0.4503 Lr: 0.03287
[2023-08-08 01:32:00,418 INFO misc.py line 115 22900] Train: [43/100][129/156] Data 0.001 (0.001) Batch 4.055 (3.382) Remain 08:22:41 loss: 0.3791 Lr: 0.03286
[2023-08-08 01:32:04,735 INFO misc.py line 115 22900] Train: [43/100][130/156] Data 0.001 (0.001) Batch 4.316 (3.389) Remain 08:23:43 loss: 0.5282 Lr: 0.03286
[2023-08-08 01:32:08,472 INFO misc.py line 115 22900] Train: [43/100][131/156] Data 0.001 (0.001) Batch 3.737 (3.392) Remain 08:24:04 loss: 0.3174 Lr: 0.03285
[2023-08-08 01:32:11,840 INFO misc.py line 115 22900] Train: [43/100][132/156] Data 0.001 (0.001) Batch 3.368 (3.392) Remain 08:23:59 loss: 0.3794 Lr: 0.03285
[2023-08-08 01:32:15,214 INFO misc.py line 115 22900] Train: [43/100][133/156] Data 0.001 (0.001) Batch 3.374 (3.391) Remain 08:23:54 loss: 0.5062 Lr: 0.03284
[2023-08-08 01:32:19,298 INFO misc.py line 115 22900] Train: [43/100][134/156] Data 0.001 (0.001) Batch 4.085 (3.397) Remain 08:24:38 loss: 0.3380 Lr: 0.03284
[2023-08-08 01:32:23,234 INFO misc.py line 115 22900] Train: [43/100][135/156] Data 0.001 (0.001) Batch 3.936 (3.401) Remain 08:25:11 loss: 0.3955 Lr: 0.03283
[2023-08-08 01:32:26,627 INFO misc.py line 115 22900] Train: [43/100][136/156] Data 0.001 (0.001) Batch 3.392 (3.401) Remain 08:25:07 loss: 0.2604 Lr: 0.03283
[2023-08-08 01:32:30,689 INFO misc.py line 115 22900] Train: [43/100][137/156] Data 0.001 (0.001) Batch 4.063 (3.406) Remain 08:25:48 loss: 0.4024 Lr: 0.03282
[2023-08-08 01:32:34,542 INFO misc.py line 115 22900] Train: [43/100][138/156] Data 0.001 (0.001) Batch 3.853 (3.409) Remain 08:26:14 loss: 0.1957 Lr: 0.03282
[2023-08-08 01:32:38,282 INFO misc.py line 115 22900] Train: [43/100][139/156] Data 0.001 (0.001) Batch 3.740 (3.411) Remain 08:26:32 loss: 0.4281 Lr: 0.03281
[2023-08-08 01:32:42,298 INFO misc.py line 115 22900] Train: [43/100][140/156] Data 0.001 (0.001) Batch 4.015 (3.416) Remain 08:27:08 loss: 0.3005 Lr: 0.03281
[2023-08-08 01:32:46,104 INFO misc.py line 115 22900] Train: [43/100][141/156] Data 0.001 (0.001) Batch 3.807 (3.419) Remain 08:27:30 loss: 0.3800 Lr: 0.03280
[2023-08-08 01:32:50,043 INFO misc.py line 115 22900] Train: [43/100][142/156] Data 0.001 (0.001) Batch 3.939 (3.422) Remain 08:28:00 loss: 0.2513 Lr: 0.03280
[2023-08-08 01:32:54,452 INFO misc.py line 115 22900] Train: [43/100][143/156] Data 0.001 (0.001) Batch 4.409 (3.429) Remain 08:28:59 loss: 0.4617 Lr: 0.03279
[2023-08-08 01:32:58,241 INFO misc.py line 115 22900] Train: [43/100][144/156] Data 0.001 (0.001) Batch 3.789 (3.432) Remain 08:29:18 loss: 0.4353 Lr: 0.03279
[2023-08-08 01:33:00,500 INFO misc.py line 115 22900] Train: [43/100][145/156] Data 0.001 (0.001) Batch 2.259 (3.424) Remain 08:28:01 loss: 0.2240 Lr: 0.03278
[2023-08-08 01:33:04,210 INFO misc.py line 115 22900] Train: [43/100][146/156] Data 0.001 (0.001) Batch 3.710 (3.426) Remain 08:28:16 loss: 0.3115 Lr: 0.03278
[2023-08-08 01:33:08,678 INFO misc.py line 115 22900] Train: [43/100][147/156] Data 0.001 (0.001) Batch 4.468 (3.433) Remain 08:29:17 loss: 0.5262 Lr: 0.03277
[2023-08-08 01:33:12,171 INFO misc.py line 115 22900] Train: [43/100][148/156] Data 0.001 (0.001) Batch 3.492 (3.433) Remain 08:29:17 loss: 0.1742 Lr: 0.03277
[2023-08-08 01:33:15,952 INFO misc.py line 115 22900] Train: [43/100][149/156] Data 0.001 (0.001) Batch 3.781 (3.436) Remain 08:29:35 loss: 0.3857 Lr: 0.03276
[2023-08-08 01:33:19,649 INFO misc.py line 115 22900] Train: [43/100][150/156] Data 0.001 (0.001) Batch 3.697 (3.438) Remain 08:29:47 loss: 0.3099 Lr: 0.03276
[2023-08-08 01:33:24,148 INFO misc.py line 115 22900] Train: [43/100][151/156] Data 0.001 (0.001) Batch 4.499 (3.445) Remain 08:30:48 loss: 0.3733 Lr: 0.03275
[2023-08-08 01:33:27,893 INFO misc.py line 115 22900] Train: [43/100][152/156] Data 0.001 (0.001) Batch 3.745 (3.447) Remain 08:31:02 loss: 0.1964 Lr: 0.03275
[2023-08-08 01:33:31,251 INFO misc.py line 115 22900] Train: [43/100][153/156] Data 0.001 (0.001) Batch 3.358 (3.446) Remain 08:30:53 loss: 0.3658 Lr: 0.03274
[2023-08-08 01:33:34,361 INFO misc.py line 115 22900] Train: [43/100][154/156] Data 0.001 (0.001) Batch 3.110 (3.444) Remain 08:30:30 loss: 0.1357 Lr: 0.03274
[2023-08-08 01:33:37,627 INFO misc.py line 115 22900] Train: [43/100][155/156] Data 0.001 (0.001) Batch 3.265 (3.443) Remain 08:30:16 loss: 0.2918 Lr: 0.03273
[2023-08-08 01:33:41,320 INFO misc.py line 115 22900] Train: [43/100][156/156] Data 0.001 (0.001) Batch 3.693 (3.444) Remain 08:30:27 loss: 0.3536 Lr: 0.03273
[2023-08-08 01:33:41,320 INFO misc.py line 129 22900] Train result: loss: 0.3200 
[2023-08-08 01:33:41,320 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 01:33:43,420 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.1429 
[2023-08-08 01:33:44,289 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.5211 
[2023-08-08 01:33:45,951 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.8639 
[2023-08-08 01:33:47,471 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.5955 
[2023-08-08 01:33:49,314 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.9935 
[2023-08-08 01:33:50,978 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.6629 
[2023-08-08 01:33:53,115 INFO evaluator.py line 122 22900] Test: [7/24] Loss 4.0548 
[2023-08-08 01:33:54,919 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.2608 
[2023-08-08 01:33:56,202 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.6546 
[2023-08-08 01:33:58,332 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.5153 
[2023-08-08 01:33:58,857 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.1745 
[2023-08-08 01:34:00,392 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.8969 
[2023-08-08 01:34:03,102 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.2363 
[2023-08-08 01:34:04,782 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.9198 
[2023-08-08 01:34:06,806 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.3363 
[2023-08-08 01:34:09,519 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.2989 
[2023-08-08 01:34:12,227 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.6674 
[2023-08-08 01:34:14,074 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.5689 
[2023-08-08 01:34:14,820 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.1978 
[2023-08-08 01:34:15,705 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.7164 
[2023-08-08 01:34:17,967 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.4044 
[2023-08-08 01:34:19,932 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.9229 
[2023-08-08 01:34:21,782 INFO evaluator.py line 122 22900] Test: [23/24] Loss 3.4450 
[2023-08-08 01:34:23,716 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.8462 
[2023-08-08 01:34:23,764 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2195/0.3049/0.6837.
[2023-08-08 01:34:23,764 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6032/0.9772
[2023-08-08 01:34:23,764 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9423/0.9919
[2023-08-08 01:34:23,764 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1663/0.3542
[2023-08-08 01:34:23,764 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1390/0.2265
[2023-08-08 01:34:23,764 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6189/0.7243
[2023-08-08 01:34:23,764 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.3444/0.4341
[2023-08-08 01:34:23,764 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5533/0.7319
[2023-08-08 01:34:23,764 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.0975/0.1057
[2023-08-08 01:34:23,764 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.0750/0.1560
[2023-08-08 01:34:23,764 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0347/0.0349
[2023-08-08 01:34:23,764 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0091/0.0176
[2023-08-08 01:34:23,764 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0344/0.0416
[2023-08-08 01:34:23,764 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1910/0.3139
[2023-08-08 01:34:23,764 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0189/0.0199
[2023-08-08 01:34:23,764 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0111/0.0123
[2023-08-08 01:34:23,764 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-08 01:34:23,764 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.1559/0.1939
[2023-08-08 01:34:23,764 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.3142/0.6293
[2023-08-08 01:34:23,764 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 01:34:23,764 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0816/0.1325
[2023-08-08 01:34:23,764 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 01:34:23,764 INFO misc.py line 152 22900] Currently Best mIoU: 0.2562
[2023-08-08 01:34:23,765 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 01:34:29,303 INFO misc.py line 115 22900] Train: [44/100][1/156] Data 0.896 (0.896) Batch 4.760 (4.760) Remain 11:45:17 loss: 0.1803 Lr: 0.03272
[2023-08-08 01:34:32,972 INFO misc.py line 115 22900] Train: [44/100][2/156] Data 0.001 (0.001) Batch 3.669 (3.669) Remain 09:03:41 loss: 0.4329 Lr: 0.03272
[2023-08-08 01:34:37,086 INFO misc.py line 115 22900] Train: [44/100][3/156] Data 0.001 (0.001) Batch 4.114 (4.114) Remain 10:09:32 loss: 0.3242 Lr: 0.03271
[2023-08-08 01:34:41,252 INFO misc.py line 115 22900] Train: [44/100][4/156] Data 0.001 (0.001) Batch 4.166 (4.166) Remain 10:17:06 loss: 0.4269 Lr: 0.03271
[2023-08-08 01:34:43,698 INFO misc.py line 115 22900] Train: [44/100][5/156] Data 0.001 (0.001) Batch 2.445 (3.305) Remain 08:09:35 loss: 0.2912 Lr: 0.03270
[2023-08-08 01:34:46,525 INFO misc.py line 115 22900] Train: [44/100][6/156] Data 0.001 (0.001) Batch 2.828 (3.146) Remain 07:45:56 loss: 0.1534 Lr: 0.03270
[2023-08-08 01:34:50,374 INFO misc.py line 115 22900] Train: [44/100][7/156] Data 0.001 (0.001) Batch 3.849 (3.322) Remain 08:11:54 loss: 0.3727 Lr: 0.03269
[2023-08-08 01:34:54,418 INFO misc.py line 115 22900] Train: [44/100][8/156] Data 0.001 (0.001) Batch 4.044 (3.466) Remain 08:33:14 loss: 0.2937 Lr: 0.03269
[2023-08-08 01:34:58,496 INFO misc.py line 115 22900] Train: [44/100][9/156] Data 0.001 (0.001) Batch 4.078 (3.568) Remain 08:48:16 loss: 0.2238 Lr: 0.03268
[2023-08-08 01:35:02,593 INFO misc.py line 115 22900] Train: [44/100][10/156] Data 0.001 (0.001) Batch 4.097 (3.644) Remain 08:59:24 loss: 0.3300 Lr: 0.03268
[2023-08-08 01:35:04,558 INFO misc.py line 115 22900] Train: [44/100][11/156] Data 0.001 (0.001) Batch 1.964 (3.434) Remain 08:28:16 loss: 0.2318 Lr: 0.03267
[2023-08-08 01:35:07,603 INFO misc.py line 115 22900] Train: [44/100][12/156] Data 0.001 (0.001) Batch 3.045 (3.391) Remain 08:21:49 loss: 0.2053 Lr: 0.03267
[2023-08-08 01:35:10,372 INFO misc.py line 115 22900] Train: [44/100][13/156] Data 0.001 (0.001) Batch 2.770 (3.329) Remain 08:12:34 loss: 0.2579 Lr: 0.03266
[2023-08-08 01:35:13,791 INFO misc.py line 115 22900] Train: [44/100][14/156] Data 0.001 (0.001) Batch 3.418 (3.337) Remain 08:13:43 loss: 0.4640 Lr: 0.03266
[2023-08-08 01:35:17,022 INFO misc.py line 115 22900] Train: [44/100][15/156] Data 0.001 (0.001) Batch 3.232 (3.328) Remain 08:12:22 loss: 0.2123 Lr: 0.03265
[2023-08-08 01:35:20,689 INFO misc.py line 115 22900] Train: [44/100][16/156] Data 0.001 (0.001) Batch 3.667 (3.354) Remain 08:16:10 loss: 0.2791 Lr: 0.03264
[2023-08-08 01:35:24,775 INFO misc.py line 115 22900] Train: [44/100][17/156] Data 0.001 (0.001) Batch 4.086 (3.406) Remain 08:23:51 loss: 0.1573 Lr: 0.03264
[2023-08-08 01:35:28,820 INFO misc.py line 115 22900] Train: [44/100][18/156] Data 0.001 (0.001) Batch 4.044 (3.449) Remain 08:30:05 loss: 0.3202 Lr: 0.03263
[2023-08-08 01:35:32,189 INFO misc.py line 115 22900] Train: [44/100][19/156] Data 0.001 (0.001) Batch 3.370 (3.444) Remain 08:29:17 loss: 0.5145 Lr: 0.03263
[2023-08-08 01:35:35,636 INFO misc.py line 115 22900] Train: [44/100][20/156] Data 0.001 (0.001) Batch 3.446 (3.444) Remain 08:29:15 loss: 0.2641 Lr: 0.03262
[2023-08-08 01:35:39,592 INFO misc.py line 115 22900] Train: [44/100][21/156] Data 0.001 (0.001) Batch 3.957 (3.473) Remain 08:33:24 loss: 0.3033 Lr: 0.03262
[2023-08-08 01:35:43,556 INFO misc.py line 115 22900] Train: [44/100][22/156] Data 0.001 (0.001) Batch 3.964 (3.498) Remain 08:37:10 loss: 0.3162 Lr: 0.03261
[2023-08-08 01:35:47,706 INFO misc.py line 115 22900] Train: [44/100][23/156] Data 0.001 (0.001) Batch 4.150 (3.531) Remain 08:41:56 loss: 0.4303 Lr: 0.03261
[2023-08-08 01:35:51,767 INFO misc.py line 115 22900] Train: [44/100][24/156] Data 0.001 (0.001) Batch 4.060 (3.556) Remain 08:45:36 loss: 0.1732 Lr: 0.03260
[2023-08-08 01:35:55,335 INFO misc.py line 115 22900] Train: [44/100][25/156] Data 0.001 (0.001) Batch 3.569 (3.557) Remain 08:45:37 loss: 0.2132 Lr: 0.03260
[2023-08-08 01:35:58,473 INFO misc.py line 115 22900] Train: [44/100][26/156] Data 0.001 (0.001) Batch 3.138 (3.539) Remain 08:42:52 loss: 0.4034 Lr: 0.03259
[2023-08-08 01:36:02,489 INFO misc.py line 115 22900] Train: [44/100][27/156] Data 0.001 (0.001) Batch 4.015 (3.558) Remain 08:45:45 loss: 0.3283 Lr: 0.03259
[2023-08-08 01:36:05,640 INFO misc.py line 115 22900] Train: [44/100][28/156] Data 0.001 (0.001) Batch 3.151 (3.542) Remain 08:43:17 loss: 0.1403 Lr: 0.03258
[2023-08-08 01:36:09,049 INFO misc.py line 115 22900] Train: [44/100][29/156] Data 0.001 (0.001) Batch 3.409 (3.537) Remain 08:42:28 loss: 0.4494 Lr: 0.03258
[2023-08-08 01:36:12,095 INFO misc.py line 115 22900] Train: [44/100][30/156] Data 0.001 (0.001) Batch 3.047 (3.519) Remain 08:39:43 loss: 0.2777 Lr: 0.03257
[2023-08-08 01:36:15,175 INFO misc.py line 115 22900] Train: [44/100][31/156] Data 0.001 (0.001) Batch 3.080 (3.503) Remain 08:37:21 loss: 0.2148 Lr: 0.03257
[2023-08-08 01:36:17,166 INFO misc.py line 115 22900] Train: [44/100][32/156] Data 0.001 (0.001) Batch 1.991 (3.451) Remain 08:29:35 loss: 0.3206 Lr: 0.03256
[2023-08-08 01:36:20,414 INFO misc.py line 115 22900] Train: [44/100][33/156] Data 0.001 (0.001) Batch 3.248 (3.444) Remain 08:28:32 loss: 0.1993 Lr: 0.03256
[2023-08-08 01:36:23,352 INFO misc.py line 115 22900] Train: [44/100][34/156] Data 0.001 (0.001) Batch 2.939 (3.428) Remain 08:26:04 loss: 0.1431 Lr: 0.03255
[2023-08-08 01:36:27,245 INFO misc.py line 115 22900] Train: [44/100][35/156] Data 0.001 (0.001) Batch 3.892 (3.442) Remain 08:28:09 loss: 0.4676 Lr: 0.03255
[2023-08-08 01:36:30,514 INFO misc.py line 115 22900] Train: [44/100][36/156] Data 0.001 (0.001) Batch 3.269 (3.437) Remain 08:27:19 loss: 0.2082 Lr: 0.03254
[2023-08-08 01:36:33,707 INFO misc.py line 115 22900] Train: [44/100][37/156] Data 0.001 (0.001) Batch 3.193 (3.430) Remain 08:26:12 loss: 0.3393 Lr: 0.03254
[2023-08-08 01:36:37,022 INFO misc.py line 115 22900] Train: [44/100][38/156] Data 0.001 (0.001) Batch 3.315 (3.427) Remain 08:25:40 loss: 0.1798 Lr: 0.03253
[2023-08-08 01:36:40,391 INFO misc.py line 115 22900] Train: [44/100][39/156] Data 0.001 (0.001) Batch 3.369 (3.425) Remain 08:25:22 loss: 0.3468 Lr: 0.03253
[2023-08-08 01:36:43,849 INFO misc.py line 115 22900] Train: [44/100][40/156] Data 0.001 (0.001) Batch 3.458 (3.426) Remain 08:25:27 loss: 0.3058 Lr: 0.03252
[2023-08-08 01:36:46,697 INFO misc.py line 115 22900] Train: [44/100][41/156] Data 0.001 (0.001) Batch 2.848 (3.411) Remain 08:23:09 loss: 0.1174 Lr: 0.03252
[2023-08-08 01:36:49,943 INFO misc.py line 115 22900] Train: [44/100][42/156] Data 0.001 (0.001) Batch 3.246 (3.407) Remain 08:22:28 loss: 0.3683 Lr: 0.03251
[2023-08-08 01:36:54,201 INFO misc.py line 115 22900] Train: [44/100][43/156] Data 0.001 (0.001) Batch 4.258 (3.428) Remain 08:25:33 loss: 0.5936 Lr: 0.03251
[2023-08-08 01:36:58,044 INFO misc.py line 115 22900] Train: [44/100][44/156] Data 0.001 (0.001) Batch 3.843 (3.438) Remain 08:26:59 loss: 0.3608 Lr: 0.03250
[2023-08-08 01:37:01,630 INFO misc.py line 115 22900] Train: [44/100][45/156] Data 0.001 (0.001) Batch 3.586 (3.442) Remain 08:27:27 loss: 0.1677 Lr: 0.03250
[2023-08-08 01:37:04,930 INFO misc.py line 115 22900] Train: [44/100][46/156] Data 0.001 (0.001) Batch 3.300 (3.438) Remain 08:26:54 loss: 0.2782 Lr: 0.03249
[2023-08-08 01:37:08,573 INFO misc.py line 115 22900] Train: [44/100][47/156] Data 0.001 (0.001) Batch 3.643 (3.443) Remain 08:27:32 loss: 0.1660 Lr: 0.03249
[2023-08-08 01:37:12,954 INFO misc.py line 115 22900] Train: [44/100][48/156] Data 0.001 (0.001) Batch 4.381 (3.464) Remain 08:30:33 loss: 0.4441 Lr: 0.03248
[2023-08-08 01:37:15,776 INFO misc.py line 115 22900] Train: [44/100][49/156] Data 0.001 (0.001) Batch 2.822 (3.450) Remain 08:28:26 loss: 0.2175 Lr: 0.03248
[2023-08-08 01:37:17,133 INFO misc.py line 115 22900] Train: [44/100][50/156] Data 0.001 (0.001) Batch 1.357 (3.405) Remain 08:21:49 loss: 0.2627 Lr: 0.03247
[2023-08-08 01:37:20,743 INFO misc.py line 115 22900] Train: [44/100][51/156] Data 0.001 (0.001) Batch 3.610 (3.409) Remain 08:22:23 loss: 0.2367 Lr: 0.03247
[2023-08-08 01:37:24,826 INFO misc.py line 115 22900] Train: [44/100][52/156] Data 0.001 (0.001) Batch 4.084 (3.423) Remain 08:24:21 loss: 0.5514 Lr: 0.03246
[2023-08-08 01:37:28,334 INFO misc.py line 115 22900] Train: [44/100][53/156] Data 0.001 (0.001) Batch 3.508 (3.425) Remain 08:24:33 loss: 0.1804 Lr: 0.03246
[2023-08-08 01:37:32,399 INFO misc.py line 115 22900] Train: [44/100][54/156] Data 0.001 (0.001) Batch 4.065 (3.437) Remain 08:26:20 loss: 0.3855 Lr: 0.03245
[2023-08-08 01:37:36,243 INFO misc.py line 115 22900] Train: [44/100][55/156] Data 0.001 (0.001) Batch 3.844 (3.445) Remain 08:27:26 loss: 0.4584 Lr: 0.03245
[2023-08-08 01:37:39,513 INFO misc.py line 115 22900] Train: [44/100][56/156] Data 0.001 (0.001) Batch 3.270 (3.442) Remain 08:26:53 loss: 0.1348 Lr: 0.03244
[2023-08-08 01:37:42,774 INFO misc.py line 115 22900] Train: [44/100][57/156] Data 0.001 (0.001) Batch 3.261 (3.439) Remain 08:26:20 loss: 0.3053 Lr: 0.03244
[2023-08-08 01:37:46,093 INFO misc.py line 115 22900] Train: [44/100][58/156] Data 0.001 (0.001) Batch 3.318 (3.436) Remain 08:25:57 loss: 0.1507 Lr: 0.03243
[2023-08-08 01:37:50,411 INFO misc.py line 115 22900] Train: [44/100][59/156] Data 0.001 (0.001) Batch 4.319 (3.452) Remain 08:28:13 loss: 0.5694 Lr: 0.03243
[2023-08-08 01:37:52,797 INFO misc.py line 115 22900] Train: [44/100][60/156] Data 0.001 (0.001) Batch 2.385 (3.434) Remain 08:25:24 loss: 0.1341 Lr: 0.03242
[2023-08-08 01:37:56,766 INFO misc.py line 115 22900] Train: [44/100][61/156] Data 0.001 (0.001) Batch 3.969 (3.443) Remain 08:26:42 loss: 0.3220 Lr: 0.03242
[2023-08-08 01:38:00,237 INFO misc.py line 115 22900] Train: [44/100][62/156] Data 0.001 (0.001) Batch 3.472 (3.443) Remain 08:26:43 loss: 0.2590 Lr: 0.03241
[2023-08-08 01:38:02,926 INFO misc.py line 115 22900] Train: [44/100][63/156] Data 0.001 (0.001) Batch 2.689 (3.431) Remain 08:24:49 loss: 0.1974 Lr: 0.03241
[2023-08-08 01:38:05,320 INFO misc.py line 115 22900] Train: [44/100][64/156] Data 0.001 (0.001) Batch 2.393 (3.414) Remain 08:22:15 loss: 0.2031 Lr: 0.03240
[2023-08-08 01:38:08,564 INFO misc.py line 115 22900] Train: [44/100][65/156] Data 0.001 (0.001) Batch 3.244 (3.411) Remain 08:21:48 loss: 0.2193 Lr: 0.03240
[2023-08-08 01:38:12,630 INFO misc.py line 115 22900] Train: [44/100][66/156] Data 0.001 (0.001) Batch 4.066 (3.421) Remain 08:23:16 loss: 0.3683 Lr: 0.03239
[2023-08-08 01:38:15,194 INFO misc.py line 115 22900] Train: [44/100][67/156] Data 0.001 (0.001) Batch 2.564 (3.408) Remain 08:21:14 loss: 0.2664 Lr: 0.03239
[2023-08-08 01:38:19,230 INFO misc.py line 115 22900] Train: [44/100][68/156] Data 0.001 (0.001) Batch 4.036 (3.418) Remain 08:22:36 loss: 0.6068 Lr: 0.03238
[2023-08-08 01:38:21,847 INFO misc.py line 115 22900] Train: [44/100][69/156] Data 0.001 (0.001) Batch 2.617 (3.405) Remain 08:20:46 loss: 0.1415 Lr: 0.03238
[2023-08-08 01:38:25,910 INFO misc.py line 115 22900] Train: [44/100][70/156] Data 0.001 (0.001) Batch 4.063 (3.415) Remain 08:22:09 loss: 0.4697 Lr: 0.03237
[2023-08-08 01:38:29,078 INFO misc.py line 115 22900] Train: [44/100][71/156] Data 0.001 (0.001) Batch 3.168 (3.412) Remain 08:21:34 loss: 0.2787 Lr: 0.03237
[2023-08-08 01:38:31,233 INFO misc.py line 115 22900] Train: [44/100][72/156] Data 0.001 (0.001) Batch 2.156 (3.393) Remain 08:18:50 loss: 0.1506 Lr: 0.03236
[2023-08-08 01:38:34,997 INFO misc.py line 115 22900] Train: [44/100][73/156] Data 0.001 (0.001) Batch 3.764 (3.399) Remain 08:19:33 loss: 0.2134 Lr: 0.03236
[2023-08-08 01:38:38,445 INFO misc.py line 115 22900] Train: [44/100][74/156] Data 0.001 (0.001) Batch 3.448 (3.399) Remain 08:19:35 loss: 0.2746 Lr: 0.03235
[2023-08-08 01:38:43,436 INFO misc.py line 115 22900] Train: [44/100][75/156] Data 0.001 (0.001) Batch 4.991 (3.422) Remain 08:22:47 loss: 0.4602 Lr: 0.03235
[2023-08-08 01:38:47,173 INFO misc.py line 115 22900] Train: [44/100][76/156] Data 0.001 (0.001) Batch 3.737 (3.426) Remain 08:23:22 loss: 0.2646 Lr: 0.03234
[2023-08-08 01:38:50,920 INFO misc.py line 115 22900] Train: [44/100][77/156] Data 0.001 (0.001) Batch 3.747 (3.430) Remain 08:23:57 loss: 0.2400 Lr: 0.03234
[2023-08-08 01:38:53,509 INFO misc.py line 115 22900] Train: [44/100][78/156] Data 0.001 (0.001) Batch 2.589 (3.419) Remain 08:22:14 loss: 0.3159 Lr: 0.03233
[2023-08-08 01:38:56,432 INFO misc.py line 115 22900] Train: [44/100][79/156] Data 0.001 (0.001) Batch 2.923 (3.412) Remain 08:21:13 loss: 0.2283 Lr: 0.03233
[2023-08-08 01:39:00,109 INFO misc.py line 115 22900] Train: [44/100][80/156] Data 0.001 (0.001) Batch 3.677 (3.416) Remain 08:21:40 loss: 0.1559 Lr: 0.03232
[2023-08-08 01:39:03,210 INFO misc.py line 115 22900] Train: [44/100][81/156] Data 0.001 (0.001) Batch 3.101 (3.412) Remain 08:21:01 loss: 0.3235 Lr: 0.03232
[2023-08-08 01:39:07,551 INFO misc.py line 115 22900] Train: [44/100][82/156] Data 0.001 (0.001) Batch 4.342 (3.424) Remain 08:22:41 loss: 0.4095 Lr: 0.03231
[2023-08-08 01:39:10,370 INFO misc.py line 115 22900] Train: [44/100][83/156] Data 0.001 (0.001) Batch 2.818 (3.416) Remain 08:21:31 loss: 0.2240 Lr: 0.03231
[2023-08-08 01:39:14,420 INFO misc.py line 115 22900] Train: [44/100][84/156] Data 0.001 (0.001) Batch 4.050 (3.424) Remain 08:22:37 loss: 0.4143 Lr: 0.03230
[2023-08-08 01:39:17,122 INFO misc.py line 115 22900] Train: [44/100][85/156] Data 0.001 (0.001) Batch 2.702 (3.415) Remain 08:21:16 loss: 0.1786 Lr: 0.03230
[2023-08-08 01:39:20,232 INFO misc.py line 115 22900] Train: [44/100][86/156] Data 0.001 (0.001) Batch 3.110 (3.411) Remain 08:20:40 loss: 0.2677 Lr: 0.03229
[2023-08-08 01:39:23,832 INFO misc.py line 115 22900] Train: [44/100][87/156] Data 0.001 (0.001) Batch 3.600 (3.414) Remain 08:20:57 loss: 0.3576 Lr: 0.03229
[2023-08-08 01:39:27,363 INFO misc.py line 115 22900] Train: [44/100][88/156] Data 0.001 (0.001) Batch 3.531 (3.415) Remain 08:21:05 loss: 0.1876 Lr: 0.03228
[2023-08-08 01:39:30,154 INFO misc.py line 115 22900] Train: [44/100][89/156] Data 0.001 (0.001) Batch 2.791 (3.408) Remain 08:19:58 loss: 0.2072 Lr: 0.03228
[2023-08-08 01:39:34,835 INFO misc.py line 115 22900] Train: [44/100][90/156] Data 0.001 (0.001) Batch 4.681 (3.422) Remain 08:22:03 loss: 0.6056 Lr: 0.03227
[2023-08-08 01:39:39,235 INFO misc.py line 115 22900] Train: [44/100][91/156] Data 0.001 (0.001) Batch 4.400 (3.434) Remain 08:23:38 loss: 0.3860 Lr: 0.03227
[2023-08-08 01:39:43,348 INFO misc.py line 115 22900] Train: [44/100][92/156] Data 0.001 (0.001) Batch 4.113 (3.441) Remain 08:24:42 loss: 0.3086 Lr: 0.03226
[2023-08-08 01:39:47,458 INFO misc.py line 115 22900] Train: [44/100][93/156] Data 0.001 (0.001) Batch 4.110 (3.449) Remain 08:25:44 loss: 0.3405 Lr: 0.03226
[2023-08-08 01:39:50,161 INFO misc.py line 115 22900] Train: [44/100][94/156] Data 0.001 (0.001) Batch 2.702 (3.440) Remain 08:24:28 loss: 0.2983 Lr: 0.03225
[2023-08-08 01:39:53,829 INFO misc.py line 115 22900] Train: [44/100][95/156] Data 0.001 (0.001) Batch 3.669 (3.443) Remain 08:24:46 loss: 0.3013 Lr: 0.03225
[2023-08-08 01:39:57,478 INFO misc.py line 115 22900] Train: [44/100][96/156] Data 0.001 (0.001) Batch 3.649 (3.445) Remain 08:25:02 loss: 0.2940 Lr: 0.03224
[2023-08-08 01:40:01,021 INFO misc.py line 115 22900] Train: [44/100][97/156] Data 0.001 (0.001) Batch 3.542 (3.446) Remain 08:25:08 loss: 0.3176 Lr: 0.03224
[2023-08-08 01:40:04,511 INFO misc.py line 115 22900] Train: [44/100][98/156] Data 0.001 (0.001) Batch 3.490 (3.447) Remain 08:25:09 loss: 0.2871 Lr: 0.03223
[2023-08-08 01:40:07,500 INFO misc.py line 115 22900] Train: [44/100][99/156] Data 0.001 (0.001) Batch 2.989 (3.442) Remain 08:24:23 loss: 0.2547 Lr: 0.03222
[2023-08-08 01:40:11,321 INFO misc.py line 115 22900] Train: [44/100][100/156] Data 0.001 (0.001) Batch 3.821 (3.446) Remain 08:24:54 loss: 0.3197 Lr: 0.03222
[2023-08-08 01:40:15,323 INFO misc.py line 115 22900] Train: [44/100][101/156] Data 0.001 (0.001) Batch 4.002 (3.451) Remain 08:25:41 loss: 0.4574 Lr: 0.03221
[2023-08-08 01:40:18,340 INFO misc.py line 115 22900] Train: [44/100][102/156] Data 0.001 (0.001) Batch 3.017 (3.447) Remain 08:24:59 loss: 0.3622 Lr: 0.03221
[2023-08-08 01:40:21,606 INFO misc.py line 115 22900] Train: [44/100][103/156] Data 0.001 (0.001) Batch 3.266 (3.445) Remain 08:24:39 loss: 0.1596 Lr: 0.03220
[2023-08-08 01:40:25,115 INFO misc.py line 115 22900] Train: [44/100][104/156] Data 0.001 (0.001) Batch 3.509 (3.446) Remain 08:24:41 loss: 0.2742 Lr: 0.03220
[2023-08-08 01:40:29,076 INFO misc.py line 115 22900] Train: [44/100][105/156] Data 0.001 (0.001) Batch 3.961 (3.451) Remain 08:25:22 loss: 0.3435 Lr: 0.03219
[2023-08-08 01:40:32,550 INFO misc.py line 115 22900] Train: [44/100][106/156] Data 0.001 (0.001) Batch 3.475 (3.451) Remain 08:25:21 loss: 0.2019 Lr: 0.03219
[2023-08-08 01:40:35,189 INFO misc.py line 115 22900] Train: [44/100][107/156] Data 0.001 (0.001) Batch 2.638 (3.443) Remain 08:24:09 loss: 0.2331 Lr: 0.03218
[2023-08-08 01:40:39,080 INFO misc.py line 115 22900] Train: [44/100][108/156] Data 0.001 (0.001) Batch 3.892 (3.448) Remain 08:24:43 loss: 0.3130 Lr: 0.03218
[2023-08-08 01:40:42,466 INFO misc.py line 115 22900] Train: [44/100][109/156] Data 0.001 (0.001) Batch 3.385 (3.447) Remain 08:24:34 loss: 0.1770 Lr: 0.03217
[2023-08-08 01:40:46,466 INFO misc.py line 115 22900] Train: [44/100][110/156] Data 0.001 (0.001) Batch 4.000 (3.452) Remain 08:25:16 loss: 0.2910 Lr: 0.03217
[2023-08-08 01:40:50,661 INFO misc.py line 115 22900] Train: [44/100][111/156] Data 0.001 (0.001) Batch 4.195 (3.459) Remain 08:26:13 loss: 0.2802 Lr: 0.03216
[2023-08-08 01:40:53,916 INFO misc.py line 115 22900] Train: [44/100][112/156] Data 0.001 (0.001) Batch 3.255 (3.457) Remain 08:25:53 loss: 0.2960 Lr: 0.03216
[2023-08-08 01:40:57,988 INFO misc.py line 115 22900] Train: [44/100][113/156] Data 0.001 (0.001) Batch 4.072 (3.463) Remain 08:26:39 loss: 0.2901 Lr: 0.03215
[2023-08-08 01:41:01,168 INFO misc.py line 115 22900] Train: [44/100][114/156] Data 0.001 (0.001) Batch 3.180 (3.460) Remain 08:26:13 loss: 0.0838 Lr: 0.03215
[2023-08-08 01:41:04,630 INFO misc.py line 115 22900] Train: [44/100][115/156] Data 0.001 (0.001) Batch 3.462 (3.460) Remain 08:26:10 loss: 0.3086 Lr: 0.03214
[2023-08-08 01:41:07,997 INFO misc.py line 115 22900] Train: [44/100][116/156] Data 0.001 (0.001) Batch 3.367 (3.459) Remain 08:25:59 loss: 0.3061 Lr: 0.03214
[2023-08-08 01:41:12,060 INFO misc.py line 115 22900] Train: [44/100][117/156] Data 0.001 (0.001) Batch 4.063 (3.465) Remain 08:26:42 loss: 0.2450 Lr: 0.03213
[2023-08-08 01:41:15,767 INFO misc.py line 115 22900] Train: [44/100][118/156] Data 0.001 (0.001) Batch 3.706 (3.467) Remain 08:26:57 loss: 0.2683 Lr: 0.03213
[2023-08-08 01:41:19,702 INFO misc.py line 115 22900] Train: [44/100][119/156] Data 0.001 (0.001) Batch 3.936 (3.471) Remain 08:27:29 loss: 0.2719 Lr: 0.03212
[2023-08-08 01:41:23,059 INFO misc.py line 115 22900] Train: [44/100][120/156] Data 0.001 (0.001) Batch 3.356 (3.470) Remain 08:27:17 loss: 0.1857 Lr: 0.03212
[2023-08-08 01:41:26,715 INFO misc.py line 115 22900] Train: [44/100][121/156] Data 0.001 (0.001) Batch 3.657 (3.471) Remain 08:27:27 loss: 0.2782 Lr: 0.03211
[2023-08-08 01:41:30,737 INFO misc.py line 115 22900] Train: [44/100][122/156] Data 0.001 (0.001) Batch 4.022 (3.476) Remain 08:28:05 loss: 0.2878 Lr: 0.03211
[2023-08-08 01:41:32,750 INFO misc.py line 115 22900] Train: [44/100][123/156] Data 0.001 (0.001) Batch 2.013 (3.464) Remain 08:26:14 loss: 0.2976 Lr: 0.03210
[2023-08-08 01:41:36,770 INFO misc.py line 115 22900] Train: [44/100][124/156] Data 0.001 (0.001) Batch 4.020 (3.468) Remain 08:26:51 loss: 0.4607 Lr: 0.03210
[2023-08-08 01:41:39,160 INFO misc.py line 115 22900] Train: [44/100][125/156] Data 0.001 (0.001) Batch 2.390 (3.460) Remain 08:25:30 loss: 0.0452 Lr: 0.03209
[2023-08-08 01:41:42,475 INFO misc.py line 115 22900] Train: [44/100][126/156] Data 0.001 (0.001) Batch 3.315 (3.458) Remain 08:25:16 loss: 0.3004 Lr: 0.03209
[2023-08-08 01:41:45,686 INFO misc.py line 115 22900] Train: [44/100][127/156] Data 0.001 (0.001) Batch 3.212 (3.456) Remain 08:24:55 loss: 0.1505 Lr: 0.03208
[2023-08-08 01:41:49,304 INFO misc.py line 115 22900] Train: [44/100][128/156] Data 0.001 (0.001) Batch 3.618 (3.458) Remain 08:25:03 loss: 0.2778 Lr: 0.03208
[2023-08-08 01:41:52,825 INFO misc.py line 115 22900] Train: [44/100][129/156] Data 0.001 (0.001) Batch 3.521 (3.458) Remain 08:25:04 loss: 0.1857 Lr: 0.03207
[2023-08-08 01:41:56,077 INFO misc.py line 115 22900] Train: [44/100][130/156] Data 0.001 (0.001) Batch 3.252 (3.457) Remain 08:24:46 loss: 0.2698 Lr: 0.03207
[2023-08-08 01:41:58,970 INFO misc.py line 115 22900] Train: [44/100][131/156] Data 0.001 (0.001) Batch 2.892 (3.452) Remain 08:24:04 loss: 0.5038 Lr: 0.03206
[2023-08-08 01:42:02,726 INFO misc.py line 115 22900] Train: [44/100][132/156] Data 0.001 (0.001) Batch 3.757 (3.455) Remain 08:24:22 loss: 0.3250 Lr: 0.03206
[2023-08-08 01:42:06,071 INFO misc.py line 115 22900] Train: [44/100][133/156] Data 0.001 (0.001) Batch 3.344 (3.454) Remain 08:24:11 loss: 0.1972 Lr: 0.03205
[2023-08-08 01:42:09,295 INFO misc.py line 115 22900] Train: [44/100][134/156] Data 0.001 (0.001) Batch 3.224 (3.452) Remain 08:23:52 loss: 0.3295 Lr: 0.03205
[2023-08-08 01:42:12,739 INFO misc.py line 115 22900] Train: [44/100][135/156] Data 0.001 (0.001) Batch 3.444 (3.452) Remain 08:23:48 loss: 0.4818 Lr: 0.03204
[2023-08-08 01:42:15,614 INFO misc.py line 115 22900] Train: [44/100][136/156] Data 0.001 (0.001) Batch 2.875 (3.448) Remain 08:23:06 loss: 0.1509 Lr: 0.03204
[2023-08-08 01:42:19,373 INFO misc.py line 115 22900] Train: [44/100][137/156] Data 0.001 (0.001) Batch 3.759 (3.450) Remain 08:23:23 loss: 0.2390 Lr: 0.03203
[2023-08-08 01:42:23,238 INFO misc.py line 115 22900] Train: [44/100][138/156] Data 0.001 (0.001) Batch 3.865 (3.453) Remain 08:23:47 loss: 0.3155 Lr: 0.03203
[2023-08-08 01:42:26,415 INFO misc.py line 115 22900] Train: [44/100][139/156] Data 0.001 (0.001) Batch 3.177 (3.451) Remain 08:23:26 loss: 0.3352 Lr: 0.03202
[2023-08-08 01:42:29,692 INFO misc.py line 115 22900] Train: [44/100][140/156] Data 0.001 (0.001) Batch 3.276 (3.450) Remain 08:23:11 loss: 0.1215 Lr: 0.03202
[2023-08-08 01:42:33,702 INFO misc.py line 115 22900] Train: [44/100][141/156] Data 0.001 (0.001) Batch 4.010 (3.454) Remain 08:23:43 loss: 0.7207 Lr: 0.03201
[2023-08-08 01:42:36,549 INFO misc.py line 115 22900] Train: [44/100][142/156] Data 0.001 (0.001) Batch 2.847 (3.449) Remain 08:23:01 loss: 0.1118 Lr: 0.03201
[2023-08-08 01:42:40,053 INFO misc.py line 115 22900] Train: [44/100][143/156] Data 0.001 (0.001) Batch 3.504 (3.450) Remain 08:23:01 loss: 0.3424 Lr: 0.03200
[2023-08-08 01:42:42,979 INFO misc.py line 115 22900] Train: [44/100][144/156] Data 0.001 (0.001) Batch 2.926 (3.446) Remain 08:22:26 loss: 0.3111 Lr: 0.03200
[2023-08-08 01:42:47,167 INFO misc.py line 115 22900] Train: [44/100][145/156] Data 0.001 (0.001) Batch 4.188 (3.451) Remain 08:23:08 loss: 0.4272 Lr: 0.03199
[2023-08-08 01:42:50,842 INFO misc.py line 115 22900] Train: [44/100][146/156] Data 0.001 (0.001) Batch 3.675 (3.453) Remain 08:23:18 loss: 0.2434 Lr: 0.03199
[2023-08-08 01:42:54,416 INFO misc.py line 115 22900] Train: [44/100][147/156] Data 0.001 (0.001) Batch 3.574 (3.454) Remain 08:23:22 loss: 0.4152 Lr: 0.03198
[2023-08-08 01:42:58,966 INFO misc.py line 115 22900] Train: [44/100][148/156] Data 0.001 (0.001) Batch 4.550 (3.461) Remain 08:24:25 loss: 0.6232 Lr: 0.03198
[2023-08-08 01:43:02,373 INFO misc.py line 115 22900] Train: [44/100][149/156] Data 0.001 (0.001) Batch 3.407 (3.461) Remain 08:24:18 loss: 0.3534 Lr: 0.03197
[2023-08-08 01:43:06,178 INFO misc.py line 115 22900] Train: [44/100][150/156] Data 0.001 (0.001) Batch 3.805 (3.463) Remain 08:24:35 loss: 0.3333 Lr: 0.03197
[2023-08-08 01:43:09,256 INFO misc.py line 115 22900] Train: [44/100][151/156] Data 0.001 (0.001) Batch 3.078 (3.461) Remain 08:24:09 loss: 0.1257 Lr: 0.03196
[2023-08-08 01:43:13,363 INFO misc.py line 115 22900] Train: [44/100][152/156] Data 0.001 (0.001) Batch 4.107 (3.465) Remain 08:24:43 loss: 0.4411 Lr: 0.03196
[2023-08-08 01:43:17,489 INFO misc.py line 115 22900] Train: [44/100][153/156] Data 0.001 (0.001) Batch 4.126 (3.469) Remain 08:25:18 loss: 0.5597 Lr: 0.03195
[2023-08-08 01:43:21,206 INFO misc.py line 115 22900] Train: [44/100][154/156] Data 0.001 (0.001) Batch 3.717 (3.471) Remain 08:25:29 loss: 0.2361 Lr: 0.03195
[2023-08-08 01:43:24,045 INFO misc.py line 115 22900] Train: [44/100][155/156] Data 0.001 (0.001) Batch 2.839 (3.467) Remain 08:24:49 loss: 0.1207 Lr: 0.03194
[2023-08-08 01:43:26,822 INFO misc.py line 115 22900] Train: [44/100][156/156] Data 0.001 (0.001) Batch 2.776 (3.462) Remain 08:24:06 loss: 0.0832 Lr: 0.03194
[2023-08-08 01:43:26,822 INFO misc.py line 129 22900] Train result: loss: 0.2952 
[2023-08-08 01:43:26,837 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 01:43:28,965 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.2064 
[2023-08-08 01:43:29,833 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.6608 
[2023-08-08 01:43:31,497 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.7888 
[2023-08-08 01:43:33,017 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.3401 
[2023-08-08 01:43:34,860 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.8141 
[2023-08-08 01:43:36,523 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.8342 
[2023-08-08 01:43:38,661 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.8171 
[2023-08-08 01:43:40,463 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.2697 
[2023-08-08 01:43:41,748 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.4380 
[2023-08-08 01:43:43,879 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3496 
[2023-08-08 01:43:44,404 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.6307 
[2023-08-08 01:43:45,937 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.7628 
[2023-08-08 01:43:48,647 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.2054 
[2023-08-08 01:43:50,328 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.8056 
[2023-08-08 01:43:52,349 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.3593 
[2023-08-08 01:43:55,061 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.2579 
[2023-08-08 01:43:57,765 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.5266 
[2023-08-08 01:43:59,611 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.5947 
[2023-08-08 01:44:00,361 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.2513 
[2023-08-08 01:44:01,245 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.7871 
[2023-08-08 01:44:03,503 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.2953 
[2023-08-08 01:44:05,468 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.8504 
[2023-08-08 01:44:07,316 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.7702 
[2023-08-08 01:44:09,252 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.9213 
[2023-08-08 01:44:09,300 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.1970/0.2705/0.6762.
[2023-08-08 01:44:09,300 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6172/0.9561
[2023-08-08 01:44:09,300 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9247/0.9915
[2023-08-08 01:44:09,300 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1818/0.4201
[2023-08-08 01:44:09,301 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1046/0.3159
[2023-08-08 01:44:09,301 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6303/0.7295
[2023-08-08 01:44:09,301 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.3269/0.4583
[2023-08-08 01:44:09,301 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.4448/0.5976
[2023-08-08 01:44:09,301 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1296/0.1397
[2023-08-08 01:44:09,301 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.0885/0.1560
[2023-08-08 01:44:09,301 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0575/0.0579
[2023-08-08 01:44:09,301 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 01:44:09,301 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0039/0.0039
[2023-08-08 01:44:09,301 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1079/0.1247
[2023-08-08 01:44:09,301 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0532/0.0623
[2023-08-08 01:44:09,301 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0048/0.0051
[2023-08-08 01:44:09,301 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0007/0.0007
[2023-08-08 01:44:09,301 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0718/0.0907
[2023-08-08 01:44:09,301 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.0883/0.0989
[2023-08-08 01:44:09,301 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 01:44:09,301 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.1031/0.2014
[2023-08-08 01:44:09,301 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 01:44:09,301 INFO misc.py line 152 22900] Currently Best mIoU: 0.2562
[2023-08-08 01:44:09,301 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 01:44:14,592 INFO misc.py line 115 22900] Train: [45/100][1/156] Data 1.139 (1.139) Batch 4.461 (4.461) Remain 10:49:27 loss: 0.2704 Lr: 0.03193
[2023-08-08 01:44:18,636 INFO misc.py line 115 22900] Train: [45/100][2/156] Data 0.001 (0.001) Batch 4.045 (4.045) Remain 09:48:45 loss: 0.2996 Lr: 0.03193
[2023-08-08 01:44:21,167 INFO misc.py line 115 22900] Train: [45/100][3/156] Data 0.001 (0.001) Batch 2.531 (2.531) Remain 06:08:24 loss: 0.2545 Lr: 0.03192
[2023-08-08 01:44:25,238 INFO misc.py line 115 22900] Train: [45/100][4/156] Data 0.001 (0.001) Batch 4.071 (4.071) Remain 09:52:27 loss: 0.3236 Lr: 0.03191
[2023-08-08 01:44:28,331 INFO misc.py line 115 22900] Train: [45/100][5/156] Data 0.001 (0.001) Batch 3.093 (3.582) Remain 08:41:14 loss: 0.5371 Lr: 0.03191
[2023-08-08 01:44:31,783 INFO misc.py line 115 22900] Train: [45/100][6/156] Data 0.001 (0.001) Batch 3.452 (3.539) Remain 08:34:51 loss: 0.2088 Lr: 0.03190
[2023-08-08 01:44:35,678 INFO misc.py line 115 22900] Train: [45/100][7/156] Data 0.001 (0.001) Batch 3.895 (3.628) Remain 08:47:44 loss: 0.3409 Lr: 0.03190
[2023-08-08 01:44:38,841 INFO misc.py line 115 22900] Train: [45/100][8/156] Data 0.001 (0.001) Batch 3.163 (3.535) Remain 08:34:11 loss: 0.3784 Lr: 0.03189
[2023-08-08 01:44:41,963 INFO misc.py line 115 22900] Train: [45/100][9/156] Data 0.001 (0.001) Batch 3.122 (3.466) Remain 08:24:07 loss: 0.3062 Lr: 0.03189
[2023-08-08 01:44:46,045 INFO misc.py line 115 22900] Train: [45/100][10/156] Data 0.001 (0.001) Batch 4.082 (3.554) Remain 08:36:52 loss: 0.3923 Lr: 0.03188
[2023-08-08 01:44:49,920 INFO misc.py line 115 22900] Train: [45/100][11/156] Data 0.001 (0.001) Batch 3.874 (3.594) Remain 08:42:37 loss: 0.2401 Lr: 0.03188
[2023-08-08 01:44:53,309 INFO misc.py line 115 22900] Train: [45/100][12/156] Data 0.001 (0.001) Batch 3.389 (3.571) Remain 08:39:15 loss: 0.4737 Lr: 0.03187
[2023-08-08 01:44:55,911 INFO misc.py line 115 22900] Train: [45/100][13/156] Data 0.001 (0.001) Batch 2.602 (3.474) Remain 08:25:06 loss: 0.2858 Lr: 0.03187
[2023-08-08 01:45:00,021 INFO misc.py line 115 22900] Train: [45/100][14/156] Data 0.001 (0.001) Batch 4.110 (3.532) Remain 08:33:26 loss: 0.2721 Lr: 0.03186
[2023-08-08 01:45:03,234 INFO misc.py line 115 22900] Train: [45/100][15/156] Data 0.001 (0.001) Batch 3.214 (3.506) Remain 08:29:32 loss: 0.2344 Lr: 0.03186
[2023-08-08 01:45:07,293 INFO misc.py line 115 22900] Train: [45/100][16/156] Data 0.001 (0.001) Batch 4.059 (3.548) Remain 08:35:39 loss: 0.2032 Lr: 0.03185
[2023-08-08 01:45:11,275 INFO misc.py line 115 22900] Train: [45/100][17/156] Data 0.001 (0.001) Batch 3.982 (3.579) Remain 08:40:06 loss: 0.2060 Lr: 0.03185
[2023-08-08 01:45:14,506 INFO misc.py line 115 22900] Train: [45/100][18/156] Data 0.001 (0.001) Batch 3.231 (3.556) Remain 08:36:40 loss: 0.3095 Lr: 0.03184
[2023-08-08 01:45:18,016 INFO misc.py line 115 22900] Train: [45/100][19/156] Data 0.001 (0.001) Batch 3.510 (3.553) Remain 08:36:11 loss: 0.1280 Lr: 0.03184
[2023-08-08 01:45:20,505 INFO misc.py line 115 22900] Train: [45/100][20/156] Data 0.001 (0.001) Batch 2.489 (3.490) Remain 08:27:02 loss: 0.2060 Lr: 0.03183
[2023-08-08 01:45:24,680 INFO misc.py line 115 22900] Train: [45/100][21/156] Data 0.001 (0.001) Batch 4.176 (3.529) Remain 08:32:30 loss: 0.5361 Lr: 0.03183
[2023-08-08 01:45:28,193 INFO misc.py line 115 22900] Train: [45/100][22/156] Data 0.001 (0.001) Batch 3.513 (3.528) Remain 08:32:20 loss: 0.2790 Lr: 0.03182
[2023-08-08 01:45:30,718 INFO misc.py line 115 22900] Train: [45/100][23/156] Data 0.001 (0.001) Batch 2.524 (3.478) Remain 08:24:59 loss: 0.2885 Lr: 0.03182
[2023-08-08 01:45:33,838 INFO misc.py line 115 22900] Train: [45/100][24/156] Data 0.001 (0.001) Batch 3.120 (3.460) Remain 08:22:27 loss: 0.1397 Lr: 0.03181
[2023-08-08 01:45:36,715 INFO misc.py line 115 22900] Train: [45/100][25/156] Data 0.001 (0.001) Batch 2.878 (3.434) Remain 08:18:33 loss: 0.1165 Lr: 0.03181
[2023-08-08 01:45:40,119 INFO misc.py line 115 22900] Train: [45/100][26/156] Data 0.001 (0.001) Batch 3.403 (3.433) Remain 08:18:18 loss: 0.2384 Lr: 0.03180
[2023-08-08 01:45:44,089 INFO misc.py line 115 22900] Train: [45/100][27/156] Data 0.001 (0.001) Batch 3.970 (3.455) Remain 08:21:30 loss: 0.3844 Lr: 0.03180
[2023-08-08 01:45:46,884 INFO misc.py line 115 22900] Train: [45/100][28/156] Data 0.001 (0.001) Batch 2.795 (3.429) Remain 08:17:36 loss: 0.2003 Lr: 0.03179
[2023-08-08 01:45:49,553 INFO misc.py line 115 22900] Train: [45/100][29/156] Data 0.001 (0.001) Batch 2.668 (3.399) Remain 08:13:18 loss: 0.2356 Lr: 0.03179
[2023-08-08 01:45:53,410 INFO misc.py line 115 22900] Train: [45/100][30/156] Data 0.001 (0.001) Batch 3.857 (3.416) Remain 08:15:43 loss: 0.2897 Lr: 0.03178
[2023-08-08 01:45:57,378 INFO misc.py line 115 22900] Train: [45/100][31/156] Data 0.001 (0.001) Batch 3.968 (3.436) Remain 08:18:31 loss: 0.2726 Lr: 0.03178
[2023-08-08 01:45:59,936 INFO misc.py line 115 22900] Train: [45/100][32/156] Data 0.001 (0.001) Batch 2.558 (3.406) Remain 08:14:04 loss: 0.1872 Lr: 0.03177
[2023-08-08 01:46:04,017 INFO misc.py line 115 22900] Train: [45/100][33/156] Data 0.001 (0.001) Batch 4.081 (3.428) Remain 08:17:16 loss: 0.3730 Lr: 0.03177
[2023-08-08 01:46:07,902 INFO misc.py line 115 22900] Train: [45/100][34/156] Data 0.001 (0.001) Batch 3.885 (3.443) Remain 08:19:21 loss: 0.4263 Lr: 0.03176
[2023-08-08 01:46:11,603 INFO misc.py line 115 22900] Train: [45/100][35/156] Data 0.001 (0.001) Batch 3.702 (3.451) Remain 08:20:28 loss: 0.2092 Lr: 0.03176
[2023-08-08 01:46:13,761 INFO misc.py line 115 22900] Train: [45/100][36/156] Data 0.001 (0.001) Batch 2.158 (3.412) Remain 08:14:43 loss: 0.2034 Lr: 0.03175
[2023-08-08 01:46:17,030 INFO misc.py line 115 22900] Train: [45/100][37/156] Data 0.001 (0.001) Batch 3.269 (3.408) Remain 08:14:03 loss: 0.3869 Lr: 0.03175
[2023-08-08 01:46:20,904 INFO misc.py line 115 22900] Train: [45/100][38/156] Data 0.001 (0.001) Batch 3.874 (3.421) Remain 08:15:56 loss: 0.3325 Lr: 0.03174
[2023-08-08 01:46:23,317 INFO misc.py line 115 22900] Train: [45/100][39/156] Data 0.001 (0.001) Batch 2.412 (3.393) Remain 08:11:49 loss: 0.0879 Lr: 0.03174
[2023-08-08 01:46:25,761 INFO misc.py line 115 22900] Train: [45/100][40/156] Data 0.001 (0.001) Batch 2.445 (3.367) Remain 08:08:02 loss: 0.1418 Lr: 0.03173
[2023-08-08 01:46:29,778 INFO misc.py line 115 22900] Train: [45/100][41/156] Data 0.001 (0.001) Batch 4.017 (3.384) Remain 08:10:28 loss: 0.3786 Lr: 0.03173
[2023-08-08 01:46:33,551 INFO misc.py line 115 22900] Train: [45/100][42/156] Data 0.001 (0.001) Batch 3.773 (3.394) Remain 08:11:51 loss: 0.3207 Lr: 0.03172
[2023-08-08 01:46:37,397 INFO misc.py line 115 22900] Train: [45/100][43/156] Data 0.001 (0.001) Batch 3.846 (3.406) Remain 08:13:26 loss: 0.2768 Lr: 0.03172
[2023-08-08 01:46:40,764 INFO misc.py line 115 22900] Train: [45/100][44/156] Data 0.001 (0.001) Batch 3.367 (3.405) Remain 08:13:14 loss: 0.5773 Lr: 0.03171
[2023-08-08 01:46:44,054 INFO misc.py line 115 22900] Train: [45/100][45/156] Data 0.001 (0.001) Batch 3.290 (3.402) Remain 08:12:47 loss: 0.3564 Lr: 0.03171
[2023-08-08 01:46:47,314 INFO misc.py line 115 22900] Train: [45/100][46/156] Data 0.001 (0.001) Batch 3.260 (3.399) Remain 08:12:15 loss: 0.1393 Lr: 0.03170
[2023-08-08 01:46:51,010 INFO misc.py line 115 22900] Train: [45/100][47/156] Data 0.001 (0.001) Batch 3.696 (3.406) Remain 08:13:10 loss: 0.4135 Lr: 0.03170
[2023-08-08 01:46:54,695 INFO misc.py line 115 22900] Train: [45/100][48/156] Data 0.001 (0.001) Batch 3.685 (3.412) Remain 08:14:01 loss: 0.3399 Lr: 0.03169
[2023-08-08 01:46:57,915 INFO misc.py line 115 22900] Train: [45/100][49/156] Data 0.001 (0.001) Batch 3.220 (3.408) Remain 08:13:21 loss: 0.1271 Lr: 0.03169
[2023-08-08 01:47:02,103 INFO misc.py line 115 22900] Train: [45/100][50/156] Data 0.001 (0.001) Batch 4.188 (3.424) Remain 08:15:42 loss: 0.4566 Lr: 0.03168
[2023-08-08 01:47:06,175 INFO misc.py line 115 22900] Train: [45/100][51/156] Data 0.001 (0.001) Batch 4.072 (3.438) Remain 08:17:35 loss: 0.2075 Lr: 0.03168
[2023-08-08 01:47:09,027 INFO misc.py line 115 22900] Train: [45/100][52/156] Data 0.001 (0.001) Batch 2.852 (3.426) Remain 08:15:48 loss: 0.4380 Lr: 0.03167
[2023-08-08 01:47:13,056 INFO misc.py line 115 22900] Train: [45/100][53/156] Data 0.001 (0.001) Batch 4.029 (3.438) Remain 08:17:30 loss: 0.4438 Lr: 0.03166
[2023-08-08 01:47:17,208 INFO misc.py line 115 22900] Train: [45/100][54/156] Data 0.001 (0.001) Batch 4.151 (3.452) Remain 08:19:28 loss: 0.3044 Lr: 0.03166
[2023-08-08 01:47:21,106 INFO misc.py line 115 22900] Train: [45/100][55/156] Data 0.001 (0.001) Batch 3.898 (3.460) Remain 08:20:39 loss: 0.3094 Lr: 0.03165
[2023-08-08 01:47:24,796 INFO misc.py line 115 22900] Train: [45/100][56/156] Data 0.001 (0.001) Batch 3.690 (3.465) Remain 08:21:13 loss: 0.2273 Lr: 0.03165
[2023-08-08 01:47:28,282 INFO misc.py line 115 22900] Train: [45/100][57/156] Data 0.001 (0.001) Batch 3.486 (3.465) Remain 08:21:13 loss: 0.1779 Lr: 0.03164
[2023-08-08 01:47:32,837 INFO misc.py line 115 22900] Train: [45/100][58/156] Data 0.001 (0.001) Batch 4.556 (3.485) Remain 08:24:01 loss: 0.3363 Lr: 0.03164
[2023-08-08 01:47:36,320 INFO misc.py line 115 22900] Train: [45/100][59/156] Data 0.001 (0.001) Batch 3.483 (3.485) Remain 08:23:58 loss: 0.3759 Lr: 0.03163
[2023-08-08 01:47:39,424 INFO misc.py line 115 22900] Train: [45/100][60/156] Data 0.001 (0.001) Batch 3.104 (3.478) Remain 08:22:56 loss: 0.1109 Lr: 0.03163
[2023-08-08 01:47:43,398 INFO misc.py line 115 22900] Train: [45/100][61/156] Data 0.001 (0.001) Batch 3.974 (3.487) Remain 08:24:07 loss: 0.3570 Lr: 0.03162
[2023-08-08 01:47:46,932 INFO misc.py line 115 22900] Train: [45/100][62/156] Data 0.001 (0.001) Batch 3.534 (3.488) Remain 08:24:10 loss: 0.2805 Lr: 0.03162
[2023-08-08 01:47:49,524 INFO misc.py line 115 22900] Train: [45/100][63/156] Data 0.001 (0.001) Batch 2.592 (3.473) Remain 08:21:57 loss: 0.1094 Lr: 0.03161
[2023-08-08 01:47:53,061 INFO misc.py line 115 22900] Train: [45/100][64/156] Data 0.001 (0.001) Batch 3.536 (3.474) Remain 08:22:03 loss: 0.2015 Lr: 0.03161
[2023-08-08 01:47:57,061 INFO misc.py line 115 22900] Train: [45/100][65/156] Data 0.001 (0.001) Batch 4.001 (3.482) Remain 08:23:13 loss: 0.4071 Lr: 0.03160
[2023-08-08 01:48:00,780 INFO misc.py line 115 22900] Train: [45/100][66/156] Data 0.001 (0.001) Batch 3.718 (3.486) Remain 08:23:42 loss: 0.3272 Lr: 0.03160
[2023-08-08 01:48:04,344 INFO misc.py line 115 22900] Train: [45/100][67/156] Data 0.001 (0.001) Batch 3.565 (3.487) Remain 08:23:50 loss: 0.3582 Lr: 0.03159
[2023-08-08 01:48:08,397 INFO misc.py line 115 22900] Train: [45/100][68/156] Data 0.001 (0.001) Batch 4.053 (3.496) Remain 08:25:01 loss: 0.3453 Lr: 0.03159
[2023-08-08 01:48:11,578 INFO misc.py line 115 22900] Train: [45/100][69/156] Data 0.001 (0.001) Batch 3.180 (3.491) Remain 08:24:17 loss: 0.2994 Lr: 0.03158
[2023-08-08 01:48:15,134 INFO misc.py line 115 22900] Train: [45/100][70/156] Data 0.001 (0.001) Batch 3.557 (3.492) Remain 08:24:22 loss: 0.2777 Lr: 0.03158
[2023-08-08 01:48:18,213 INFO misc.py line 115 22900] Train: [45/100][71/156] Data 0.001 (0.001) Batch 3.079 (3.486) Remain 08:23:25 loss: 0.2769 Lr: 0.03157
[2023-08-08 01:48:21,878 INFO misc.py line 115 22900] Train: [45/100][72/156] Data 0.001 (0.001) Batch 3.665 (3.489) Remain 08:23:44 loss: 0.2541 Lr: 0.03157
[2023-08-08 01:48:25,339 INFO misc.py line 115 22900] Train: [45/100][73/156] Data 0.001 (0.001) Batch 3.461 (3.488) Remain 08:23:37 loss: 0.3009 Lr: 0.03156
[2023-08-08 01:48:28,688 INFO misc.py line 115 22900] Train: [45/100][74/156] Data 0.001 (0.001) Batch 3.349 (3.486) Remain 08:23:17 loss: 0.2756 Lr: 0.03156
[2023-08-08 01:48:32,485 INFO misc.py line 115 22900] Train: [45/100][75/156] Data 0.001 (0.001) Batch 3.797 (3.491) Remain 08:23:51 loss: 0.3623 Lr: 0.03155
[2023-08-08 01:48:35,466 INFO misc.py line 115 22900] Train: [45/100][76/156] Data 0.001 (0.001) Batch 2.981 (3.484) Remain 08:22:47 loss: 0.1714 Lr: 0.03155
[2023-08-08 01:48:39,212 INFO misc.py line 115 22900] Train: [45/100][77/156] Data 0.001 (0.001) Batch 3.746 (3.487) Remain 08:23:14 loss: 0.1930 Lr: 0.03154
[2023-08-08 01:48:42,372 INFO misc.py line 115 22900] Train: [45/100][78/156] Data 0.001 (0.001) Batch 3.160 (3.483) Remain 08:22:33 loss: 0.2513 Lr: 0.03154
[2023-08-08 01:48:46,343 INFO misc.py line 115 22900] Train: [45/100][79/156] Data 0.001 (0.001) Batch 3.971 (3.489) Remain 08:23:25 loss: 0.3283 Lr: 0.03153
[2023-08-08 01:48:50,449 INFO misc.py line 115 22900] Train: [45/100][80/156] Data 0.001 (0.001) Batch 4.106 (3.497) Remain 08:24:31 loss: 0.2588 Lr: 0.03153
[2023-08-08 01:48:53,515 INFO misc.py line 115 22900] Train: [45/100][81/156] Data 0.001 (0.001) Batch 3.066 (3.492) Remain 08:23:40 loss: 0.2726 Lr: 0.03152
[2023-08-08 01:48:57,080 INFO misc.py line 115 22900] Train: [45/100][82/156] Data 0.001 (0.001) Batch 3.565 (3.493) Remain 08:23:44 loss: 0.1962 Lr: 0.03152
[2023-08-08 01:49:00,093 INFO misc.py line 115 22900] Train: [45/100][83/156] Data 0.001 (0.001) Batch 3.012 (3.487) Remain 08:22:49 loss: 0.2953 Lr: 0.03151
[2023-08-08 01:49:03,863 INFO misc.py line 115 22900] Train: [45/100][84/156] Data 0.001 (0.001) Batch 3.771 (3.490) Remain 08:23:16 loss: 0.2085 Lr: 0.03151
[2023-08-08 01:49:07,959 INFO misc.py line 115 22900] Train: [45/100][85/156] Data 0.001 (0.001) Batch 4.096 (3.497) Remain 08:24:16 loss: 0.4179 Lr: 0.03150
[2023-08-08 01:49:11,604 INFO misc.py line 115 22900] Train: [45/100][86/156] Data 0.001 (0.001) Batch 3.644 (3.499) Remain 08:24:28 loss: 0.2670 Lr: 0.03150
[2023-08-08 01:49:15,280 INFO misc.py line 115 22900] Train: [45/100][87/156] Data 0.001 (0.001) Batch 3.676 (3.501) Remain 08:24:43 loss: 0.1495 Lr: 0.03149
[2023-08-08 01:49:20,128 INFO misc.py line 115 22900] Train: [45/100][88/156] Data 0.001 (0.001) Batch 4.848 (3.517) Remain 08:26:56 loss: 0.6254 Lr: 0.03149
[2023-08-08 01:49:23,407 INFO misc.py line 115 22900] Train: [45/100][89/156] Data 0.001 (0.001) Batch 3.278 (3.514) Remain 08:26:29 loss: 0.2551 Lr: 0.03148
[2023-08-08 01:49:27,622 INFO misc.py line 115 22900] Train: [45/100][90/156] Data 0.001 (0.001) Batch 4.215 (3.522) Remain 08:27:35 loss: 0.5636 Lr: 0.03148
[2023-08-08 01:49:30,333 INFO misc.py line 115 22900] Train: [45/100][91/156] Data 0.001 (0.001) Batch 2.711 (3.513) Remain 08:26:11 loss: 0.2615 Lr: 0.03147
[2023-08-08 01:49:34,072 INFO misc.py line 115 22900] Train: [45/100][92/156] Data 0.001 (0.001) Batch 3.739 (3.516) Remain 08:26:30 loss: 0.2188 Lr: 0.03147
[2023-08-08 01:49:36,993 INFO misc.py line 115 22900] Train: [45/100][93/156] Data 0.001 (0.001) Batch 2.921 (3.509) Remain 08:25:29 loss: 0.0829 Lr: 0.03146
[2023-08-08 01:49:39,654 INFO misc.py line 115 22900] Train: [45/100][94/156] Data 0.001 (0.001) Batch 2.660 (3.500) Remain 08:24:05 loss: 0.1752 Lr: 0.03146
[2023-08-08 01:49:43,501 INFO misc.py line 115 22900] Train: [45/100][95/156] Data 0.001 (0.001) Batch 3.847 (3.504) Remain 08:24:34 loss: 0.4177 Lr: 0.03145
[2023-08-08 01:49:48,187 INFO misc.py line 115 22900] Train: [45/100][96/156] Data 0.001 (0.001) Batch 4.686 (3.516) Remain 08:26:21 loss: 0.3294 Lr: 0.03145
[2023-08-08 01:49:51,412 INFO misc.py line 115 22900] Train: [45/100][97/156] Data 0.001 (0.001) Batch 3.226 (3.513) Remain 08:25:50 loss: 0.3256 Lr: 0.03144
[2023-08-08 01:49:54,640 INFO misc.py line 115 22900] Train: [45/100][98/156] Data 0.001 (0.001) Batch 3.228 (3.510) Remain 08:25:21 loss: 0.2673 Lr: 0.03143
[2023-08-08 01:49:58,354 INFO misc.py line 115 22900] Train: [45/100][99/156] Data 0.001 (0.001) Batch 3.713 (3.512) Remain 08:25:36 loss: 0.3151 Lr: 0.03143
[2023-08-08 01:50:02,429 INFO misc.py line 115 22900] Train: [45/100][100/156] Data 0.001 (0.001) Batch 4.075 (3.518) Remain 08:26:22 loss: 0.2791 Lr: 0.03142
[2023-08-08 01:50:05,090 INFO misc.py line 115 22900] Train: [45/100][101/156] Data 0.001 (0.001) Batch 2.661 (3.509) Remain 08:25:03 loss: 0.2179 Lr: 0.03142
[2023-08-08 01:50:08,693 INFO misc.py line 115 22900] Train: [45/100][102/156] Data 0.001 (0.001) Batch 3.603 (3.510) Remain 08:25:08 loss: 0.4046 Lr: 0.03141
[2023-08-08 01:50:12,593 INFO misc.py line 115 22900] Train: [45/100][103/156] Data 0.001 (0.001) Batch 3.899 (3.514) Remain 08:25:38 loss: 0.2568 Lr: 0.03141
[2023-08-08 01:50:15,796 INFO misc.py line 115 22900] Train: [45/100][104/156] Data 0.001 (0.001) Batch 3.203 (3.511) Remain 08:25:08 loss: 0.1991 Lr: 0.03140
[2023-08-08 01:50:18,923 INFO misc.py line 115 22900] Train: [45/100][105/156] Data 0.001 (0.001) Batch 3.127 (3.507) Remain 08:24:32 loss: 0.3928 Lr: 0.03140
[2023-08-08 01:50:21,080 INFO misc.py line 115 22900] Train: [45/100][106/156] Data 0.001 (0.001) Batch 2.158 (3.494) Remain 08:22:35 loss: 0.1756 Lr: 0.03139
[2023-08-08 01:50:24,847 INFO misc.py line 115 22900] Train: [45/100][107/156] Data 0.001 (0.001) Batch 3.767 (3.497) Remain 08:22:54 loss: 0.3921 Lr: 0.03139
[2023-08-08 01:50:27,649 INFO misc.py line 115 22900] Train: [45/100][108/156] Data 0.001 (0.001) Batch 2.802 (3.490) Remain 08:21:54 loss: 0.1255 Lr: 0.03138
[2023-08-08 01:50:30,938 INFO misc.py line 115 22900] Train: [45/100][109/156] Data 0.001 (0.001) Batch 3.290 (3.488) Remain 08:21:34 loss: 0.2685 Lr: 0.03138
[2023-08-08 01:50:34,970 INFO misc.py line 115 22900] Train: [45/100][110/156] Data 0.001 (0.001) Batch 4.031 (3.493) Remain 08:22:14 loss: 0.3137 Lr: 0.03137
[2023-08-08 01:50:38,852 INFO misc.py line 115 22900] Train: [45/100][111/156] Data 0.001 (0.001) Batch 3.883 (3.497) Remain 08:22:42 loss: 0.2707 Lr: 0.03137
[2023-08-08 01:50:42,645 INFO misc.py line 115 22900] Train: [45/100][112/156] Data 0.001 (0.001) Batch 3.792 (3.500) Remain 08:23:02 loss: 0.3507 Lr: 0.03136
[2023-08-08 01:50:45,189 INFO misc.py line 115 22900] Train: [45/100][113/156] Data 0.001 (0.001) Batch 2.544 (3.491) Remain 08:21:43 loss: 0.5545 Lr: 0.03136
[2023-08-08 01:50:48,767 INFO misc.py line 115 22900] Train: [45/100][114/156] Data 0.001 (0.001) Batch 3.578 (3.492) Remain 08:21:47 loss: 0.2153 Lr: 0.03135
[2023-08-08 01:50:51,285 INFO misc.py line 115 22900] Train: [45/100][115/156] Data 0.001 (0.001) Batch 2.518 (3.483) Remain 08:20:28 loss: 0.1800 Lr: 0.03135
[2023-08-08 01:50:54,468 INFO misc.py line 115 22900] Train: [45/100][116/156] Data 0.001 (0.001) Batch 3.183 (3.481) Remain 08:20:02 loss: 0.1097 Lr: 0.03134
[2023-08-08 01:50:57,744 INFO misc.py line 115 22900] Train: [45/100][117/156] Data 0.001 (0.001) Batch 3.276 (3.479) Remain 08:19:43 loss: 0.3829 Lr: 0.03134
[2023-08-08 01:51:00,821 INFO misc.py line 115 22900] Train: [45/100][118/156] Data 0.001 (0.001) Batch 3.077 (3.475) Remain 08:19:09 loss: 0.2587 Lr: 0.03133
[2023-08-08 01:51:04,750 INFO misc.py line 115 22900] Train: [45/100][119/156] Data 0.001 (0.001) Batch 3.929 (3.479) Remain 08:19:39 loss: 0.2535 Lr: 0.03133
[2023-08-08 01:51:08,740 INFO misc.py line 115 22900] Train: [45/100][120/156] Data 0.001 (0.001) Batch 3.990 (3.484) Remain 08:20:14 loss: 0.2831 Lr: 0.03132
[2023-08-08 01:51:12,313 INFO misc.py line 115 22900] Train: [45/100][121/156] Data 0.001 (0.001) Batch 3.573 (3.484) Remain 08:20:17 loss: 0.2376 Lr: 0.03132
[2023-08-08 01:51:16,043 INFO misc.py line 115 22900] Train: [45/100][122/156] Data 0.001 (0.001) Batch 3.730 (3.486) Remain 08:20:31 loss: 0.1931 Lr: 0.03131
[2023-08-08 01:51:19,240 INFO misc.py line 115 22900] Train: [45/100][123/156] Data 0.001 (0.001) Batch 3.197 (3.484) Remain 08:20:07 loss: 0.1979 Lr: 0.03131
[2023-08-08 01:51:22,737 INFO misc.py line 115 22900] Train: [45/100][124/156] Data 0.001 (0.001) Batch 3.497 (3.484) Remain 08:20:04 loss: 0.4261 Lr: 0.03130
[2023-08-08 01:51:26,791 INFO misc.py line 115 22900] Train: [45/100][125/156] Data 0.001 (0.001) Batch 4.054 (3.489) Remain 08:20:41 loss: 0.6132 Lr: 0.03130
[2023-08-08 01:51:30,218 INFO misc.py line 115 22900] Train: [45/100][126/156] Data 0.001 (0.001) Batch 3.427 (3.488) Remain 08:20:33 loss: 0.2001 Lr: 0.03129
[2023-08-08 01:51:32,800 INFO misc.py line 115 22900] Train: [45/100][127/156] Data 0.001 (0.001) Batch 2.582 (3.481) Remain 08:19:27 loss: 0.4187 Lr: 0.03129
[2023-08-08 01:51:36,201 INFO misc.py line 115 22900] Train: [45/100][128/156] Data 0.001 (0.001) Batch 3.402 (3.480) Remain 08:19:18 loss: 0.3035 Lr: 0.03128
[2023-08-08 01:51:39,628 INFO misc.py line 115 22900] Train: [45/100][129/156] Data 0.001 (0.001) Batch 3.426 (3.480) Remain 08:19:10 loss: 0.4896 Lr: 0.03128
[2023-08-08 01:51:42,890 INFO misc.py line 115 22900] Train: [45/100][130/156] Data 0.001 (0.001) Batch 3.262 (3.478) Remain 08:18:52 loss: 0.3864 Lr: 0.03127
[2023-08-08 01:51:46,776 INFO misc.py line 115 22900] Train: [45/100][131/156] Data 0.001 (0.001) Batch 3.886 (3.481) Remain 08:19:16 loss: 0.3561 Lr: 0.03127
[2023-08-08 01:51:50,339 INFO misc.py line 115 22900] Train: [45/100][132/156] Data 0.001 (0.001) Batch 3.562 (3.482) Remain 08:19:18 loss: 0.1560 Lr: 0.03126
[2023-08-08 01:51:53,014 INFO misc.py line 115 22900] Train: [45/100][133/156] Data 0.001 (0.001) Batch 2.675 (3.476) Remain 08:18:21 loss: 0.5529 Lr: 0.03126
[2023-08-08 01:51:55,806 INFO misc.py line 115 22900] Train: [45/100][134/156] Data 0.001 (0.001) Batch 2.792 (3.471) Remain 08:17:33 loss: 0.1064 Lr: 0.03125
[2023-08-08 01:51:59,910 INFO misc.py line 115 22900] Train: [45/100][135/156] Data 0.001 (0.001) Batch 4.103 (3.475) Remain 08:18:11 loss: 0.3439 Lr: 0.03125
[2023-08-08 01:52:04,036 INFO misc.py line 115 22900] Train: [45/100][136/156] Data 0.001 (0.001) Batch 4.127 (3.480) Remain 08:18:49 loss: 0.2725 Lr: 0.03124
[2023-08-08 01:52:08,211 INFO misc.py line 115 22900] Train: [45/100][137/156] Data 0.001 (0.001) Batch 4.175 (3.485) Remain 08:19:30 loss: 0.3891 Lr: 0.03123
[2023-08-08 01:52:10,965 INFO misc.py line 115 22900] Train: [45/100][138/156] Data 0.001 (0.001) Batch 2.753 (3.480) Remain 08:18:40 loss: 0.2805 Lr: 0.03123
[2023-08-08 01:52:14,443 INFO misc.py line 115 22900] Train: [45/100][139/156] Data 0.001 (0.001) Batch 3.479 (3.480) Remain 08:18:37 loss: 0.4099 Lr: 0.03122
[2023-08-08 01:52:18,107 INFO misc.py line 115 22900] Train: [45/100][140/156] Data 0.001 (0.001) Batch 3.664 (3.481) Remain 08:18:45 loss: 0.2360 Lr: 0.03122
[2023-08-08 01:52:22,573 INFO misc.py line 115 22900] Train: [45/100][141/156] Data 0.001 (0.001) Batch 4.466 (3.488) Remain 08:19:43 loss: 0.3885 Lr: 0.03121
[2023-08-08 01:52:25,788 INFO misc.py line 115 22900] Train: [45/100][142/156] Data 0.001 (0.001) Batch 3.215 (3.486) Remain 08:19:22 loss: 0.2941 Lr: 0.03121
[2023-08-08 01:52:28,670 INFO misc.py line 115 22900] Train: [45/100][143/156] Data 0.001 (0.001) Batch 2.882 (3.482) Remain 08:18:42 loss: 0.2039 Lr: 0.03120
[2023-08-08 01:52:31,592 INFO misc.py line 115 22900] Train: [45/100][144/156] Data 0.001 (0.001) Batch 2.923 (3.478) Remain 08:18:04 loss: 0.2755 Lr: 0.03120
[2023-08-08 01:52:33,632 INFO misc.py line 115 22900] Train: [45/100][145/156] Data 0.001 (0.001) Batch 2.039 (3.468) Remain 08:16:34 loss: 0.2288 Lr: 0.03119
[2023-08-08 01:52:37,386 INFO misc.py line 115 22900] Train: [45/100][146/156] Data 0.001 (0.001) Batch 3.754 (3.470) Remain 08:16:47 loss: 0.4875 Lr: 0.03119
[2023-08-08 01:52:40,488 INFO misc.py line 115 22900] Train: [45/100][147/156] Data 0.001 (0.001) Batch 3.102 (3.468) Remain 08:16:22 loss: 0.3546 Lr: 0.03118
[2023-08-08 01:52:43,834 INFO misc.py line 115 22900] Train: [45/100][148/156] Data 0.001 (0.001) Batch 3.346 (3.467) Remain 08:16:11 loss: 0.2265 Lr: 0.03118
[2023-08-08 01:52:47,483 INFO misc.py line 115 22900] Train: [45/100][149/156] Data 0.001 (0.001) Batch 3.649 (3.468) Remain 08:16:18 loss: 0.2875 Lr: 0.03117
[2023-08-08 01:52:50,616 INFO misc.py line 115 22900] Train: [45/100][150/156] Data 0.001 (0.001) Batch 3.133 (3.466) Remain 08:15:55 loss: 0.1766 Lr: 0.03117
[2023-08-08 01:52:53,466 INFO misc.py line 115 22900] Train: [45/100][151/156] Data 0.001 (0.001) Batch 2.850 (3.461) Remain 08:15:16 loss: 0.2180 Lr: 0.03116
[2023-08-08 01:52:56,891 INFO misc.py line 115 22900] Train: [45/100][152/156] Data 0.001 (0.001) Batch 3.426 (3.461) Remain 08:15:11 loss: 0.2390 Lr: 0.03116
[2023-08-08 01:53:00,397 INFO misc.py line 115 22900] Train: [45/100][153/156] Data 0.001 (0.001) Batch 3.505 (3.462) Remain 08:15:10 loss: 0.1897 Lr: 0.03115
[2023-08-08 01:53:04,459 INFO misc.py line 115 22900] Train: [45/100][154/156] Data 0.001 (0.001) Batch 4.063 (3.466) Remain 08:15:40 loss: 0.6107 Lr: 0.03115
[2023-08-08 01:53:08,653 INFO misc.py line 115 22900] Train: [45/100][155/156] Data 0.001 (0.001) Batch 4.194 (3.470) Remain 08:16:18 loss: 0.4562 Lr: 0.03114
[2023-08-08 01:53:12,443 INFO misc.py line 115 22900] Train: [45/100][156/156] Data 0.001 (0.001) Batch 3.790 (3.472) Remain 08:16:33 loss: 0.2685 Lr: 0.03114
[2023-08-08 01:53:12,444 INFO misc.py line 129 22900] Train result: loss: 0.2959 
[2023-08-08 01:53:12,444 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 01:53:14,535 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.4440 
[2023-08-08 01:53:15,404 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.4554 
[2023-08-08 01:53:17,068 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.8209 
[2023-08-08 01:53:18,590 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.1525 
[2023-08-08 01:53:20,435 INFO evaluator.py line 122 22900] Test: [5/24] Loss 2.0029 
[2023-08-08 01:53:22,098 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.5245 
[2023-08-08 01:53:24,238 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.8128 
[2023-08-08 01:53:26,039 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.4858 
[2023-08-08 01:53:27,321 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.1944 
[2023-08-08 01:53:29,452 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.4610 
[2023-08-08 01:53:29,977 INFO evaluator.py line 122 22900] Test: [11/24] Loss 0.7729 
[2023-08-08 01:53:31,510 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.7003 
[2023-08-08 01:53:34,224 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.1088 
[2023-08-08 01:53:35,903 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.8073 
[2023-08-08 01:53:37,925 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.3058 
[2023-08-08 01:53:40,635 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.1543 
[2023-08-08 01:53:43,342 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.4939 
[2023-08-08 01:53:45,188 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.3302 
[2023-08-08 01:53:45,936 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.1478 
[2023-08-08 01:53:46,821 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.6738 
[2023-08-08 01:53:49,082 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3856 
[2023-08-08 01:53:51,047 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.8233 
[2023-08-08 01:53:52,895 INFO evaluator.py line 122 22900] Test: [23/24] Loss 3.6901 
[2023-08-08 01:53:54,830 INFO evaluator.py line 122 22900] Test: [24/24] Loss 2.1247 
[2023-08-08 01:53:54,877 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2177/0.3180/0.6906.
[2023-08-08 01:53:54,877 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6554/0.9614
[2023-08-08 01:53:54,877 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9248/0.9861
[2023-08-08 01:53:54,877 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1895/0.3671
[2023-08-08 01:53:54,877 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.0845/0.1124
[2023-08-08 01:53:54,877 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6262/0.7984
[2023-08-08 01:53:54,877 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.3330/0.3971
[2023-08-08 01:53:54,877 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.4431/0.6113
[2023-08-08 01:53:54,877 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.2118/0.2300
[2023-08-08 01:53:54,877 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1411/0.3737
[2023-08-08 01:53:54,877 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0041/0.0041
[2023-08-08 01:53:54,877 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 01:53:54,877 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0043/0.0044
[2023-08-08 01:53:54,877 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1653/0.4175
[2023-08-08 01:53:54,877 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0341/0.0375
[2023-08-08 01:53:54,877 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0239/0.0249
[2023-08-08 01:53:54,877 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-08 01:53:54,877 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.1732/0.2016
[2023-08-08 01:53:54,877 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.2439/0.6679
[2023-08-08 01:53:54,877 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 01:53:54,877 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0965/0.1653
[2023-08-08 01:53:54,877 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 01:53:54,878 INFO misc.py line 152 22900] Currently Best mIoU: 0.2562
[2023-08-08 01:53:54,878 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 01:54:00,799 INFO misc.py line 115 22900] Train: [46/100][1/156] Data 0.932 (0.932) Batch 5.142 (5.142) Remain 12:15:09 loss: 0.3946 Lr: 0.03113
[2023-08-08 01:54:04,881 INFO misc.py line 115 22900] Train: [46/100][2/156] Data 0.001 (0.001) Batch 4.081 (4.081) Remain 09:43:28 loss: 0.4837 Lr: 0.03113
[2023-08-08 01:54:07,206 INFO misc.py line 115 22900] Train: [46/100][3/156] Data 0.001 (0.001) Batch 2.325 (2.325) Remain 05:32:21 loss: 0.1391 Lr: 0.03112
[2023-08-08 01:54:11,774 INFO misc.py line 115 22900] Train: [46/100][4/156] Data 0.001 (0.001) Batch 4.568 (4.568) Remain 10:52:56 loss: 0.4229 Lr: 0.03112
[2023-08-08 01:54:15,044 INFO misc.py line 115 22900] Train: [46/100][5/156] Data 0.001 (0.001) Batch 3.270 (3.919) Remain 09:20:06 loss: 0.2297 Lr: 0.03111
[2023-08-08 01:54:18,456 INFO misc.py line 115 22900] Train: [46/100][6/156] Data 0.001 (0.001) Batch 3.412 (3.750) Remain 08:55:54 loss: 0.2869 Lr: 0.03111
[2023-08-08 01:54:21,025 INFO misc.py line 115 22900] Train: [46/100][7/156] Data 0.001 (0.001) Batch 2.569 (3.455) Remain 08:13:38 loss: 0.1354 Lr: 0.03110
[2023-08-08 01:54:23,764 INFO misc.py line 115 22900] Train: [46/100][8/156] Data 0.001 (0.001) Batch 2.740 (3.312) Remain 07:53:08 loss: 0.2162 Lr: 0.03110
[2023-08-08 01:54:27,452 INFO misc.py line 115 22900] Train: [46/100][9/156] Data 0.001 (0.001) Batch 3.687 (3.374) Remain 08:02:01 loss: 0.3320 Lr: 0.03109
[2023-08-08 01:54:31,074 INFO misc.py line 115 22900] Train: [46/100][10/156] Data 0.001 (0.001) Batch 3.622 (3.410) Remain 08:07:01 loss: 0.1963 Lr: 0.03109
[2023-08-08 01:54:34,965 INFO misc.py line 115 22900] Train: [46/100][11/156] Data 0.001 (0.001) Batch 3.891 (3.470) Remain 08:15:33 loss: 0.4440 Lr: 0.03108
[2023-08-08 01:54:39,532 INFO misc.py line 115 22900] Train: [46/100][12/156] Data 0.001 (0.001) Batch 4.568 (3.592) Remain 08:32:55 loss: 0.3970 Lr: 0.03108
[2023-08-08 01:54:43,540 INFO misc.py line 115 22900] Train: [46/100][13/156] Data 0.001 (0.001) Batch 4.008 (3.633) Remain 08:38:47 loss: 0.3919 Lr: 0.03107
[2023-08-08 01:54:46,442 INFO misc.py line 115 22900] Train: [46/100][14/156] Data 0.001 (0.001) Batch 2.901 (3.567) Remain 08:29:14 loss: 0.3037 Lr: 0.03107
[2023-08-08 01:54:50,420 INFO misc.py line 115 22900] Train: [46/100][15/156] Data 0.001 (0.001) Batch 3.978 (3.601) Remain 08:34:04 loss: 0.3343 Lr: 0.03106
[2023-08-08 01:54:54,718 INFO misc.py line 115 22900] Train: [46/100][16/156] Data 0.001 (0.001) Batch 4.298 (3.655) Remain 08:41:39 loss: 0.3194 Lr: 0.03106
[2023-08-08 01:54:58,614 INFO misc.py line 115 22900] Train: [46/100][17/156] Data 0.001 (0.001) Batch 3.896 (3.672) Remain 08:44:03 loss: 0.1642 Lr: 0.03105
[2023-08-08 01:55:02,696 INFO misc.py line 115 22900] Train: [46/100][18/156] Data 0.001 (0.001) Batch 4.082 (3.699) Remain 08:47:53 loss: 0.2566 Lr: 0.03104
[2023-08-08 01:55:06,138 INFO misc.py line 115 22900] Train: [46/100][19/156] Data 0.001 (0.001) Batch 3.442 (3.683) Remain 08:45:32 loss: 0.2897 Lr: 0.03104
[2023-08-08 01:55:09,389 INFO misc.py line 115 22900] Train: [46/100][20/156] Data 0.001 (0.001) Batch 3.251 (3.658) Remain 08:41:51 loss: 0.2395 Lr: 0.03103
[2023-08-08 01:55:12,596 INFO misc.py line 115 22900] Train: [46/100][21/156] Data 0.001 (0.001) Batch 3.207 (3.633) Remain 08:38:13 loss: 0.3586 Lr: 0.03103
[2023-08-08 01:55:16,033 INFO misc.py line 115 22900] Train: [46/100][22/156] Data 0.001 (0.001) Batch 3.436 (3.622) Remain 08:36:41 loss: 0.2253 Lr: 0.03102
[2023-08-08 01:55:20,069 INFO misc.py line 115 22900] Train: [46/100][23/156] Data 0.001 (0.001) Batch 4.037 (3.643) Remain 08:39:34 loss: 0.6065 Lr: 0.03102
[2023-08-08 01:55:23,073 INFO misc.py line 115 22900] Train: [46/100][24/156] Data 0.001 (0.001) Batch 3.004 (3.613) Remain 08:35:10 loss: 0.2028 Lr: 0.03101
[2023-08-08 01:55:25,798 INFO misc.py line 115 22900] Train: [46/100][25/156] Data 0.001 (0.001) Batch 2.725 (3.572) Remain 08:29:21 loss: 0.1441 Lr: 0.03101
[2023-08-08 01:55:29,338 INFO misc.py line 115 22900] Train: [46/100][26/156] Data 0.001 (0.001) Batch 3.541 (3.571) Remain 08:29:06 loss: 0.3936 Lr: 0.03100
[2023-08-08 01:55:32,562 INFO misc.py line 115 22900] Train: [46/100][27/156] Data 0.001 (0.001) Batch 3.223 (3.557) Remain 08:26:58 loss: 0.2030 Lr: 0.03100
[2023-08-08 01:55:35,762 INFO misc.py line 115 22900] Train: [46/100][28/156] Data 0.001 (0.001) Batch 3.201 (3.542) Remain 08:24:53 loss: 0.3225 Lr: 0.03099
[2023-08-08 01:55:38,690 INFO misc.py line 115 22900] Train: [46/100][29/156] Data 0.001 (0.001) Batch 2.927 (3.519) Remain 08:21:27 loss: 0.2283 Lr: 0.03099
[2023-08-08 01:55:41,727 INFO misc.py line 115 22900] Train: [46/100][30/156] Data 0.001 (0.001) Batch 3.037 (3.501) Remain 08:18:51 loss: 0.1242 Lr: 0.03098
[2023-08-08 01:55:45,832 INFO misc.py line 115 22900] Train: [46/100][31/156] Data 0.001 (0.001) Batch 4.105 (3.522) Remain 08:21:52 loss: 0.2523 Lr: 0.03098
[2023-08-08 01:55:49,687 INFO misc.py line 115 22900] Train: [46/100][32/156] Data 0.001 (0.001) Batch 3.855 (3.534) Remain 08:23:27 loss: 0.2109 Lr: 0.03097
[2023-08-08 01:55:53,201 INFO misc.py line 115 22900] Train: [46/100][33/156] Data 0.001 (0.001) Batch 3.513 (3.533) Remain 08:23:17 loss: 0.3605 Lr: 0.03097
[2023-08-08 01:55:56,872 INFO misc.py line 115 22900] Train: [46/100][34/156] Data 0.001 (0.001) Batch 3.672 (3.538) Remain 08:23:52 loss: 0.6263 Lr: 0.03096
[2023-08-08 01:56:00,087 INFO misc.py line 115 22900] Train: [46/100][35/156] Data 0.001 (0.001) Batch 3.215 (3.528) Remain 08:22:22 loss: 0.3763 Lr: 0.03096
[2023-08-08 01:56:03,830 INFO misc.py line 115 22900] Train: [46/100][36/156] Data 0.001 (0.001) Batch 3.743 (3.534) Remain 08:23:15 loss: 0.3492 Lr: 0.03095
[2023-08-08 01:56:07,274 INFO misc.py line 115 22900] Train: [46/100][37/156] Data 0.001 (0.001) Batch 3.444 (3.531) Remain 08:22:48 loss: 0.3311 Lr: 0.03095
[2023-08-08 01:56:10,960 INFO misc.py line 115 22900] Train: [46/100][38/156] Data 0.001 (0.001) Batch 3.686 (3.536) Remain 08:23:23 loss: 0.2699 Lr: 0.03094
[2023-08-08 01:56:14,678 INFO misc.py line 115 22900] Train: [46/100][39/156] Data 0.001 (0.001) Batch 3.718 (3.541) Remain 08:24:02 loss: 0.2372 Lr: 0.03094
[2023-08-08 01:56:18,367 INFO misc.py line 115 22900] Train: [46/100][40/156] Data 0.001 (0.001) Batch 3.689 (3.545) Remain 08:24:33 loss: 0.2865 Lr: 0.03093
[2023-08-08 01:56:22,564 INFO misc.py line 115 22900] Train: [46/100][41/156] Data 0.001 (0.001) Batch 4.198 (3.562) Remain 08:26:56 loss: 0.3222 Lr: 0.03093
[2023-08-08 01:56:26,646 INFO misc.py line 115 22900] Train: [46/100][42/156] Data 0.001 (0.001) Batch 4.082 (3.575) Remain 08:28:46 loss: 0.4024 Lr: 0.03092
[2023-08-08 01:56:29,892 INFO misc.py line 115 22900] Train: [46/100][43/156] Data 0.001 (0.001) Batch 3.246 (3.567) Remain 08:27:32 loss: 0.4773 Lr: 0.03092
[2023-08-08 01:56:32,873 INFO misc.py line 115 22900] Train: [46/100][44/156] Data 0.001 (0.001) Batch 2.981 (3.553) Remain 08:25:27 loss: 0.2022 Lr: 0.03091
[2023-08-08 01:56:35,646 INFO misc.py line 115 22900] Train: [46/100][45/156] Data 0.001 (0.001) Batch 2.773 (3.534) Remain 08:22:45 loss: 0.1831 Lr: 0.03091
[2023-08-08 01:56:38,602 INFO misc.py line 115 22900] Train: [46/100][46/156] Data 0.001 (0.001) Batch 2.957 (3.521) Remain 08:20:46 loss: 0.0902 Lr: 0.03090
[2023-08-08 01:56:40,927 INFO misc.py line 115 22900] Train: [46/100][47/156] Data 0.001 (0.001) Batch 2.324 (3.494) Remain 08:16:51 loss: 0.2332 Lr: 0.03090
[2023-08-08 01:56:44,472 INFO misc.py line 115 22900] Train: [46/100][48/156] Data 0.001 (0.001) Batch 3.545 (3.495) Remain 08:16:57 loss: 0.2463 Lr: 0.03089
[2023-08-08 01:56:47,256 INFO misc.py line 115 22900] Train: [46/100][49/156] Data 0.001 (0.001) Batch 2.784 (3.479) Remain 08:14:42 loss: 0.1595 Lr: 0.03089
[2023-08-08 01:56:49,847 INFO misc.py line 115 22900] Train: [46/100][50/156] Data 0.001 (0.001) Batch 2.591 (3.460) Remain 08:11:57 loss: 0.3661 Lr: 0.03088
[2023-08-08 01:56:53,481 INFO misc.py line 115 22900] Train: [46/100][51/156] Data 0.001 (0.001) Batch 3.634 (3.464) Remain 08:12:25 loss: 0.2743 Lr: 0.03087
[2023-08-08 01:56:57,140 INFO misc.py line 115 22900] Train: [46/100][52/156] Data 0.001 (0.001) Batch 3.659 (3.468) Remain 08:12:55 loss: 0.3468 Lr: 0.03087
[2023-08-08 01:57:00,941 INFO misc.py line 115 22900] Train: [46/100][53/156] Data 0.001 (0.001) Batch 3.802 (3.475) Remain 08:13:48 loss: 0.2355 Lr: 0.03086
[2023-08-08 01:57:03,851 INFO misc.py line 115 22900] Train: [46/100][54/156] Data 0.001 (0.001) Batch 2.910 (3.464) Remain 08:12:10 loss: 0.0761 Lr: 0.03086
[2023-08-08 01:57:07,489 INFO misc.py line 115 22900] Train: [46/100][55/156] Data 0.001 (0.001) Batch 3.638 (3.467) Remain 08:12:36 loss: 0.2394 Lr: 0.03085
[2023-08-08 01:57:10,982 INFO misc.py line 115 22900] Train: [46/100][56/156] Data 0.001 (0.001) Batch 3.493 (3.467) Remain 08:12:36 loss: 0.1980 Lr: 0.03085
[2023-08-08 01:57:14,765 INFO misc.py line 115 22900] Train: [46/100][57/156] Data 0.001 (0.001) Batch 3.783 (3.473) Remain 08:13:23 loss: 0.2753 Lr: 0.03084
[2023-08-08 01:57:17,458 INFO misc.py line 115 22900] Train: [46/100][58/156] Data 0.001 (0.001) Batch 2.693 (3.459) Remain 08:11:18 loss: 0.5500 Lr: 0.03084
[2023-08-08 01:57:21,264 INFO misc.py line 115 22900] Train: [46/100][59/156] Data 0.001 (0.001) Batch 3.805 (3.465) Remain 08:12:07 loss: 0.2189 Lr: 0.03083
[2023-08-08 01:57:24,419 INFO misc.py line 115 22900] Train: [46/100][60/156] Data 0.001 (0.001) Batch 3.156 (3.460) Remain 08:11:18 loss: 0.2309 Lr: 0.03083
[2023-08-08 01:57:28,423 INFO misc.py line 115 22900] Train: [46/100][61/156] Data 0.001 (0.001) Batch 4.003 (3.469) Remain 08:12:34 loss: 0.3456 Lr: 0.03082
[2023-08-08 01:57:31,791 INFO misc.py line 115 22900] Train: [46/100][62/156] Data 0.001 (0.001) Batch 3.369 (3.468) Remain 08:12:16 loss: 0.2211 Lr: 0.03082
[2023-08-08 01:57:34,508 INFO misc.py line 115 22900] Train: [46/100][63/156] Data 0.001 (0.001) Batch 2.716 (3.455) Remain 08:10:26 loss: 0.2356 Lr: 0.03081
[2023-08-08 01:57:38,990 INFO misc.py line 115 22900] Train: [46/100][64/156] Data 0.001 (0.001) Batch 4.482 (3.472) Remain 08:12:46 loss: 0.5039 Lr: 0.03081
[2023-08-08 01:57:42,212 INFO misc.py line 115 22900] Train: [46/100][65/156] Data 0.001 (0.001) Batch 3.222 (3.468) Remain 08:12:08 loss: 0.3154 Lr: 0.03080
[2023-08-08 01:57:46,151 INFO misc.py line 115 22900] Train: [46/100][66/156] Data 0.001 (0.001) Batch 3.938 (3.475) Remain 08:13:08 loss: 0.2848 Lr: 0.03080
[2023-08-08 01:57:49,693 INFO misc.py line 115 22900] Train: [46/100][67/156] Data 0.001 (0.001) Batch 3.543 (3.476) Remain 08:13:14 loss: 0.2488 Lr: 0.03079
[2023-08-08 01:57:53,535 INFO misc.py line 115 22900] Train: [46/100][68/156] Data 0.001 (0.001) Batch 3.841 (3.482) Remain 08:13:58 loss: 0.2261 Lr: 0.03079
[2023-08-08 01:57:56,908 INFO misc.py line 115 22900] Train: [46/100][69/156] Data 0.001 (0.001) Batch 3.373 (3.480) Remain 08:13:41 loss: 0.2088 Lr: 0.03078
[2023-08-08 01:58:00,318 INFO misc.py line 115 22900] Train: [46/100][70/156] Data 0.001 (0.001) Batch 3.410 (3.479) Remain 08:13:28 loss: 0.2228 Lr: 0.03078
[2023-08-08 01:58:03,977 INFO misc.py line 115 22900] Train: [46/100][71/156] Data 0.001 (0.001) Batch 3.659 (3.482) Remain 08:13:47 loss: 0.3363 Lr: 0.03077
[2023-08-08 01:58:07,019 INFO misc.py line 115 22900] Train: [46/100][72/156] Data 0.001 (0.001) Batch 3.043 (3.476) Remain 08:12:50 loss: 0.2654 Lr: 0.03077
[2023-08-08 01:58:09,768 INFO misc.py line 115 22900] Train: [46/100][73/156] Data 0.001 (0.001) Batch 2.749 (3.465) Remain 08:11:18 loss: 0.1601 Lr: 0.03076
[2023-08-08 01:58:13,778 INFO misc.py line 115 22900] Train: [46/100][74/156] Data 0.001 (0.001) Batch 4.010 (3.473) Remain 08:12:20 loss: 0.3527 Lr: 0.03076
[2023-08-08 01:58:16,804 INFO misc.py line 115 22900] Train: [46/100][75/156] Data 0.001 (0.001) Batch 3.027 (3.467) Remain 08:11:23 loss: 0.3290 Lr: 0.03075
[2023-08-08 01:58:19,226 INFO misc.py line 115 22900] Train: [46/100][76/156] Data 0.001 (0.001) Batch 2.421 (3.452) Remain 08:09:18 loss: 0.1270 Lr: 0.03075
[2023-08-08 01:58:22,474 INFO misc.py line 115 22900] Train: [46/100][77/156] Data 0.001 (0.001) Batch 3.248 (3.450) Remain 08:08:51 loss: 0.1202 Lr: 0.03074
[2023-08-08 01:58:24,540 INFO misc.py line 115 22900] Train: [46/100][78/156] Data 0.001 (0.001) Batch 2.066 (3.431) Remain 08:06:11 loss: 0.1739 Lr: 0.03074
[2023-08-08 01:58:28,029 INFO misc.py line 115 22900] Train: [46/100][79/156] Data 0.001 (0.001) Batch 3.489 (3.432) Remain 08:06:14 loss: 0.2360 Lr: 0.03073
[2023-08-08 01:58:31,352 INFO misc.py line 115 22900] Train: [46/100][80/156] Data 0.001 (0.001) Batch 3.323 (3.430) Remain 08:05:59 loss: 0.1335 Lr: 0.03073
[2023-08-08 01:58:34,230 INFO misc.py line 115 22900] Train: [46/100][81/156] Data 0.001 (0.001) Batch 2.878 (3.423) Remain 08:04:55 loss: 0.2380 Lr: 0.03072
[2023-08-08 01:58:38,412 INFO misc.py line 115 22900] Train: [46/100][82/156] Data 0.001 (0.001) Batch 4.181 (3.433) Remain 08:06:13 loss: 0.1815 Lr: 0.03072
[2023-08-08 01:58:42,141 INFO misc.py line 115 22900] Train: [46/100][83/156] Data 0.001 (0.001) Batch 3.730 (3.437) Remain 08:06:41 loss: 0.2348 Lr: 0.03071
[2023-08-08 01:58:45,464 INFO misc.py line 115 22900] Train: [46/100][84/156] Data 0.001 (0.001) Batch 3.323 (3.435) Remain 08:06:26 loss: 0.1304 Lr: 0.03070
[2023-08-08 01:58:49,402 INFO misc.py line 115 22900] Train: [46/100][85/156] Data 0.001 (0.001) Batch 3.938 (3.441) Remain 08:07:14 loss: 0.4215 Lr: 0.03070
[2023-08-08 01:58:52,792 INFO misc.py line 115 22900] Train: [46/100][86/156] Data 0.001 (0.001) Batch 3.390 (3.441) Remain 08:07:06 loss: 0.2592 Lr: 0.03069
[2023-08-08 01:58:56,441 INFO misc.py line 115 22900] Train: [46/100][87/156] Data 0.001 (0.001) Batch 3.649 (3.443) Remain 08:07:23 loss: 0.2327 Lr: 0.03069
[2023-08-08 01:58:59,915 INFO misc.py line 115 22900] Train: [46/100][88/156] Data 0.001 (0.001) Batch 3.474 (3.444) Remain 08:07:23 loss: 0.2952 Lr: 0.03068
[2023-08-08 01:59:03,936 INFO misc.py line 115 22900] Train: [46/100][89/156] Data 0.001 (0.001) Batch 4.021 (3.450) Remain 08:08:16 loss: 0.4059 Lr: 0.03068
[2023-08-08 01:59:08,058 INFO misc.py line 115 22900] Train: [46/100][90/156] Data 0.001 (0.001) Batch 4.123 (3.458) Remain 08:09:19 loss: 0.3989 Lr: 0.03067
[2023-08-08 01:59:11,697 INFO misc.py line 115 22900] Train: [46/100][91/156] Data 0.001 (0.001) Batch 3.638 (3.460) Remain 08:09:33 loss: 0.2802 Lr: 0.03067
[2023-08-08 01:59:14,229 INFO misc.py line 115 22900] Train: [46/100][92/156] Data 0.001 (0.001) Batch 2.532 (3.450) Remain 08:08:01 loss: 0.1483 Lr: 0.03066
[2023-08-08 01:59:19,002 INFO misc.py line 115 22900] Train: [46/100][93/156] Data 0.001 (0.001) Batch 4.773 (3.464) Remain 08:10:02 loss: 0.5965 Lr: 0.03066
[2023-08-08 01:59:22,575 INFO misc.py line 115 22900] Train: [46/100][94/156] Data 0.001 (0.001) Batch 3.573 (3.466) Remain 08:10:09 loss: 0.2315 Lr: 0.03065
[2023-08-08 01:59:26,046 INFO misc.py line 115 22900] Train: [46/100][95/156] Data 0.001 (0.001) Batch 3.471 (3.466) Remain 08:10:06 loss: 0.3078 Lr: 0.03065
[2023-08-08 01:59:30,122 INFO misc.py line 115 22900] Train: [46/100][96/156] Data 0.001 (0.001) Batch 4.076 (3.472) Remain 08:10:58 loss: 0.2935 Lr: 0.03064
[2023-08-08 01:59:33,821 INFO misc.py line 115 22900] Train: [46/100][97/156] Data 0.001 (0.001) Batch 3.699 (3.475) Remain 08:11:15 loss: 0.2400 Lr: 0.03064
[2023-08-08 01:59:37,345 INFO misc.py line 115 22900] Train: [46/100][98/156] Data 0.001 (0.001) Batch 3.525 (3.475) Remain 08:11:16 loss: 0.3107 Lr: 0.03063
[2023-08-08 01:59:40,931 INFO misc.py line 115 22900] Train: [46/100][99/156] Data 0.001 (0.001) Batch 3.586 (3.476) Remain 08:11:22 loss: 0.2676 Lr: 0.03063
[2023-08-08 01:59:44,567 INFO misc.py line 115 22900] Train: [46/100][100/156] Data 0.001 (0.001) Batch 3.636 (3.478) Remain 08:11:33 loss: 0.2111 Lr: 0.03062
[2023-08-08 01:59:48,564 INFO misc.py line 115 22900] Train: [46/100][101/156] Data 0.001 (0.001) Batch 3.997 (3.483) Remain 08:12:14 loss: 0.5679 Lr: 0.03062
[2023-08-08 01:59:51,633 INFO misc.py line 115 22900] Train: [46/100][102/156] Data 0.001 (0.001) Batch 3.068 (3.479) Remain 08:11:35 loss: 0.2065 Lr: 0.03061
[2023-08-08 01:59:55,311 INFO misc.py line 115 22900] Train: [46/100][103/156] Data 0.001 (0.001) Batch 3.678 (3.481) Remain 08:11:48 loss: 0.3572 Lr: 0.03061
[2023-08-08 01:59:59,435 INFO misc.py line 115 22900] Train: [46/100][104/156] Data 0.001 (0.001) Batch 4.124 (3.487) Remain 08:12:39 loss: 0.4582 Lr: 0.03060
[2023-08-08 02:00:01,873 INFO misc.py line 115 22900] Train: [46/100][105/156] Data 0.001 (0.001) Batch 2.438 (3.477) Remain 08:11:08 loss: 0.2665 Lr: 0.03060
[2023-08-08 02:00:05,188 INFO misc.py line 115 22900] Train: [46/100][106/156] Data 0.001 (0.001) Batch 3.315 (3.476) Remain 08:10:51 loss: 0.2240 Lr: 0.03059
[2023-08-08 02:00:08,199 INFO misc.py line 115 22900] Train: [46/100][107/156] Data 0.001 (0.001) Batch 3.011 (3.471) Remain 08:10:10 loss: 0.2307 Lr: 0.03059
[2023-08-08 02:00:11,754 INFO misc.py line 115 22900] Train: [46/100][108/156] Data 0.001 (0.001) Batch 3.555 (3.472) Remain 08:10:13 loss: 0.2451 Lr: 0.03058
[2023-08-08 02:00:15,213 INFO misc.py line 115 22900] Train: [46/100][109/156] Data 0.001 (0.001) Batch 3.459 (3.472) Remain 08:10:09 loss: 0.3811 Lr: 0.03058
[2023-08-08 02:00:19,285 INFO misc.py line 115 22900] Train: [46/100][110/156] Data 0.001 (0.001) Batch 4.071 (3.477) Remain 08:10:53 loss: 0.3280 Lr: 0.03057
[2023-08-08 02:00:23,289 INFO misc.py line 115 22900] Train: [46/100][111/156] Data 0.001 (0.001) Batch 4.004 (3.482) Remain 08:11:31 loss: 0.2531 Lr: 0.03057
[2023-08-08 02:00:25,107 INFO misc.py line 115 22900] Train: [46/100][112/156] Data 0.001 (0.001) Batch 1.818 (3.467) Remain 08:09:18 loss: 0.1487 Lr: 0.03056
[2023-08-08 02:00:28,011 INFO misc.py line 115 22900] Train: [46/100][113/156] Data 0.001 (0.001) Batch 2.903 (3.462) Remain 08:08:31 loss: 0.1905 Lr: 0.03056
[2023-08-08 02:00:29,777 INFO misc.py line 115 22900] Train: [46/100][114/156] Data 0.001 (0.001) Batch 1.767 (3.447) Remain 08:06:18 loss: 0.0989 Lr: 0.03055
[2023-08-08 02:00:33,100 INFO misc.py line 115 22900] Train: [46/100][115/156] Data 0.001 (0.001) Batch 3.322 (3.445) Remain 08:06:05 loss: 0.2501 Lr: 0.03054
[2023-08-08 02:00:36,513 INFO misc.py line 115 22900] Train: [46/100][116/156] Data 0.001 (0.001) Batch 3.414 (3.445) Remain 08:06:00 loss: 0.2658 Lr: 0.03054
[2023-08-08 02:00:40,139 INFO misc.py line 115 22900] Train: [46/100][117/156] Data 0.001 (0.001) Batch 3.626 (3.447) Remain 08:06:10 loss: 0.2264 Lr: 0.03053
[2023-08-08 02:00:42,475 INFO misc.py line 115 22900] Train: [46/100][118/156] Data 0.001 (0.001) Batch 2.335 (3.437) Remain 08:04:44 loss: 0.1014 Lr: 0.03053
[2023-08-08 02:00:46,591 INFO misc.py line 115 22900] Train: [46/100][119/156] Data 0.001 (0.001) Batch 4.116 (3.443) Remain 08:05:30 loss: 0.3603 Lr: 0.03052
[2023-08-08 02:00:49,225 INFO misc.py line 115 22900] Train: [46/100][120/156] Data 0.001 (0.001) Batch 2.634 (3.436) Remain 08:04:29 loss: 0.2437 Lr: 0.03052
[2023-08-08 02:00:53,050 INFO misc.py line 115 22900] Train: [46/100][121/156] Data 0.001 (0.001) Batch 3.825 (3.439) Remain 08:04:53 loss: 0.4066 Lr: 0.03051
[2023-08-08 02:00:56,840 INFO misc.py line 115 22900] Train: [46/100][122/156] Data 0.001 (0.001) Batch 3.790 (3.442) Remain 08:05:14 loss: 0.2519 Lr: 0.03051
[2023-08-08 02:01:00,886 INFO misc.py line 115 22900] Train: [46/100][123/156] Data 0.001 (0.001) Batch 4.046 (3.447) Remain 08:05:54 loss: 0.3266 Lr: 0.03050
[2023-08-08 02:01:04,913 INFO misc.py line 115 22900] Train: [46/100][124/156] Data 0.001 (0.001) Batch 4.027 (3.452) Remain 08:06:31 loss: 0.5483 Lr: 0.03050
[2023-08-08 02:01:08,790 INFO misc.py line 115 22900] Train: [46/100][125/156] Data 0.001 (0.001) Batch 3.877 (3.456) Remain 08:06:57 loss: 0.3570 Lr: 0.03049
[2023-08-08 02:01:12,853 INFO misc.py line 115 22900] Train: [46/100][126/156] Data 0.001 (0.001) Batch 4.063 (3.461) Remain 08:07:35 loss: 0.3841 Lr: 0.03049
[2023-08-08 02:01:15,847 INFO misc.py line 115 22900] Train: [46/100][127/156] Data 0.001 (0.001) Batch 2.994 (3.457) Remain 08:07:00 loss: 0.2527 Lr: 0.03048
[2023-08-08 02:01:20,287 INFO misc.py line 115 22900] Train: [46/100][128/156] Data 0.001 (0.001) Batch 4.440 (3.465) Remain 08:08:03 loss: 0.4804 Lr: 0.03048
[2023-08-08 02:01:24,329 INFO misc.py line 115 22900] Train: [46/100][129/156] Data 0.001 (0.001) Batch 4.042 (3.469) Remain 08:08:38 loss: 0.2697 Lr: 0.03047
[2023-08-08 02:01:27,080 INFO misc.py line 115 22900] Train: [46/100][130/156] Data 0.001 (0.001) Batch 2.751 (3.464) Remain 08:07:47 loss: 0.1233 Lr: 0.03047
[2023-08-08 02:01:30,485 INFO misc.py line 115 22900] Train: [46/100][131/156] Data 0.001 (0.001) Batch 3.405 (3.463) Remain 08:07:39 loss: 0.2470 Lr: 0.03046
[2023-08-08 02:01:33,674 INFO misc.py line 115 22900] Train: [46/100][132/156] Data 0.001 (0.001) Batch 3.190 (3.461) Remain 08:07:18 loss: 0.4262 Lr: 0.03046
[2023-08-08 02:01:37,105 INFO misc.py line 115 22900] Train: [46/100][133/156] Data 0.001 (0.001) Batch 3.431 (3.461) Remain 08:07:13 loss: 0.3067 Lr: 0.03045
[2023-08-08 02:01:40,707 INFO misc.py line 115 22900] Train: [46/100][134/156] Data 0.001 (0.001) Batch 3.602 (3.462) Remain 08:07:18 loss: 0.2458 Lr: 0.03045
[2023-08-08 02:01:43,643 INFO misc.py line 115 22900] Train: [46/100][135/156] Data 0.001 (0.001) Batch 2.936 (3.458) Remain 08:06:41 loss: 0.1342 Lr: 0.03044
[2023-08-08 02:01:45,458 INFO misc.py line 115 22900] Train: [46/100][136/156] Data 0.001 (0.001) Batch 1.815 (3.446) Remain 08:04:53 loss: 0.2319 Lr: 0.03044
[2023-08-08 02:01:48,656 INFO misc.py line 115 22900] Train: [46/100][137/156] Data 0.001 (0.001) Batch 3.198 (3.444) Remain 08:04:34 loss: 0.2239 Lr: 0.03043
[2023-08-08 02:01:52,219 INFO misc.py line 115 22900] Train: [46/100][138/156] Data 0.001 (0.001) Batch 3.563 (3.445) Remain 08:04:38 loss: 0.1749 Lr: 0.03043
[2023-08-08 02:01:55,636 INFO misc.py line 115 22900] Train: [46/100][139/156] Data 0.001 (0.001) Batch 3.417 (3.444) Remain 08:04:33 loss: 0.2670 Lr: 0.03042
[2023-08-08 02:01:59,411 INFO misc.py line 115 22900] Train: [46/100][140/156] Data 0.001 (0.001) Batch 3.775 (3.447) Remain 08:04:50 loss: 0.3495 Lr: 0.03042
[2023-08-08 02:02:01,285 INFO misc.py line 115 22900] Train: [46/100][141/156] Data 0.001 (0.001) Batch 1.874 (3.435) Remain 08:03:10 loss: 0.1756 Lr: 0.03041
[2023-08-08 02:02:04,459 INFO misc.py line 115 22900] Train: [46/100][142/156] Data 0.001 (0.001) Batch 3.174 (3.433) Remain 08:02:51 loss: 0.2068 Lr: 0.03041
[2023-08-08 02:02:08,358 INFO misc.py line 115 22900] Train: [46/100][143/156] Data 0.001 (0.001) Batch 3.899 (3.437) Remain 08:03:16 loss: 0.2099 Lr: 0.03040
[2023-08-08 02:02:12,347 INFO misc.py line 115 22900] Train: [46/100][144/156] Data 0.001 (0.001) Batch 3.989 (3.441) Remain 08:03:45 loss: 0.5931 Lr: 0.03039
[2023-08-08 02:02:16,354 INFO misc.py line 115 22900] Train: [46/100][145/156] Data 0.001 (0.001) Batch 4.007 (3.445) Remain 08:04:16 loss: 0.4970 Lr: 0.03039
[2023-08-08 02:02:20,462 INFO misc.py line 115 22900] Train: [46/100][146/156] Data 0.001 (0.001) Batch 4.107 (3.449) Remain 08:04:51 loss: 0.2538 Lr: 0.03038
[2023-08-08 02:02:24,580 INFO misc.py line 115 22900] Train: [46/100][147/156] Data 0.001 (0.001) Batch 4.118 (3.454) Remain 08:05:27 loss: 0.2861 Lr: 0.03038
[2023-08-08 02:02:29,172 INFO misc.py line 115 22900] Train: [46/100][148/156] Data 0.001 (0.001) Batch 4.592 (3.462) Remain 08:06:30 loss: 0.3308 Lr: 0.03037
[2023-08-08 02:02:33,231 INFO misc.py line 115 22900] Train: [46/100][149/156] Data 0.001 (0.001) Batch 4.059 (3.466) Remain 08:07:01 loss: 0.1705 Lr: 0.03037
[2023-08-08 02:02:36,902 INFO misc.py line 115 22900] Train: [46/100][150/156] Data 0.001 (0.001) Batch 3.671 (3.467) Remain 08:07:09 loss: 0.2854 Lr: 0.03036
[2023-08-08 02:02:40,062 INFO misc.py line 115 22900] Train: [46/100][151/156] Data 0.001 (0.001) Batch 3.160 (3.465) Remain 08:06:48 loss: 0.2526 Lr: 0.03036
[2023-08-08 02:02:43,686 INFO misc.py line 115 22900] Train: [46/100][152/156] Data 0.001 (0.001) Batch 3.624 (3.466) Remain 08:06:54 loss: 0.3930 Lr: 0.03035
[2023-08-08 02:02:47,183 INFO misc.py line 115 22900] Train: [46/100][153/156] Data 0.001 (0.001) Batch 3.497 (3.467) Remain 08:06:52 loss: 0.2319 Lr: 0.03035
[2023-08-08 02:02:50,827 INFO misc.py line 115 22900] Train: [46/100][154/156] Data 0.001 (0.001) Batch 3.644 (3.468) Remain 08:06:58 loss: 0.2123 Lr: 0.03034
[2023-08-08 02:02:55,812 INFO misc.py line 115 22900] Train: [46/100][155/156] Data 0.001 (0.001) Batch 4.985 (3.478) Remain 08:08:19 loss: 0.6986 Lr: 0.03034
[2023-08-08 02:02:58,787 INFO misc.py line 115 22900] Train: [46/100][156/156] Data 0.001 (0.001) Batch 2.975 (3.474) Remain 08:07:48 loss: 0.1890 Lr: 0.03033
[2023-08-08 02:02:58,787 INFO misc.py line 129 22900] Train result: loss: 0.2855 
[2023-08-08 02:02:58,787 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 02:03:00,882 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.8222 
[2023-08-08 02:03:01,753 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.4892 
[2023-08-08 02:03:03,418 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.6617 
[2023-08-08 02:03:04,939 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.1206 
[2023-08-08 02:03:06,783 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.9677 
[2023-08-08 02:03:08,446 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.4798 
[2023-08-08 02:03:10,585 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.3051 
[2023-08-08 02:03:12,390 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.9564 
[2023-08-08 02:03:13,672 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.2172 
[2023-08-08 02:03:15,800 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.2229 
[2023-08-08 02:03:16,326 INFO evaluator.py line 122 22900] Test: [11/24] Loss 0.9286 
[2023-08-08 02:03:17,859 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.6775 
[2023-08-08 02:03:20,568 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.0477 
[2023-08-08 02:03:22,248 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.8887 
[2023-08-08 02:03:24,270 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.4032 
[2023-08-08 02:03:26,983 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.1739 
[2023-08-08 02:03:29,688 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.4490 
[2023-08-08 02:03:31,535 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.3777 
[2023-08-08 02:03:32,284 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.0369 
[2023-08-08 02:03:33,170 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.6015 
[2023-08-08 02:03:35,428 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.2899 
[2023-08-08 02:03:37,392 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.6615 
[2023-08-08 02:03:39,239 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.8660 
[2023-08-08 02:03:41,178 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.4951 
[2023-08-08 02:03:41,225 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2526/0.3519/0.7000.
[2023-08-08 02:03:41,225 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6629/0.9285
[2023-08-08 02:03:41,225 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9297/0.9925
[2023-08-08 02:03:41,225 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1789/0.4296
[2023-08-08 02:03:41,225 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1059/0.1850
[2023-08-08 02:03:41,225 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6212/0.8053
[2023-08-08 02:03:41,225 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.3437/0.4540
[2023-08-08 02:03:41,225 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5377/0.6697
[2023-08-08 02:03:41,225 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.2231/0.2729
[2023-08-08 02:03:41,225 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.2103/0.4182
[2023-08-08 02:03:41,225 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0091/0.0091
[2023-08-08 02:03:41,225 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 02:03:41,225 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0252/0.0265
[2023-08-08 02:03:41,225 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.3005/0.3929
[2023-08-08 02:03:41,225 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.1668/0.2098
[2023-08-08 02:03:41,225 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0155/0.0161
[2023-08-08 02:03:41,225 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.1585/0.1593
[2023-08-08 02:03:41,225 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.2042/0.3188
[2023-08-08 02:03:41,225 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.2845/0.5921
[2023-08-08 02:03:41,225 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 02:03:41,225 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0734/0.1580
[2023-08-08 02:03:41,226 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 02:03:41,226 INFO misc.py line 152 22900] Currently Best mIoU: 0.2562
[2023-08-08 02:03:41,226 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 02:03:46,955 INFO misc.py line 115 22900] Train: [47/100][1/156] Data 1.228 (1.228) Batch 4.914 (4.914) Remain 11:29:50 loss: 0.2869 Lr: 0.03033
[2023-08-08 02:03:50,419 INFO misc.py line 115 22900] Train: [47/100][2/156] Data 0.001 (0.001) Batch 3.464 (3.464) Remain 08:06:14 loss: 0.1523 Lr: 0.03032
[2023-08-08 02:03:53,658 INFO misc.py line 115 22900] Train: [47/100][3/156] Data 0.001 (0.001) Batch 3.240 (3.240) Remain 07:34:41 loss: 0.3257 Lr: 0.03032
[2023-08-08 02:03:56,807 INFO misc.py line 115 22900] Train: [47/100][4/156] Data 0.001 (0.001) Batch 3.149 (3.149) Remain 07:21:50 loss: 0.1910 Lr: 0.03031
[2023-08-08 02:03:59,529 INFO misc.py line 115 22900] Train: [47/100][5/156] Data 0.001 (0.001) Batch 2.722 (2.935) Remain 06:51:53 loss: 0.1717 Lr: 0.03031
[2023-08-08 02:04:01,634 INFO misc.py line 115 22900] Train: [47/100][6/156] Data 0.001 (0.001) Batch 2.105 (2.659) Remain 06:13:00 loss: 0.0623 Lr: 0.03030
[2023-08-08 02:04:05,193 INFO misc.py line 115 22900] Train: [47/100][7/156] Data 0.001 (0.001) Batch 3.558 (2.884) Remain 06:44:31 loss: 0.3179 Lr: 0.03030
[2023-08-08 02:04:08,983 INFO misc.py line 115 22900] Train: [47/100][8/156] Data 0.001 (0.001) Batch 3.791 (3.065) Remain 07:09:55 loss: 0.2794 Lr: 0.03029
[2023-08-08 02:04:13,035 INFO misc.py line 115 22900] Train: [47/100][9/156] Data 0.001 (0.001) Batch 4.051 (3.229) Remain 07:32:54 loss: 0.4173 Lr: 0.03029
[2023-08-08 02:04:15,662 INFO misc.py line 115 22900] Train: [47/100][10/156] Data 0.001 (0.001) Batch 2.628 (3.143) Remain 07:20:48 loss: 0.2533 Lr: 0.03028
[2023-08-08 02:04:19,301 INFO misc.py line 115 22900] Train: [47/100][11/156] Data 0.001 (0.001) Batch 3.638 (3.205) Remain 07:29:25 loss: 0.2879 Lr: 0.03028
[2023-08-08 02:04:22,251 INFO misc.py line 115 22900] Train: [47/100][12/156] Data 0.001 (0.001) Batch 2.951 (3.177) Remain 07:25:24 loss: 0.1472 Lr: 0.03027
[2023-08-08 02:04:24,879 INFO misc.py line 115 22900] Train: [47/100][13/156] Data 0.001 (0.001) Batch 2.627 (3.122) Remain 07:17:39 loss: 0.2266 Lr: 0.03027
[2023-08-08 02:04:28,071 INFO misc.py line 115 22900] Train: [47/100][14/156] Data 0.001 (0.001) Batch 3.192 (3.128) Remain 07:18:29 loss: 0.1236 Lr: 0.03026
[2023-08-08 02:04:30,620 INFO misc.py line 115 22900] Train: [47/100][15/156] Data 0.001 (0.001) Batch 2.549 (3.080) Remain 07:11:40 loss: 0.2972 Lr: 0.03025
[2023-08-08 02:04:33,958 INFO misc.py line 115 22900] Train: [47/100][16/156] Data 0.001 (0.001) Batch 3.338 (3.100) Remain 07:14:24 loss: 0.3644 Lr: 0.03025
[2023-08-08 02:04:37,388 INFO misc.py line 115 22900] Train: [47/100][17/156] Data 0.001 (0.001) Batch 3.430 (3.124) Remain 07:17:39 loss: 0.2755 Lr: 0.03024
[2023-08-08 02:04:40,702 INFO misc.py line 115 22900] Train: [47/100][18/156] Data 0.001 (0.001) Batch 3.314 (3.136) Remain 07:19:23 loss: 0.2550 Lr: 0.03024
[2023-08-08 02:04:44,454 INFO misc.py line 115 22900] Train: [47/100][19/156] Data 0.001 (0.001) Batch 3.751 (3.175) Remain 07:24:43 loss: 0.3566 Lr: 0.03023
[2023-08-08 02:04:47,033 INFO misc.py line 115 22900] Train: [47/100][20/156] Data 0.001 (0.001) Batch 2.579 (3.140) Remain 07:19:45 loss: 0.2017 Lr: 0.03023
[2023-08-08 02:04:50,914 INFO misc.py line 115 22900] Train: [47/100][21/156] Data 0.001 (0.001) Batch 3.881 (3.181) Remain 07:25:28 loss: 0.2819 Lr: 0.03022
[2023-08-08 02:04:53,939 INFO misc.py line 115 22900] Train: [47/100][22/156] Data 0.001 (0.001) Batch 3.025 (3.173) Remain 07:24:16 loss: 0.1953 Lr: 0.03022
[2023-08-08 02:04:56,977 INFO misc.py line 115 22900] Train: [47/100][23/156] Data 0.001 (0.001) Batch 3.038 (3.166) Remain 07:23:17 loss: 0.2779 Lr: 0.03021
[2023-08-08 02:05:01,015 INFO misc.py line 115 22900] Train: [47/100][24/156] Data 0.001 (0.001) Batch 4.038 (3.207) Remain 07:29:02 loss: 0.2674 Lr: 0.03021
[2023-08-08 02:05:04,309 INFO misc.py line 115 22900] Train: [47/100][25/156] Data 0.001 (0.001) Batch 3.294 (3.211) Remain 07:29:32 loss: 0.2804 Lr: 0.03020
[2023-08-08 02:05:08,343 INFO misc.py line 115 22900] Train: [47/100][26/156] Data 0.001 (0.001) Batch 4.034 (3.247) Remain 07:34:29 loss: 0.2515 Lr: 0.03020
[2023-08-08 02:05:12,196 INFO misc.py line 115 22900] Train: [47/100][27/156] Data 0.001 (0.001) Batch 3.853 (3.272) Remain 07:37:58 loss: 0.2265 Lr: 0.03019
[2023-08-08 02:05:14,810 INFO misc.py line 115 22900] Train: [47/100][28/156] Data 0.001 (0.001) Batch 2.615 (3.246) Remain 07:34:14 loss: 0.1172 Lr: 0.03019
[2023-08-08 02:05:18,197 INFO misc.py line 115 22900] Train: [47/100][29/156] Data 0.001 (0.001) Batch 3.387 (3.251) Remain 07:34:56 loss: 0.1468 Lr: 0.03018
[2023-08-08 02:05:21,731 INFO misc.py line 115 22900] Train: [47/100][30/156] Data 0.001 (0.001) Batch 3.534 (3.262) Remain 07:36:20 loss: 0.1247 Lr: 0.03018
[2023-08-08 02:05:24,772 INFO misc.py line 115 22900] Train: [47/100][31/156] Data 0.001 (0.001) Batch 3.041 (3.254) Remain 07:35:11 loss: 0.2256 Lr: 0.03017
[2023-08-08 02:05:28,865 INFO misc.py line 115 22900] Train: [47/100][32/156] Data 0.001 (0.001) Batch 4.093 (3.283) Remain 07:39:10 loss: 0.1902 Lr: 0.03017
[2023-08-08 02:05:31,446 INFO misc.py line 115 22900] Train: [47/100][33/156] Data 0.001 (0.001) Batch 2.581 (3.260) Remain 07:35:51 loss: 0.1709 Lr: 0.03016
[2023-08-08 02:05:34,394 INFO misc.py line 115 22900] Train: [47/100][34/156] Data 0.001 (0.001) Batch 2.948 (3.250) Remain 07:34:23 loss: 0.1709 Lr: 0.03016
[2023-08-08 02:05:38,382 INFO misc.py line 115 22900] Train: [47/100][35/156] Data 0.001 (0.001) Batch 3.988 (3.273) Remain 07:37:33 loss: 0.4740 Lr: 0.03015
[2023-08-08 02:05:42,407 INFO misc.py line 115 22900] Train: [47/100][36/156] Data 0.001 (0.001) Batch 4.025 (3.295) Remain 07:40:41 loss: 0.4332 Lr: 0.03015
[2023-08-08 02:05:45,525 INFO misc.py line 115 22900] Train: [47/100][37/156] Data 0.001 (0.001) Batch 3.118 (3.290) Remain 07:39:54 loss: 0.1619 Lr: 0.03014
[2023-08-08 02:05:49,009 INFO misc.py line 115 22900] Train: [47/100][38/156] Data 0.001 (0.001) Batch 3.484 (3.296) Remain 07:40:37 loss: 0.2075 Lr: 0.03014
[2023-08-08 02:05:51,560 INFO misc.py line 115 22900] Train: [47/100][39/156] Data 0.001 (0.001) Batch 2.551 (3.275) Remain 07:37:41 loss: 0.0855 Lr: 0.03013
[2023-08-08 02:05:55,048 INFO misc.py line 115 22900] Train: [47/100][40/156] Data 0.001 (0.001) Batch 3.488 (3.281) Remain 07:38:26 loss: 0.3069 Lr: 0.03013
[2023-08-08 02:05:58,235 INFO misc.py line 115 22900] Train: [47/100][41/156] Data 0.001 (0.001) Batch 3.186 (3.278) Remain 07:38:02 loss: 0.0984 Lr: 0.03012
[2023-08-08 02:06:02,428 INFO misc.py line 115 22900] Train: [47/100][42/156] Data 0.001 (0.001) Batch 4.193 (3.302) Remain 07:41:15 loss: 0.2947 Lr: 0.03012
[2023-08-08 02:06:06,550 INFO misc.py line 115 22900] Train: [47/100][43/156] Data 0.001 (0.001) Batch 4.122 (3.322) Remain 07:44:04 loss: 0.2534 Lr: 0.03011
[2023-08-08 02:06:10,025 INFO misc.py line 115 22900] Train: [47/100][44/156] Data 0.001 (0.001) Batch 3.475 (3.326) Remain 07:44:31 loss: 0.4993 Lr: 0.03010
[2023-08-08 02:06:14,018 INFO misc.py line 115 22900] Train: [47/100][45/156] Data 0.001 (0.001) Batch 3.993 (3.342) Remain 07:46:41 loss: 0.2574 Lr: 0.03010
[2023-08-08 02:06:17,953 INFO misc.py line 115 22900] Train: [47/100][46/156] Data 0.001 (0.001) Batch 3.934 (3.356) Remain 07:48:33 loss: 0.2383 Lr: 0.03009
[2023-08-08 02:06:21,918 INFO misc.py line 115 22900] Train: [47/100][47/156] Data 0.001 (0.001) Batch 3.966 (3.370) Remain 07:50:26 loss: 0.5255 Lr: 0.03009
[2023-08-08 02:06:25,628 INFO misc.py line 115 22900] Train: [47/100][48/156] Data 0.001 (0.001) Batch 3.709 (3.377) Remain 07:51:26 loss: 0.3826 Lr: 0.03008
[2023-08-08 02:06:28,727 INFO misc.py line 115 22900] Train: [47/100][49/156] Data 0.001 (0.001) Batch 3.099 (3.371) Remain 07:50:32 loss: 0.2175 Lr: 0.03008
[2023-08-08 02:06:32,382 INFO misc.py line 115 22900] Train: [47/100][50/156] Data 0.001 (0.001) Batch 3.655 (3.377) Remain 07:51:19 loss: 0.2544 Lr: 0.03007
[2023-08-08 02:06:36,471 INFO misc.py line 115 22900] Train: [47/100][51/156] Data 0.001 (0.001) Batch 4.089 (3.392) Remain 07:53:20 loss: 0.3987 Lr: 0.03007
[2023-08-08 02:06:39,441 INFO misc.py line 115 22900] Train: [47/100][52/156] Data 0.001 (0.001) Batch 2.970 (3.383) Remain 07:52:05 loss: 0.0851 Lr: 0.03006
[2023-08-08 02:06:42,272 INFO misc.py line 115 22900] Train: [47/100][53/156] Data 0.001 (0.001) Batch 2.831 (3.372) Remain 07:50:29 loss: 0.1884 Lr: 0.03006
[2023-08-08 02:06:46,018 INFO misc.py line 115 22900] Train: [47/100][54/156] Data 0.001 (0.001) Batch 3.746 (3.380) Remain 07:51:27 loss: 0.2332 Lr: 0.03005
[2023-08-08 02:06:48,470 INFO misc.py line 115 22900] Train: [47/100][55/156] Data 0.001 (0.001) Batch 2.452 (3.362) Remain 07:48:54 loss: 0.1748 Lr: 0.03005
[2023-08-08 02:06:51,728 INFO misc.py line 115 22900] Train: [47/100][56/156] Data 0.001 (0.001) Batch 3.259 (3.360) Remain 07:48:34 loss: 0.1674 Lr: 0.03004
[2023-08-08 02:06:55,716 INFO misc.py line 115 22900] Train: [47/100][57/156] Data 0.001 (0.001) Batch 3.988 (3.371) Remain 07:50:08 loss: 0.2017 Lr: 0.03004
[2023-08-08 02:06:59,488 INFO misc.py line 115 22900] Train: [47/100][58/156] Data 0.001 (0.001) Batch 3.772 (3.379) Remain 07:51:06 loss: 0.2551 Lr: 0.03003
[2023-08-08 02:07:03,057 INFO misc.py line 115 22900] Train: [47/100][59/156] Data 0.001 (0.001) Batch 3.569 (3.382) Remain 07:51:31 loss: 0.1320 Lr: 0.03003
[2023-08-08 02:07:07,107 INFO misc.py line 115 22900] Train: [47/100][60/156] Data 0.001 (0.001) Batch 4.051 (3.394) Remain 07:53:06 loss: 0.4125 Lr: 0.03002
[2023-08-08 02:07:11,452 INFO misc.py line 115 22900] Train: [47/100][61/156] Data 0.001 (0.001) Batch 4.344 (3.410) Remain 07:55:19 loss: 0.4038 Lr: 0.03002
[2023-08-08 02:07:14,744 INFO misc.py line 115 22900] Train: [47/100][62/156] Data 0.001 (0.001) Batch 3.293 (3.408) Remain 07:54:59 loss: 0.1516 Lr: 0.03001
[2023-08-08 02:07:17,754 INFO misc.py line 115 22900] Train: [47/100][63/156] Data 0.001 (0.001) Batch 3.010 (3.402) Remain 07:54:00 loss: 0.1999 Lr: 0.03001
[2023-08-08 02:07:21,141 INFO misc.py line 115 22900] Train: [47/100][64/156] Data 0.001 (0.001) Batch 3.387 (3.401) Remain 07:53:55 loss: 0.4129 Lr: 0.03000
[2023-08-08 02:07:25,047 INFO misc.py line 115 22900] Train: [47/100][65/156] Data 0.001 (0.001) Batch 3.906 (3.409) Remain 07:54:59 loss: 0.3131 Lr: 0.03000
[2023-08-08 02:07:27,805 INFO misc.py line 115 22900] Train: [47/100][66/156] Data 0.001 (0.001) Batch 2.758 (3.399) Remain 07:53:30 loss: 0.2600 Lr: 0.02999
[2023-08-08 02:07:32,519 INFO misc.py line 115 22900] Train: [47/100][67/156] Data 0.001 (0.001) Batch 4.714 (3.420) Remain 07:56:18 loss: 0.4079 Lr: 0.02999
[2023-08-08 02:07:36,505 INFO misc.py line 115 22900] Train: [47/100][68/156] Data 0.001 (0.001) Batch 3.986 (3.428) Remain 07:57:27 loss: 0.2750 Lr: 0.02998
[2023-08-08 02:07:40,621 INFO misc.py line 115 22900] Train: [47/100][69/156] Data 0.001 (0.001) Batch 4.116 (3.439) Remain 07:58:51 loss: 0.1182 Lr: 0.02997
[2023-08-08 02:07:44,117 INFO misc.py line 115 22900] Train: [47/100][70/156] Data 0.001 (0.001) Batch 3.496 (3.440) Remain 07:58:55 loss: 0.3795 Lr: 0.02997
[2023-08-08 02:07:48,057 INFO misc.py line 115 22900] Train: [47/100][71/156] Data 0.001 (0.001) Batch 3.940 (3.447) Remain 07:59:53 loss: 0.4903 Lr: 0.02996
[2023-08-08 02:07:51,656 INFO misc.py line 115 22900] Train: [47/100][72/156] Data 0.001 (0.001) Batch 3.599 (3.449) Remain 08:00:08 loss: 0.2704 Lr: 0.02996
[2023-08-08 02:07:55,678 INFO misc.py line 115 22900] Train: [47/100][73/156] Data 0.001 (0.001) Batch 4.022 (3.457) Remain 08:01:12 loss: 0.3391 Lr: 0.02995
[2023-08-08 02:07:58,792 INFO misc.py line 115 22900] Train: [47/100][74/156] Data 0.001 (0.001) Batch 3.115 (3.453) Remain 08:00:29 loss: 0.3150 Lr: 0.02995
[2023-08-08 02:08:01,375 INFO misc.py line 115 22900] Train: [47/100][75/156] Data 0.001 (0.001) Batch 2.583 (3.441) Remain 07:58:44 loss: 0.2415 Lr: 0.02994
[2023-08-08 02:08:03,782 INFO misc.py line 115 22900] Train: [47/100][76/156] Data 0.001 (0.001) Batch 2.407 (3.426) Remain 07:56:43 loss: 0.1592 Lr: 0.02994
[2023-08-08 02:08:07,812 INFO misc.py line 115 22900] Train: [47/100][77/156] Data 0.001 (0.001) Batch 4.030 (3.435) Remain 07:57:47 loss: 0.2873 Lr: 0.02993
[2023-08-08 02:08:11,880 INFO misc.py line 115 22900] Train: [47/100][78/156] Data 0.001 (0.001) Batch 4.068 (3.443) Remain 07:58:54 loss: 0.3052 Lr: 0.02993
[2023-08-08 02:08:15,485 INFO misc.py line 115 22900] Train: [47/100][79/156] Data 0.001 (0.001) Batch 3.605 (3.445) Remain 07:59:09 loss: 0.1805 Lr: 0.02992
[2023-08-08 02:08:19,253 INFO misc.py line 115 22900] Train: [47/100][80/156] Data 0.001 (0.001) Batch 3.768 (3.449) Remain 07:59:40 loss: 0.2107 Lr: 0.02992
[2023-08-08 02:08:22,530 INFO misc.py line 115 22900] Train: [47/100][81/156] Data 0.001 (0.001) Batch 3.277 (3.447) Remain 07:59:18 loss: 0.2523 Lr: 0.02991
[2023-08-08 02:08:27,123 INFO misc.py line 115 22900] Train: [47/100][82/156] Data 0.001 (0.001) Batch 4.593 (3.462) Remain 08:01:16 loss: 0.6284 Lr: 0.02991
[2023-08-08 02:08:30,545 INFO misc.py line 115 22900] Train: [47/100][83/156] Data 0.001 (0.001) Batch 3.421 (3.461) Remain 08:01:08 loss: 0.1437 Lr: 0.02990
[2023-08-08 02:08:34,504 INFO misc.py line 115 22900] Train: [47/100][84/156] Data 0.001 (0.001) Batch 3.959 (3.467) Remain 08:01:56 loss: 0.3322 Lr: 0.02990
[2023-08-08 02:08:38,192 INFO misc.py line 115 22900] Train: [47/100][85/156] Data 0.001 (0.001) Batch 3.688 (3.470) Remain 08:02:15 loss: 0.1956 Lr: 0.02989
[2023-08-08 02:08:42,306 INFO misc.py line 115 22900] Train: [47/100][86/156] Data 0.001 (0.001) Batch 4.114 (3.478) Remain 08:03:16 loss: 0.5481 Lr: 0.02989
[2023-08-08 02:08:45,803 INFO misc.py line 115 22900] Train: [47/100][87/156] Data 0.001 (0.001) Batch 3.497 (3.478) Remain 08:03:15 loss: 0.1119 Lr: 0.02988
[2023-08-08 02:08:49,052 INFO misc.py line 115 22900] Train: [47/100][88/156] Data 0.001 (0.001) Batch 3.249 (3.475) Remain 08:02:49 loss: 0.3045 Lr: 0.02988
[2023-08-08 02:08:51,861 INFO misc.py line 115 22900] Train: [47/100][89/156] Data 0.001 (0.001) Batch 2.809 (3.467) Remain 08:01:41 loss: 0.1370 Lr: 0.02987
[2023-08-08 02:08:55,958 INFO misc.py line 115 22900] Train: [47/100][90/156] Data 0.001 (0.001) Batch 4.097 (3.475) Remain 08:02:38 loss: 0.2805 Lr: 0.02987
[2023-08-08 02:09:00,051 INFO misc.py line 115 22900] Train: [47/100][91/156] Data 0.001 (0.001) Batch 4.094 (3.482) Remain 08:03:33 loss: 0.3090 Lr: 0.02986
[2023-08-08 02:09:04,545 INFO misc.py line 115 22900] Train: [47/100][92/156] Data 0.001 (0.001) Batch 4.494 (3.493) Remain 08:05:04 loss: 0.4433 Lr: 0.02986
[2023-08-08 02:09:07,637 INFO misc.py line 115 22900] Train: [47/100][93/156] Data 0.001 (0.001) Batch 3.092 (3.489) Remain 08:04:23 loss: 0.1605 Lr: 0.02985
[2023-08-08 02:09:10,597 INFO misc.py line 115 22900] Train: [47/100][94/156] Data 0.001 (0.001) Batch 2.960 (3.483) Remain 08:03:32 loss: 0.1480 Lr: 0.02984
[2023-08-08 02:09:13,731 INFO misc.py line 115 22900] Train: [47/100][95/156] Data 0.001 (0.001) Batch 3.134 (3.479) Remain 08:02:57 loss: 0.1849 Lr: 0.02984
[2023-08-08 02:09:17,260 INFO misc.py line 115 22900] Train: [47/100][96/156] Data 0.001 (0.001) Batch 3.528 (3.480) Remain 08:02:57 loss: 0.7065 Lr: 0.02983
[2023-08-08 02:09:20,711 INFO misc.py line 115 22900] Train: [47/100][97/156] Data 0.001 (0.001) Batch 3.451 (3.479) Remain 08:02:51 loss: 0.3522 Lr: 0.02983
[2023-08-08 02:09:24,397 INFO misc.py line 115 22900] Train: [47/100][98/156] Data 0.001 (0.001) Batch 3.686 (3.481) Remain 08:03:06 loss: 0.3398 Lr: 0.02982
[2023-08-08 02:09:26,996 INFO misc.py line 115 22900] Train: [47/100][99/156] Data 0.001 (0.001) Batch 2.599 (3.472) Remain 08:01:46 loss: 0.2331 Lr: 0.02982
[2023-08-08 02:09:30,615 INFO misc.py line 115 22900] Train: [47/100][100/156] Data 0.001 (0.001) Batch 3.620 (3.474) Remain 08:01:55 loss: 0.1611 Lr: 0.02981
[2023-08-08 02:09:33,271 INFO misc.py line 115 22900] Train: [47/100][101/156] Data 0.001 (0.001) Batch 2.656 (3.465) Remain 08:00:42 loss: 0.2017 Lr: 0.02981
[2023-08-08 02:09:36,923 INFO misc.py line 115 22900] Train: [47/100][102/156] Data 0.001 (0.001) Batch 3.652 (3.467) Remain 08:00:55 loss: 0.2977 Lr: 0.02980
[2023-08-08 02:09:40,629 INFO misc.py line 115 22900] Train: [47/100][103/156] Data 0.001 (0.001) Batch 3.706 (3.470) Remain 08:01:11 loss: 0.2466 Lr: 0.02980
[2023-08-08 02:09:44,742 INFO misc.py line 115 22900] Train: [47/100][104/156] Data 0.001 (0.001) Batch 4.113 (3.476) Remain 08:02:00 loss: 0.4314 Lr: 0.02979
[2023-08-08 02:09:48,763 INFO misc.py line 115 22900] Train: [47/100][105/156] Data 0.001 (0.001) Batch 4.020 (3.481) Remain 08:02:41 loss: 0.4476 Lr: 0.02979
[2023-08-08 02:09:52,866 INFO misc.py line 115 22900] Train: [47/100][106/156] Data 0.001 (0.001) Batch 4.103 (3.487) Remain 08:03:28 loss: 0.3193 Lr: 0.02978
[2023-08-08 02:09:54,631 INFO misc.py line 115 22900] Train: [47/100][107/156] Data 0.001 (0.001) Batch 1.765 (3.471) Remain 08:01:07 loss: 0.0837 Lr: 0.02978
[2023-08-08 02:09:57,472 INFO misc.py line 115 22900] Train: [47/100][108/156] Data 0.001 (0.001) Batch 2.840 (3.465) Remain 08:00:13 loss: 0.1430 Lr: 0.02977
[2023-08-08 02:10:01,466 INFO misc.py line 115 22900] Train: [47/100][109/156] Data 0.001 (0.001) Batch 3.994 (3.470) Remain 08:00:52 loss: 0.4735 Lr: 0.02977
[2023-08-08 02:10:04,638 INFO misc.py line 115 22900] Train: [47/100][110/156] Data 0.001 (0.001) Batch 3.172 (3.467) Remain 08:00:25 loss: 0.4678 Lr: 0.02976
[2023-08-08 02:10:08,104 INFO misc.py line 115 22900] Train: [47/100][111/156] Data 0.001 (0.001) Batch 3.466 (3.467) Remain 08:00:21 loss: 0.3590 Lr: 0.02976
[2023-08-08 02:10:11,298 INFO misc.py line 115 22900] Train: [47/100][112/156] Data 0.001 (0.001) Batch 3.193 (3.465) Remain 07:59:57 loss: 0.2242 Lr: 0.02975
[2023-08-08 02:10:14,170 INFO misc.py line 115 22900] Train: [47/100][113/156] Data 0.001 (0.001) Batch 2.872 (3.459) Remain 07:59:09 loss: 0.2789 Lr: 0.02975
[2023-08-08 02:10:18,436 INFO misc.py line 115 22900] Train: [47/100][114/156] Data 0.001 (0.001) Batch 4.266 (3.466) Remain 08:00:06 loss: 0.4101 Lr: 0.02974
[2023-08-08 02:10:21,703 INFO misc.py line 115 22900] Train: [47/100][115/156] Data 0.001 (0.001) Batch 3.267 (3.465) Remain 07:59:48 loss: 0.2325 Lr: 0.02974
[2023-08-08 02:10:24,067 INFO misc.py line 115 22900] Train: [47/100][116/156] Data 0.001 (0.001) Batch 2.364 (3.455) Remain 07:58:23 loss: 0.1329 Lr: 0.02973
[2023-08-08 02:10:28,634 INFO misc.py line 115 22900] Train: [47/100][117/156] Data 0.001 (0.001) Batch 4.567 (3.465) Remain 07:59:41 loss: 0.5743 Lr: 0.02973
[2023-08-08 02:10:32,616 INFO misc.py line 115 22900] Train: [47/100][118/156] Data 0.001 (0.001) Batch 3.982 (3.469) Remain 08:00:15 loss: 0.2916 Lr: 0.02972
[2023-08-08 02:10:35,631 INFO misc.py line 115 22900] Train: [47/100][119/156] Data 0.001 (0.001) Batch 3.015 (3.465) Remain 07:59:39 loss: 0.2021 Lr: 0.02971
[2023-08-08 02:10:39,361 INFO misc.py line 115 22900] Train: [47/100][120/156] Data 0.001 (0.001) Batch 3.730 (3.468) Remain 07:59:54 loss: 0.3673 Lr: 0.02971
[2023-08-08 02:10:43,765 INFO misc.py line 115 22900] Train: [47/100][121/156] Data 0.001 (0.001) Batch 4.404 (3.475) Remain 08:00:56 loss: 0.4170 Lr: 0.02970
[2023-08-08 02:10:46,944 INFO misc.py line 115 22900] Train: [47/100][122/156] Data 0.001 (0.001) Batch 3.180 (3.473) Remain 08:00:32 loss: 0.3631 Lr: 0.02970
[2023-08-08 02:10:50,939 INFO misc.py line 115 22900] Train: [47/100][123/156] Data 0.001 (0.001) Batch 3.994 (3.477) Remain 08:01:05 loss: 0.4039 Lr: 0.02969
[2023-08-08 02:10:55,516 INFO misc.py line 115 22900] Train: [47/100][124/156] Data 0.001 (0.001) Batch 4.577 (3.486) Remain 08:02:17 loss: 0.4137 Lr: 0.02969
[2023-08-08 02:10:58,809 INFO misc.py line 115 22900] Train: [47/100][125/156] Data 0.001 (0.001) Batch 3.293 (3.485) Remain 08:02:00 loss: 0.1149 Lr: 0.02968
[2023-08-08 02:11:01,514 INFO misc.py line 115 22900] Train: [47/100][126/156] Data 0.001 (0.001) Batch 2.705 (3.478) Remain 08:01:04 loss: 0.1633 Lr: 0.02968
[2023-08-08 02:11:05,233 INFO misc.py line 115 22900] Train: [47/100][127/156] Data 0.001 (0.001) Batch 3.719 (3.480) Remain 08:01:17 loss: 0.2541 Lr: 0.02967
[2023-08-08 02:11:08,844 INFO misc.py line 115 22900] Train: [47/100][128/156] Data 0.001 (0.001) Batch 3.611 (3.481) Remain 08:01:22 loss: 0.3567 Lr: 0.02967
[2023-08-08 02:11:12,848 INFO misc.py line 115 22900] Train: [47/100][129/156] Data 0.001 (0.001) Batch 4.003 (3.486) Remain 08:01:53 loss: 0.1771 Lr: 0.02966
[2023-08-08 02:11:16,933 INFO misc.py line 115 22900] Train: [47/100][130/156] Data 0.001 (0.001) Batch 4.085 (3.490) Remain 08:02:28 loss: 0.2305 Lr: 0.02966
[2023-08-08 02:11:19,967 INFO misc.py line 115 22900] Train: [47/100][131/156] Data 0.001 (0.001) Batch 3.034 (3.487) Remain 08:01:55 loss: 0.0800 Lr: 0.02965
[2023-08-08 02:11:23,179 INFO misc.py line 115 22900] Train: [47/100][132/156] Data 0.001 (0.001) Batch 3.212 (3.485) Remain 08:01:34 loss: 0.3341 Lr: 0.02965
[2023-08-08 02:11:26,588 INFO misc.py line 115 22900] Train: [47/100][133/156] Data 0.001 (0.001) Batch 3.409 (3.484) Remain 08:01:26 loss: 0.1793 Lr: 0.02964
[2023-08-08 02:11:29,958 INFO misc.py line 115 22900] Train: [47/100][134/156] Data 0.001 (0.001) Batch 3.370 (3.483) Remain 08:01:15 loss: 0.1751 Lr: 0.02964
[2023-08-08 02:11:34,051 INFO misc.py line 115 22900] Train: [47/100][135/156] Data 0.001 (0.001) Batch 4.093 (3.488) Remain 08:01:50 loss: 0.2844 Lr: 0.02963
[2023-08-08 02:11:37,683 INFO misc.py line 115 22900] Train: [47/100][136/156] Data 0.001 (0.001) Batch 3.632 (3.489) Remain 08:01:56 loss: 0.4517 Lr: 0.02963
[2023-08-08 02:11:40,179 INFO misc.py line 115 22900] Train: [47/100][137/156] Data 0.001 (0.001) Batch 2.496 (3.481) Remain 08:00:51 loss: 0.0805 Lr: 0.02962
[2023-08-08 02:11:42,062 INFO misc.py line 115 22900] Train: [47/100][138/156] Data 0.001 (0.001) Batch 1.883 (3.470) Remain 07:59:09 loss: 0.1155 Lr: 0.02962
[2023-08-08 02:11:45,110 INFO misc.py line 115 22900] Train: [47/100][139/156] Data 0.001 (0.001) Batch 3.048 (3.467) Remain 07:58:40 loss: 0.1757 Lr: 0.02961
[2023-08-08 02:11:48,951 INFO misc.py line 115 22900] Train: [47/100][140/156] Data 0.001 (0.001) Batch 3.841 (3.469) Remain 07:58:59 loss: 0.2604 Lr: 0.02961
[2023-08-08 02:11:52,591 INFO misc.py line 115 22900] Train: [47/100][141/156] Data 0.001 (0.001) Batch 3.640 (3.471) Remain 07:59:06 loss: 0.1434 Lr: 0.02960
[2023-08-08 02:11:56,101 INFO misc.py line 115 22900] Train: [47/100][142/156] Data 0.001 (0.001) Batch 3.510 (3.471) Remain 07:59:05 loss: 0.2092 Lr: 0.02960
[2023-08-08 02:11:59,885 INFO misc.py line 115 22900] Train: [47/100][143/156] Data 0.001 (0.001) Batch 3.784 (3.473) Remain 07:59:20 loss: 0.1291 Lr: 0.02959
[2023-08-08 02:12:03,937 INFO misc.py line 115 22900] Train: [47/100][144/156] Data 0.001 (0.001) Batch 4.052 (3.477) Remain 07:59:50 loss: 0.3983 Lr: 0.02958
[2023-08-08 02:12:06,498 INFO misc.py line 115 22900] Train: [47/100][145/156] Data 0.001 (0.001) Batch 2.562 (3.471) Remain 07:58:53 loss: 0.1338 Lr: 0.02958
[2023-08-08 02:12:10,204 INFO misc.py line 115 22900] Train: [47/100][146/156] Data 0.001 (0.001) Batch 3.706 (3.472) Remain 07:59:04 loss: 0.2799 Lr: 0.02957
[2023-08-08 02:12:13,616 INFO misc.py line 115 22900] Train: [47/100][147/156] Data 0.001 (0.001) Batch 3.412 (3.472) Remain 07:58:57 loss: 0.2599 Lr: 0.02957
[2023-08-08 02:12:17,349 INFO misc.py line 115 22900] Train: [47/100][148/156] Data 0.001 (0.001) Batch 3.733 (3.474) Remain 07:59:08 loss: 0.3025 Lr: 0.02956
[2023-08-08 02:12:19,944 INFO misc.py line 115 22900] Train: [47/100][149/156] Data 0.001 (0.001) Batch 2.595 (3.468) Remain 07:58:15 loss: 0.1951 Lr: 0.02956
[2023-08-08 02:12:21,740 INFO misc.py line 115 22900] Train: [47/100][150/156] Data 0.001 (0.001) Batch 1.796 (3.456) Remain 07:56:37 loss: 0.1735 Lr: 0.02955
[2023-08-08 02:12:24,580 INFO misc.py line 115 22900] Train: [47/100][151/156] Data 0.001 (0.001) Batch 2.840 (3.452) Remain 07:55:59 loss: 0.1898 Lr: 0.02955
[2023-08-08 02:12:27,343 INFO misc.py line 115 22900] Train: [47/100][152/156] Data 0.001 (0.001) Batch 2.763 (3.448) Remain 07:55:18 loss: 0.3200 Lr: 0.02954
[2023-08-08 02:12:31,309 INFO misc.py line 115 22900] Train: [47/100][153/156] Data 0.001 (0.001) Batch 3.966 (3.451) Remain 07:55:43 loss: 0.5752 Lr: 0.02954
[2023-08-08 02:12:35,342 INFO misc.py line 115 22900] Train: [47/100][154/156] Data 0.001 (0.001) Batch 4.033 (3.455) Remain 07:56:11 loss: 0.3750 Lr: 0.02953
[2023-08-08 02:12:39,082 INFO misc.py line 115 22900] Train: [47/100][155/156] Data 0.001 (0.001) Batch 3.740 (3.457) Remain 07:56:23 loss: 0.3988 Lr: 0.02953
[2023-08-08 02:12:43,484 INFO misc.py line 115 22900] Train: [47/100][156/156] Data 0.001 (0.001) Batch 4.402 (3.463) Remain 07:57:11 loss: 0.3032 Lr: 0.02952
[2023-08-08 02:12:43,484 INFO misc.py line 129 22900] Train result: loss: 0.2702 
[2023-08-08 02:12:43,484 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 02:12:45,592 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.3928 
[2023-08-08 02:12:46,460 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.6204 
[2023-08-08 02:12:48,126 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.6280 
[2023-08-08 02:12:49,650 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.5773 
[2023-08-08 02:12:51,491 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.9176 
[2023-08-08 02:12:53,154 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.8841 
[2023-08-08 02:12:55,290 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.3393 
[2023-08-08 02:12:57,093 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.3692 
[2023-08-08 02:12:58,375 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.7237 
[2023-08-08 02:13:00,510 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.4811 
[2023-08-08 02:13:01,035 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.1511 
[2023-08-08 02:13:02,568 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.7961 
[2023-08-08 02:13:05,280 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.3705 
[2023-08-08 02:13:06,961 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.7181 
[2023-08-08 02:13:08,986 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.4038 
[2023-08-08 02:13:11,698 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.3922 
[2023-08-08 02:13:14,406 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.7594 
[2023-08-08 02:13:16,252 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.5846 
[2023-08-08 02:13:17,000 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.3516 
[2023-08-08 02:13:17,886 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.7802 
[2023-08-08 02:13:20,145 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.4573 
[2023-08-08 02:13:22,109 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.9160 
[2023-08-08 02:13:23,955 INFO evaluator.py line 122 22900] Test: [23/24] Loss 3.1271 
[2023-08-08 02:13:25,893 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.9833 
[2023-08-08 02:13:25,941 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2113/0.2949/0.6743.
[2023-08-08 02:13:25,941 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6081/0.9792
[2023-08-08 02:13:25,941 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9333/0.9924
[2023-08-08 02:13:25,941 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1597/0.3898
[2023-08-08 02:13:25,941 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1231/0.2416
[2023-08-08 02:13:25,941 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.5617/0.5861
[2023-08-08 02:13:25,941 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.3484/0.6469
[2023-08-08 02:13:25,941 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5079/0.5766
[2023-08-08 02:13:25,941 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1155/0.1208
[2023-08-08 02:13:25,941 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1340/0.2823
[2023-08-08 02:13:25,941 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0230/0.0231
[2023-08-08 02:13:25,941 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 02:13:25,941 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0077/0.0080
[2023-08-08 02:13:25,941 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.2721/0.5125
[2023-08-08 02:13:25,941 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0376/0.0406
[2023-08-08 02:13:25,941 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0015/0.0017
[2023-08-08 02:13:25,941 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-08 02:13:25,941 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.1344/0.1470
[2023-08-08 02:13:25,941 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.1648/0.1798
[2023-08-08 02:13:25,941 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 02:13:25,941 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0937/0.1688
[2023-08-08 02:13:25,941 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 02:13:25,942 INFO misc.py line 152 22900] Currently Best mIoU: 0.2562
[2023-08-08 02:13:25,942 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 02:13:29,992 INFO misc.py line 115 22900] Train: [48/100][1/156] Data 0.623 (0.623) Batch 3.273 (3.273) Remain 07:30:59 loss: 0.2020 Lr: 0.02952
[2023-08-08 02:13:33,066 INFO misc.py line 115 22900] Train: [48/100][2/156] Data 0.001 (0.001) Batch 3.074 (3.074) Remain 07:03:26 loss: 0.2891 Lr: 0.02951
[2023-08-08 02:13:36,472 INFO misc.py line 115 22900] Train: [48/100][3/156] Data 0.001 (0.001) Batch 3.406 (3.406) Remain 07:49:11 loss: 0.1018 Lr: 0.02951
[2023-08-08 02:13:40,485 INFO misc.py line 115 22900] Train: [48/100][4/156] Data 0.001 (0.001) Batch 4.013 (4.013) Remain 09:12:44 loss: 0.4045 Lr: 0.02950
[2023-08-08 02:13:43,657 INFO misc.py line 115 22900] Train: [48/100][5/156] Data 0.001 (0.001) Batch 3.172 (3.593) Remain 08:14:44 loss: 0.2436 Lr: 0.02950
[2023-08-08 02:13:46,936 INFO misc.py line 115 22900] Train: [48/100][6/156] Data 0.001 (0.001) Batch 3.279 (3.488) Remain 08:00:18 loss: 0.1869 Lr: 0.02949
[2023-08-08 02:13:49,664 INFO misc.py line 115 22900] Train: [48/100][7/156] Data 0.001 (0.001) Batch 2.728 (3.298) Remain 07:34:05 loss: 0.2550 Lr: 0.02949
[2023-08-08 02:13:53,560 INFO misc.py line 115 22900] Train: [48/100][8/156] Data 0.001 (0.001) Batch 3.896 (3.418) Remain 07:50:29 loss: 0.4849 Lr: 0.02948
[2023-08-08 02:13:57,399 INFO misc.py line 115 22900] Train: [48/100][9/156] Data 0.001 (0.001) Batch 3.839 (3.488) Remain 08:00:06 loss: 0.2994 Lr: 0.02948
[2023-08-08 02:14:00,776 INFO misc.py line 115 22900] Train: [48/100][10/156] Data 0.001 (0.001) Batch 3.377 (3.472) Remain 07:57:52 loss: 0.2797 Lr: 0.02947
[2023-08-08 02:14:04,865 INFO misc.py line 115 22900] Train: [48/100][11/156] Data 0.001 (0.001) Batch 4.089 (3.549) Remain 08:08:25 loss: 0.5270 Lr: 0.02946
[2023-08-08 02:14:08,727 INFO misc.py line 115 22900] Train: [48/100][12/156] Data 0.001 (0.001) Batch 3.862 (3.584) Remain 08:13:08 loss: 0.2451 Lr: 0.02946
[2023-08-08 02:14:12,546 INFO misc.py line 115 22900] Train: [48/100][13/156] Data 0.001 (0.001) Batch 3.819 (3.607) Remain 08:16:19 loss: 0.2432 Lr: 0.02945
[2023-08-08 02:14:15,094 INFO misc.py line 115 22900] Train: [48/100][14/156] Data 0.001 (0.001) Batch 2.548 (3.511) Remain 08:03:00 loss: 0.0716 Lr: 0.02945
[2023-08-08 02:14:18,412 INFO misc.py line 115 22900] Train: [48/100][15/156] Data 0.001 (0.001) Batch 3.318 (3.495) Remain 08:00:44 loss: 0.1486 Lr: 0.02944
[2023-08-08 02:14:21,662 INFO misc.py line 115 22900] Train: [48/100][16/156] Data 0.001 (0.001) Batch 3.251 (3.476) Remain 07:58:05 loss: 0.2290 Lr: 0.02944
[2023-08-08 02:14:25,633 INFO misc.py line 115 22900] Train: [48/100][17/156] Data 0.001 (0.001) Batch 3.971 (3.512) Remain 08:02:53 loss: 0.3645 Lr: 0.02943
[2023-08-08 02:14:27,495 INFO misc.py line 115 22900] Train: [48/100][18/156] Data 0.001 (0.001) Batch 1.861 (3.402) Remain 07:47:42 loss: 0.3058 Lr: 0.02943
[2023-08-08 02:14:30,987 INFO misc.py line 115 22900] Train: [48/100][19/156] Data 0.001 (0.001) Batch 3.493 (3.407) Remain 07:48:26 loss: 0.2518 Lr: 0.02942
[2023-08-08 02:14:34,357 INFO misc.py line 115 22900] Train: [48/100][20/156] Data 0.001 (0.001) Batch 3.369 (3.405) Remain 07:48:04 loss: 0.2005 Lr: 0.02942
[2023-08-08 02:14:37,527 INFO misc.py line 115 22900] Train: [48/100][21/156] Data 0.001 (0.001) Batch 3.170 (3.392) Remain 07:46:13 loss: 0.2713 Lr: 0.02941
[2023-08-08 02:14:40,672 INFO misc.py line 115 22900] Train: [48/100][22/156] Data 0.001 (0.001) Batch 3.145 (3.379) Remain 07:44:22 loss: 0.2337 Lr: 0.02941
[2023-08-08 02:14:43,209 INFO misc.py line 115 22900] Train: [48/100][23/156] Data 0.001 (0.001) Batch 2.537 (3.337) Remain 07:38:32 loss: 0.3075 Lr: 0.02940
[2023-08-08 02:14:46,076 INFO misc.py line 115 22900] Train: [48/100][24/156] Data 0.001 (0.001) Batch 2.866 (3.314) Remain 07:35:24 loss: 0.2044 Lr: 0.02940
[2023-08-08 02:14:49,798 INFO misc.py line 115 22900] Train: [48/100][25/156] Data 0.001 (0.001) Batch 3.722 (3.333) Remain 07:37:53 loss: 0.2441 Lr: 0.02939
[2023-08-08 02:14:52,540 INFO misc.py line 115 22900] Train: [48/100][26/156] Data 0.001 (0.001) Batch 2.743 (3.307) Remain 07:34:18 loss: 0.1928 Lr: 0.02939
[2023-08-08 02:14:56,770 INFO misc.py line 115 22900] Train: [48/100][27/156] Data 0.001 (0.001) Batch 4.230 (3.346) Remain 07:39:32 loss: 0.3547 Lr: 0.02938
[2023-08-08 02:15:00,475 INFO misc.py line 115 22900] Train: [48/100][28/156] Data 0.001 (0.001) Batch 3.705 (3.360) Remain 07:41:27 loss: 0.2208 Lr: 0.02938
[2023-08-08 02:15:04,521 INFO misc.py line 115 22900] Train: [48/100][29/156] Data 0.001 (0.001) Batch 4.046 (3.387) Remain 07:45:01 loss: 0.3092 Lr: 0.02937
[2023-08-08 02:15:06,865 INFO misc.py line 115 22900] Train: [48/100][30/156] Data 0.001 (0.001) Batch 2.344 (3.348) Remain 07:39:40 loss: 0.0945 Lr: 0.02937
[2023-08-08 02:15:10,711 INFO misc.py line 115 22900] Train: [48/100][31/156] Data 0.001 (0.001) Batch 3.846 (3.366) Remain 07:42:03 loss: 0.1548 Lr: 0.02936
[2023-08-08 02:15:14,032 INFO misc.py line 115 22900] Train: [48/100][32/156] Data 0.001 (0.001) Batch 3.321 (3.364) Remain 07:41:47 loss: 0.2126 Lr: 0.02936
[2023-08-08 02:15:18,101 INFO misc.py line 115 22900] Train: [48/100][33/156] Data 0.001 (0.001) Batch 4.069 (3.388) Remain 07:44:57 loss: 0.2813 Lr: 0.02935
[2023-08-08 02:15:22,240 INFO misc.py line 115 22900] Train: [48/100][34/156] Data 0.001 (0.001) Batch 4.139 (3.412) Remain 07:48:13 loss: 0.2383 Lr: 0.02934
[2023-08-08 02:15:25,712 INFO misc.py line 115 22900] Train: [48/100][35/156] Data 0.001 (0.001) Batch 3.472 (3.414) Remain 07:48:25 loss: 0.2562 Lr: 0.02934
[2023-08-08 02:15:29,726 INFO misc.py line 115 22900] Train: [48/100][36/156] Data 0.001 (0.001) Batch 4.013 (3.432) Remain 07:50:51 loss: 0.2141 Lr: 0.02933
[2023-08-08 02:15:33,375 INFO misc.py line 115 22900] Train: [48/100][37/156] Data 0.001 (0.001) Batch 3.650 (3.438) Remain 07:51:40 loss: 0.2896 Lr: 0.02933
[2023-08-08 02:15:36,440 INFO misc.py line 115 22900] Train: [48/100][38/156] Data 0.001 (0.001) Batch 3.065 (3.428) Remain 07:50:09 loss: 0.2432 Lr: 0.02932
[2023-08-08 02:15:39,465 INFO misc.py line 115 22900] Train: [48/100][39/156] Data 0.001 (0.001) Batch 3.024 (3.416) Remain 07:48:34 loss: 0.2128 Lr: 0.02932
[2023-08-08 02:15:43,083 INFO misc.py line 115 22900] Train: [48/100][40/156] Data 0.001 (0.001) Batch 3.619 (3.422) Remain 07:49:15 loss: 0.4855 Lr: 0.02931
[2023-08-08 02:15:47,049 INFO misc.py line 115 22900] Train: [48/100][41/156] Data 0.001 (0.001) Batch 3.965 (3.436) Remain 07:51:09 loss: 0.2740 Lr: 0.02931
[2023-08-08 02:15:49,805 INFO misc.py line 115 22900] Train: [48/100][42/156] Data 0.001 (0.001) Batch 2.757 (3.419) Remain 07:48:43 loss: 0.2929 Lr: 0.02930
[2023-08-08 02:15:53,479 INFO misc.py line 115 22900] Train: [48/100][43/156] Data 0.001 (0.001) Batch 3.674 (3.425) Remain 07:49:32 loss: 0.3167 Lr: 0.02930
[2023-08-08 02:15:57,134 INFO misc.py line 115 22900] Train: [48/100][44/156] Data 0.002 (0.001) Batch 3.654 (3.431) Remain 07:50:14 loss: 0.6306 Lr: 0.02929
[2023-08-08 02:16:01,273 INFO misc.py line 115 22900] Train: [48/100][45/156] Data 0.001 (0.001) Batch 4.139 (3.448) Remain 07:52:30 loss: 0.1697 Lr: 0.02929
[2023-08-08 02:16:05,301 INFO misc.py line 115 22900] Train: [48/100][46/156] Data 0.001 (0.001) Batch 4.028 (3.461) Remain 07:54:17 loss: 0.2975 Lr: 0.02928
[2023-08-08 02:16:08,459 INFO misc.py line 115 22900] Train: [48/100][47/156] Data 0.001 (0.001) Batch 3.158 (3.454) Remain 07:53:17 loss: 0.2805 Lr: 0.02928
[2023-08-08 02:16:11,890 INFO misc.py line 115 22900] Train: [48/100][48/156] Data 0.001 (0.001) Batch 3.431 (3.454) Remain 07:53:09 loss: 0.1992 Lr: 0.02927
[2023-08-08 02:16:15,728 INFO misc.py line 115 22900] Train: [48/100][49/156] Data 0.001 (0.001) Batch 3.838 (3.462) Remain 07:54:14 loss: 0.1763 Lr: 0.02927
[2023-08-08 02:16:19,254 INFO misc.py line 115 22900] Train: [48/100][50/156] Data 0.001 (0.001) Batch 3.526 (3.463) Remain 07:54:22 loss: 0.3450 Lr: 0.02926
[2023-08-08 02:16:23,984 INFO misc.py line 115 22900] Train: [48/100][51/156] Data 0.001 (0.001) Batch 4.730 (3.490) Remain 07:57:56 loss: 0.3136 Lr: 0.02926
[2023-08-08 02:16:28,122 INFO misc.py line 115 22900] Train: [48/100][52/156] Data 0.001 (0.001) Batch 4.138 (3.503) Remain 07:59:41 loss: 0.1864 Lr: 0.02925
[2023-08-08 02:16:32,243 INFO misc.py line 115 22900] Train: [48/100][53/156] Data 0.001 (0.001) Batch 4.120 (3.515) Remain 08:01:19 loss: 0.2994 Lr: 0.02925
[2023-08-08 02:16:34,200 INFO misc.py line 115 22900] Train: [48/100][54/156] Data 0.001 (0.001) Batch 1.958 (3.485) Remain 07:57:04 loss: 0.3460 Lr: 0.02924
[2023-08-08 02:16:37,823 INFO misc.py line 115 22900] Train: [48/100][55/156] Data 0.001 (0.001) Batch 3.622 (3.488) Remain 07:57:22 loss: 0.1605 Lr: 0.02924
[2023-08-08 02:16:41,924 INFO misc.py line 115 22900] Train: [48/100][56/156] Data 0.001 (0.001) Batch 4.101 (3.499) Remain 07:58:54 loss: 0.2445 Lr: 0.02923
[2023-08-08 02:16:45,363 INFO misc.py line 115 22900] Train: [48/100][57/156] Data 0.001 (0.001) Batch 3.439 (3.498) Remain 07:58:41 loss: 0.3410 Lr: 0.02922
[2023-08-08 02:16:48,425 INFO misc.py line 115 22900] Train: [48/100][58/156] Data 0.001 (0.001) Batch 3.062 (3.490) Remain 07:57:33 loss: 0.3999 Lr: 0.02922
[2023-08-08 02:16:51,424 INFO misc.py line 115 22900] Train: [48/100][59/156] Data 0.001 (0.001) Batch 2.999 (3.481) Remain 07:56:17 loss: 0.1986 Lr: 0.02921
[2023-08-08 02:16:55,394 INFO misc.py line 115 22900] Train: [48/100][60/156] Data 0.001 (0.001) Batch 3.970 (3.490) Remain 07:57:24 loss: 0.2657 Lr: 0.02921
[2023-08-08 02:16:58,784 INFO misc.py line 115 22900] Train: [48/100][61/156] Data 0.001 (0.001) Batch 3.390 (3.488) Remain 07:57:07 loss: 0.2475 Lr: 0.02920
[2023-08-08 02:17:02,431 INFO misc.py line 115 22900] Train: [48/100][62/156] Data 0.001 (0.001) Batch 3.647 (3.491) Remain 07:57:25 loss: 0.3262 Lr: 0.02920
[2023-08-08 02:17:05,318 INFO misc.py line 115 22900] Train: [48/100][63/156] Data 0.001 (0.001) Batch 2.886 (3.481) Remain 07:55:59 loss: 0.1602 Lr: 0.02919
[2023-08-08 02:17:08,204 INFO misc.py line 115 22900] Train: [48/100][64/156] Data 0.001 (0.001) Batch 2.887 (3.471) Remain 07:54:36 loss: 0.1652 Lr: 0.02919
[2023-08-08 02:17:11,053 INFO misc.py line 115 22900] Train: [48/100][65/156] Data 0.001 (0.001) Batch 2.849 (3.461) Remain 07:53:10 loss: 0.2685 Lr: 0.02918
[2023-08-08 02:17:14,343 INFO misc.py line 115 22900] Train: [48/100][66/156] Data 0.001 (0.001) Batch 3.289 (3.458) Remain 07:52:44 loss: 0.1843 Lr: 0.02918
[2023-08-08 02:17:18,025 INFO misc.py line 115 22900] Train: [48/100][67/156] Data 0.001 (0.001) Batch 3.683 (3.462) Remain 07:53:09 loss: 0.2990 Lr: 0.02917
[2023-08-08 02:17:21,306 INFO misc.py line 115 22900] Train: [48/100][68/156] Data 0.001 (0.001) Batch 3.281 (3.459) Remain 07:52:43 loss: 0.2280 Lr: 0.02917
[2023-08-08 02:17:25,132 INFO misc.py line 115 22900] Train: [48/100][69/156] Data 0.001 (0.001) Batch 3.825 (3.465) Remain 07:53:25 loss: 0.2889 Lr: 0.02916
[2023-08-08 02:17:28,764 INFO misc.py line 115 22900] Train: [48/100][70/156] Data 0.001 (0.001) Batch 3.633 (3.467) Remain 07:53:42 loss: 0.1717 Lr: 0.02916
[2023-08-08 02:17:31,362 INFO misc.py line 115 22900] Train: [48/100][71/156] Data 0.001 (0.001) Batch 2.598 (3.454) Remain 07:51:54 loss: 0.0659 Lr: 0.02915
[2023-08-08 02:17:34,503 INFO misc.py line 115 22900] Train: [48/100][72/156] Data 0.001 (0.001) Batch 3.141 (3.450) Remain 07:51:13 loss: 0.2990 Lr: 0.02915
[2023-08-08 02:17:37,437 INFO misc.py line 115 22900] Train: [48/100][73/156] Data 0.001 (0.001) Batch 2.934 (3.442) Remain 07:50:10 loss: 0.2188 Lr: 0.02914
[2023-08-08 02:17:41,572 INFO misc.py line 115 22900] Train: [48/100][74/156] Data 0.001 (0.001) Batch 4.136 (3.452) Remain 07:51:26 loss: 0.7960 Lr: 0.02914
[2023-08-08 02:17:45,187 INFO misc.py line 115 22900] Train: [48/100][75/156] Data 0.001 (0.001) Batch 3.614 (3.454) Remain 07:51:41 loss: 0.1543 Lr: 0.02913
[2023-08-08 02:17:47,517 INFO misc.py line 115 22900] Train: [48/100][76/156] Data 0.001 (0.001) Batch 2.331 (3.439) Remain 07:49:32 loss: 0.1594 Lr: 0.02913
[2023-08-08 02:17:51,000 INFO misc.py line 115 22900] Train: [48/100][77/156] Data 0.001 (0.001) Batch 3.483 (3.440) Remain 07:49:33 loss: 0.2090 Lr: 0.02912
[2023-08-08 02:17:53,784 INFO misc.py line 115 22900] Train: [48/100][78/156] Data 0.001 (0.001) Batch 2.784 (3.431) Remain 07:48:18 loss: 0.1866 Lr: 0.02912
[2023-08-08 02:17:57,501 INFO misc.py line 115 22900] Train: [48/100][79/156] Data 0.001 (0.001) Batch 3.717 (3.435) Remain 07:48:45 loss: 0.2589 Lr: 0.02911
[2023-08-08 02:18:01,048 INFO misc.py line 115 22900] Train: [48/100][80/156] Data 0.001 (0.001) Batch 3.547 (3.436) Remain 07:48:54 loss: 0.2501 Lr: 0.02910
[2023-08-08 02:18:04,375 INFO misc.py line 115 22900] Train: [48/100][81/156] Data 0.001 (0.001) Batch 3.327 (3.435) Remain 07:48:39 loss: 0.1842 Lr: 0.02910
[2023-08-08 02:18:07,558 INFO misc.py line 115 22900] Train: [48/100][82/156] Data 0.001 (0.001) Batch 3.183 (3.431) Remain 07:48:09 loss: 0.3156 Lr: 0.02909
[2023-08-08 02:18:11,620 INFO misc.py line 115 22900] Train: [48/100][83/156] Data 0.001 (0.001) Batch 4.062 (3.439) Remain 07:49:11 loss: 0.1781 Lr: 0.02909
[2023-08-08 02:18:15,741 INFO misc.py line 115 22900] Train: [48/100][84/156] Data 0.001 (0.001) Batch 4.121 (3.448) Remain 07:50:16 loss: 0.2941 Lr: 0.02908
[2023-08-08 02:18:19,788 INFO misc.py line 115 22900] Train: [48/100][85/156] Data 0.001 (0.001) Batch 4.048 (3.455) Remain 07:51:12 loss: 0.4004 Lr: 0.02908
[2023-08-08 02:18:23,263 INFO misc.py line 115 22900] Train: [48/100][86/156] Data 0.001 (0.001) Batch 3.475 (3.455) Remain 07:51:11 loss: 0.1566 Lr: 0.02907
[2023-08-08 02:18:26,966 INFO misc.py line 115 22900] Train: [48/100][87/156] Data 0.001 (0.001) Batch 3.703 (3.458) Remain 07:51:32 loss: 0.2414 Lr: 0.02907
[2023-08-08 02:18:29,410 INFO misc.py line 115 22900] Train: [48/100][88/156] Data 0.001 (0.001) Batch 2.444 (3.446) Remain 07:49:51 loss: 0.2359 Lr: 0.02906
[2023-08-08 02:18:33,186 INFO misc.py line 115 22900] Train: [48/100][89/156] Data 0.001 (0.001) Batch 3.775 (3.450) Remain 07:50:18 loss: 0.2680 Lr: 0.02906
[2023-08-08 02:18:36,594 INFO misc.py line 115 22900] Train: [48/100][90/156] Data 0.001 (0.001) Batch 3.408 (3.450) Remain 07:50:11 loss: 0.2415 Lr: 0.02905
[2023-08-08 02:18:40,443 INFO misc.py line 115 22900] Train: [48/100][91/156] Data 0.001 (0.001) Batch 3.849 (3.454) Remain 07:50:45 loss: 0.1537 Lr: 0.02905
[2023-08-08 02:18:44,452 INFO misc.py line 115 22900] Train: [48/100][92/156] Data 0.001 (0.001) Batch 4.010 (3.460) Remain 07:51:32 loss: 0.3421 Lr: 0.02904
[2023-08-08 02:18:47,851 INFO misc.py line 115 22900] Train: [48/100][93/156] Data 0.001 (0.001) Batch 3.399 (3.460) Remain 07:51:23 loss: 0.2360 Lr: 0.02904
[2023-08-08 02:18:52,260 INFO misc.py line 115 22900] Train: [48/100][94/156] Data 0.001 (0.001) Batch 4.409 (3.470) Remain 07:52:45 loss: 0.3474 Lr: 0.02903
[2023-08-08 02:18:56,637 INFO misc.py line 115 22900] Train: [48/100][95/156] Data 0.001 (0.001) Batch 4.377 (3.480) Remain 07:54:02 loss: 0.3472 Lr: 0.02903
[2023-08-08 02:19:00,710 INFO misc.py line 115 22900] Train: [48/100][96/156] Data 0.001 (0.001) Batch 4.073 (3.486) Remain 07:54:51 loss: 0.2473 Lr: 0.02902
[2023-08-08 02:19:03,543 INFO misc.py line 115 22900] Train: [48/100][97/156] Data 0.001 (0.001) Batch 2.834 (3.479) Remain 07:53:50 loss: 0.0994 Lr: 0.02902
[2023-08-08 02:19:05,814 INFO misc.py line 115 22900] Train: [48/100][98/156] Data 0.001 (0.001) Batch 2.270 (3.467) Remain 07:52:03 loss: 0.1515 Lr: 0.02901
[2023-08-08 02:19:08,975 INFO misc.py line 115 22900] Train: [48/100][99/156] Data 0.001 (0.001) Batch 3.161 (3.464) Remain 07:51:33 loss: 0.0711 Lr: 0.02901
[2023-08-08 02:19:13,354 INFO misc.py line 115 22900] Train: [48/100][100/156] Data 0.001 (0.001) Batch 4.380 (3.473) Remain 07:52:47 loss: 0.3660 Lr: 0.02900
[2023-08-08 02:19:17,417 INFO misc.py line 115 22900] Train: [48/100][101/156] Data 0.001 (0.001) Batch 4.062 (3.479) Remain 07:53:33 loss: 0.1720 Lr: 0.02899
[2023-08-08 02:19:21,473 INFO misc.py line 115 22900] Train: [48/100][102/156] Data 0.001 (0.001) Batch 4.056 (3.485) Remain 07:54:17 loss: 0.2336 Lr: 0.02899
[2023-08-08 02:19:24,698 INFO misc.py line 115 22900] Train: [48/100][103/156] Data 0.001 (0.001) Batch 3.225 (3.482) Remain 07:53:52 loss: 0.1256 Lr: 0.02898
[2023-08-08 02:19:28,464 INFO misc.py line 115 22900] Train: [48/100][104/156] Data 0.001 (0.001) Batch 3.766 (3.485) Remain 07:54:12 loss: 0.2945 Lr: 0.02898
[2023-08-08 02:19:31,125 INFO misc.py line 115 22900] Train: [48/100][105/156] Data 0.001 (0.001) Batch 2.661 (3.477) Remain 07:53:02 loss: 0.4317 Lr: 0.02897
[2023-08-08 02:19:34,751 INFO misc.py line 115 22900] Train: [48/100][106/156] Data 0.001 (0.001) Batch 3.626 (3.478) Remain 07:53:11 loss: 0.1342 Lr: 0.02897
[2023-08-08 02:19:38,427 INFO misc.py line 115 22900] Train: [48/100][107/156] Data 0.002 (0.001) Batch 3.676 (3.480) Remain 07:53:23 loss: 0.2657 Lr: 0.02896
[2023-08-08 02:19:42,011 INFO misc.py line 115 22900] Train: [48/100][108/156] Data 0.001 (0.001) Batch 3.584 (3.481) Remain 07:53:27 loss: 0.2948 Lr: 0.02896
[2023-08-08 02:19:45,846 INFO misc.py line 115 22900] Train: [48/100][109/156] Data 0.001 (0.001) Batch 3.835 (3.485) Remain 07:53:51 loss: 0.2367 Lr: 0.02895
[2023-08-08 02:19:49,667 INFO misc.py line 115 22900] Train: [48/100][110/156] Data 0.001 (0.001) Batch 3.821 (3.488) Remain 07:54:13 loss: 0.2598 Lr: 0.02895
[2023-08-08 02:19:53,259 INFO misc.py line 115 22900] Train: [48/100][111/156] Data 0.001 (0.001) Batch 3.593 (3.489) Remain 07:54:17 loss: 0.3296 Lr: 0.02894
[2023-08-08 02:19:57,390 INFO misc.py line 115 22900] Train: [48/100][112/156] Data 0.001 (0.001) Batch 4.131 (3.495) Remain 07:55:02 loss: 0.2843 Lr: 0.02894
[2023-08-08 02:20:00,900 INFO misc.py line 115 22900] Train: [48/100][113/156] Data 0.001 (0.001) Batch 3.509 (3.495) Remain 07:55:00 loss: 0.3152 Lr: 0.02893
[2023-08-08 02:20:04,461 INFO misc.py line 115 22900] Train: [48/100][114/156] Data 0.001 (0.001) Batch 3.562 (3.495) Remain 07:55:01 loss: 0.1534 Lr: 0.02893
[2023-08-08 02:20:07,616 INFO misc.py line 115 22900] Train: [48/100][115/156] Data 0.001 (0.001) Batch 3.154 (3.492) Remain 07:54:33 loss: 0.1572 Lr: 0.02892
[2023-08-08 02:20:10,606 INFO misc.py line 115 22900] Train: [48/100][116/156] Data 0.001 (0.001) Batch 2.990 (3.488) Remain 07:53:53 loss: 0.1160 Lr: 0.02892
[2023-08-08 02:20:13,238 INFO misc.py line 115 22900] Train: [48/100][117/156] Data 0.001 (0.001) Batch 2.633 (3.480) Remain 07:52:48 loss: 0.1952 Lr: 0.02891
[2023-08-08 02:20:17,129 INFO misc.py line 115 22900] Train: [48/100][118/156] Data 0.001 (0.001) Batch 3.891 (3.484) Remain 07:53:14 loss: 0.5190 Lr: 0.02891
[2023-08-08 02:20:19,929 INFO misc.py line 115 22900] Train: [48/100][119/156] Data 0.001 (0.001) Batch 2.800 (3.478) Remain 07:52:22 loss: 0.1879 Lr: 0.02890
[2023-08-08 02:20:22,668 INFO misc.py line 115 22900] Train: [48/100][120/156] Data 0.001 (0.001) Batch 2.740 (3.472) Remain 07:51:27 loss: 0.1230 Lr: 0.02890
[2023-08-08 02:20:26,340 INFO misc.py line 115 22900] Train: [48/100][121/156] Data 0.001 (0.001) Batch 3.671 (3.473) Remain 07:51:38 loss: 0.3004 Lr: 0.02889
[2023-08-08 02:20:29,764 INFO misc.py line 115 22900] Train: [48/100][122/156] Data 0.001 (0.001) Batch 3.424 (3.473) Remain 07:51:31 loss: 0.2022 Lr: 0.02888
[2023-08-08 02:20:33,119 INFO misc.py line 115 22900] Train: [48/100][123/156] Data 0.001 (0.001) Batch 3.355 (3.472) Remain 07:51:19 loss: 0.5153 Lr: 0.02888
[2023-08-08 02:20:35,965 INFO misc.py line 115 22900] Train: [48/100][124/156] Data 0.001 (0.001) Batch 2.846 (3.467) Remain 07:50:34 loss: 0.0940 Lr: 0.02887
[2023-08-08 02:20:39,802 INFO misc.py line 115 22900] Train: [48/100][125/156] Data 0.001 (0.001) Batch 3.837 (3.470) Remain 07:50:55 loss: 0.1619 Lr: 0.02887
[2023-08-08 02:20:42,927 INFO misc.py line 115 22900] Train: [48/100][126/156] Data 0.001 (0.001) Batch 3.125 (3.467) Remain 07:50:29 loss: 0.2919 Lr: 0.02886
[2023-08-08 02:20:46,510 INFO misc.py line 115 22900] Train: [48/100][127/156] Data 0.001 (0.001) Batch 3.583 (3.468) Remain 07:50:33 loss: 0.1980 Lr: 0.02886
[2023-08-08 02:20:49,967 INFO misc.py line 115 22900] Train: [48/100][128/156] Data 0.001 (0.001) Batch 3.457 (3.468) Remain 07:50:29 loss: 0.2885 Lr: 0.02885
[2023-08-08 02:20:53,968 INFO misc.py line 115 22900] Train: [48/100][129/156] Data 0.001 (0.001) Batch 4.001 (3.472) Remain 07:51:00 loss: 0.3302 Lr: 0.02885
[2023-08-08 02:20:56,906 INFO misc.py line 115 22900] Train: [48/100][130/156] Data 0.001 (0.001) Batch 2.939 (3.468) Remain 07:50:22 loss: 0.1721 Lr: 0.02884
[2023-08-08 02:21:00,940 INFO misc.py line 115 22900] Train: [48/100][131/156] Data 0.001 (0.001) Batch 4.034 (3.472) Remain 07:50:54 loss: 0.8087 Lr: 0.02884
[2023-08-08 02:21:04,279 INFO misc.py line 115 22900] Train: [48/100][132/156] Data 0.001 (0.001) Batch 3.338 (3.471) Remain 07:50:43 loss: 0.2740 Lr: 0.02883
[2023-08-08 02:21:06,678 INFO misc.py line 115 22900] Train: [48/100][133/156] Data 0.001 (0.001) Batch 2.399 (3.463) Remain 07:49:32 loss: 0.1336 Lr: 0.02883
[2023-08-08 02:21:10,391 INFO misc.py line 115 22900] Train: [48/100][134/156] Data 0.001 (0.001) Batch 3.712 (3.465) Remain 07:49:44 loss: 0.3220 Lr: 0.02882
[2023-08-08 02:21:13,915 INFO misc.py line 115 22900] Train: [48/100][135/156] Data 0.001 (0.001) Batch 3.524 (3.465) Remain 07:49:44 loss: 0.2825 Lr: 0.02882
[2023-08-08 02:21:16,558 INFO misc.py line 115 22900] Train: [48/100][136/156] Data 0.001 (0.001) Batch 2.644 (3.459) Remain 07:48:50 loss: 0.2978 Lr: 0.02881
[2023-08-08 02:21:20,096 INFO misc.py line 115 22900] Train: [48/100][137/156] Data 0.001 (0.001) Batch 3.538 (3.460) Remain 07:48:52 loss: 0.2399 Lr: 0.02881
[2023-08-08 02:21:22,688 INFO misc.py line 115 22900] Train: [48/100][138/156] Data 0.001 (0.001) Batch 2.592 (3.453) Remain 07:47:56 loss: 0.1920 Lr: 0.02880
[2023-08-08 02:21:26,148 INFO misc.py line 115 22900] Train: [48/100][139/156] Data 0.001 (0.001) Batch 3.460 (3.453) Remain 07:47:53 loss: 0.3509 Lr: 0.02880
[2023-08-08 02:21:28,747 INFO misc.py line 115 22900] Train: [48/100][140/156] Data 0.001 (0.001) Batch 2.599 (3.447) Remain 07:46:59 loss: 0.1300 Lr: 0.02879
[2023-08-08 02:21:31,770 INFO misc.py line 115 22900] Train: [48/100][141/156] Data 0.001 (0.001) Batch 3.024 (3.444) Remain 07:46:30 loss: 0.1027 Lr: 0.02879
[2023-08-08 02:21:35,501 INFO misc.py line 115 22900] Train: [48/100][142/156] Data 0.001 (0.001) Batch 3.731 (3.446) Remain 07:46:44 loss: 0.3049 Lr: 0.02878
[2023-08-08 02:21:39,468 INFO misc.py line 115 22900] Train: [48/100][143/156] Data 0.001 (0.001) Batch 3.966 (3.450) Remain 07:47:11 loss: 0.3823 Lr: 0.02877
[2023-08-08 02:21:42,834 INFO misc.py line 115 22900] Train: [48/100][144/156] Data 0.001 (0.001) Batch 3.366 (3.449) Remain 07:47:02 loss: 0.2179 Lr: 0.02877
[2023-08-08 02:21:46,549 INFO misc.py line 115 22900] Train: [48/100][145/156] Data 0.001 (0.001) Batch 3.715 (3.451) Remain 07:47:14 loss: 0.2508 Lr: 0.02876
[2023-08-08 02:21:49,442 INFO misc.py line 115 22900] Train: [48/100][146/156] Data 0.001 (0.001) Batch 2.893 (3.447) Remain 07:46:39 loss: 0.2603 Lr: 0.02876
[2023-08-08 02:21:53,422 INFO misc.py line 115 22900] Train: [48/100][147/156] Data 0.001 (0.001) Batch 3.980 (3.451) Remain 07:47:05 loss: 0.3958 Lr: 0.02875
[2023-08-08 02:21:55,900 INFO misc.py line 115 22900] Train: [48/100][148/156] Data 0.001 (0.001) Batch 2.478 (3.444) Remain 07:46:07 loss: 0.1895 Lr: 0.02875
[2023-08-08 02:21:59,831 INFO misc.py line 115 22900] Train: [48/100][149/156] Data 0.001 (0.001) Batch 3.931 (3.448) Remain 07:46:31 loss: 0.4113 Lr: 0.02874
[2023-08-08 02:22:03,775 INFO misc.py line 115 22900] Train: [48/100][150/156] Data 0.001 (0.001) Batch 3.944 (3.451) Remain 07:46:55 loss: 0.2223 Lr: 0.02874
[2023-08-08 02:22:07,778 INFO misc.py line 115 22900] Train: [48/100][151/156] Data 0.001 (0.001) Batch 4.003 (3.455) Remain 07:47:22 loss: 0.2277 Lr: 0.02873
[2023-08-08 02:22:11,830 INFO misc.py line 115 22900] Train: [48/100][152/156] Data 0.001 (0.001) Batch 4.052 (3.459) Remain 07:47:51 loss: 0.2688 Lr: 0.02873
[2023-08-08 02:22:13,960 INFO misc.py line 115 22900] Train: [48/100][153/156] Data 0.001 (0.001) Batch 2.131 (3.450) Remain 07:46:36 loss: 0.1061 Lr: 0.02872
[2023-08-08 02:22:17,278 INFO misc.py line 115 22900] Train: [48/100][154/156] Data 0.001 (0.001) Batch 3.318 (3.449) Remain 07:46:25 loss: 0.2315 Lr: 0.02872
[2023-08-08 02:22:21,158 INFO misc.py line 115 22900] Train: [48/100][155/156] Data 0.001 (0.001) Batch 3.880 (3.452) Remain 07:46:45 loss: 0.3183 Lr: 0.02871
[2023-08-08 02:22:23,968 INFO misc.py line 115 22900] Train: [48/100][156/156] Data 0.001 (0.001) Batch 2.810 (3.448) Remain 07:46:07 loss: 0.1394 Lr: 0.02871
[2023-08-08 02:22:23,969 INFO misc.py line 129 22900] Train result: loss: 0.2587 
[2023-08-08 02:22:23,969 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 02:22:26,062 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.1789 
[2023-08-08 02:22:26,930 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.5923 
[2023-08-08 02:22:28,594 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.5687 
[2023-08-08 02:22:30,117 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.2429 
[2023-08-08 02:22:31,963 INFO evaluator.py line 122 22900] Test: [5/24] Loss 2.0702 
[2023-08-08 02:22:33,626 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.6867 
[2023-08-08 02:22:35,765 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.5853 
[2023-08-08 02:22:37,567 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.2551 
[2023-08-08 02:22:38,849 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.4614 
[2023-08-08 02:22:40,978 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3692 
[2023-08-08 02:22:41,503 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.4141 
[2023-08-08 02:22:43,034 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.5898 
[2023-08-08 02:22:45,748 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.1164 
[2023-08-08 02:22:47,430 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.6618 
[2023-08-08 02:22:49,451 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.4483 
[2023-08-08 02:22:52,161 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.1482 
[2023-08-08 02:22:54,867 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.4639 
[2023-08-08 02:22:56,713 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.6709 
[2023-08-08 02:22:57,462 INFO evaluator.py line 122 22900] Test: [19/24] Loss 0.9446 
[2023-08-08 02:22:58,346 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.6971 
[2023-08-08 02:23:00,606 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3452 
[2023-08-08 02:23:02,572 INFO evaluator.py line 122 22900] Test: [22/24] Loss 2.0181 
[2023-08-08 02:23:04,420 INFO evaluator.py line 122 22900] Test: [23/24] Loss 3.3167 
[2023-08-08 02:23:06,353 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.8795 
[2023-08-08 02:23:06,404 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2393/0.3287/0.6932.
[2023-08-08 02:23:06,405 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6717/0.9589
[2023-08-08 02:23:06,405 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9428/0.9926
[2023-08-08 02:23:06,405 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1883/0.5481
[2023-08-08 02:23:06,405 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1646/0.2014
[2023-08-08 02:23:06,405 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6076/0.7561
[2023-08-08 02:23:06,405 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.2791/0.3234
[2023-08-08 02:23:06,405 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5218/0.7032
[2023-08-08 02:23:06,405 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1498/0.1545
[2023-08-08 02:23:06,405 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1415/0.4077
[2023-08-08 02:23:06,405 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0359/0.0360
[2023-08-08 02:23:06,405 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 02:23:06,405 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0011/0.0011
[2023-08-08 02:23:06,405 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1967/0.5020
[2023-08-08 02:23:06,405 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0315/0.0335
[2023-08-08 02:23:06,405 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0369/0.0422
[2023-08-08 02:23:06,405 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.3852/0.4013
[2023-08-08 02:23:06,405 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.1675/0.1776
[2023-08-08 02:23:06,405 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.1840/0.2008
[2023-08-08 02:23:06,405 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 02:23:06,405 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0806/0.1334
[2023-08-08 02:23:06,405 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 02:23:06,405 INFO misc.py line 152 22900] Currently Best mIoU: 0.2562
[2023-08-08 02:23:06,406 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 02:23:12,205 INFO misc.py line 115 22900] Train: [49/100][1/156] Data 1.358 (1.358) Batch 4.998 (4.998) Remain 11:15:41 loss: 0.1644 Lr: 0.02870
[2023-08-08 02:23:16,578 INFO misc.py line 115 22900] Train: [49/100][2/156] Data 0.001 (0.001) Batch 4.373 (4.373) Remain 09:51:07 loss: 0.5639 Lr: 0.02870
[2023-08-08 02:23:19,146 INFO misc.py line 115 22900] Train: [49/100][3/156] Data 0.001 (0.001) Batch 2.569 (2.569) Remain 05:47:08 loss: 0.2086 Lr: 0.02869
[2023-08-08 02:23:21,842 INFO misc.py line 115 22900] Train: [49/100][4/156] Data 0.001 (0.001) Batch 2.696 (2.696) Remain 06:04:16 loss: 0.1611 Lr: 0.02869
[2023-08-08 02:23:25,561 INFO misc.py line 115 22900] Train: [49/100][5/156] Data 0.001 (0.001) Batch 3.719 (3.207) Remain 07:13:21 loss: 0.2589 Lr: 0.02868
[2023-08-08 02:23:28,106 INFO misc.py line 115 22900] Train: [49/100][6/156] Data 0.001 (0.001) Batch 2.545 (2.987) Remain 06:43:28 loss: 0.1773 Lr: 0.02868
[2023-08-08 02:23:30,755 INFO misc.py line 115 22900] Train: [49/100][7/156] Data 0.001 (0.001) Batch 2.649 (2.902) Remain 06:32:01 loss: 0.2948 Lr: 0.02867
[2023-08-08 02:23:33,610 INFO misc.py line 115 22900] Train: [49/100][8/156] Data 0.001 (0.001) Batch 2.855 (2.893) Remain 06:30:43 loss: 0.2062 Lr: 0.02866
[2023-08-08 02:23:37,328 INFO misc.py line 115 22900] Train: [49/100][9/156] Data 0.001 (0.001) Batch 3.718 (3.030) Remain 06:49:14 loss: 0.2440 Lr: 0.02866
[2023-08-08 02:23:40,185 INFO misc.py line 115 22900] Train: [49/100][10/156] Data 0.001 (0.001) Batch 2.857 (3.006) Remain 06:45:50 loss: 0.1099 Lr: 0.02865
[2023-08-08 02:23:43,885 INFO misc.py line 115 22900] Train: [49/100][11/156] Data 0.001 (0.001) Batch 3.700 (3.092) Remain 06:57:30 loss: 0.1369 Lr: 0.02865
[2023-08-08 02:23:46,299 INFO misc.py line 115 22900] Train: [49/100][12/156] Data 0.001 (0.001) Batch 2.414 (3.017) Remain 06:47:17 loss: 0.2501 Lr: 0.02864
[2023-08-08 02:23:49,782 INFO misc.py line 115 22900] Train: [49/100][13/156] Data 0.001 (0.001) Batch 3.483 (3.064) Remain 06:53:31 loss: 0.1816 Lr: 0.02864
[2023-08-08 02:23:53,808 INFO misc.py line 115 22900] Train: [49/100][14/156] Data 0.001 (0.001) Batch 4.026 (3.151) Remain 07:05:16 loss: 0.1870 Lr: 0.02863
[2023-08-08 02:23:56,604 INFO misc.py line 115 22900] Train: [49/100][15/156] Data 0.001 (0.001) Batch 2.796 (3.121) Remain 07:01:14 loss: 0.3219 Lr: 0.02863
[2023-08-08 02:23:59,404 INFO misc.py line 115 22900] Train: [49/100][16/156] Data 0.001 (0.001) Batch 2.800 (3.097) Remain 06:57:51 loss: 0.1967 Lr: 0.02862
[2023-08-08 02:24:02,951 INFO misc.py line 115 22900] Train: [49/100][17/156] Data 0.001 (0.001) Batch 3.547 (3.129) Remain 07:02:08 loss: 0.1409 Lr: 0.02862
[2023-08-08 02:24:06,506 INFO misc.py line 115 22900] Train: [49/100][18/156] Data 0.001 (0.001) Batch 3.555 (3.157) Remain 07:05:55 loss: 0.2174 Lr: 0.02861
[2023-08-08 02:24:10,126 INFO misc.py line 115 22900] Train: [49/100][19/156] Data 0.001 (0.001) Batch 3.619 (3.186) Remain 07:09:45 loss: 0.2038 Lr: 0.02861
[2023-08-08 02:24:14,187 INFO misc.py line 115 22900] Train: [49/100][20/156] Data 0.001 (0.001) Batch 4.061 (3.238) Remain 07:16:39 loss: 0.4213 Lr: 0.02860
[2023-08-08 02:24:16,388 INFO misc.py line 115 22900] Train: [49/100][21/156] Data 0.001 (0.001) Batch 2.201 (3.180) Remain 07:08:49 loss: 0.0673 Lr: 0.02860
[2023-08-08 02:24:19,905 INFO misc.py line 115 22900] Train: [49/100][22/156] Data 0.001 (0.001) Batch 3.517 (3.198) Remain 07:11:10 loss: 0.3363 Lr: 0.02859
[2023-08-08 02:24:24,500 INFO misc.py line 115 22900] Train: [49/100][23/156] Data 0.001 (0.001) Batch 4.595 (3.268) Remain 07:20:32 loss: 0.2541 Lr: 0.02859
[2023-08-08 02:24:27,973 INFO misc.py line 115 22900] Train: [49/100][24/156] Data 0.001 (0.001) Batch 3.474 (3.277) Remain 07:21:48 loss: 0.1967 Lr: 0.02858
[2023-08-08 02:24:31,378 INFO misc.py line 115 22900] Train: [49/100][25/156] Data 0.001 (0.001) Batch 3.405 (3.283) Remain 07:22:31 loss: 0.2645 Lr: 0.02858
[2023-08-08 02:24:35,205 INFO misc.py line 115 22900] Train: [49/100][26/156] Data 0.001 (0.001) Batch 3.827 (3.307) Remain 07:25:39 loss: 0.4596 Lr: 0.02857
[2023-08-08 02:24:38,194 INFO misc.py line 115 22900] Train: [49/100][27/156] Data 0.001 (0.001) Batch 2.988 (3.294) Remain 07:23:49 loss: 0.1999 Lr: 0.02857
[2023-08-08 02:24:42,171 INFO misc.py line 115 22900] Train: [49/100][28/156] Data 0.001 (0.001) Batch 3.977 (3.321) Remain 07:27:26 loss: 0.5125 Lr: 0.02856
[2023-08-08 02:24:46,155 INFO misc.py line 115 22900] Train: [49/100][29/156] Data 0.001 (0.001) Batch 3.984 (3.346) Remain 07:30:49 loss: 0.3734 Lr: 0.02855
[2023-08-08 02:24:48,562 INFO misc.py line 115 22900] Train: [49/100][30/156] Data 0.001 (0.001) Batch 2.408 (3.312) Remain 07:26:05 loss: 0.1557 Lr: 0.02855
[2023-08-08 02:24:52,109 INFO misc.py line 115 22900] Train: [49/100][31/156] Data 0.001 (0.001) Batch 3.546 (3.320) Remain 07:27:09 loss: 0.3514 Lr: 0.02854
[2023-08-08 02:24:55,507 INFO misc.py line 115 22900] Train: [49/100][32/156] Data 0.001 (0.001) Batch 3.399 (3.323) Remain 07:27:28 loss: 0.2892 Lr: 0.02854
[2023-08-08 02:24:58,813 INFO misc.py line 115 22900] Train: [49/100][33/156] Data 0.001 (0.001) Batch 3.306 (3.322) Remain 07:27:20 loss: 0.3381 Lr: 0.02853
[2023-08-08 02:25:02,794 INFO misc.py line 115 22900] Train: [49/100][34/156] Data 0.001 (0.001) Batch 3.981 (3.343) Remain 07:30:08 loss: 0.1925 Lr: 0.02853
[2023-08-08 02:25:07,604 INFO misc.py line 115 22900] Train: [49/100][35/156] Data 0.001 (0.001) Batch 4.809 (3.389) Remain 07:36:15 loss: 0.6031 Lr: 0.02852
[2023-08-08 02:25:11,108 INFO misc.py line 115 22900] Train: [49/100][36/156] Data 0.001 (0.001) Batch 3.504 (3.393) Remain 07:36:39 loss: 0.2915 Lr: 0.02852
[2023-08-08 02:25:15,224 INFO misc.py line 115 22900] Train: [49/100][37/156] Data 0.001 (0.001) Batch 4.117 (3.414) Remain 07:39:28 loss: 0.2295 Lr: 0.02851
[2023-08-08 02:25:19,343 INFO misc.py line 115 22900] Train: [49/100][38/156] Data 0.001 (0.001) Batch 4.119 (3.434) Remain 07:42:07 loss: 0.2428 Lr: 0.02851
[2023-08-08 02:25:23,346 INFO misc.py line 115 22900] Train: [49/100][39/156] Data 0.001 (0.001) Batch 4.003 (3.450) Remain 07:44:11 loss: 0.2974 Lr: 0.02850
[2023-08-08 02:25:26,036 INFO misc.py line 115 22900] Train: [49/100][40/156] Data 0.001 (0.001) Batch 2.690 (3.429) Remain 07:41:22 loss: 0.2701 Lr: 0.02850
[2023-08-08 02:25:29,978 INFO misc.py line 115 22900] Train: [49/100][41/156] Data 0.001 (0.001) Batch 3.942 (3.443) Remain 07:43:07 loss: 0.4279 Lr: 0.02849
[2023-08-08 02:25:33,102 INFO misc.py line 115 22900] Train: [49/100][42/156] Data 0.001 (0.001) Batch 3.124 (3.435) Remain 07:41:58 loss: 0.1237 Lr: 0.02849
[2023-08-08 02:25:36,032 INFO misc.py line 115 22900] Train: [49/100][43/156] Data 0.001 (0.001) Batch 2.930 (3.422) Remain 07:40:13 loss: 0.1570 Lr: 0.02848
[2023-08-08 02:25:39,104 INFO misc.py line 115 22900] Train: [49/100][44/156] Data 0.001 (0.001) Batch 3.072 (3.414) Remain 07:39:00 loss: 0.1819 Lr: 0.02848
[2023-08-08 02:25:42,778 INFO misc.py line 115 22900] Train: [49/100][45/156] Data 0.001 (0.001) Batch 3.674 (3.420) Remain 07:39:47 loss: 0.3032 Lr: 0.02847
[2023-08-08 02:25:45,674 INFO misc.py line 115 22900] Train: [49/100][46/156] Data 0.001 (0.001) Batch 2.896 (3.408) Remain 07:38:05 loss: 0.1477 Lr: 0.02847
[2023-08-08 02:25:49,094 INFO misc.py line 115 22900] Train: [49/100][47/156] Data 0.001 (0.001) Batch 3.420 (3.408) Remain 07:38:04 loss: 0.2081 Lr: 0.02846
[2023-08-08 02:25:52,737 INFO misc.py line 115 22900] Train: [49/100][48/156] Data 0.001 (0.001) Batch 3.643 (3.413) Remain 07:38:43 loss: 0.5540 Lr: 0.02846
[2023-08-08 02:25:56,769 INFO misc.py line 115 22900] Train: [49/100][49/156] Data 0.001 (0.001) Batch 4.031 (3.427) Remain 07:40:28 loss: 0.3900 Lr: 0.02845
[2023-08-08 02:25:59,887 INFO misc.py line 115 22900] Train: [49/100][50/156] Data 0.001 (0.001) Batch 3.118 (3.420) Remain 07:39:32 loss: 0.3014 Lr: 0.02844
[2023-08-08 02:26:03,703 INFO misc.py line 115 22900] Train: [49/100][51/156] Data 0.001 (0.001) Batch 3.816 (3.428) Remain 07:40:35 loss: 0.2799 Lr: 0.02844
[2023-08-08 02:26:07,884 INFO misc.py line 115 22900] Train: [49/100][52/156] Data 0.001 (0.001) Batch 4.181 (3.444) Remain 07:42:35 loss: 0.2753 Lr: 0.02843
[2023-08-08 02:26:10,781 INFO misc.py line 115 22900] Train: [49/100][53/156] Data 0.001 (0.001) Batch 2.897 (3.433) Remain 07:41:04 loss: 0.1076 Lr: 0.02843
[2023-08-08 02:26:14,412 INFO misc.py line 115 22900] Train: [49/100][54/156] Data 0.001 (0.001) Batch 3.631 (3.437) Remain 07:41:31 loss: 0.1863 Lr: 0.02842
[2023-08-08 02:26:17,826 INFO misc.py line 115 22900] Train: [49/100][55/156] Data 0.001 (0.001) Batch 3.414 (3.436) Remain 07:41:25 loss: 0.2417 Lr: 0.02842
[2023-08-08 02:26:20,892 INFO misc.py line 115 22900] Train: [49/100][56/156] Data 0.001 (0.001) Batch 3.066 (3.429) Remain 07:40:25 loss: 0.1287 Lr: 0.02841
[2023-08-08 02:26:24,993 INFO misc.py line 115 22900] Train: [49/100][57/156] Data 0.001 (0.001) Batch 4.101 (3.442) Remain 07:42:02 loss: 0.2320 Lr: 0.02841
[2023-08-08 02:26:28,947 INFO misc.py line 115 22900] Train: [49/100][58/156] Data 0.001 (0.001) Batch 3.954 (3.451) Remain 07:43:13 loss: 0.2903 Lr: 0.02840
[2023-08-08 02:26:32,113 INFO misc.py line 115 22900] Train: [49/100][59/156] Data 0.001 (0.001) Batch 3.166 (3.446) Remain 07:42:29 loss: 0.1177 Lr: 0.02840
[2023-08-08 02:26:36,266 INFO misc.py line 115 22900] Train: [49/100][60/156] Data 0.001 (0.001) Batch 4.153 (3.458) Remain 07:44:05 loss: 0.1811 Lr: 0.02839
[2023-08-08 02:26:38,846 INFO misc.py line 115 22900] Train: [49/100][61/156] Data 0.001 (0.001) Batch 2.580 (3.443) Remain 07:42:00 loss: 0.3094 Lr: 0.02839
[2023-08-08 02:26:42,770 INFO misc.py line 115 22900] Train: [49/100][62/156] Data 0.001 (0.001) Batch 3.925 (3.451) Remain 07:43:02 loss: 0.3734 Lr: 0.02838
[2023-08-08 02:26:46,053 INFO misc.py line 115 22900] Train: [49/100][63/156] Data 0.001 (0.001) Batch 3.282 (3.448) Remain 07:42:36 loss: 0.1836 Lr: 0.02838
[2023-08-08 02:26:49,873 INFO misc.py line 115 22900] Train: [49/100][64/156] Data 0.001 (0.001) Batch 3.820 (3.455) Remain 07:43:22 loss: 0.3195 Lr: 0.02837
[2023-08-08 02:26:54,170 INFO misc.py line 115 22900] Train: [49/100][65/156] Data 0.001 (0.001) Batch 4.298 (3.468) Remain 07:45:07 loss: 0.4652 Lr: 0.02837
[2023-08-08 02:26:57,305 INFO misc.py line 115 22900] Train: [49/100][66/156] Data 0.001 (0.001) Batch 3.134 (3.463) Remain 07:44:21 loss: 0.3454 Lr: 0.02836
[2023-08-08 02:27:01,327 INFO misc.py line 115 22900] Train: [49/100][67/156] Data 0.001 (0.001) Batch 4.023 (3.472) Remain 07:45:28 loss: 0.2602 Lr: 0.02836
[2023-08-08 02:27:03,982 INFO misc.py line 115 22900] Train: [49/100][68/156] Data 0.001 (0.001) Batch 2.655 (3.459) Remain 07:43:44 loss: 0.2177 Lr: 0.02835
[2023-08-08 02:27:08,039 INFO misc.py line 115 22900] Train: [49/100][69/156] Data 0.001 (0.001) Batch 4.057 (3.468) Remain 07:44:53 loss: 0.3368 Lr: 0.02834
[2023-08-08 02:27:11,663 INFO misc.py line 115 22900] Train: [49/100][70/156] Data 0.001 (0.001) Batch 3.624 (3.470) Remain 07:45:08 loss: 0.1722 Lr: 0.02834
[2023-08-08 02:27:15,755 INFO misc.py line 115 22900] Train: [49/100][71/156] Data 0.001 (0.001) Batch 4.092 (3.480) Remain 07:46:18 loss: 0.4022 Lr: 0.02833
[2023-08-08 02:27:18,585 INFO misc.py line 115 22900] Train: [49/100][72/156] Data 0.001 (0.001) Batch 2.830 (3.470) Remain 07:44:59 loss: 0.0626 Lr: 0.02833
[2023-08-08 02:27:22,909 INFO misc.py line 115 22900] Train: [49/100][73/156] Data 0.001 (0.001) Batch 4.324 (3.482) Remain 07:46:34 loss: 0.3596 Lr: 0.02832
[2023-08-08 02:27:26,038 INFO misc.py line 115 22900] Train: [49/100][74/156] Data 0.001 (0.001) Batch 3.129 (3.477) Remain 07:45:50 loss: 0.1226 Lr: 0.02832
[2023-08-08 02:27:29,218 INFO misc.py line 115 22900] Train: [49/100][75/156] Data 0.001 (0.001) Batch 3.181 (3.473) Remain 07:45:14 loss: 0.3591 Lr: 0.02831
[2023-08-08 02:27:32,427 INFO misc.py line 115 22900] Train: [49/100][76/156] Data 0.001 (0.001) Batch 3.208 (3.470) Remain 07:44:41 loss: 0.1242 Lr: 0.02831
[2023-08-08 02:27:35,643 INFO misc.py line 115 22900] Train: [49/100][77/156] Data 0.001 (0.001) Batch 3.216 (3.466) Remain 07:44:10 loss: 0.1461 Lr: 0.02830
[2023-08-08 02:27:39,134 INFO misc.py line 115 22900] Train: [49/100][78/156] Data 0.001 (0.001) Batch 3.491 (3.466) Remain 07:44:09 loss: 0.2649 Lr: 0.02830
[2023-08-08 02:27:41,987 INFO misc.py line 115 22900] Train: [49/100][79/156] Data 0.001 (0.001) Batch 2.854 (3.458) Remain 07:43:01 loss: 0.2263 Lr: 0.02829
[2023-08-08 02:27:45,956 INFO misc.py line 115 22900] Train: [49/100][80/156] Data 0.001 (0.001) Batch 3.968 (3.465) Remain 07:43:51 loss: 0.3608 Lr: 0.02829
[2023-08-08 02:27:49,520 INFO misc.py line 115 22900] Train: [49/100][81/156] Data 0.001 (0.001) Batch 3.565 (3.466) Remain 07:43:58 loss: 0.2174 Lr: 0.02828
[2023-08-08 02:27:52,993 INFO misc.py line 115 22900] Train: [49/100][82/156] Data 0.001 (0.001) Batch 3.473 (3.466) Remain 07:43:55 loss: 0.2107 Lr: 0.02828
[2023-08-08 02:27:56,880 INFO misc.py line 115 22900] Train: [49/100][83/156] Data 0.001 (0.001) Batch 3.886 (3.472) Remain 07:44:33 loss: 0.2588 Lr: 0.02827
[2023-08-08 02:28:00,782 INFO misc.py line 115 22900] Train: [49/100][84/156] Data 0.001 (0.001) Batch 3.902 (3.477) Remain 07:45:13 loss: 0.3414 Lr: 0.02827
[2023-08-08 02:28:03,589 INFO misc.py line 115 22900] Train: [49/100][85/156] Data 0.001 (0.001) Batch 2.808 (3.469) Remain 07:44:04 loss: 0.0782 Lr: 0.02826
[2023-08-08 02:28:06,108 INFO misc.py line 115 22900] Train: [49/100][86/156] Data 0.001 (0.001) Batch 2.518 (3.457) Remain 07:42:28 loss: 0.2144 Lr: 0.02826
[2023-08-08 02:28:08,784 INFO misc.py line 115 22900] Train: [49/100][87/156] Data 0.001 (0.001) Batch 2.676 (3.448) Remain 07:41:10 loss: 0.2219 Lr: 0.02825
[2023-08-08 02:28:12,713 INFO misc.py line 115 22900] Train: [49/100][88/156] Data 0.001 (0.001) Batch 3.929 (3.454) Remain 07:41:52 loss: 0.3691 Lr: 0.02824
[2023-08-08 02:28:16,267 INFO misc.py line 115 22900] Train: [49/100][89/156] Data 0.001 (0.001) Batch 3.554 (3.455) Remain 07:41:58 loss: 0.3077 Lr: 0.02824
[2023-08-08 02:28:19,560 INFO misc.py line 115 22900] Train: [49/100][90/156] Data 0.001 (0.001) Batch 3.293 (3.453) Remain 07:41:40 loss: 0.1056 Lr: 0.02823
[2023-08-08 02:28:22,983 INFO misc.py line 115 22900] Train: [49/100][91/156] Data 0.001 (0.001) Batch 3.423 (3.453) Remain 07:41:34 loss: 0.1751 Lr: 0.02823
[2023-08-08 02:28:26,575 INFO misc.py line 115 22900] Train: [49/100][92/156] Data 0.001 (0.001) Batch 3.591 (3.454) Remain 07:41:43 loss: 0.3241 Lr: 0.02822
[2023-08-08 02:28:31,024 INFO misc.py line 115 22900] Train: [49/100][93/156] Data 0.001 (0.001) Batch 4.449 (3.465) Remain 07:43:08 loss: 0.2577 Lr: 0.02822
[2023-08-08 02:28:33,955 INFO misc.py line 115 22900] Train: [49/100][94/156] Data 0.001 (0.001) Batch 2.931 (3.459) Remain 07:42:17 loss: 0.3706 Lr: 0.02821
[2023-08-08 02:28:37,565 INFO misc.py line 115 22900] Train: [49/100][95/156] Data 0.001 (0.001) Batch 3.611 (3.461) Remain 07:42:27 loss: 0.3567 Lr: 0.02821
[2023-08-08 02:28:41,592 INFO misc.py line 115 22900] Train: [49/100][96/156] Data 0.001 (0.001) Batch 4.027 (3.467) Remain 07:43:12 loss: 0.4632 Lr: 0.02820
[2023-08-08 02:28:44,462 INFO misc.py line 115 22900] Train: [49/100][97/156] Data 0.001 (0.001) Batch 2.870 (3.461) Remain 07:42:18 loss: 0.2053 Lr: 0.02820
[2023-08-08 02:28:47,488 INFO misc.py line 115 22900] Train: [49/100][98/156] Data 0.001 (0.001) Batch 3.026 (3.456) Remain 07:41:38 loss: 0.1886 Lr: 0.02819
[2023-08-08 02:28:51,564 INFO misc.py line 115 22900] Train: [49/100][99/156] Data 0.001 (0.001) Batch 4.075 (3.463) Remain 07:42:26 loss: 0.1391 Lr: 0.02819
[2023-08-08 02:28:55,726 INFO misc.py line 115 22900] Train: [49/100][100/156] Data 0.001 (0.001) Batch 4.163 (3.470) Remain 07:43:20 loss: 0.3385 Lr: 0.02818
[2023-08-08 02:28:58,323 INFO misc.py line 115 22900] Train: [49/100][101/156] Data 0.001 (0.001) Batch 2.597 (3.461) Remain 07:42:05 loss: 0.2134 Lr: 0.02818
[2023-08-08 02:29:02,369 INFO misc.py line 115 22900] Train: [49/100][102/156] Data 0.001 (0.001) Batch 4.046 (3.467) Remain 07:42:49 loss: 0.4674 Lr: 0.02817
[2023-08-08 02:29:04,370 INFO misc.py line 115 22900] Train: [49/100][103/156] Data 0.001 (0.001) Batch 2.001 (3.452) Remain 07:40:48 loss: 0.0930 Lr: 0.02817
[2023-08-08 02:29:07,344 INFO misc.py line 115 22900] Train: [49/100][104/156] Data 0.001 (0.001) Batch 2.974 (3.448) Remain 07:40:07 loss: 0.2568 Lr: 0.02816
[2023-08-08 02:29:10,764 INFO misc.py line 115 22900] Train: [49/100][105/156] Data 0.001 (0.001) Batch 3.420 (3.447) Remain 07:40:01 loss: 0.2517 Lr: 0.02816
[2023-08-08 02:29:13,932 INFO misc.py line 115 22900] Train: [49/100][106/156] Data 0.001 (0.001) Batch 3.168 (3.445) Remain 07:39:36 loss: 0.2947 Lr: 0.02815
[2023-08-08 02:29:16,890 INFO misc.py line 115 22900] Train: [49/100][107/156] Data 0.001 (0.001) Batch 2.958 (3.440) Remain 07:38:55 loss: 0.3480 Lr: 0.02815
[2023-08-08 02:29:20,219 INFO misc.py line 115 22900] Train: [49/100][108/156] Data 0.001 (0.001) Batch 3.329 (3.439) Remain 07:38:44 loss: 0.2719 Lr: 0.02814
[2023-08-08 02:29:23,784 INFO misc.py line 115 22900] Train: [49/100][109/156] Data 0.001 (0.001) Batch 3.565 (3.440) Remain 07:38:50 loss: 0.2366 Lr: 0.02813
[2023-08-08 02:29:27,258 INFO misc.py line 115 22900] Train: [49/100][110/156] Data 0.001 (0.001) Batch 3.474 (3.440) Remain 07:38:49 loss: 0.3619 Lr: 0.02813
[2023-08-08 02:29:29,711 INFO misc.py line 115 22900] Train: [49/100][111/156] Data 0.001 (0.001) Batch 2.452 (3.431) Remain 07:37:32 loss: 0.1920 Lr: 0.02812
[2023-08-08 02:29:32,112 INFO misc.py line 115 22900] Train: [49/100][112/156] Data 0.001 (0.001) Batch 2.401 (3.422) Remain 07:36:13 loss: 0.2273 Lr: 0.02812
[2023-08-08 02:29:34,152 INFO misc.py line 115 22900] Train: [49/100][113/156] Data 0.001 (0.001) Batch 2.041 (3.409) Remain 07:34:29 loss: 0.0910 Lr: 0.02811
[2023-08-08 02:29:38,248 INFO misc.py line 115 22900] Train: [49/100][114/156] Data 0.001 (0.001) Batch 4.096 (3.415) Remain 07:35:15 loss: 0.1578 Lr: 0.02811
[2023-08-08 02:29:42,361 INFO misc.py line 115 22900] Train: [49/100][115/156] Data 0.001 (0.001) Batch 4.113 (3.422) Remain 07:36:02 loss: 0.2947 Lr: 0.02810
[2023-08-08 02:29:46,900 INFO misc.py line 115 22900] Train: [49/100][116/156] Data 0.001 (0.001) Batch 4.540 (3.431) Remain 07:37:17 loss: 0.3659 Lr: 0.02810
[2023-08-08 02:29:50,233 INFO misc.py line 115 22900] Train: [49/100][117/156] Data 0.001 (0.001) Batch 3.332 (3.431) Remain 07:37:07 loss: 0.4975 Lr: 0.02809
[2023-08-08 02:29:53,818 INFO misc.py line 115 22900] Train: [49/100][118/156] Data 0.001 (0.001) Batch 3.585 (3.432) Remain 07:37:14 loss: 0.2305 Lr: 0.02809
[2023-08-08 02:29:57,996 INFO misc.py line 115 22900] Train: [49/100][119/156] Data 0.001 (0.001) Batch 4.179 (3.438) Remain 07:38:02 loss: 0.3577 Lr: 0.02808
[2023-08-08 02:30:01,249 INFO misc.py line 115 22900] Train: [49/100][120/156] Data 0.001 (0.001) Batch 3.252 (3.437) Remain 07:37:46 loss: 0.2292 Lr: 0.02808
[2023-08-08 02:30:04,727 INFO misc.py line 115 22900] Train: [49/100][121/156] Data 0.001 (0.001) Batch 3.478 (3.437) Remain 07:37:46 loss: 0.3035 Lr: 0.02807
[2023-08-08 02:30:07,600 INFO misc.py line 115 22900] Train: [49/100][122/156] Data 0.001 (0.001) Batch 2.873 (3.432) Remain 07:37:04 loss: 0.2926 Lr: 0.02807
[2023-08-08 02:30:10,967 INFO misc.py line 115 22900] Train: [49/100][123/156] Data 0.001 (0.001) Batch 3.367 (3.432) Remain 07:36:56 loss: 0.3050 Lr: 0.02806
[2023-08-08 02:30:14,580 INFO misc.py line 115 22900] Train: [49/100][124/156] Data 0.001 (0.001) Batch 3.612 (3.433) Remain 07:37:05 loss: 0.1858 Lr: 0.02806
[2023-08-08 02:30:17,883 INFO misc.py line 115 22900] Train: [49/100][125/156] Data 0.001 (0.001) Batch 3.303 (3.432) Remain 07:36:53 loss: 0.1570 Lr: 0.02805
[2023-08-08 02:30:21,455 INFO misc.py line 115 22900] Train: [49/100][126/156] Data 0.001 (0.001) Batch 3.572 (3.433) Remain 07:36:59 loss: 0.2779 Lr: 0.02805
[2023-08-08 02:30:24,456 INFO misc.py line 115 22900] Train: [49/100][127/156] Data 0.001 (0.001) Batch 3.001 (3.430) Remain 07:36:27 loss: 0.2046 Lr: 0.02804
[2023-08-08 02:30:28,146 INFO misc.py line 115 22900] Train: [49/100][128/156] Data 0.001 (0.001) Batch 3.690 (3.432) Remain 07:36:41 loss: 0.3548 Lr: 0.02803
[2023-08-08 02:30:32,130 INFO misc.py line 115 22900] Train: [49/100][129/156] Data 0.001 (0.001) Batch 3.984 (3.436) Remain 07:37:12 loss: 0.2802 Lr: 0.02803
[2023-08-08 02:30:35,668 INFO misc.py line 115 22900] Train: [49/100][130/156] Data 0.001 (0.001) Batch 3.538 (3.437) Remain 07:37:15 loss: 0.3076 Lr: 0.02802
[2023-08-08 02:30:39,374 INFO misc.py line 115 22900] Train: [49/100][131/156] Data 0.001 (0.001) Batch 3.706 (3.439) Remain 07:37:28 loss: 0.2540 Lr: 0.02802
[2023-08-08 02:30:42,413 INFO misc.py line 115 22900] Train: [49/100][132/156] Data 0.001 (0.001) Batch 3.039 (3.436) Remain 07:37:00 loss: 0.1175 Lr: 0.02801
[2023-08-08 02:30:45,312 INFO misc.py line 115 22900] Train: [49/100][133/156] Data 0.001 (0.001) Batch 2.899 (3.432) Remain 07:36:24 loss: 0.1421 Lr: 0.02801
[2023-08-08 02:30:49,693 INFO misc.py line 115 22900] Train: [49/100][134/156] Data 0.001 (0.001) Batch 4.381 (3.439) Remain 07:37:18 loss: 0.2096 Lr: 0.02800
[2023-08-08 02:30:52,991 INFO misc.py line 115 22900] Train: [49/100][135/156] Data 0.001 (0.001) Batch 3.298 (3.438) Remain 07:37:06 loss: 0.1334 Lr: 0.02800
[2023-08-08 02:30:57,046 INFO misc.py line 115 22900] Train: [49/100][136/156] Data 0.001 (0.001) Batch 4.055 (3.443) Remain 07:37:40 loss: 0.4027 Lr: 0.02799
[2023-08-08 02:30:59,663 INFO misc.py line 115 22900] Train: [49/100][137/156] Data 0.001 (0.001) Batch 2.617 (3.437) Remain 07:36:47 loss: 0.1924 Lr: 0.02799
[2023-08-08 02:31:02,779 INFO misc.py line 115 22900] Train: [49/100][138/156] Data 0.001 (0.001) Batch 3.116 (3.434) Remain 07:36:25 loss: 0.2367 Lr: 0.02798
[2023-08-08 02:31:06,313 INFO misc.py line 115 22900] Train: [49/100][139/156] Data 0.001 (0.001) Batch 3.533 (3.435) Remain 07:36:27 loss: 0.1562 Lr: 0.02798
[2023-08-08 02:31:09,603 INFO misc.py line 115 22900] Train: [49/100][140/156] Data 0.001 (0.001) Batch 3.290 (3.434) Remain 07:36:15 loss: 0.2155 Lr: 0.02797
[2023-08-08 02:31:13,173 INFO misc.py line 115 22900] Train: [49/100][141/156] Data 0.001 (0.001) Batch 3.570 (3.435) Remain 07:36:20 loss: 0.2769 Lr: 0.02797
[2023-08-08 02:31:17,383 INFO misc.py line 115 22900] Train: [49/100][142/156] Data 0.001 (0.001) Batch 4.210 (3.441) Remain 07:37:01 loss: 0.4179 Lr: 0.02796
[2023-08-08 02:31:21,712 INFO misc.py line 115 22900] Train: [49/100][143/156] Data 0.001 (0.001) Batch 4.329 (3.447) Remain 07:37:48 loss: 0.3867 Lr: 0.02796
[2023-08-08 02:31:25,413 INFO misc.py line 115 22900] Train: [49/100][144/156] Data 0.001 (0.001) Batch 3.701 (3.449) Remain 07:37:59 loss: 0.1152 Lr: 0.02795
[2023-08-08 02:31:28,726 INFO misc.py line 115 22900] Train: [49/100][145/156] Data 0.001 (0.001) Batch 3.313 (3.448) Remain 07:37:48 loss: 0.4052 Lr: 0.02795
[2023-08-08 02:31:32,190 INFO misc.py line 115 22900] Train: [49/100][146/156] Data 0.001 (0.001) Batch 3.464 (3.448) Remain 07:37:45 loss: 0.3457 Lr: 0.02794
[2023-08-08 02:31:36,189 INFO misc.py line 115 22900] Train: [49/100][147/156] Data 0.001 (0.001) Batch 3.999 (3.452) Remain 07:38:12 loss: 0.2692 Lr: 0.02793
[2023-08-08 02:31:39,202 INFO misc.py line 115 22900] Train: [49/100][148/156] Data 0.001 (0.001) Batch 3.014 (3.449) Remain 07:37:45 loss: 0.2239 Lr: 0.02793
[2023-08-08 02:31:43,244 INFO misc.py line 115 22900] Train: [49/100][149/156] Data 0.001 (0.001) Batch 4.041 (3.453) Remain 07:38:13 loss: 0.3617 Lr: 0.02792
[2023-08-08 02:31:46,646 INFO misc.py line 115 22900] Train: [49/100][150/156] Data 0.001 (0.001) Batch 3.403 (3.452) Remain 07:38:07 loss: 0.1688 Lr: 0.02792
[2023-08-08 02:31:49,585 INFO misc.py line 115 22900] Train: [49/100][151/156] Data 0.001 (0.001) Batch 2.939 (3.449) Remain 07:37:36 loss: 0.1249 Lr: 0.02791
[2023-08-08 02:31:53,558 INFO misc.py line 115 22900] Train: [49/100][152/156] Data 0.001 (0.001) Batch 3.972 (3.452) Remain 07:38:01 loss: 0.5268 Lr: 0.02791
[2023-08-08 02:31:56,911 INFO misc.py line 115 22900] Train: [49/100][153/156] Data 0.001 (0.001) Batch 3.354 (3.452) Remain 07:37:52 loss: 0.3021 Lr: 0.02790
[2023-08-08 02:32:00,964 INFO misc.py line 115 22900] Train: [49/100][154/156] Data 0.001 (0.001) Batch 4.053 (3.456) Remain 07:38:20 loss: 0.2869 Lr: 0.02790
[2023-08-08 02:32:05,007 INFO misc.py line 115 22900] Train: [49/100][155/156] Data 0.001 (0.001) Batch 4.043 (3.460) Remain 07:38:48 loss: 0.2011 Lr: 0.02789
[2023-08-08 02:32:08,066 INFO misc.py line 115 22900] Train: [49/100][156/156] Data 0.001 (0.001) Batch 3.059 (3.457) Remain 07:38:23 loss: 0.2874 Lr: 0.02789
[2023-08-08 02:32:08,066 INFO misc.py line 129 22900] Train result: loss: 0.2617 
[2023-08-08 02:32:08,067 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 02:32:10,171 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.2137 
[2023-08-08 02:32:11,041 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.4655 
[2023-08-08 02:32:12,704 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.7078 
[2023-08-08 02:32:14,225 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.1864 
[2023-08-08 02:32:16,068 INFO evaluator.py line 122 22900] Test: [5/24] Loss 2.2449 
[2023-08-08 02:32:17,732 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.5737 
[2023-08-08 02:32:19,872 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.1957 
[2023-08-08 02:32:21,676 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.3015 
[2023-08-08 02:32:22,960 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.1696 
[2023-08-08 02:32:25,089 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.4685 
[2023-08-08 02:32:25,614 INFO evaluator.py line 122 22900] Test: [11/24] Loss 0.6761 
[2023-08-08 02:32:27,150 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.7432 
[2023-08-08 02:32:29,864 INFO evaluator.py line 122 22900] Test: [13/24] Loss 0.9710 
[2023-08-08 02:32:31,543 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.7667 
[2023-08-08 02:32:33,566 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.3577 
[2023-08-08 02:32:36,276 INFO evaluator.py line 122 22900] Test: [16/24] Loss 0.9460 
[2023-08-08 02:32:38,984 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.3621 
[2023-08-08 02:32:40,831 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.4925 
[2023-08-08 02:32:41,580 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.0777 
[2023-08-08 02:32:42,465 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.8153 
[2023-08-08 02:32:44,727 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.4119 
[2023-08-08 02:32:46,691 INFO evaluator.py line 122 22900] Test: [22/24] Loss 2.0071 
[2023-08-08 02:32:48,540 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.8900 
[2023-08-08 02:32:50,475 INFO evaluator.py line 122 22900] Test: [24/24] Loss 2.0673 
[2023-08-08 02:32:50,523 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2409/0.3314/0.6976.
[2023-08-08 02:32:50,523 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6238/0.9518
[2023-08-08 02:32:50,523 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9401/0.9922
[2023-08-08 02:32:50,523 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.2014/0.3568
[2023-08-08 02:32:50,523 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1381/0.2231
[2023-08-08 02:32:50,523 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6350/0.8086
[2023-08-08 02:32:50,523 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.4006/0.6105
[2023-08-08 02:32:50,523 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5447/0.7211
[2023-08-08 02:32:50,523 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1516/0.1622
[2023-08-08 02:32:50,523 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1461/0.3332
[2023-08-08 02:32:50,523 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0235/0.0237
[2023-08-08 02:32:50,523 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0026/0.0436
[2023-08-08 02:32:50,523 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0504/0.0545
[2023-08-08 02:32:50,523 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1440/0.1998
[2023-08-08 02:32:50,523 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0195/0.0209
[2023-08-08 02:32:50,523 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0352/0.0414
[2023-08-08 02:32:50,523 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.2986/0.3183
[2023-08-08 02:32:50,523 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0548/0.0734
[2023-08-08 02:32:50,523 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.3198/0.5618
[2023-08-08 02:32:50,523 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 02:32:50,523 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0883/0.1310
[2023-08-08 02:32:50,524 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 02:32:50,524 INFO misc.py line 152 22900] Currently Best mIoU: 0.2562
[2023-08-08 02:32:50,524 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 02:32:54,556 INFO misc.py line 115 22900] Train: [50/100][1/156] Data 0.717 (0.717) Batch 3.248 (3.248) Remain 07:10:34 loss: 0.3247 Lr: 0.02788
[2023-08-08 02:32:58,053 INFO misc.py line 115 22900] Train: [50/100][2/156] Data 0.001 (0.001) Batch 3.497 (3.497) Remain 07:43:35 loss: 0.2686 Lr: 0.02788
[2023-08-08 02:33:01,891 INFO misc.py line 115 22900] Train: [50/100][3/156] Data 0.001 (0.001) Batch 3.838 (3.838) Remain 08:28:41 loss: 0.2796 Lr: 0.02787
[2023-08-08 02:33:03,977 INFO misc.py line 115 22900] Train: [50/100][4/156] Data 0.001 (0.001) Batch 2.086 (2.086) Remain 04:36:25 loss: 0.1588 Lr: 0.02787
[2023-08-08 02:33:08,062 INFO misc.py line 115 22900] Train: [50/100][5/156] Data 0.001 (0.001) Batch 4.086 (3.086) Remain 06:48:53 loss: 0.3963 Lr: 0.02786
[2023-08-08 02:33:10,654 INFO misc.py line 115 22900] Train: [50/100][6/156] Data 0.001 (0.001) Batch 2.591 (2.921) Remain 06:27:00 loss: 0.1747 Lr: 0.02786
[2023-08-08 02:33:14,031 INFO misc.py line 115 22900] Train: [50/100][7/156] Data 0.001 (0.001) Batch 3.377 (3.035) Remain 06:42:04 loss: 0.1294 Lr: 0.02785
[2023-08-08 02:33:17,561 INFO misc.py line 115 22900] Train: [50/100][8/156] Data 0.001 (0.001) Batch 3.530 (3.134) Remain 06:55:09 loss: 0.1698 Lr: 0.02785
[2023-08-08 02:33:20,875 INFO misc.py line 115 22900] Train: [50/100][9/156] Data 0.001 (0.001) Batch 3.314 (3.164) Remain 06:59:04 loss: 0.3543 Lr: 0.02784
[2023-08-08 02:33:24,054 INFO misc.py line 115 22900] Train: [50/100][10/156] Data 0.001 (0.001) Batch 3.179 (3.166) Remain 06:59:18 loss: 0.2780 Lr: 0.02783
[2023-08-08 02:33:27,920 INFO misc.py line 115 22900] Train: [50/100][11/156] Data 0.001 (0.001) Batch 3.866 (3.254) Remain 07:10:50 loss: 0.2612 Lr: 0.02783
[2023-08-08 02:33:31,409 INFO misc.py line 115 22900] Train: [50/100][12/156] Data 0.001 (0.001) Batch 3.489 (3.280) Remain 07:14:14 loss: 0.2578 Lr: 0.02782
[2023-08-08 02:33:34,454 INFO misc.py line 115 22900] Train: [50/100][13/156] Data 0.001 (0.001) Batch 3.045 (3.256) Remain 07:11:04 loss: 0.0856 Lr: 0.02782
[2023-08-08 02:33:38,312 INFO misc.py line 115 22900] Train: [50/100][14/156] Data 0.001 (0.001) Batch 3.858 (3.311) Remain 07:18:16 loss: 0.2216 Lr: 0.02781
[2023-08-08 02:33:42,270 INFO misc.py line 115 22900] Train: [50/100][15/156] Data 0.001 (0.001) Batch 3.958 (3.365) Remain 07:25:20 loss: 0.3287 Lr: 0.02781
[2023-08-08 02:33:46,011 INFO misc.py line 115 22900] Train: [50/100][16/156] Data 0.001 (0.001) Batch 3.741 (3.394) Remain 07:29:07 loss: 0.1651 Lr: 0.02780
[2023-08-08 02:33:50,505 INFO misc.py line 115 22900] Train: [50/100][17/156] Data 0.001 (0.001) Batch 4.494 (3.472) Remain 07:39:27 loss: 0.3913 Lr: 0.02780
[2023-08-08 02:33:54,239 INFO misc.py line 115 22900] Train: [50/100][18/156] Data 0.001 (0.001) Batch 3.734 (3.490) Remain 07:41:42 loss: 0.2980 Lr: 0.02779
[2023-08-08 02:33:57,906 INFO misc.py line 115 22900] Train: [50/100][19/156] Data 0.001 (0.001) Batch 3.667 (3.501) Remain 07:43:06 loss: 0.2935 Lr: 0.02779
[2023-08-08 02:34:02,001 INFO misc.py line 115 22900] Train: [50/100][20/156] Data 0.001 (0.001) Batch 4.095 (3.536) Remain 07:47:40 loss: 0.5128 Lr: 0.02778
[2023-08-08 02:34:06,045 INFO misc.py line 115 22900] Train: [50/100][21/156] Data 0.001 (0.001) Batch 4.044 (3.564) Remain 07:51:20 loss: 0.3662 Lr: 0.02778
[2023-08-08 02:34:10,654 INFO misc.py line 115 22900] Train: [50/100][22/156] Data 0.001 (0.001) Batch 4.610 (3.619) Remain 07:58:34 loss: 0.3527 Lr: 0.02777
[2023-08-08 02:34:14,420 INFO misc.py line 115 22900] Train: [50/100][23/156] Data 0.001 (0.001) Batch 3.766 (3.626) Remain 07:59:28 loss: 0.2874 Lr: 0.02777
[2023-08-08 02:34:17,782 INFO misc.py line 115 22900] Train: [50/100][24/156] Data 0.001 (0.001) Batch 3.362 (3.614) Remain 07:57:44 loss: 0.2097 Lr: 0.02776
[2023-08-08 02:34:21,658 INFO misc.py line 115 22900] Train: [50/100][25/156] Data 0.001 (0.001) Batch 3.876 (3.626) Remain 07:59:15 loss: 0.1533 Lr: 0.02776
[2023-08-08 02:34:24,985 INFO misc.py line 115 22900] Train: [50/100][26/156] Data 0.001 (0.001) Batch 3.327 (3.613) Remain 07:57:29 loss: 0.2142 Lr: 0.02775
[2023-08-08 02:34:28,972 INFO misc.py line 115 22900] Train: [50/100][27/156] Data 0.001 (0.001) Batch 3.987 (3.628) Remain 07:59:29 loss: 0.2814 Lr: 0.02775
[2023-08-08 02:34:31,318 INFO misc.py line 115 22900] Train: [50/100][28/156] Data 0.001 (0.001) Batch 2.346 (3.577) Remain 07:52:38 loss: 0.1697 Lr: 0.02774
[2023-08-08 02:34:33,731 INFO misc.py line 115 22900] Train: [50/100][29/156] Data 0.001 (0.001) Batch 2.413 (3.532) Remain 07:46:40 loss: 0.2253 Lr: 0.02773
[2023-08-08 02:34:37,097 INFO misc.py line 115 22900] Train: [50/100][30/156] Data 0.001 (0.001) Batch 3.366 (3.526) Remain 07:45:48 loss: 0.3047 Lr: 0.02773
[2023-08-08 02:34:40,785 INFO misc.py line 115 22900] Train: [50/100][31/156] Data 0.001 (0.001) Batch 3.688 (3.532) Remain 07:46:30 loss: 0.2387 Lr: 0.02772
[2023-08-08 02:34:44,440 INFO misc.py line 115 22900] Train: [50/100][32/156] Data 0.001 (0.001) Batch 3.656 (3.536) Remain 07:47:00 loss: 0.1448 Lr: 0.02772
[2023-08-08 02:34:48,415 INFO misc.py line 115 22900] Train: [50/100][33/156] Data 0.001 (0.001) Batch 3.975 (3.551) Remain 07:48:52 loss: 0.2167 Lr: 0.02771
[2023-08-08 02:34:52,468 INFO misc.py line 115 22900] Train: [50/100][34/156] Data 0.001 (0.001) Batch 4.053 (3.567) Remain 07:50:57 loss: 0.1600 Lr: 0.02771
[2023-08-08 02:34:56,466 INFO misc.py line 115 22900] Train: [50/100][35/156] Data 0.001 (0.001) Batch 3.998 (3.580) Remain 07:52:40 loss: 0.3203 Lr: 0.02770
[2023-08-08 02:35:00,104 INFO misc.py line 115 22900] Train: [50/100][36/156] Data 0.001 (0.001) Batch 3.638 (3.582) Remain 07:52:51 loss: 0.2483 Lr: 0.02770
[2023-08-08 02:35:03,077 INFO misc.py line 115 22900] Train: [50/100][37/156] Data 0.001 (0.001) Batch 2.973 (3.564) Remain 07:50:25 loss: 0.1480 Lr: 0.02769
[2023-08-08 02:35:06,177 INFO misc.py line 115 22900] Train: [50/100][38/156] Data 0.001 (0.001) Batch 3.100 (3.551) Remain 07:48:37 loss: 0.2227 Lr: 0.02769
[2023-08-08 02:35:08,183 INFO misc.py line 115 22900] Train: [50/100][39/156] Data 0.001 (0.001) Batch 2.005 (3.508) Remain 07:42:53 loss: 0.0636 Lr: 0.02768
[2023-08-08 02:35:12,510 INFO misc.py line 115 22900] Train: [50/100][40/156] Data 0.001 (0.001) Batch 4.327 (3.530) Remain 07:45:45 loss: 0.2677 Lr: 0.02768
[2023-08-08 02:35:15,921 INFO misc.py line 115 22900] Train: [50/100][41/156] Data 0.001 (0.001) Batch 3.411 (3.527) Remain 07:45:16 loss: 0.1328 Lr: 0.02767
[2023-08-08 02:35:18,914 INFO misc.py line 115 22900] Train: [50/100][42/156] Data 0.001 (0.001) Batch 2.994 (3.513) Remain 07:43:25 loss: 0.2081 Lr: 0.02767
[2023-08-08 02:35:21,898 INFO misc.py line 115 22900] Train: [50/100][43/156] Data 0.001 (0.001) Batch 2.983 (3.500) Remain 07:41:36 loss: 0.0975 Lr: 0.02766
[2023-08-08 02:35:25,087 INFO misc.py line 115 22900] Train: [50/100][44/156] Data 0.001 (0.001) Batch 3.190 (3.493) Remain 07:40:33 loss: 0.2109 Lr: 0.02766
[2023-08-08 02:35:29,276 INFO misc.py line 115 22900] Train: [50/100][45/156] Data 0.001 (0.001) Batch 4.189 (3.509) Remain 07:42:41 loss: 0.2618 Lr: 0.02765
[2023-08-08 02:35:33,267 INFO misc.py line 115 22900] Train: [50/100][46/156] Data 0.001 (0.001) Batch 3.991 (3.520) Remain 07:44:06 loss: 0.3034 Lr: 0.02765
[2023-08-08 02:35:37,236 INFO misc.py line 115 22900] Train: [50/100][47/156] Data 0.001 (0.001) Batch 3.969 (3.531) Remain 07:45:23 loss: 0.1301 Lr: 0.02764
[2023-08-08 02:35:40,899 INFO misc.py line 115 22900] Train: [50/100][48/156] Data 0.001 (0.001) Batch 3.663 (3.534) Remain 07:45:42 loss: 0.3009 Lr: 0.02763
[2023-08-08 02:35:44,290 INFO misc.py line 115 22900] Train: [50/100][49/156] Data 0.001 (0.001) Batch 3.391 (3.530) Remain 07:45:14 loss: 0.1134 Lr: 0.02763
[2023-08-08 02:35:46,826 INFO misc.py line 115 22900] Train: [50/100][50/156] Data 0.001 (0.001) Batch 2.536 (3.509) Remain 07:42:24 loss: 0.0920 Lr: 0.02762
[2023-08-08 02:35:50,722 INFO misc.py line 115 22900] Train: [50/100][51/156] Data 0.001 (0.001) Batch 3.895 (3.517) Remain 07:43:24 loss: 0.2748 Lr: 0.02762
[2023-08-08 02:35:54,701 INFO misc.py line 115 22900] Train: [50/100][52/156] Data 0.001 (0.001) Batch 3.980 (3.527) Remain 07:44:35 loss: 0.2451 Lr: 0.02761
[2023-08-08 02:35:58,323 INFO misc.py line 115 22900] Train: [50/100][53/156] Data 0.001 (0.001) Batch 3.621 (3.529) Remain 07:44:46 loss: 0.3572 Lr: 0.02761
[2023-08-08 02:35:59,925 INFO misc.py line 115 22900] Train: [50/100][54/156] Data 0.001 (0.001) Batch 1.603 (3.491) Remain 07:39:44 loss: 0.0859 Lr: 0.02760
[2023-08-08 02:36:03,622 INFO misc.py line 115 22900] Train: [50/100][55/156] Data 0.001 (0.001) Batch 3.697 (3.495) Remain 07:40:12 loss: 0.1568 Lr: 0.02760
[2023-08-08 02:36:05,915 INFO misc.py line 115 22900] Train: [50/100][56/156] Data 0.001 (0.001) Batch 2.293 (3.472) Remain 07:37:10 loss: 0.1255 Lr: 0.02759
[2023-08-08 02:36:09,434 INFO misc.py line 115 22900] Train: [50/100][57/156] Data 0.001 (0.001) Batch 3.519 (3.473) Remain 07:37:13 loss: 0.5591 Lr: 0.02759
[2023-08-08 02:36:12,795 INFO misc.py line 115 22900] Train: [50/100][58/156] Data 0.001 (0.001) Batch 3.361 (3.471) Remain 07:36:53 loss: 0.5033 Lr: 0.02758
[2023-08-08 02:36:16,678 INFO misc.py line 115 22900] Train: [50/100][59/156] Data 0.001 (0.001) Batch 3.883 (3.478) Remain 07:37:48 loss: 0.3322 Lr: 0.02758
[2023-08-08 02:36:20,169 INFO misc.py line 115 22900] Train: [50/100][60/156] Data 0.001 (0.001) Batch 3.491 (3.479) Remain 07:37:46 loss: 0.2262 Lr: 0.02757
[2023-08-08 02:36:23,904 INFO misc.py line 115 22900] Train: [50/100][61/156] Data 0.001 (0.001) Batch 3.735 (3.483) Remain 07:38:18 loss: 0.1630 Lr: 0.02757
[2023-08-08 02:36:27,093 INFO misc.py line 115 22900] Train: [50/100][62/156] Data 0.001 (0.001) Batch 3.189 (3.478) Remain 07:37:35 loss: 0.2233 Lr: 0.02756
[2023-08-08 02:36:31,166 INFO misc.py line 115 22900] Train: [50/100][63/156] Data 0.001 (0.001) Batch 4.073 (3.488) Remain 07:38:50 loss: 0.3210 Lr: 0.02756
[2023-08-08 02:36:35,177 INFO misc.py line 115 22900] Train: [50/100][64/156] Data 0.001 (0.001) Batch 4.011 (3.496) Remain 07:39:54 loss: 0.3562 Lr: 0.02755
[2023-08-08 02:36:38,611 INFO misc.py line 115 22900] Train: [50/100][65/156] Data 0.001 (0.001) Batch 3.434 (3.495) Remain 07:39:42 loss: 0.2825 Lr: 0.02754
[2023-08-08 02:36:42,703 INFO misc.py line 115 22900] Train: [50/100][66/156] Data 0.001 (0.001) Batch 4.092 (3.505) Remain 07:40:54 loss: 0.1682 Lr: 0.02754
[2023-08-08 02:36:47,014 INFO misc.py line 115 22900] Train: [50/100][67/156] Data 0.001 (0.001) Batch 4.311 (3.518) Remain 07:42:29 loss: 0.4987 Lr: 0.02753
[2023-08-08 02:36:50,149 INFO misc.py line 115 22900] Train: [50/100][68/156] Data 0.001 (0.001) Batch 3.135 (3.512) Remain 07:41:40 loss: 0.2088 Lr: 0.02753
[2023-08-08 02:36:53,073 INFO misc.py line 115 22900] Train: [50/100][69/156] Data 0.001 (0.001) Batch 2.923 (3.503) Remain 07:40:26 loss: 0.1933 Lr: 0.02752
[2023-08-08 02:36:55,997 INFO misc.py line 115 22900] Train: [50/100][70/156] Data 0.001 (0.001) Batch 2.924 (3.494) Remain 07:39:14 loss: 0.3501 Lr: 0.02752
[2023-08-08 02:37:00,067 INFO misc.py line 115 22900] Train: [50/100][71/156] Data 0.001 (0.001) Batch 4.070 (3.503) Remain 07:40:17 loss: 0.2997 Lr: 0.02751
[2023-08-08 02:37:03,751 INFO misc.py line 115 22900] Train: [50/100][72/156] Data 0.001 (0.001) Batch 3.684 (3.505) Remain 07:40:35 loss: 0.1282 Lr: 0.02751
[2023-08-08 02:37:07,619 INFO misc.py line 115 22900] Train: [50/100][73/156] Data 0.001 (0.001) Batch 3.868 (3.510) Remain 07:41:12 loss: 0.2046 Lr: 0.02750
[2023-08-08 02:37:10,799 INFO misc.py line 115 22900] Train: [50/100][74/156] Data 0.001 (0.001) Batch 3.180 (3.506) Remain 07:40:32 loss: 0.2225 Lr: 0.02750
[2023-08-08 02:37:14,994 INFO misc.py line 115 22900] Train: [50/100][75/156] Data 0.001 (0.001) Batch 4.195 (3.515) Remain 07:41:44 loss: 0.3279 Lr: 0.02749
[2023-08-08 02:37:16,861 INFO misc.py line 115 22900] Train: [50/100][76/156] Data 0.001 (0.001) Batch 1.866 (3.493) Remain 07:38:42 loss: 0.3852 Lr: 0.02749
[2023-08-08 02:37:20,134 INFO misc.py line 115 22900] Train: [50/100][77/156] Data 0.001 (0.001) Batch 3.273 (3.490) Remain 07:38:15 loss: 0.3087 Lr: 0.02748
[2023-08-08 02:37:23,074 INFO misc.py line 115 22900] Train: [50/100][78/156] Data 0.001 (0.001) Batch 2.940 (3.482) Remain 07:37:14 loss: 0.2371 Lr: 0.02748
[2023-08-08 02:37:25,676 INFO misc.py line 115 22900] Train: [50/100][79/156] Data 0.001 (0.001) Batch 2.602 (3.471) Remain 07:35:39 loss: 0.3052 Lr: 0.02747
[2023-08-08 02:37:28,811 INFO misc.py line 115 22900] Train: [50/100][80/156] Data 0.001 (0.001) Batch 3.135 (3.466) Remain 07:35:02 loss: 0.3586 Lr: 0.02747
[2023-08-08 02:37:32,437 INFO misc.py line 115 22900] Train: [50/100][81/156] Data 0.001 (0.001) Batch 3.626 (3.469) Remain 07:35:14 loss: 0.1809 Lr: 0.02746
[2023-08-08 02:37:37,039 INFO misc.py line 115 22900] Train: [50/100][82/156] Data 0.001 (0.001) Batch 4.602 (3.483) Remain 07:37:04 loss: 0.3640 Lr: 0.02746
[2023-08-08 02:37:40,586 INFO misc.py line 115 22900] Train: [50/100][83/156] Data 0.001 (0.001) Batch 3.547 (3.484) Remain 07:37:07 loss: 0.2254 Lr: 0.02745
[2023-08-08 02:37:44,716 INFO misc.py line 115 22900] Train: [50/100][84/156] Data 0.001 (0.001) Batch 4.130 (3.492) Remain 07:38:06 loss: 0.3391 Lr: 0.02744
[2023-08-08 02:37:48,264 INFO misc.py line 115 22900] Train: [50/100][85/156] Data 0.001 (0.001) Batch 3.547 (3.492) Remain 07:38:08 loss: 0.1455 Lr: 0.02744
[2023-08-08 02:37:51,481 INFO misc.py line 115 22900] Train: [50/100][86/156] Data 0.001 (0.001) Batch 3.217 (3.489) Remain 07:37:38 loss: 0.4012 Lr: 0.02743
[2023-08-08 02:37:55,850 INFO misc.py line 115 22900] Train: [50/100][87/156] Data 0.001 (0.001) Batch 4.368 (3.500) Remain 07:38:57 loss: 0.4140 Lr: 0.02743
[2023-08-08 02:37:58,338 INFO misc.py line 115 22900] Train: [50/100][88/156] Data 0.001 (0.001) Batch 2.489 (3.488) Remain 07:37:20 loss: 0.2767 Lr: 0.02742
[2023-08-08 02:38:02,153 INFO misc.py line 115 22900] Train: [50/100][89/156] Data 0.001 (0.001) Batch 3.815 (3.491) Remain 07:37:47 loss: 0.2438 Lr: 0.02742
[2023-08-08 02:38:05,518 INFO misc.py line 115 22900] Train: [50/100][90/156] Data 0.001 (0.001) Batch 3.365 (3.490) Remain 07:37:32 loss: 0.2537 Lr: 0.02741
[2023-08-08 02:38:07,771 INFO misc.py line 115 22900] Train: [50/100][91/156] Data 0.001 (0.001) Batch 2.252 (3.476) Remain 07:35:37 loss: 0.1029 Lr: 0.02741
[2023-08-08 02:38:11,432 INFO misc.py line 115 22900] Train: [50/100][92/156] Data 0.001 (0.001) Batch 3.661 (3.478) Remain 07:35:50 loss: 0.3259 Lr: 0.02740
[2023-08-08 02:38:14,683 INFO misc.py line 115 22900] Train: [50/100][93/156] Data 0.001 (0.001) Batch 3.251 (3.475) Remain 07:35:27 loss: 0.1608 Lr: 0.02740
[2023-08-08 02:38:17,343 INFO misc.py line 115 22900] Train: [50/100][94/156] Data 0.001 (0.001) Batch 2.661 (3.467) Remain 07:34:13 loss: 0.2588 Lr: 0.02739
[2023-08-08 02:38:20,198 INFO misc.py line 115 22900] Train: [50/100][95/156] Data 0.001 (0.001) Batch 2.854 (3.460) Remain 07:33:17 loss: 0.2268 Lr: 0.02739
[2023-08-08 02:38:23,446 INFO misc.py line 115 22900] Train: [50/100][96/156] Data 0.001 (0.001) Batch 3.248 (3.458) Remain 07:32:56 loss: 0.2582 Lr: 0.02738
[2023-08-08 02:38:27,472 INFO misc.py line 115 22900] Train: [50/100][97/156] Data 0.001 (0.001) Batch 4.025 (3.464) Remain 07:33:40 loss: 0.4493 Lr: 0.02738
[2023-08-08 02:38:31,544 INFO misc.py line 115 22900] Train: [50/100][98/156] Data 0.001 (0.001) Batch 4.072 (3.470) Remain 07:34:27 loss: 0.2499 Lr: 0.02737
[2023-08-08 02:38:35,606 INFO misc.py line 115 22900] Train: [50/100][99/156] Data 0.001 (0.001) Batch 4.062 (3.476) Remain 07:35:12 loss: 0.3638 Lr: 0.02737
[2023-08-08 02:38:39,693 INFO misc.py line 115 22900] Train: [50/100][100/156] Data 0.001 (0.001) Batch 4.087 (3.482) Remain 07:35:58 loss: 0.2761 Lr: 0.02736
[2023-08-08 02:38:43,132 INFO misc.py line 115 22900] Train: [50/100][101/156] Data 0.001 (0.001) Batch 3.439 (3.482) Remain 07:35:51 loss: 0.1783 Lr: 0.02736
[2023-08-08 02:38:45,737 INFO misc.py line 115 22900] Train: [50/100][102/156] Data 0.001 (0.001) Batch 2.606 (3.473) Remain 07:34:38 loss: 0.1917 Lr: 0.02735
[2023-08-08 02:38:49,279 INFO misc.py line 115 22900] Train: [50/100][103/156] Data 0.001 (0.001) Batch 3.541 (3.474) Remain 07:34:40 loss: 0.1999 Lr: 0.02734
[2023-08-08 02:38:52,677 INFO misc.py line 115 22900] Train: [50/100][104/156] Data 0.001 (0.001) Batch 3.399 (3.473) Remain 07:34:31 loss: 0.3033 Lr: 0.02734
[2023-08-08 02:38:55,657 INFO misc.py line 115 22900] Train: [50/100][105/156] Data 0.001 (0.001) Batch 2.979 (3.468) Remain 07:33:49 loss: 0.2359 Lr: 0.02733
[2023-08-08 02:38:59,950 INFO misc.py line 115 22900] Train: [50/100][106/156] Data 0.001 (0.001) Batch 4.293 (3.476) Remain 07:34:48 loss: 0.4540 Lr: 0.02733
[2023-08-08 02:39:02,796 INFO misc.py line 115 22900] Train: [50/100][107/156] Data 0.001 (0.001) Batch 2.847 (3.470) Remain 07:33:57 loss: 0.2133 Lr: 0.02732
[2023-08-08 02:39:06,193 INFO misc.py line 115 22900] Train: [50/100][108/156] Data 0.001 (0.001) Batch 3.396 (3.470) Remain 07:33:48 loss: 0.1692 Lr: 0.02732
[2023-08-08 02:39:10,100 INFO misc.py line 115 22900] Train: [50/100][109/156] Data 0.001 (0.001) Batch 3.907 (3.474) Remain 07:34:17 loss: 0.2957 Lr: 0.02731
[2023-08-08 02:39:13,604 INFO misc.py line 115 22900] Train: [50/100][110/156] Data 0.001 (0.001) Batch 3.504 (3.474) Remain 07:34:16 loss: 0.3222 Lr: 0.02731
[2023-08-08 02:39:16,799 INFO misc.py line 115 22900] Train: [50/100][111/156] Data 0.001 (0.001) Batch 3.196 (3.471) Remain 07:33:52 loss: 0.1827 Lr: 0.02730
[2023-08-08 02:39:20,119 INFO misc.py line 115 22900] Train: [50/100][112/156] Data 0.002 (0.001) Batch 3.319 (3.470) Remain 07:33:38 loss: 0.2789 Lr: 0.02730
[2023-08-08 02:39:24,059 INFO misc.py line 115 22900] Train: [50/100][113/156] Data 0.001 (0.001) Batch 3.941 (3.474) Remain 07:34:08 loss: 0.2488 Lr: 0.02729
[2023-08-08 02:39:28,114 INFO misc.py line 115 22900] Train: [50/100][114/156] Data 0.001 (0.001) Batch 4.055 (3.479) Remain 07:34:46 loss: 0.2468 Lr: 0.02729
[2023-08-08 02:39:31,278 INFO misc.py line 115 22900] Train: [50/100][115/156] Data 0.001 (0.001) Batch 3.164 (3.477) Remain 07:34:20 loss: 0.2280 Lr: 0.02728
[2023-08-08 02:39:35,357 INFO misc.py line 115 22900] Train: [50/100][116/156] Data 0.001 (0.001) Batch 4.079 (3.482) Remain 07:34:58 loss: 0.2814 Lr: 0.02728
[2023-08-08 02:39:39,399 INFO misc.py line 115 22900] Train: [50/100][117/156] Data 0.001 (0.001) Batch 4.043 (3.487) Remain 07:35:33 loss: 0.3838 Lr: 0.02727
[2023-08-08 02:39:42,784 INFO misc.py line 115 22900] Train: [50/100][118/156] Data 0.001 (0.001) Batch 3.385 (3.486) Remain 07:35:23 loss: 0.1210 Lr: 0.02727
[2023-08-08 02:39:47,126 INFO misc.py line 115 22900] Train: [50/100][119/156] Data 0.001 (0.001) Batch 4.341 (3.493) Remain 07:36:17 loss: 0.4096 Lr: 0.02726
[2023-08-08 02:39:50,867 INFO misc.py line 115 22900] Train: [50/100][120/156] Data 0.001 (0.001) Batch 3.741 (3.496) Remain 07:36:30 loss: 0.3464 Lr: 0.02725
[2023-08-08 02:39:55,108 INFO misc.py line 115 22900] Train: [50/100][121/156] Data 0.001 (0.001) Batch 4.241 (3.502) Remain 07:37:16 loss: 0.3110 Lr: 0.02725
[2023-08-08 02:39:57,815 INFO misc.py line 115 22900] Train: [50/100][122/156] Data 0.001 (0.001) Batch 2.708 (3.495) Remain 07:36:21 loss: 0.1318 Lr: 0.02724
[2023-08-08 02:40:01,486 INFO misc.py line 115 22900] Train: [50/100][123/156] Data 0.001 (0.001) Batch 3.671 (3.497) Remain 07:36:29 loss: 0.3221 Lr: 0.02724
[2023-08-08 02:40:05,302 INFO misc.py line 115 22900] Train: [50/100][124/156] Data 0.001 (0.001) Batch 3.816 (3.499) Remain 07:36:46 loss: 0.2025 Lr: 0.02723
[2023-08-08 02:40:08,676 INFO misc.py line 115 22900] Train: [50/100][125/156] Data 0.001 (0.001) Batch 3.374 (3.498) Remain 07:36:34 loss: 0.4882 Lr: 0.02723
[2023-08-08 02:40:11,491 INFO misc.py line 115 22900] Train: [50/100][126/156] Data 0.001 (0.001) Batch 2.815 (3.493) Remain 07:35:47 loss: 0.2174 Lr: 0.02722
[2023-08-08 02:40:14,360 INFO misc.py line 115 22900] Train: [50/100][127/156] Data 0.001 (0.001) Batch 2.869 (3.488) Remain 07:35:04 loss: 0.1951 Lr: 0.02722
[2023-08-08 02:40:18,311 INFO misc.py line 115 22900] Train: [50/100][128/156] Data 0.001 (0.001) Batch 3.951 (3.491) Remain 07:35:30 loss: 0.1660 Lr: 0.02721
[2023-08-08 02:40:21,311 INFO misc.py line 115 22900] Train: [50/100][129/156] Data 0.001 (0.001) Batch 3.000 (3.487) Remain 07:34:56 loss: 0.1463 Lr: 0.02721
[2023-08-08 02:40:24,327 INFO misc.py line 115 22900] Train: [50/100][130/156] Data 0.001 (0.001) Batch 3.016 (3.484) Remain 07:34:23 loss: 0.2896 Lr: 0.02720
[2023-08-08 02:40:28,367 INFO misc.py line 115 22900] Train: [50/100][131/156] Data 0.001 (0.001) Batch 4.039 (3.488) Remain 07:34:54 loss: 0.5693 Lr: 0.02720
[2023-08-08 02:40:32,348 INFO misc.py line 115 22900] Train: [50/100][132/156] Data 0.001 (0.001) Batch 3.981 (3.492) Remain 07:35:20 loss: 0.3984 Lr: 0.02719
[2023-08-08 02:40:35,892 INFO misc.py line 115 22900] Train: [50/100][133/156] Data 0.001 (0.001) Batch 3.544 (3.492) Remain 07:35:20 loss: 0.2307 Lr: 0.02719
[2023-08-08 02:40:38,606 INFO misc.py line 115 22900] Train: [50/100][134/156] Data 0.001 (0.001) Batch 2.714 (3.486) Remain 07:34:30 loss: 0.2876 Lr: 0.02718
[2023-08-08 02:40:41,737 INFO misc.py line 115 22900] Train: [50/100][135/156] Data 0.001 (0.001) Batch 3.131 (3.484) Remain 07:34:05 loss: 0.1497 Lr: 0.02718
[2023-08-08 02:40:44,333 INFO misc.py line 115 22900] Train: [50/100][136/156] Data 0.001 (0.001) Batch 2.595 (3.477) Remain 07:33:10 loss: 0.2056 Lr: 0.02717
[2023-08-08 02:40:47,580 INFO misc.py line 115 22900] Train: [50/100][137/156] Data 0.001 (0.001) Batch 3.247 (3.475) Remain 07:32:53 loss: 0.1535 Lr: 0.02717
[2023-08-08 02:40:51,322 INFO misc.py line 115 22900] Train: [50/100][138/156] Data 0.001 (0.001) Batch 3.742 (3.477) Remain 07:33:05 loss: 0.3263 Lr: 0.02716
[2023-08-08 02:40:54,787 INFO misc.py line 115 22900] Train: [50/100][139/156] Data 0.001 (0.001) Batch 3.464 (3.477) Remain 07:33:01 loss: 0.3326 Lr: 0.02715
[2023-08-08 02:40:58,384 INFO misc.py line 115 22900] Train: [50/100][140/156] Data 0.001 (0.001) Batch 3.598 (3.478) Remain 07:33:04 loss: 0.3511 Lr: 0.02715
[2023-08-08 02:41:01,761 INFO misc.py line 115 22900] Train: [50/100][141/156] Data 0.001 (0.001) Batch 3.377 (3.477) Remain 07:32:55 loss: 0.2752 Lr: 0.02714
[2023-08-08 02:41:06,093 INFO misc.py line 115 22900] Train: [50/100][142/156] Data 0.001 (0.001) Batch 4.332 (3.483) Remain 07:33:39 loss: 0.3882 Lr: 0.02714
[2023-08-08 02:41:09,346 INFO misc.py line 115 22900] Train: [50/100][143/156] Data 0.001 (0.001) Batch 3.253 (3.482) Remain 07:33:23 loss: 0.0655 Lr: 0.02713
[2023-08-08 02:41:12,269 INFO misc.py line 115 22900] Train: [50/100][144/156] Data 0.001 (0.001) Batch 2.923 (3.478) Remain 07:32:49 loss: 0.2019 Lr: 0.02713
[2023-08-08 02:41:14,611 INFO misc.py line 115 22900] Train: [50/100][145/156] Data 0.001 (0.001) Batch 2.342 (3.470) Remain 07:31:43 loss: 0.1494 Lr: 0.02712
[2023-08-08 02:41:18,967 INFO misc.py line 115 22900] Train: [50/100][146/156] Data 0.001 (0.001) Batch 4.356 (3.476) Remain 07:32:27 loss: 0.5145 Lr: 0.02712
[2023-08-08 02:41:22,196 INFO misc.py line 115 22900] Train: [50/100][147/156] Data 0.001 (0.001) Batch 3.229 (3.474) Remain 07:32:11 loss: 0.1257 Lr: 0.02711
[2023-08-08 02:41:25,325 INFO misc.py line 115 22900] Train: [50/100][148/156] Data 0.001 (0.001) Batch 3.129 (3.472) Remain 07:31:49 loss: 0.2665 Lr: 0.02711
[2023-08-08 02:41:29,289 INFO misc.py line 115 22900] Train: [50/100][149/156] Data 0.001 (0.001) Batch 3.964 (3.475) Remain 07:32:11 loss: 0.2938 Lr: 0.02710
[2023-08-08 02:41:32,759 INFO misc.py line 115 22900] Train: [50/100][150/156] Data 0.001 (0.001) Batch 3.469 (3.475) Remain 07:32:08 loss: 0.1847 Lr: 0.02710
[2023-08-08 02:41:36,007 INFO misc.py line 115 22900] Train: [50/100][151/156] Data 0.001 (0.001) Batch 3.249 (3.474) Remain 07:31:52 loss: 0.1769 Lr: 0.02709
[2023-08-08 02:41:39,149 INFO misc.py line 115 22900] Train: [50/100][152/156] Data 0.001 (0.001) Batch 3.141 (3.472) Remain 07:31:31 loss: 0.2822 Lr: 0.02709
[2023-08-08 02:41:42,365 INFO misc.py line 115 22900] Train: [50/100][153/156] Data 0.001 (0.001) Batch 3.217 (3.470) Remain 07:31:15 loss: 0.3025 Lr: 0.02708
[2023-08-08 02:41:45,892 INFO misc.py line 115 22900] Train: [50/100][154/156] Data 0.001 (0.001) Batch 3.527 (3.470) Remain 07:31:14 loss: 0.3906 Lr: 0.02708
[2023-08-08 02:41:48,861 INFO misc.py line 115 22900] Train: [50/100][155/156] Data 0.001 (0.001) Batch 2.968 (3.467) Remain 07:30:45 loss: 0.1682 Lr: 0.02707
[2023-08-08 02:41:52,522 INFO misc.py line 115 22900] Train: [50/100][156/156] Data 0.001 (0.001) Batch 3.661 (3.468) Remain 07:30:51 loss: 0.2485 Lr: 0.02706
[2023-08-08 02:41:52,522 INFO misc.py line 129 22900] Train result: loss: 0.2598 
[2023-08-08 02:41:52,522 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 02:41:54,648 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.9697 
[2023-08-08 02:41:55,517 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.6509 
[2023-08-08 02:41:57,181 INFO evaluator.py line 122 22900] Test: [3/24] Loss 1.1394 
[2023-08-08 02:41:58,705 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.4759 
[2023-08-08 02:42:00,550 INFO evaluator.py line 122 22900] Test: [5/24] Loss 2.0152 
[2023-08-08 02:42:02,213 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.5543 
[2023-08-08 02:42:04,352 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.9287 
[2023-08-08 02:42:06,156 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.9442 
[2023-08-08 02:42:07,438 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.3240 
[2023-08-08 02:42:09,571 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.5644 
[2023-08-08 02:42:10,097 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.6535 
[2023-08-08 02:42:11,628 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.9715 
[2023-08-08 02:42:14,340 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.3568 
[2023-08-08 02:42:16,022 INFO evaluator.py line 122 22900] Test: [14/24] Loss 1.0694 
[2023-08-08 02:42:18,046 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.3232 
[2023-08-08 02:42:20,755 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.4396 
[2023-08-08 02:42:23,461 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.6201 
[2023-08-08 02:42:25,308 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.4758 
[2023-08-08 02:42:26,057 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.3721 
[2023-08-08 02:42:26,942 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.7947 
[2023-08-08 02:42:29,200 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.6265 
[2023-08-08 02:42:31,165 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.7635 
[2023-08-08 02:42:33,012 INFO evaluator.py line 122 22900] Test: [23/24] Loss 4.1339 
[2023-08-08 02:42:34,948 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.7244 
[2023-08-08 02:42:34,996 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2316/0.3560/0.6849.
[2023-08-08 02:42:34,996 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6568/0.9377
[2023-08-08 02:42:34,996 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9468/0.9886
[2023-08-08 02:42:34,996 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1287/0.2229
[2023-08-08 02:42:34,996 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.2264/0.3716
[2023-08-08 02:42:34,996 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6267/0.7560
[2023-08-08 02:42:34,996 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.2569/0.2995
[2023-08-08 02:42:34,996 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.4478/0.7722
[2023-08-08 02:42:34,996 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1554/0.1836
[2023-08-08 02:42:34,996 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1834/0.5431
[2023-08-08 02:42:34,996 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0103/0.0103
[2023-08-08 02:42:34,996 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0554/0.4742
[2023-08-08 02:42:34,996 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0178/0.0186
[2023-08-08 02:42:34,996 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1343/0.3515
[2023-08-08 02:42:34,997 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0238/0.0275
[2023-08-08 02:42:34,997 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0020/0.0021
[2023-08-08 02:42:34,997 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.1656/0.1667
[2023-08-08 02:42:34,997 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.2077/0.2905
[2023-08-08 02:42:34,997 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.3130/0.6003
[2023-08-08 02:42:34,997 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 02:42:34,997 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0730/0.1025
[2023-08-08 02:42:34,997 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 02:42:34,997 INFO misc.py line 152 22900] Currently Best mIoU: 0.2562
[2023-08-08 02:42:34,997 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 02:42:40,726 INFO misc.py line 115 22900] Train: [51/100][1/156] Data 0.852 (0.852) Batch 4.915 (4.915) Remain 10:38:55 loss: 0.2946 Lr: 0.02706
[2023-08-08 02:42:44,358 INFO misc.py line 115 22900] Train: [51/100][2/156] Data 0.001 (0.001) Batch 3.632 (3.632) Remain 07:52:04 loss: 0.3550 Lr: 0.02705
[2023-08-08 02:42:47,155 INFO misc.py line 115 22900] Train: [51/100][3/156] Data 0.001 (0.001) Batch 2.797 (2.797) Remain 06:03:25 loss: 0.2542 Lr: 0.02705
[2023-08-08 02:42:51,222 INFO misc.py line 115 22900] Train: [51/100][4/156] Data 0.001 (0.001) Batch 4.067 (4.067) Remain 08:48:27 loss: 0.7101 Lr: 0.02704
[2023-08-08 02:42:55,248 INFO misc.py line 115 22900] Train: [51/100][5/156] Data 0.001 (0.001) Batch 4.026 (4.047) Remain 08:45:44 loss: 0.1613 Lr: 0.02704
[2023-08-08 02:42:57,442 INFO misc.py line 115 22900] Train: [51/100][6/156] Data 0.001 (0.001) Batch 2.193 (3.429) Remain 07:25:25 loss: 0.1415 Lr: 0.02703
[2023-08-08 02:43:01,487 INFO misc.py line 115 22900] Train: [51/100][7/156] Data 0.001 (0.001) Batch 4.046 (3.583) Remain 07:45:23 loss: 0.5047 Lr: 0.02703
[2023-08-08 02:43:03,695 INFO misc.py line 115 22900] Train: [51/100][8/156] Data 0.001 (0.001) Batch 2.207 (3.308) Remain 07:09:35 loss: 0.1198 Lr: 0.02702
[2023-08-08 02:43:08,332 INFO misc.py line 115 22900] Train: [51/100][9/156] Data 0.001 (0.001) Batch 4.637 (3.529) Remain 07:38:18 loss: 0.3734 Lr: 0.02702
[2023-08-08 02:43:11,662 INFO misc.py line 115 22900] Train: [51/100][10/156] Data 0.001 (0.001) Batch 3.330 (3.501) Remain 07:34:33 loss: 0.2216 Lr: 0.02701
[2023-08-08 02:43:15,042 INFO misc.py line 115 22900] Train: [51/100][11/156] Data 0.001 (0.001) Batch 3.380 (3.486) Remain 07:32:31 loss: 0.3785 Lr: 0.02701
[2023-08-08 02:43:18,858 INFO misc.py line 115 22900] Train: [51/100][12/156] Data 0.001 (0.001) Batch 3.815 (3.523) Remain 07:37:13 loss: 0.2659 Lr: 0.02700
[2023-08-08 02:43:22,462 INFO misc.py line 115 22900] Train: [51/100][13/156] Data 0.001 (0.001) Batch 3.604 (3.531) Remain 07:38:13 loss: 0.2894 Lr: 0.02700
[2023-08-08 02:43:25,311 INFO misc.py line 115 22900] Train: [51/100][14/156] Data 0.001 (0.001) Batch 2.849 (3.469) Remain 07:30:07 loss: 0.1268 Lr: 0.02699
[2023-08-08 02:43:28,335 INFO misc.py line 115 22900] Train: [51/100][15/156] Data 0.001 (0.001) Batch 3.024 (3.432) Remain 07:25:15 loss: 0.1199 Lr: 0.02699
[2023-08-08 02:43:32,029 INFO misc.py line 115 22900] Train: [51/100][16/156] Data 0.001 (0.001) Batch 3.694 (3.452) Remain 07:27:49 loss: 0.2487 Lr: 0.02698
[2023-08-08 02:43:36,287 INFO misc.py line 115 22900] Train: [51/100][17/156] Data 0.001 (0.001) Batch 4.258 (3.509) Remain 07:35:13 loss: 0.7017 Lr: 0.02697
[2023-08-08 02:43:39,386 INFO misc.py line 115 22900] Train: [51/100][18/156] Data 0.001 (0.001) Batch 3.099 (3.482) Remain 07:31:37 loss: 0.2077 Lr: 0.02697
[2023-08-08 02:43:43,062 INFO misc.py line 115 22900] Train: [51/100][19/156] Data 0.001 (0.001) Batch 3.676 (3.494) Remain 07:33:08 loss: 0.3663 Lr: 0.02696
[2023-08-08 02:43:44,933 INFO misc.py line 115 22900] Train: [51/100][20/156] Data 0.001 (0.001) Batch 1.871 (3.399) Remain 07:20:41 loss: 0.2302 Lr: 0.02696
[2023-08-08 02:43:48,977 INFO misc.py line 115 22900] Train: [51/100][21/156] Data 0.001 (0.001) Batch 4.044 (3.435) Remain 07:25:17 loss: 0.1195 Lr: 0.02695
[2023-08-08 02:43:52,927 INFO misc.py line 115 22900] Train: [51/100][22/156] Data 0.001 (0.001) Batch 3.950 (3.462) Remain 07:28:45 loss: 0.2599 Lr: 0.02695
[2023-08-08 02:43:56,923 INFO misc.py line 115 22900] Train: [51/100][23/156] Data 0.001 (0.001) Batch 3.996 (3.488) Remain 07:32:09 loss: 0.3719 Lr: 0.02694
[2023-08-08 02:44:00,790 INFO misc.py line 115 22900] Train: [51/100][24/156] Data 0.001 (0.001) Batch 3.866 (3.506) Remain 07:34:25 loss: 0.3424 Lr: 0.02694
[2023-08-08 02:44:04,808 INFO misc.py line 115 22900] Train: [51/100][25/156] Data 0.001 (0.001) Batch 4.019 (3.530) Remain 07:37:23 loss: 0.3472 Lr: 0.02693
[2023-08-08 02:44:06,931 INFO misc.py line 115 22900] Train: [51/100][26/156] Data 0.001 (0.001) Batch 2.123 (3.469) Remain 07:29:24 loss: 0.1771 Lr: 0.02693
[2023-08-08 02:44:10,774 INFO misc.py line 115 22900] Train: [51/100][27/156] Data 0.001 (0.001) Batch 3.843 (3.484) Remain 07:31:22 loss: 0.3269 Lr: 0.02692
[2023-08-08 02:44:15,274 INFO misc.py line 115 22900] Train: [51/100][28/156] Data 0.001 (0.001) Batch 4.499 (3.525) Remain 07:36:34 loss: 0.5714 Lr: 0.02692
[2023-08-08 02:44:18,878 INFO misc.py line 115 22900] Train: [51/100][29/156] Data 0.001 (0.001) Batch 3.604 (3.528) Remain 07:36:54 loss: 0.1283 Lr: 0.02691
[2023-08-08 02:44:22,314 INFO misc.py line 115 22900] Train: [51/100][30/156] Data 0.001 (0.001) Batch 3.435 (3.524) Remain 07:36:24 loss: 0.3095 Lr: 0.02691
[2023-08-08 02:44:26,356 INFO misc.py line 115 22900] Train: [51/100][31/156] Data 0.001 (0.001) Batch 4.043 (3.543) Remain 07:38:44 loss: 0.4546 Lr: 0.02690
[2023-08-08 02:44:29,842 INFO misc.py line 115 22900] Train: [51/100][32/156] Data 0.001 (0.001) Batch 3.485 (3.541) Remain 07:38:25 loss: 0.3017 Lr: 0.02690
[2023-08-08 02:44:32,916 INFO misc.py line 115 22900] Train: [51/100][33/156] Data 0.001 (0.001) Batch 3.075 (3.525) Remain 07:36:21 loss: 0.2502 Lr: 0.02689
[2023-08-08 02:44:36,630 INFO misc.py line 115 22900] Train: [51/100][34/156] Data 0.001 (0.001) Batch 3.714 (3.531) Remain 07:37:05 loss: 0.2484 Lr: 0.02689
[2023-08-08 02:44:39,642 INFO misc.py line 115 22900] Train: [51/100][35/156] Data 0.001 (0.001) Batch 3.012 (3.515) Remain 07:34:55 loss: 0.1733 Lr: 0.02688
[2023-08-08 02:44:44,110 INFO misc.py line 115 22900] Train: [51/100][36/156] Data 0.001 (0.001) Batch 4.469 (3.544) Remain 07:38:36 loss: 0.4174 Lr: 0.02687
[2023-08-08 02:44:47,899 INFO misc.py line 115 22900] Train: [51/100][37/156] Data 0.001 (0.001) Batch 3.789 (3.551) Remain 07:39:28 loss: 0.2044 Lr: 0.02687
[2023-08-08 02:44:51,088 INFO misc.py line 115 22900] Train: [51/100][38/156] Data 0.001 (0.001) Batch 3.188 (3.541) Remain 07:38:04 loss: 0.2358 Lr: 0.02686
[2023-08-08 02:44:54,519 INFO misc.py line 115 22900] Train: [51/100][39/156] Data 0.001 (0.001) Batch 3.431 (3.538) Remain 07:37:37 loss: 0.3196 Lr: 0.02686
[2023-08-08 02:44:58,880 INFO misc.py line 115 22900] Train: [51/100][40/156] Data 0.001 (0.001) Batch 4.361 (3.560) Remain 07:40:26 loss: 0.4099 Lr: 0.02685
[2023-08-08 02:45:01,715 INFO misc.py line 115 22900] Train: [51/100][41/156] Data 0.001 (0.001) Batch 2.836 (3.541) Remain 07:37:55 loss: 0.2145 Lr: 0.02685
[2023-08-08 02:45:04,642 INFO misc.py line 115 22900] Train: [51/100][42/156] Data 0.001 (0.001) Batch 2.927 (3.525) Remain 07:35:49 loss: 0.1920 Lr: 0.02684
[2023-08-08 02:45:07,265 INFO misc.py line 115 22900] Train: [51/100][43/156] Data 0.001 (0.001) Batch 2.622 (3.503) Remain 07:32:50 loss: 0.1395 Lr: 0.02684
[2023-08-08 02:45:11,381 INFO misc.py line 115 22900] Train: [51/100][44/156] Data 0.001 (0.001) Batch 4.117 (3.518) Remain 07:34:43 loss: 0.2088 Lr: 0.02683
[2023-08-08 02:45:13,251 INFO misc.py line 115 22900] Train: [51/100][45/156] Data 0.001 (0.001) Batch 1.869 (3.478) Remain 07:29:35 loss: 0.1314 Lr: 0.02683
[2023-08-08 02:45:16,445 INFO misc.py line 115 22900] Train: [51/100][46/156] Data 0.001 (0.001) Batch 3.194 (3.472) Remain 07:28:40 loss: 0.2514 Lr: 0.02682
[2023-08-08 02:45:19,921 INFO misc.py line 115 22900] Train: [51/100][47/156] Data 0.001 (0.001) Batch 3.476 (3.472) Remain 07:28:38 loss: 0.5225 Lr: 0.02682
[2023-08-08 02:45:23,998 INFO misc.py line 115 22900] Train: [51/100][48/156] Data 0.001 (0.001) Batch 4.078 (3.485) Remain 07:30:18 loss: 0.2708 Lr: 0.02681
[2023-08-08 02:45:28,050 INFO misc.py line 115 22900] Train: [51/100][49/156] Data 0.001 (0.001) Batch 4.052 (3.498) Remain 07:31:50 loss: 0.1572 Lr: 0.02681
[2023-08-08 02:45:31,474 INFO misc.py line 115 22900] Train: [51/100][50/156] Data 0.001 (0.001) Batch 3.423 (3.496) Remain 07:31:35 loss: 0.1588 Lr: 0.02680
[2023-08-08 02:45:35,486 INFO misc.py line 115 22900] Train: [51/100][51/156] Data 0.001 (0.001) Batch 4.012 (3.507) Remain 07:32:54 loss: 0.2924 Lr: 0.02680
[2023-08-08 02:45:38,471 INFO misc.py line 115 22900] Train: [51/100][52/156] Data 0.001 (0.001) Batch 2.985 (3.496) Remain 07:31:28 loss: 0.3110 Lr: 0.02679
[2023-08-08 02:45:42,513 INFO misc.py line 115 22900] Train: [51/100][53/156] Data 0.001 (0.001) Batch 4.042 (3.507) Remain 07:32:49 loss: 0.3390 Lr: 0.02678
[2023-08-08 02:45:45,322 INFO misc.py line 115 22900] Train: [51/100][54/156] Data 0.001 (0.001) Batch 2.810 (3.493) Remain 07:31:00 loss: 0.1520 Lr: 0.02678
[2023-08-08 02:45:48,554 INFO misc.py line 115 22900] Train: [51/100][55/156] Data 0.001 (0.001) Batch 3.232 (3.488) Remain 07:30:17 loss: 0.2120 Lr: 0.02677
[2023-08-08 02:45:52,452 INFO misc.py line 115 22900] Train: [51/100][56/156] Data 0.001 (0.001) Batch 3.898 (3.496) Remain 07:31:14 loss: 0.3607 Lr: 0.02677
[2023-08-08 02:45:55,424 INFO misc.py line 115 22900] Train: [51/100][57/156] Data 0.001 (0.001) Batch 2.971 (3.486) Remain 07:29:55 loss: 0.1518 Lr: 0.02676
[2023-08-08 02:45:59,663 INFO misc.py line 115 22900] Train: [51/100][58/156] Data 0.001 (0.001) Batch 4.239 (3.500) Remain 07:31:38 loss: 0.3223 Lr: 0.02676
[2023-08-08 02:46:03,903 INFO misc.py line 115 22900] Train: [51/100][59/156] Data 0.001 (0.001) Batch 4.240 (3.513) Remain 07:33:16 loss: 0.2512 Lr: 0.02675
[2023-08-08 02:46:06,341 INFO misc.py line 115 22900] Train: [51/100][60/156] Data 0.001 (0.001) Batch 2.438 (3.494) Remain 07:30:47 loss: 0.1444 Lr: 0.02675
[2023-08-08 02:46:09,538 INFO misc.py line 115 22900] Train: [51/100][61/156] Data 0.001 (0.001) Batch 3.197 (3.489) Remain 07:30:04 loss: 0.4218 Lr: 0.02674
[2023-08-08 02:46:13,263 INFO misc.py line 115 22900] Train: [51/100][62/156] Data 0.001 (0.001) Batch 3.725 (3.493) Remain 07:30:31 loss: 0.2805 Lr: 0.02674
[2023-08-08 02:46:16,929 INFO misc.py line 115 22900] Train: [51/100][63/156] Data 0.001 (0.001) Batch 3.667 (3.496) Remain 07:30:50 loss: 0.3083 Lr: 0.02673
[2023-08-08 02:46:20,914 INFO misc.py line 115 22900] Train: [51/100][64/156] Data 0.001 (0.001) Batch 3.985 (3.504) Remain 07:31:48 loss: 0.2400 Lr: 0.02673
[2023-08-08 02:46:24,960 INFO misc.py line 115 22900] Train: [51/100][65/156] Data 0.001 (0.001) Batch 4.046 (3.513) Remain 07:32:52 loss: 0.2606 Lr: 0.02672
[2023-08-08 02:46:28,532 INFO misc.py line 115 22900] Train: [51/100][66/156] Data 0.001 (0.001) Batch 3.572 (3.514) Remain 07:32:56 loss: 0.1710 Lr: 0.02672
[2023-08-08 02:46:32,525 INFO misc.py line 115 22900] Train: [51/100][67/156] Data 0.001 (0.001) Batch 3.993 (3.521) Remain 07:33:50 loss: 0.2972 Lr: 0.02671
[2023-08-08 02:46:35,903 INFO misc.py line 115 22900] Train: [51/100][68/156] Data 0.001 (0.001) Batch 3.378 (3.519) Remain 07:33:30 loss: 0.3566 Lr: 0.02671
[2023-08-08 02:46:38,873 INFO misc.py line 115 22900] Train: [51/100][69/156] Data 0.001 (0.001) Batch 2.970 (3.511) Remain 07:32:22 loss: 0.2821 Lr: 0.02670
[2023-08-08 02:46:42,304 INFO misc.py line 115 22900] Train: [51/100][70/156] Data 0.001 (0.001) Batch 3.431 (3.510) Remain 07:32:09 loss: 0.2160 Lr: 0.02669
[2023-08-08 02:46:45,761 INFO misc.py line 115 22900] Train: [51/100][71/156] Data 0.001 (0.001) Batch 3.457 (3.509) Remain 07:32:00 loss: 0.2297 Lr: 0.02669
[2023-08-08 02:46:49,402 INFO misc.py line 115 22900] Train: [51/100][72/156] Data 0.001 (0.001) Batch 3.641 (3.511) Remain 07:32:11 loss: 0.2378 Lr: 0.02668
[2023-08-08 02:46:52,491 INFO misc.py line 115 22900] Train: [51/100][73/156] Data 0.001 (0.001) Batch 3.089 (3.505) Remain 07:31:21 loss: 0.3525 Lr: 0.02668
[2023-08-08 02:46:56,058 INFO misc.py line 115 22900] Train: [51/100][74/156] Data 0.001 (0.001) Batch 3.567 (3.506) Remain 07:31:24 loss: 0.2489 Lr: 0.02667
[2023-08-08 02:46:59,422 INFO misc.py line 115 22900] Train: [51/100][75/156] Data 0.001 (0.001) Batch 3.364 (3.504) Remain 07:31:06 loss: 0.1889 Lr: 0.02667
[2023-08-08 02:47:03,473 INFO misc.py line 115 22900] Train: [51/100][76/156] Data 0.001 (0.001) Batch 4.051 (3.511) Remain 07:32:00 loss: 0.1542 Lr: 0.02666
[2023-08-08 02:47:07,922 INFO misc.py line 115 22900] Train: [51/100][77/156] Data 0.001 (0.001) Batch 4.449 (3.524) Remain 07:33:34 loss: 0.2727 Lr: 0.02666
[2023-08-08 02:47:11,106 INFO misc.py line 115 22900] Train: [51/100][78/156] Data 0.001 (0.001) Batch 3.184 (3.519) Remain 07:32:56 loss: 0.2513 Lr: 0.02665
[2023-08-08 02:47:15,097 INFO misc.py line 115 22900] Train: [51/100][79/156] Data 0.001 (0.001) Batch 3.992 (3.526) Remain 07:33:40 loss: 0.3609 Lr: 0.02665
[2023-08-08 02:47:18,448 INFO misc.py line 115 22900] Train: [51/100][80/156] Data 0.001 (0.001) Batch 3.351 (3.523) Remain 07:33:19 loss: 0.1227 Lr: 0.02664
[2023-08-08 02:47:20,774 INFO misc.py line 115 22900] Train: [51/100][81/156] Data 0.001 (0.001) Batch 2.326 (3.508) Remain 07:31:17 loss: 0.0781 Lr: 0.02664
[2023-08-08 02:47:23,376 INFO misc.py line 115 22900] Train: [51/100][82/156] Data 0.001 (0.001) Batch 2.602 (3.496) Remain 07:29:45 loss: 0.3146 Lr: 0.02663
[2023-08-08 02:47:27,448 INFO misc.py line 115 22900] Train: [51/100][83/156] Data 0.001 (0.001) Batch 4.072 (3.504) Remain 07:30:37 loss: 0.3030 Lr: 0.02663
[2023-08-08 02:47:30,572 INFO misc.py line 115 22900] Train: [51/100][84/156] Data 0.001 (0.001) Batch 3.123 (3.499) Remain 07:29:58 loss: 0.3869 Lr: 0.02662
[2023-08-08 02:47:34,727 INFO misc.py line 115 22900] Train: [51/100][85/156] Data 0.001 (0.001) Batch 4.155 (3.507) Remain 07:30:56 loss: 0.2680 Lr: 0.02662
[2023-08-08 02:47:38,251 INFO misc.py line 115 22900] Train: [51/100][86/156] Data 0.001 (0.001) Batch 3.524 (3.507) Remain 07:30:54 loss: 0.1877 Lr: 0.02661
[2023-08-08 02:47:42,380 INFO misc.py line 115 22900] Train: [51/100][87/156] Data 0.001 (0.001) Batch 4.129 (3.515) Remain 07:31:48 loss: 0.2609 Lr: 0.02660
[2023-08-08 02:47:45,563 INFO misc.py line 115 22900] Train: [51/100][88/156] Data 0.001 (0.001) Batch 3.183 (3.511) Remain 07:31:14 loss: 0.1200 Lr: 0.02660
[2023-08-08 02:47:48,962 INFO misc.py line 115 22900] Train: [51/100][89/156] Data 0.001 (0.001) Batch 3.399 (3.509) Remain 07:31:00 loss: 0.1793 Lr: 0.02659
[2023-08-08 02:47:52,472 INFO misc.py line 115 22900] Train: [51/100][90/156] Data 0.001 (0.001) Batch 3.510 (3.509) Remain 07:30:57 loss: 0.3060 Lr: 0.02659
[2023-08-08 02:47:56,866 INFO misc.py line 115 22900] Train: [51/100][91/156] Data 0.001 (0.001) Batch 4.394 (3.519) Remain 07:32:11 loss: 0.3918 Lr: 0.02658
[2023-08-08 02:48:00,013 INFO misc.py line 115 22900] Train: [51/100][92/156] Data 0.001 (0.001) Batch 3.148 (3.515) Remain 07:31:35 loss: 0.2371 Lr: 0.02658
[2023-08-08 02:48:03,523 INFO misc.py line 115 22900] Train: [51/100][93/156] Data 0.001 (0.001) Batch 3.510 (3.515) Remain 07:31:31 loss: 0.2368 Lr: 0.02657
[2023-08-08 02:48:07,139 INFO misc.py line 115 22900] Train: [51/100][94/156] Data 0.001 (0.001) Batch 3.616 (3.516) Remain 07:31:36 loss: 0.2207 Lr: 0.02657
[2023-08-08 02:48:10,444 INFO misc.py line 115 22900] Train: [51/100][95/156] Data 0.001 (0.001) Batch 3.305 (3.514) Remain 07:31:15 loss: 0.2166 Lr: 0.02656
[2023-08-08 02:48:13,590 INFO misc.py line 115 22900] Train: [51/100][96/156] Data 0.001 (0.001) Batch 3.146 (3.510) Remain 07:30:41 loss: 0.2653 Lr: 0.02656
[2023-08-08 02:48:17,499 INFO misc.py line 115 22900] Train: [51/100][97/156] Data 0.001 (0.001) Batch 3.909 (3.514) Remain 07:31:10 loss: 0.2048 Lr: 0.02655
[2023-08-08 02:48:20,365 INFO misc.py line 115 22900] Train: [51/100][98/156] Data 0.001 (0.001) Batch 2.866 (3.507) Remain 07:30:14 loss: 0.1128 Lr: 0.02655
[2023-08-08 02:48:23,515 INFO misc.py line 115 22900] Train: [51/100][99/156] Data 0.001 (0.001) Batch 3.150 (3.504) Remain 07:29:42 loss: 0.3661 Lr: 0.02654
[2023-08-08 02:48:27,166 INFO misc.py line 115 22900] Train: [51/100][100/156] Data 0.001 (0.001) Batch 3.651 (3.505) Remain 07:29:50 loss: 0.2002 Lr: 0.02654
[2023-08-08 02:48:29,664 INFO misc.py line 115 22900] Train: [51/100][101/156] Data 0.001 (0.001) Batch 2.497 (3.495) Remain 07:28:27 loss: 0.1232 Lr: 0.02653
[2023-08-08 02:48:33,080 INFO misc.py line 115 22900] Train: [51/100][102/156] Data 0.001 (0.001) Batch 3.416 (3.494) Remain 07:28:18 loss: 0.1728 Lr: 0.02653
[2023-08-08 02:48:36,345 INFO misc.py line 115 22900] Train: [51/100][103/156] Data 0.001 (0.001) Batch 3.265 (3.492) Remain 07:27:57 loss: 0.2316 Lr: 0.02652
[2023-08-08 02:48:39,503 INFO misc.py line 115 22900] Train: [51/100][104/156] Data 0.001 (0.001) Batch 3.159 (3.489) Remain 07:27:28 loss: 0.1653 Lr: 0.02651
[2023-08-08 02:48:43,497 INFO misc.py line 115 22900] Train: [51/100][105/156] Data 0.001 (0.001) Batch 3.994 (3.494) Remain 07:28:02 loss: 0.2753 Lr: 0.02651
[2023-08-08 02:48:46,950 INFO misc.py line 115 22900] Train: [51/100][106/156] Data 0.001 (0.001) Batch 3.452 (3.493) Remain 07:27:56 loss: 0.2501 Lr: 0.02650
[2023-08-08 02:48:50,558 INFO misc.py line 115 22900] Train: [51/100][107/156] Data 0.001 (0.001) Batch 3.609 (3.494) Remain 07:28:01 loss: 0.2828 Lr: 0.02650
[2023-08-08 02:48:54,082 INFO misc.py line 115 22900] Train: [51/100][108/156] Data 0.001 (0.001) Batch 3.524 (3.495) Remain 07:28:00 loss: 0.1605 Lr: 0.02649
[2023-08-08 02:48:56,841 INFO misc.py line 115 22900] Train: [51/100][109/156] Data 0.001 (0.001) Batch 2.758 (3.488) Remain 07:27:03 loss: 0.1403 Lr: 0.02649
[2023-08-08 02:49:00,644 INFO misc.py line 115 22900] Train: [51/100][110/156] Data 0.001 (0.001) Batch 3.804 (3.491) Remain 07:27:22 loss: 0.2038 Lr: 0.02648
[2023-08-08 02:49:03,908 INFO misc.py line 115 22900] Train: [51/100][111/156] Data 0.001 (0.001) Batch 3.264 (3.488) Remain 07:27:02 loss: 0.4489 Lr: 0.02648
[2023-08-08 02:49:07,867 INFO misc.py line 115 22900] Train: [51/100][112/156] Data 0.001 (0.001) Batch 3.959 (3.493) Remain 07:27:32 loss: 0.2269 Lr: 0.02647
[2023-08-08 02:49:11,313 INFO misc.py line 115 22900] Train: [51/100][113/156] Data 0.002 (0.001) Batch 3.446 (3.492) Remain 07:27:25 loss: 0.1127 Lr: 0.02647
[2023-08-08 02:49:15,320 INFO misc.py line 115 22900] Train: [51/100][114/156] Data 0.001 (0.001) Batch 4.006 (3.497) Remain 07:27:57 loss: 0.3145 Lr: 0.02646
[2023-08-08 02:49:18,997 INFO misc.py line 115 22900] Train: [51/100][115/156] Data 0.001 (0.001) Batch 3.677 (3.499) Remain 07:28:06 loss: 0.2268 Lr: 0.02646
[2023-08-08 02:49:22,041 INFO misc.py line 115 22900] Train: [51/100][116/156] Data 0.001 (0.001) Batch 3.043 (3.495) Remain 07:27:32 loss: 0.1658 Lr: 0.02645
[2023-08-08 02:49:25,156 INFO misc.py line 115 22900] Train: [51/100][117/156] Data 0.001 (0.001) Batch 3.115 (3.491) Remain 07:27:03 loss: 0.2064 Lr: 0.02645
[2023-08-08 02:49:28,988 INFO misc.py line 115 22900] Train: [51/100][118/156] Data 0.001 (0.001) Batch 3.833 (3.494) Remain 07:27:22 loss: 0.2738 Lr: 0.02644
[2023-08-08 02:49:32,236 INFO misc.py line 115 22900] Train: [51/100][119/156] Data 0.001 (0.001) Batch 3.248 (3.492) Remain 07:27:02 loss: 0.2729 Lr: 0.02644
[2023-08-08 02:49:35,933 INFO misc.py line 115 22900] Train: [51/100][120/156] Data 0.001 (0.001) Batch 3.697 (3.494) Remain 07:27:12 loss: 0.3030 Lr: 0.02643
[2023-08-08 02:49:39,012 INFO misc.py line 115 22900] Train: [51/100][121/156] Data 0.001 (0.001) Batch 3.080 (3.490) Remain 07:26:42 loss: 0.2056 Lr: 0.02643
[2023-08-08 02:49:43,151 INFO misc.py line 115 22900] Train: [51/100][122/156] Data 0.001 (0.001) Batch 4.139 (3.496) Remain 07:27:20 loss: 0.2744 Lr: 0.02642
[2023-08-08 02:49:46,633 INFO misc.py line 115 22900] Train: [51/100][123/156] Data 0.001 (0.001) Batch 3.482 (3.496) Remain 07:27:16 loss: 0.3570 Lr: 0.02641
[2023-08-08 02:49:50,016 INFO misc.py line 115 22900] Train: [51/100][124/156] Data 0.001 (0.001) Batch 3.382 (3.495) Remain 07:27:05 loss: 0.2254 Lr: 0.02641
[2023-08-08 02:49:53,338 INFO misc.py line 115 22900] Train: [51/100][125/156] Data 0.001 (0.001) Batch 3.322 (3.493) Remain 07:26:51 loss: 0.2054 Lr: 0.02640
[2023-08-08 02:49:55,823 INFO misc.py line 115 22900] Train: [51/100][126/156] Data 0.001 (0.001) Batch 2.485 (3.485) Remain 07:25:44 loss: 0.1361 Lr: 0.02640
[2023-08-08 02:49:58,951 INFO misc.py line 115 22900] Train: [51/100][127/156] Data 0.001 (0.001) Batch 3.128 (3.482) Remain 07:25:19 loss: 0.0949 Lr: 0.02639
[2023-08-08 02:50:02,245 INFO misc.py line 115 22900] Train: [51/100][128/156] Data 0.001 (0.001) Batch 3.294 (3.481) Remain 07:25:04 loss: 0.2305 Lr: 0.02639
[2023-08-08 02:50:06,894 INFO misc.py line 115 22900] Train: [51/100][129/156] Data 0.001 (0.001) Batch 4.649 (3.490) Remain 07:26:11 loss: 0.5272 Lr: 0.02638
[2023-08-08 02:50:09,841 INFO misc.py line 115 22900] Train: [51/100][130/156] Data 0.001 (0.001) Batch 2.946 (3.486) Remain 07:25:35 loss: 0.1145 Lr: 0.02638
[2023-08-08 02:50:13,400 INFO misc.py line 115 22900] Train: [51/100][131/156] Data 0.001 (0.001) Batch 3.560 (3.486) Remain 07:25:36 loss: 0.2096 Lr: 0.02637
[2023-08-08 02:50:16,567 INFO misc.py line 115 22900] Train: [51/100][132/156] Data 0.001 (0.001) Batch 3.167 (3.484) Remain 07:25:13 loss: 0.1456 Lr: 0.02637
[2023-08-08 02:50:20,691 INFO misc.py line 115 22900] Train: [51/100][133/156] Data 0.001 (0.001) Batch 4.123 (3.489) Remain 07:25:48 loss: 0.4882 Lr: 0.02636
[2023-08-08 02:50:24,081 INFO misc.py line 115 22900] Train: [51/100][134/156] Data 0.001 (0.001) Batch 3.390 (3.488) Remain 07:25:38 loss: 0.3488 Lr: 0.02636
[2023-08-08 02:50:27,396 INFO misc.py line 115 22900] Train: [51/100][135/156] Data 0.001 (0.001) Batch 3.315 (3.487) Remain 07:25:25 loss: 0.2525 Lr: 0.02635
[2023-08-08 02:50:30,286 INFO misc.py line 115 22900] Train: [51/100][136/156] Data 0.001 (0.001) Batch 2.891 (3.482) Remain 07:24:47 loss: 0.1973 Lr: 0.02635
[2023-08-08 02:50:33,903 INFO misc.py line 115 22900] Train: [51/100][137/156] Data 0.001 (0.001) Batch 3.616 (3.483) Remain 07:24:51 loss: 0.2104 Lr: 0.02634
[2023-08-08 02:50:37,373 INFO misc.py line 115 22900] Train: [51/100][138/156] Data 0.001 (0.001) Batch 3.470 (3.483) Remain 07:24:47 loss: 0.1951 Lr: 0.02634
[2023-08-08 02:50:40,235 INFO misc.py line 115 22900] Train: [51/100][139/156] Data 0.001 (0.001) Batch 2.862 (3.479) Remain 07:24:08 loss: 0.1030 Lr: 0.02633
[2023-08-08 02:50:44,945 INFO misc.py line 115 22900] Train: [51/100][140/156] Data 0.001 (0.001) Batch 4.710 (3.488) Remain 07:25:14 loss: 0.4650 Lr: 0.02632
[2023-08-08 02:50:47,810 INFO misc.py line 115 22900] Train: [51/100][141/156] Data 0.001 (0.001) Batch 2.865 (3.483) Remain 07:24:36 loss: 0.2165 Lr: 0.02632
[2023-08-08 02:50:50,765 INFO misc.py line 115 22900] Train: [51/100][142/156] Data 0.001 (0.001) Batch 2.955 (3.479) Remain 07:24:03 loss: 0.1380 Lr: 0.02631
[2023-08-08 02:50:53,986 INFO misc.py line 115 22900] Train: [51/100][143/156] Data 0.001 (0.001) Batch 3.221 (3.477) Remain 07:23:46 loss: 0.1240 Lr: 0.02631
[2023-08-08 02:50:57,495 INFO misc.py line 115 22900] Train: [51/100][144/156] Data 0.001 (0.001) Batch 3.509 (3.478) Remain 07:23:44 loss: 0.1749 Lr: 0.02630
[2023-08-08 02:51:01,401 INFO misc.py line 115 22900] Train: [51/100][145/156] Data 0.001 (0.001) Batch 3.906 (3.481) Remain 07:24:04 loss: 0.3459 Lr: 0.02630
[2023-08-08 02:51:05,008 INFO misc.py line 115 22900] Train: [51/100][146/156] Data 0.001 (0.001) Batch 3.606 (3.481) Remain 07:24:07 loss: 0.1305 Lr: 0.02629
[2023-08-08 02:51:08,310 INFO misc.py line 115 22900] Train: [51/100][147/156] Data 0.001 (0.001) Batch 3.303 (3.480) Remain 07:23:54 loss: 0.1957 Lr: 0.02629
[2023-08-08 02:51:11,894 INFO misc.py line 115 22900] Train: [51/100][148/156] Data 0.001 (0.001) Batch 3.584 (3.481) Remain 07:23:56 loss: 0.1962 Lr: 0.02628
[2023-08-08 02:51:16,038 INFO misc.py line 115 22900] Train: [51/100][149/156] Data 0.001 (0.001) Batch 4.144 (3.485) Remain 07:24:27 loss: 0.2530 Lr: 0.02628
[2023-08-08 02:51:18,762 INFO misc.py line 115 22900] Train: [51/100][150/156] Data 0.001 (0.001) Batch 2.724 (3.480) Remain 07:23:44 loss: 0.1271 Lr: 0.02627
[2023-08-08 02:51:22,280 INFO misc.py line 115 22900] Train: [51/100][151/156] Data 0.001 (0.001) Batch 3.519 (3.481) Remain 07:23:42 loss: 0.2021 Lr: 0.02627
[2023-08-08 02:51:26,059 INFO misc.py line 115 22900] Train: [51/100][152/156] Data 0.001 (0.001) Batch 3.779 (3.483) Remain 07:23:54 loss: 0.2196 Lr: 0.02626
[2023-08-08 02:51:29,455 INFO misc.py line 115 22900] Train: [51/100][153/156] Data 0.001 (0.001) Batch 3.396 (3.482) Remain 07:23:46 loss: 0.2574 Lr: 0.02626
[2023-08-08 02:51:32,373 INFO misc.py line 115 22900] Train: [51/100][154/156] Data 0.001 (0.001) Batch 2.918 (3.478) Remain 07:23:14 loss: 0.3653 Lr: 0.02625
[2023-08-08 02:51:34,779 INFO misc.py line 115 22900] Train: [51/100][155/156] Data 0.001 (0.001) Batch 2.406 (3.471) Remain 07:22:17 loss: 0.1539 Lr: 0.02625
[2023-08-08 02:51:38,117 INFO misc.py line 115 22900] Train: [51/100][156/156] Data 0.001 (0.001) Batch 3.338 (3.470) Remain 07:22:07 loss: 0.2940 Lr: 0.02624
[2023-08-08 02:51:38,118 INFO misc.py line 129 22900] Train result: loss: 0.2556 
[2023-08-08 02:51:38,118 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 02:51:40,214 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.9965 
[2023-08-08 02:51:41,083 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.5942 
[2023-08-08 02:51:42,747 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.6793 
[2023-08-08 02:51:44,268 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.4853 
[2023-08-08 02:51:46,112 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.8622 
[2023-08-08 02:51:47,775 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.8022 
[2023-08-08 02:51:49,913 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.8532 
[2023-08-08 02:51:51,718 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.0552 
[2023-08-08 02:51:53,001 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.6637 
[2023-08-08 02:51:55,132 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.4572 
[2023-08-08 02:51:55,657 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.0836 
[2023-08-08 02:51:57,189 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.8240 
[2023-08-08 02:51:59,903 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.2226 
[2023-08-08 02:52:01,586 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.8988 
[2023-08-08 02:52:03,608 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.3812 
[2023-08-08 02:52:06,317 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.2287 
[2023-08-08 02:52:09,030 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.5720 
[2023-08-08 02:52:10,877 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.5561 
[2023-08-08 02:52:11,624 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.3465 
[2023-08-08 02:52:12,509 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.6371 
[2023-08-08 02:52:14,772 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3893 
[2023-08-08 02:52:16,737 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.8267 
[2023-08-08 02:52:18,585 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.6935 
[2023-08-08 02:52:20,521 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.7978 
[2023-08-08 02:52:20,570 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2433/0.3394/0.6876.
[2023-08-08 02:52:20,570 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6317/0.9702
[2023-08-08 02:52:20,570 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9519/0.9913
[2023-08-08 02:52:20,570 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1499/0.3788
[2023-08-08 02:52:20,570 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.0807/0.1284
[2023-08-08 02:52:20,570 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6001/0.6764
[2023-08-08 02:52:20,570 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.3612/0.6024
[2023-08-08 02:52:20,570 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5677/0.6680
[2023-08-08 02:52:20,570 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1396/0.1607
[2023-08-08 02:52:20,570 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1694/0.3637
[2023-08-08 02:52:20,570 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0233/0.0234
[2023-08-08 02:52:20,570 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 02:52:20,570 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0021/0.0021
[2023-08-08 02:52:20,570 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.2036/0.2852
[2023-08-08 02:52:20,570 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0178/0.0181
[2023-08-08 02:52:20,570 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0051/0.0075
[2023-08-08 02:52:20,570 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.5564/0.6105
[2023-08-08 02:52:20,570 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0369/0.0448
[2023-08-08 02:52:20,570 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.2709/0.6664
[2023-08-08 02:52:20,570 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 02:52:20,570 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0974/0.1889
[2023-08-08 02:52:20,570 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 02:52:20,571 INFO misc.py line 152 22900] Currently Best mIoU: 0.2562
[2023-08-08 02:52:20,571 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 02:52:27,280 INFO misc.py line 115 22900] Train: [52/100][1/156] Data 1.817 (1.817) Batch 5.886 (5.886) Remain 12:29:45 loss: 0.3264 Lr: 0.02623
[2023-08-08 02:52:30,894 INFO misc.py line 115 22900] Train: [52/100][2/156] Data 0.001 (0.001) Batch 3.614 (3.614) Remain 07:40:19 loss: 0.2696 Lr: 0.02623
[2023-08-08 02:52:33,496 INFO misc.py line 115 22900] Train: [52/100][3/156] Data 0.001 (0.001) Batch 2.603 (2.603) Remain 05:31:26 loss: 0.2110 Lr: 0.02622
[2023-08-08 02:52:37,602 INFO misc.py line 115 22900] Train: [52/100][4/156] Data 0.001 (0.001) Batch 4.105 (4.105) Remain 08:42:44 loss: 0.2537 Lr: 0.02622
[2023-08-08 02:52:41,625 INFO misc.py line 115 22900] Train: [52/100][5/156] Data 0.001 (0.001) Batch 4.023 (4.064) Remain 08:37:26 loss: 0.3453 Lr: 0.02621
[2023-08-08 02:52:45,067 INFO misc.py line 115 22900] Train: [52/100][6/156] Data 0.001 (0.001) Batch 3.442 (3.857) Remain 08:10:59 loss: 0.2230 Lr: 0.02621
[2023-08-08 02:52:48,125 INFO misc.py line 115 22900] Train: [52/100][7/156] Data 0.001 (0.001) Batch 3.058 (3.657) Remain 07:45:30 loss: 0.1049 Lr: 0.02620
[2023-08-08 02:52:52,467 INFO misc.py line 115 22900] Train: [52/100][8/156] Data 0.001 (0.001) Batch 4.342 (3.794) Remain 08:02:52 loss: 0.4010 Lr: 0.02620
[2023-08-08 02:52:55,527 INFO misc.py line 115 22900] Train: [52/100][9/156] Data 0.001 (0.001) Batch 3.060 (3.672) Remain 07:47:13 loss: 0.3355 Lr: 0.02619
[2023-08-08 02:52:58,791 INFO misc.py line 115 22900] Train: [52/100][10/156] Data 0.001 (0.001) Batch 3.264 (3.614) Remain 07:39:45 loss: 0.1373 Lr: 0.02619
[2023-08-08 02:53:01,178 INFO misc.py line 115 22900] Train: [52/100][11/156] Data 0.001 (0.001) Batch 2.387 (3.460) Remain 07:20:11 loss: 0.2613 Lr: 0.02618
[2023-08-08 02:53:04,709 INFO misc.py line 115 22900] Train: [52/100][12/156] Data 0.001 (0.001) Batch 3.531 (3.468) Remain 07:21:07 loss: 0.1262 Lr: 0.02618
[2023-08-08 02:53:07,321 INFO misc.py line 115 22900] Train: [52/100][13/156] Data 0.001 (0.001) Batch 2.612 (3.382) Remain 07:10:11 loss: 0.2355 Lr: 0.02617
[2023-08-08 02:53:11,340 INFO misc.py line 115 22900] Train: [52/100][14/156] Data 0.001 (0.001) Batch 4.019 (3.440) Remain 07:17:29 loss: 0.2389 Lr: 0.02617
[2023-08-08 02:53:15,007 INFO misc.py line 115 22900] Train: [52/100][15/156] Data 0.001 (0.001) Batch 3.667 (3.459) Remain 07:19:50 loss: 0.4110 Lr: 0.02616
[2023-08-08 02:53:18,284 INFO misc.py line 115 22900] Train: [52/100][16/156] Data 0.001 (0.001) Batch 3.277 (3.445) Remain 07:17:59 loss: 0.1384 Lr: 0.02616
[2023-08-08 02:53:22,022 INFO misc.py line 115 22900] Train: [52/100][17/156] Data 0.001 (0.001) Batch 3.738 (3.466) Remain 07:20:36 loss: 0.2008 Lr: 0.02615
[2023-08-08 02:53:25,129 INFO misc.py line 115 22900] Train: [52/100][18/156] Data 0.001 (0.001) Batch 3.107 (3.442) Remain 07:17:30 loss: 0.2115 Lr: 0.02614
[2023-08-08 02:53:28,054 INFO misc.py line 115 22900] Train: [52/100][19/156] Data 0.001 (0.001) Batch 2.925 (3.410) Remain 07:13:20 loss: 0.3193 Lr: 0.02614
[2023-08-08 02:53:32,142 INFO misc.py line 115 22900] Train: [52/100][20/156] Data 0.001 (0.001) Batch 4.088 (3.450) Remain 07:18:20 loss: 0.4414 Lr: 0.02613
[2023-08-08 02:53:34,761 INFO misc.py line 115 22900] Train: [52/100][21/156] Data 0.001 (0.001) Batch 2.619 (3.404) Remain 07:12:25 loss: 0.2229 Lr: 0.02613
[2023-08-08 02:53:38,650 INFO misc.py line 115 22900] Train: [52/100][22/156] Data 0.001 (0.001) Batch 3.889 (3.429) Remain 07:15:36 loss: 0.1963 Lr: 0.02612
[2023-08-08 02:53:40,802 INFO misc.py line 115 22900] Train: [52/100][23/156] Data 0.001 (0.001) Batch 2.151 (3.365) Remain 07:07:26 loss: 0.0696 Lr: 0.02612
[2023-08-08 02:53:45,537 INFO misc.py line 115 22900] Train: [52/100][24/156] Data 0.001 (0.001) Batch 4.736 (3.431) Remain 07:15:40 loss: 0.5217 Lr: 0.02611
[2023-08-08 02:53:49,491 INFO misc.py line 115 22900] Train: [52/100][25/156] Data 0.001 (0.001) Batch 3.953 (3.454) Remain 07:18:38 loss: 0.5018 Lr: 0.02611
[2023-08-08 02:53:51,765 INFO misc.py line 115 22900] Train: [52/100][26/156] Data 0.001 (0.001) Batch 2.275 (3.403) Remain 07:12:04 loss: 0.1933 Lr: 0.02610
[2023-08-08 02:53:55,206 INFO misc.py line 115 22900] Train: [52/100][27/156] Data 0.001 (0.001) Batch 3.441 (3.405) Remain 07:12:12 loss: 0.2680 Lr: 0.02610
[2023-08-08 02:53:58,940 INFO misc.py line 115 22900] Train: [52/100][28/156] Data 0.001 (0.001) Batch 3.734 (3.418) Remain 07:13:49 loss: 0.3057 Lr: 0.02609
[2023-08-08 02:54:02,571 INFO misc.py line 115 22900] Train: [52/100][29/156] Data 0.001 (0.001) Batch 3.630 (3.426) Remain 07:14:48 loss: 0.1253 Lr: 0.02609
[2023-08-08 02:54:06,224 INFO misc.py line 115 22900] Train: [52/100][30/156] Data 0.001 (0.001) Batch 3.653 (3.434) Remain 07:15:49 loss: 0.1675 Lr: 0.02608
[2023-08-08 02:54:09,814 INFO misc.py line 115 22900] Train: [52/100][31/156] Data 0.001 (0.001) Batch 3.590 (3.440) Remain 07:16:28 loss: 0.2456 Lr: 0.02608
[2023-08-08 02:54:12,169 INFO misc.py line 115 22900] Train: [52/100][32/156] Data 0.001 (0.001) Batch 2.355 (3.403) Remain 07:11:39 loss: 0.1694 Lr: 0.02607
[2023-08-08 02:54:16,247 INFO misc.py line 115 22900] Train: [52/100][33/156] Data 0.001 (0.001) Batch 4.078 (3.425) Remain 07:14:27 loss: 0.3063 Lr: 0.02607
[2023-08-08 02:54:20,230 INFO misc.py line 115 22900] Train: [52/100][34/156] Data 0.001 (0.001) Batch 3.983 (3.443) Remain 07:16:41 loss: 0.3807 Lr: 0.02606
[2023-08-08 02:54:23,580 INFO misc.py line 115 22900] Train: [52/100][35/156] Data 0.001 (0.001) Batch 3.350 (3.440) Remain 07:16:15 loss: 0.3409 Lr: 0.02605
[2023-08-08 02:54:26,292 INFO misc.py line 115 22900] Train: [52/100][36/156] Data 0.001 (0.001) Batch 2.712 (3.418) Remain 07:13:24 loss: 0.1304 Lr: 0.02605
[2023-08-08 02:54:30,427 INFO misc.py line 115 22900] Train: [52/100][37/156] Data 0.001 (0.001) Batch 4.135 (3.439) Remain 07:16:01 loss: 0.2428 Lr: 0.02604
[2023-08-08 02:54:33,285 INFO misc.py line 115 22900] Train: [52/100][38/156] Data 0.001 (0.001) Batch 2.859 (3.423) Remain 07:13:51 loss: 0.1215 Lr: 0.02604
[2023-08-08 02:54:37,035 INFO misc.py line 115 22900] Train: [52/100][39/156] Data 0.001 (0.001) Batch 3.750 (3.432) Remain 07:14:57 loss: 0.1902 Lr: 0.02603
[2023-08-08 02:54:39,503 INFO misc.py line 115 22900] Train: [52/100][40/156] Data 0.001 (0.001) Batch 2.468 (3.406) Remain 07:11:35 loss: 0.2409 Lr: 0.02603
[2023-08-08 02:54:43,580 INFO misc.py line 115 22900] Train: [52/100][41/156] Data 0.001 (0.001) Batch 4.077 (3.423) Remain 07:13:46 loss: 0.3023 Lr: 0.02602
[2023-08-08 02:54:46,737 INFO misc.py line 115 22900] Train: [52/100][42/156] Data 0.001 (0.001) Batch 3.157 (3.416) Remain 07:12:51 loss: 0.1857 Lr: 0.02602
[2023-08-08 02:54:50,009 INFO misc.py line 115 22900] Train: [52/100][43/156] Data 0.001 (0.001) Batch 3.272 (3.413) Remain 07:12:20 loss: 0.2375 Lr: 0.02601
[2023-08-08 02:54:53,796 INFO misc.py line 115 22900] Train: [52/100][44/156] Data 0.001 (0.001) Batch 3.787 (3.422) Remain 07:13:26 loss: 0.3404 Lr: 0.02601
[2023-08-08 02:54:56,436 INFO misc.py line 115 22900] Train: [52/100][45/156] Data 0.001 (0.001) Batch 2.640 (3.403) Remain 07:11:01 loss: 0.2952 Lr: 0.02600
[2023-08-08 02:54:59,858 INFO misc.py line 115 22900] Train: [52/100][46/156] Data 0.001 (0.001) Batch 3.422 (3.404) Remain 07:11:01 loss: 0.2330 Lr: 0.02600
[2023-08-08 02:55:02,805 INFO misc.py line 115 22900] Train: [52/100][47/156] Data 0.001 (0.001) Batch 2.947 (3.393) Remain 07:09:39 loss: 0.3192 Lr: 0.02599
[2023-08-08 02:55:05,760 INFO misc.py line 115 22900] Train: [52/100][48/156] Data 0.001 (0.001) Batch 2.955 (3.384) Remain 07:08:22 loss: 0.2194 Lr: 0.02599
[2023-08-08 02:55:09,233 INFO misc.py line 115 22900] Train: [52/100][49/156] Data 0.001 (0.001) Batch 3.474 (3.386) Remain 07:08:33 loss: 0.2068 Lr: 0.02598
[2023-08-08 02:55:11,897 INFO misc.py line 115 22900] Train: [52/100][50/156] Data 0.001 (0.001) Batch 2.664 (3.370) Remain 07:06:33 loss: 0.1288 Lr: 0.02598
[2023-08-08 02:55:15,856 INFO misc.py line 115 22900] Train: [52/100][51/156] Data 0.001 (0.001) Batch 3.959 (3.382) Remain 07:08:03 loss: 0.2981 Lr: 0.02597
[2023-08-08 02:55:19,854 INFO misc.py line 115 22900] Train: [52/100][52/156] Data 0.001 (0.001) Batch 3.998 (3.395) Remain 07:09:35 loss: 0.3601 Lr: 0.02596
[2023-08-08 02:55:23,878 INFO misc.py line 115 22900] Train: [52/100][53/156] Data 0.001 (0.001) Batch 4.023 (3.408) Remain 07:11:07 loss: 0.3879 Lr: 0.02596
[2023-08-08 02:55:27,117 INFO misc.py line 115 22900] Train: [52/100][54/156] Data 0.001 (0.001) Batch 3.239 (3.404) Remain 07:10:38 loss: 0.0926 Lr: 0.02595
[2023-08-08 02:55:31,860 INFO misc.py line 115 22900] Train: [52/100][55/156] Data 0.001 (0.001) Batch 4.742 (3.430) Remain 07:13:50 loss: 0.2956 Lr: 0.02595
[2023-08-08 02:55:35,794 INFO misc.py line 115 22900] Train: [52/100][56/156] Data 0.001 (0.001) Batch 3.934 (3.440) Remain 07:14:59 loss: 0.2146 Lr: 0.02594
[2023-08-08 02:55:40,186 INFO misc.py line 115 22900] Train: [52/100][57/156] Data 0.001 (0.001) Batch 4.393 (3.457) Remain 07:17:09 loss: 0.3934 Lr: 0.02594
[2023-08-08 02:55:42,969 INFO misc.py line 115 22900] Train: [52/100][58/156] Data 0.001 (0.001) Batch 2.782 (3.445) Remain 07:15:33 loss: 0.1570 Lr: 0.02593
[2023-08-08 02:55:45,846 INFO misc.py line 115 22900] Train: [52/100][59/156] Data 0.001 (0.001) Batch 2.877 (3.435) Remain 07:14:12 loss: 0.1161 Lr: 0.02593
[2023-08-08 02:55:49,897 INFO misc.py line 115 22900] Train: [52/100][60/156] Data 0.001 (0.001) Batch 4.051 (3.446) Remain 07:15:31 loss: 0.3089 Lr: 0.02592
[2023-08-08 02:55:54,060 INFO misc.py line 115 22900] Train: [52/100][61/156] Data 0.001 (0.001) Batch 4.164 (3.458) Remain 07:17:01 loss: 0.4545 Lr: 0.02592
[2023-08-08 02:55:57,683 INFO misc.py line 115 22900] Train: [52/100][62/156] Data 0.001 (0.001) Batch 3.622 (3.461) Remain 07:17:19 loss: 0.2670 Lr: 0.02591
[2023-08-08 02:56:00,815 INFO misc.py line 115 22900] Train: [52/100][63/156] Data 0.001 (0.001) Batch 3.132 (3.455) Remain 07:16:34 loss: 0.2675 Lr: 0.02591
[2023-08-08 02:56:04,343 INFO misc.py line 115 22900] Train: [52/100][64/156] Data 0.001 (0.001) Batch 3.527 (3.456) Remain 07:16:40 loss: 0.1776 Lr: 0.02590
[2023-08-08 02:56:08,772 INFO misc.py line 115 22900] Train: [52/100][65/156] Data 0.001 (0.001) Batch 4.430 (3.472) Remain 07:18:35 loss: 0.2421 Lr: 0.02590
[2023-08-08 02:56:11,479 INFO misc.py line 115 22900] Train: [52/100][66/156] Data 0.001 (0.001) Batch 2.707 (3.460) Remain 07:17:00 loss: 0.1150 Lr: 0.02589
[2023-08-08 02:56:14,771 INFO misc.py line 115 22900] Train: [52/100][67/156] Data 0.001 (0.001) Batch 3.292 (3.457) Remain 07:16:36 loss: 0.3160 Lr: 0.02589
[2023-08-08 02:56:18,848 INFO misc.py line 115 22900] Train: [52/100][68/156] Data 0.001 (0.001) Batch 4.077 (3.467) Remain 07:17:45 loss: 0.3159 Lr: 0.02588
[2023-08-08 02:56:22,054 INFO misc.py line 115 22900] Train: [52/100][69/156] Data 0.001 (0.001) Batch 3.206 (3.463) Remain 07:17:12 loss: 0.4299 Lr: 0.02587
[2023-08-08 02:56:25,026 INFO misc.py line 115 22900] Train: [52/100][70/156] Data 0.001 (0.001) Batch 2.972 (3.456) Remain 07:16:13 loss: 0.1117 Lr: 0.02587
[2023-08-08 02:56:28,102 INFO misc.py line 115 22900] Train: [52/100][71/156] Data 0.001 (0.001) Batch 3.076 (3.450) Remain 07:15:27 loss: 0.0812 Lr: 0.02586
[2023-08-08 02:56:31,553 INFO misc.py line 115 22900] Train: [52/100][72/156] Data 0.001 (0.001) Batch 3.452 (3.450) Remain 07:15:24 loss: 0.2745 Lr: 0.02586
[2023-08-08 02:56:34,284 INFO misc.py line 115 22900] Train: [52/100][73/156] Data 0.001 (0.001) Batch 2.731 (3.440) Remain 07:14:02 loss: 0.1572 Lr: 0.02585
[2023-08-08 02:56:37,983 INFO misc.py line 115 22900] Train: [52/100][74/156] Data 0.001 (0.001) Batch 3.699 (3.443) Remain 07:14:27 loss: 0.3127 Lr: 0.02585
[2023-08-08 02:56:41,352 INFO misc.py line 115 22900] Train: [52/100][75/156] Data 0.001 (0.001) Batch 3.368 (3.442) Remain 07:14:15 loss: 0.1629 Lr: 0.02584
[2023-08-08 02:56:44,574 INFO misc.py line 115 22900] Train: [52/100][76/156] Data 0.001 (0.001) Batch 3.222 (3.439) Remain 07:13:49 loss: 0.0921 Lr: 0.02584
[2023-08-08 02:56:47,149 INFO misc.py line 115 22900] Train: [52/100][77/156] Data 0.001 (0.001) Batch 2.575 (3.428) Remain 07:12:17 loss: 0.1424 Lr: 0.02583
[2023-08-08 02:56:50,864 INFO misc.py line 115 22900] Train: [52/100][78/156] Data 0.001 (0.001) Batch 3.716 (3.432) Remain 07:12:43 loss: 0.2440 Lr: 0.02583
[2023-08-08 02:56:54,940 INFO misc.py line 115 22900] Train: [52/100][79/156] Data 0.001 (0.001) Batch 4.075 (3.440) Remain 07:13:43 loss: 0.3334 Lr: 0.02582
[2023-08-08 02:56:58,735 INFO misc.py line 115 22900] Train: [52/100][80/156] Data 0.001 (0.001) Batch 3.795 (3.445) Remain 07:14:15 loss: 0.4170 Lr: 0.02582
[2023-08-08 02:57:02,608 INFO misc.py line 115 22900] Train: [52/100][81/156] Data 0.001 (0.001) Batch 3.873 (3.450) Remain 07:14:53 loss: 0.2760 Lr: 0.02581
[2023-08-08 02:57:05,556 INFO misc.py line 115 22900] Train: [52/100][82/156] Data 0.001 (0.001) Batch 2.948 (3.444) Remain 07:14:01 loss: 0.0667 Lr: 0.02581
[2023-08-08 02:57:09,043 INFO misc.py line 115 22900] Train: [52/100][83/156] Data 0.001 (0.001) Batch 3.486 (3.444) Remain 07:14:02 loss: 0.1657 Lr: 0.02580
[2023-08-08 02:57:12,632 INFO misc.py line 115 22900] Train: [52/100][84/156] Data 0.001 (0.001) Batch 3.589 (3.446) Remain 07:14:12 loss: 0.1870 Lr: 0.02580
[2023-08-08 02:57:16,645 INFO misc.py line 115 22900] Train: [52/100][85/156] Data 0.001 (0.001) Batch 4.014 (3.453) Remain 07:15:01 loss: 0.3020 Lr: 0.02579
[2023-08-08 02:57:19,651 INFO misc.py line 115 22900] Train: [52/100][86/156] Data 0.001 (0.001) Batch 3.006 (3.448) Remain 07:14:17 loss: 0.2583 Lr: 0.02578
[2023-08-08 02:57:22,928 INFO misc.py line 115 22900] Train: [52/100][87/156] Data 0.001 (0.001) Batch 3.277 (3.446) Remain 07:13:58 loss: 0.1909 Lr: 0.02578
[2023-08-08 02:57:26,484 INFO misc.py line 115 22900] Train: [52/100][88/156] Data 0.001 (0.001) Batch 3.555 (3.447) Remain 07:14:04 loss: 0.2295 Lr: 0.02577
[2023-08-08 02:57:29,360 INFO misc.py line 115 22900] Train: [52/100][89/156] Data 0.001 (0.001) Batch 2.876 (3.440) Remain 07:13:11 loss: 0.1642 Lr: 0.02577
[2023-08-08 02:57:32,159 INFO misc.py line 115 22900] Train: [52/100][90/156] Data 0.001 (0.001) Batch 2.799 (3.433) Remain 07:12:12 loss: 0.0588 Lr: 0.02576
[2023-08-08 02:57:35,673 INFO misc.py line 115 22900] Train: [52/100][91/156] Data 0.001 (0.001) Batch 3.514 (3.434) Remain 07:12:15 loss: 0.2584 Lr: 0.02576
[2023-08-08 02:57:39,053 INFO misc.py line 115 22900] Train: [52/100][92/156] Data 0.001 (0.001) Batch 3.380 (3.433) Remain 07:12:07 loss: 0.1513 Lr: 0.02575
[2023-08-08 02:57:43,550 INFO misc.py line 115 22900] Train: [52/100][93/156] Data 0.001 (0.001) Batch 4.497 (3.445) Remain 07:13:33 loss: 0.4340 Lr: 0.02575
[2023-08-08 02:57:47,117 INFO misc.py line 115 22900] Train: [52/100][94/156] Data 0.001 (0.001) Batch 3.567 (3.446) Remain 07:13:40 loss: 0.0969 Lr: 0.02574
[2023-08-08 02:57:50,848 INFO misc.py line 115 22900] Train: [52/100][95/156] Data 0.001 (0.001) Batch 3.731 (3.449) Remain 07:14:00 loss: 0.2012 Lr: 0.02574
[2023-08-08 02:57:54,912 INFO misc.py line 115 22900] Train: [52/100][96/156] Data 0.001 (0.001) Batch 4.064 (3.456) Remain 07:14:46 loss: 0.1736 Lr: 0.02573
[2023-08-08 02:57:58,311 INFO misc.py line 115 22900] Train: [52/100][97/156] Data 0.001 (0.001) Batch 3.399 (3.455) Remain 07:14:38 loss: 0.1852 Lr: 0.02573
[2023-08-08 02:58:01,628 INFO misc.py line 115 22900] Train: [52/100][98/156] Data 0.001 (0.001) Batch 3.317 (3.454) Remain 07:14:23 loss: 0.3316 Lr: 0.02572
[2023-08-08 02:58:05,453 INFO misc.py line 115 22900] Train: [52/100][99/156] Data 0.001 (0.001) Batch 3.826 (3.458) Remain 07:14:49 loss: 0.4457 Lr: 0.02572
[2023-08-08 02:58:09,175 INFO misc.py line 115 22900] Train: [52/100][100/156] Data 0.001 (0.001) Batch 3.722 (3.461) Remain 07:15:06 loss: 0.2600 Lr: 0.02571
[2023-08-08 02:58:11,991 INFO misc.py line 115 22900] Train: [52/100][101/156] Data 0.001 (0.001) Batch 2.816 (3.454) Remain 07:14:13 loss: 0.1505 Lr: 0.02570
[2023-08-08 02:58:15,483 INFO misc.py line 115 22900] Train: [52/100][102/156] Data 0.001 (0.001) Batch 3.491 (3.454) Remain 07:14:13 loss: 0.1716 Lr: 0.02570
[2023-08-08 02:58:18,361 INFO misc.py line 115 22900] Train: [52/100][103/156] Data 0.001 (0.001) Batch 2.879 (3.449) Remain 07:13:26 loss: 0.1505 Lr: 0.02569
[2023-08-08 02:58:22,663 INFO misc.py line 115 22900] Train: [52/100][104/156] Data 0.001 (0.001) Batch 4.301 (3.457) Remain 07:14:26 loss: 0.2358 Lr: 0.02569
[2023-08-08 02:58:26,462 INFO misc.py line 115 22900] Train: [52/100][105/156] Data 0.001 (0.001) Batch 3.799 (3.460) Remain 07:14:48 loss: 0.3358 Lr: 0.02568
[2023-08-08 02:58:30,722 INFO misc.py line 115 22900] Train: [52/100][106/156] Data 0.001 (0.001) Batch 4.260 (3.468) Remain 07:15:43 loss: 0.3469 Lr: 0.02568
[2023-08-08 02:58:34,803 INFO misc.py line 115 22900] Train: [52/100][107/156] Data 0.001 (0.001) Batch 4.081 (3.474) Remain 07:16:24 loss: 0.5406 Lr: 0.02567
[2023-08-08 02:58:38,170 INFO misc.py line 115 22900] Train: [52/100][108/156] Data 0.001 (0.001) Batch 3.367 (3.473) Remain 07:16:13 loss: 0.1566 Lr: 0.02567
[2023-08-08 02:58:42,180 INFO misc.py line 115 22900] Train: [52/100][109/156] Data 0.001 (0.001) Batch 4.009 (3.478) Remain 07:16:47 loss: 0.2085 Lr: 0.02566
[2023-08-08 02:58:46,698 INFO misc.py line 115 22900] Train: [52/100][110/156] Data 0.001 (0.001) Batch 4.518 (3.488) Remain 07:17:57 loss: 0.4004 Lr: 0.02566
[2023-08-08 02:58:49,984 INFO misc.py line 115 22900] Train: [52/100][111/156] Data 0.001 (0.001) Batch 3.286 (3.486) Remain 07:17:40 loss: 0.3791 Lr: 0.02565
[2023-08-08 02:58:52,307 INFO misc.py line 115 22900] Train: [52/100][112/156] Data 0.001 (0.001) Batch 2.323 (3.475) Remain 07:16:16 loss: 0.1510 Lr: 0.02565
[2023-08-08 02:58:56,306 INFO misc.py line 115 22900] Train: [52/100][113/156] Data 0.001 (0.001) Batch 3.998 (3.480) Remain 07:16:48 loss: 0.4168 Lr: 0.02564
[2023-08-08 02:58:59,865 INFO misc.py line 115 22900] Train: [52/100][114/156] Data 0.001 (0.001) Batch 3.559 (3.481) Remain 07:16:50 loss: 0.2024 Lr: 0.02564
[2023-08-08 02:59:03,162 INFO misc.py line 115 22900] Train: [52/100][115/156] Data 0.001 (0.001) Batch 3.297 (3.479) Remain 07:16:34 loss: 0.0685 Lr: 0.02563
[2023-08-08 02:59:06,602 INFO misc.py line 115 22900] Train: [52/100][116/156] Data 0.001 (0.001) Batch 3.440 (3.479) Remain 07:16:28 loss: 0.4157 Lr: 0.02563
[2023-08-08 02:59:09,841 INFO misc.py line 115 22900] Train: [52/100][117/156] Data 0.001 (0.001) Batch 3.239 (3.477) Remain 07:16:09 loss: 0.2680 Lr: 0.02562
[2023-08-08 02:59:13,324 INFO misc.py line 115 22900] Train: [52/100][118/156] Data 0.001 (0.001) Batch 3.483 (3.477) Remain 07:16:06 loss: 0.2005 Lr: 0.02561
[2023-08-08 02:59:17,206 INFO misc.py line 115 22900] Train: [52/100][119/156] Data 0.001 (0.001) Batch 3.882 (3.480) Remain 07:16:28 loss: 0.3465 Lr: 0.02561
[2023-08-08 02:59:20,551 INFO misc.py line 115 22900] Train: [52/100][120/156] Data 0.001 (0.001) Batch 3.345 (3.479) Remain 07:16:16 loss: 0.2068 Lr: 0.02560
[2023-08-08 02:59:24,140 INFO misc.py line 115 22900] Train: [52/100][121/156] Data 0.001 (0.001) Batch 3.589 (3.480) Remain 07:16:20 loss: 0.2777 Lr: 0.02560
[2023-08-08 02:59:28,598 INFO misc.py line 115 22900] Train: [52/100][122/156] Data 0.001 (0.001) Batch 4.458 (3.488) Remain 07:17:18 loss: 0.3964 Lr: 0.02559
[2023-08-08 02:59:32,357 INFO misc.py line 115 22900] Train: [52/100][123/156] Data 0.001 (0.001) Batch 3.759 (3.491) Remain 07:17:32 loss: 0.2554 Lr: 0.02559
[2023-08-08 02:59:34,816 INFO misc.py line 115 22900] Train: [52/100][124/156] Data 0.001 (0.001) Batch 2.459 (3.482) Remain 07:16:24 loss: 0.2771 Lr: 0.02558
[2023-08-08 02:59:38,866 INFO misc.py line 115 22900] Train: [52/100][125/156] Data 0.001 (0.001) Batch 4.050 (3.487) Remain 07:16:56 loss: 0.2673 Lr: 0.02558
[2023-08-08 02:59:40,941 INFO misc.py line 115 22900] Train: [52/100][126/156] Data 0.001 (0.001) Batch 2.075 (3.475) Remain 07:15:26 loss: 0.1794 Lr: 0.02557
[2023-08-08 02:59:45,675 INFO misc.py line 115 22900] Train: [52/100][127/156] Data 0.001 (0.001) Batch 4.733 (3.485) Remain 07:16:39 loss: 0.3603 Lr: 0.02557
[2023-08-08 02:59:49,798 INFO misc.py line 115 22900] Train: [52/100][128/156] Data 0.001 (0.001) Batch 4.123 (3.490) Remain 07:17:13 loss: 0.2650 Lr: 0.02556
[2023-08-08 02:59:53,049 INFO misc.py line 115 22900] Train: [52/100][129/156] Data 0.001 (0.001) Batch 3.251 (3.489) Remain 07:16:56 loss: 0.1562 Lr: 0.02556
[2023-08-08 02:59:55,821 INFO misc.py line 115 22900] Train: [52/100][130/156] Data 0.001 (0.001) Batch 2.772 (3.483) Remain 07:16:10 loss: 0.2611 Lr: 0.02555
[2023-08-08 02:59:58,503 INFO misc.py line 115 22900] Train: [52/100][131/156] Data 0.001 (0.001) Batch 2.681 (3.477) Remain 07:15:19 loss: 0.1538 Lr: 0.02555
[2023-08-08 03:00:01,260 INFO misc.py line 115 22900] Train: [52/100][132/156] Data 0.001 (0.001) Batch 2.758 (3.471) Remain 07:14:34 loss: 0.2138 Lr: 0.02554
[2023-08-08 03:00:03,203 INFO misc.py line 115 22900] Train: [52/100][133/156] Data 0.001 (0.001) Batch 1.942 (3.459) Remain 07:13:02 loss: 0.1847 Lr: 0.02554
[2023-08-08 03:00:06,639 INFO misc.py line 115 22900] Train: [52/100][134/156] Data 0.001 (0.001) Batch 3.436 (3.459) Remain 07:12:57 loss: 0.3419 Lr: 0.02553
[2023-08-08 03:00:10,685 INFO misc.py line 115 22900] Train: [52/100][135/156] Data 0.001 (0.001) Batch 4.046 (3.464) Remain 07:13:27 loss: 0.3913 Lr: 0.02552
[2023-08-08 03:00:13,455 INFO misc.py line 115 22900] Train: [52/100][136/156] Data 0.001 (0.001) Batch 2.770 (3.458) Remain 07:12:45 loss: 0.2393 Lr: 0.02552
[2023-08-08 03:00:16,700 INFO misc.py line 115 22900] Train: [52/100][137/156] Data 0.001 (0.001) Batch 3.244 (3.457) Remain 07:12:29 loss: 0.3506 Lr: 0.02551
[2023-08-08 03:00:20,437 INFO misc.py line 115 22900] Train: [52/100][138/156] Data 0.001 (0.001) Batch 3.738 (3.459) Remain 07:12:41 loss: 0.2166 Lr: 0.02551
[2023-08-08 03:00:23,496 INFO misc.py line 115 22900] Train: [52/100][139/156] Data 0.001 (0.001) Batch 3.059 (3.456) Remain 07:12:16 loss: 0.2011 Lr: 0.02550
[2023-08-08 03:00:26,896 INFO misc.py line 115 22900] Train: [52/100][140/156] Data 0.001 (0.001) Batch 3.400 (3.455) Remain 07:12:09 loss: 0.1954 Lr: 0.02550
[2023-08-08 03:00:30,471 INFO misc.py line 115 22900] Train: [52/100][141/156] Data 0.001 (0.001) Batch 3.574 (3.456) Remain 07:12:12 loss: 0.1909 Lr: 0.02549
[2023-08-08 03:00:33,759 INFO misc.py line 115 22900] Train: [52/100][142/156] Data 0.001 (0.001) Batch 3.289 (3.455) Remain 07:12:00 loss: 0.3561 Lr: 0.02549
[2023-08-08 03:00:37,311 INFO misc.py line 115 22900] Train: [52/100][143/156] Data 0.001 (0.001) Batch 3.552 (3.456) Remain 07:12:02 loss: 0.2206 Lr: 0.02548
[2023-08-08 03:00:41,572 INFO misc.py line 115 22900] Train: [52/100][144/156] Data 0.001 (0.001) Batch 4.261 (3.462) Remain 07:12:41 loss: 0.3321 Lr: 0.02548
[2023-08-08 03:00:45,321 INFO misc.py line 115 22900] Train: [52/100][145/156] Data 0.001 (0.001) Batch 3.749 (3.464) Remain 07:12:53 loss: 0.3851 Lr: 0.02547
[2023-08-08 03:00:49,268 INFO misc.py line 115 22900] Train: [52/100][146/156] Data 0.001 (0.001) Batch 3.947 (3.467) Remain 07:13:15 loss: 0.2290 Lr: 0.02547
[2023-08-08 03:00:52,165 INFO misc.py line 115 22900] Train: [52/100][147/156] Data 0.001 (0.001) Batch 2.897 (3.463) Remain 07:12:41 loss: 0.1340 Lr: 0.02546
[2023-08-08 03:00:56,220 INFO misc.py line 115 22900] Train: [52/100][148/156] Data 0.001 (0.001) Batch 4.056 (3.467) Remain 07:13:09 loss: 0.1744 Lr: 0.02546
[2023-08-08 03:00:59,411 INFO misc.py line 115 22900] Train: [52/100][149/156] Data 0.001 (0.001) Batch 3.191 (3.465) Remain 07:12:51 loss: 0.1955 Lr: 0.02545
[2023-08-08 03:01:03,014 INFO misc.py line 115 22900] Train: [52/100][150/156] Data 0.001 (0.001) Batch 3.603 (3.466) Remain 07:12:54 loss: 0.1942 Lr: 0.02545
[2023-08-08 03:01:06,364 INFO misc.py line 115 22900] Train: [52/100][151/156] Data 0.001 (0.001) Batch 3.349 (3.465) Remain 07:12:45 loss: 0.2167 Lr: 0.02544
[2023-08-08 03:01:09,669 INFO misc.py line 115 22900] Train: [52/100][152/156] Data 0.001 (0.001) Batch 3.306 (3.464) Remain 07:12:34 loss: 0.2554 Lr: 0.02543
[2023-08-08 03:01:13,307 INFO misc.py line 115 22900] Train: [52/100][153/156] Data 0.001 (0.001) Batch 3.638 (3.465) Remain 07:12:39 loss: 0.1260 Lr: 0.02543
[2023-08-08 03:01:16,589 INFO misc.py line 115 22900] Train: [52/100][154/156] Data 0.001 (0.001) Batch 3.282 (3.464) Remain 07:12:26 loss: 0.2175 Lr: 0.02542
[2023-08-08 03:01:20,203 INFO misc.py line 115 22900] Train: [52/100][155/156] Data 0.001 (0.001) Batch 3.613 (3.465) Remain 07:12:30 loss: 0.1087 Lr: 0.02542
[2023-08-08 03:01:23,629 INFO misc.py line 115 22900] Train: [52/100][156/156] Data 0.001 (0.001) Batch 3.427 (3.465) Remain 07:12:25 loss: 0.2426 Lr: 0.02541
[2023-08-08 03:01:23,630 INFO misc.py line 129 22900] Train result: loss: 0.2480 
[2023-08-08 03:01:23,630 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 03:01:25,727 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.9374 
[2023-08-08 03:01:26,594 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.5839 
[2023-08-08 03:01:28,256 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.7260 
[2023-08-08 03:01:29,777 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.3175 
[2023-08-08 03:01:31,622 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.6411 
[2023-08-08 03:01:33,287 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.8512 
[2023-08-08 03:01:35,427 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.2763 
[2023-08-08 03:01:37,232 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.1033 
[2023-08-08 03:01:38,514 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.5073 
[2023-08-08 03:01:40,644 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.4442 
[2023-08-08 03:01:41,170 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.5326 
[2023-08-08 03:01:42,699 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.7467 
[2023-08-08 03:01:45,407 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.2368 
[2023-08-08 03:01:47,087 INFO evaluator.py line 122 22900] Test: [14/24] Loss 1.0578 
[2023-08-08 03:01:49,109 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.4074 
[2023-08-08 03:01:51,819 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.3078 
[2023-08-08 03:01:54,525 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.5590 
[2023-08-08 03:01:56,372 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.4656 
[2023-08-08 03:01:57,121 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.0476 
[2023-08-08 03:01:58,007 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.7079 
[2023-08-08 03:02:00,269 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.4312 
[2023-08-08 03:02:02,234 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.7736 
[2023-08-08 03:02:04,082 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.9982 
[2023-08-08 03:02:06,018 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.6500 
[2023-08-08 03:02:06,065 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2354/0.3312/0.6860.
[2023-08-08 03:02:06,066 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6458/0.9617
[2023-08-08 03:02:06,066 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9444/0.9882
[2023-08-08 03:02:06,066 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1936/0.4325
[2023-08-08 03:02:06,066 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1758/0.3771
[2023-08-08 03:02:06,066 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6070/0.6895
[2023-08-08 03:02:06,066 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.3230/0.4224
[2023-08-08 03:02:06,066 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5041/0.6517
[2023-08-08 03:02:06,066 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1769/0.1931
[2023-08-08 03:02:06,066 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1013/0.1951
[2023-08-08 03:02:06,066 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0145/0.0146
[2023-08-08 03:02:06,066 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 03:02:06,066 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0441/0.0509
[2023-08-08 03:02:06,066 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.2481/0.5290
[2023-08-08 03:02:06,066 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0754/0.0859
[2023-08-08 03:02:06,066 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0219/0.0305
[2023-08-08 03:02:06,066 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0260/0.0276
[2023-08-08 03:02:06,066 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.2298/0.3406
[2023-08-08 03:02:06,066 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.2864/0.4157
[2023-08-08 03:02:06,066 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 03:02:06,066 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0900/0.2175
[2023-08-08 03:02:06,066 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 03:02:06,066 INFO misc.py line 152 22900] Currently Best mIoU: 0.2562
[2023-08-08 03:02:06,066 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 03:02:11,741 INFO misc.py line 115 22900] Train: [53/100][1/156] Data 0.851 (0.851) Batch 4.874 (4.874) Remain 10:08:09 loss: 0.3346 Lr: 0.02541
[2023-08-08 03:02:14,556 INFO misc.py line 115 22900] Train: [53/100][2/156] Data 0.001 (0.001) Batch 2.815 (2.815) Remain 05:51:14 loss: 0.0723 Lr: 0.02540
[2023-08-08 03:02:18,174 INFO misc.py line 115 22900] Train: [53/100][3/156] Data 0.001 (0.001) Batch 3.618 (3.618) Remain 07:31:23 loss: 0.1794 Lr: 0.02540
[2023-08-08 03:02:22,260 INFO misc.py line 115 22900] Train: [53/100][4/156] Data 0.001 (0.001) Batch 4.086 (4.086) Remain 08:29:37 loss: 0.3022 Lr: 0.02539
[2023-08-08 03:02:25,551 INFO misc.py line 115 22900] Train: [53/100][5/156] Data 0.001 (0.001) Batch 3.291 (3.688) Remain 07:39:59 loss: 0.1610 Lr: 0.02539
[2023-08-08 03:02:28,612 INFO misc.py line 115 22900] Train: [53/100][6/156] Data 0.001 (0.001) Batch 3.061 (3.479) Remain 07:13:50 loss: 0.1681 Lr: 0.02538
[2023-08-08 03:02:31,443 INFO misc.py line 115 22900] Train: [53/100][7/156] Data 0.001 (0.001) Batch 2.831 (3.317) Remain 06:53:35 loss: 0.1963 Lr: 0.02538
[2023-08-08 03:02:35,549 INFO misc.py line 115 22900] Train: [53/100][8/156] Data 0.001 (0.001) Batch 4.106 (3.475) Remain 07:13:12 loss: 0.1932 Lr: 0.02537
[2023-08-08 03:02:39,028 INFO misc.py line 115 22900] Train: [53/100][9/156] Data 0.001 (0.001) Batch 3.479 (3.476) Remain 07:13:14 loss: 0.3045 Lr: 0.02537
[2023-08-08 03:02:43,040 INFO misc.py line 115 22900] Train: [53/100][10/156] Data 0.001 (0.001) Batch 4.012 (3.552) Remain 07:22:43 loss: 0.4459 Lr: 0.02536
[2023-08-08 03:02:46,623 INFO misc.py line 115 22900] Train: [53/100][11/156] Data 0.001 (0.001) Batch 3.583 (3.556) Remain 07:23:08 loss: 0.3517 Lr: 0.02536
[2023-08-08 03:02:50,134 INFO misc.py line 115 22900] Train: [53/100][12/156] Data 0.001 (0.001) Batch 3.511 (3.551) Remain 07:22:27 loss: 0.1525 Lr: 0.02535
[2023-08-08 03:02:54,697 INFO misc.py line 115 22900] Train: [53/100][13/156] Data 0.001 (0.001) Batch 4.563 (3.652) Remain 07:35:00 loss: 0.3387 Lr: 0.02534
[2023-08-08 03:02:58,267 INFO misc.py line 115 22900] Train: [53/100][14/156] Data 0.001 (0.001) Batch 3.570 (3.645) Remain 07:34:01 loss: 0.1217 Lr: 0.02534
[2023-08-08 03:03:01,996 INFO misc.py line 115 22900] Train: [53/100][15/156] Data 0.001 (0.001) Batch 3.730 (3.652) Remain 07:34:50 loss: 0.2471 Lr: 0.02533
[2023-08-08 03:03:04,513 INFO misc.py line 115 22900] Train: [53/100][16/156] Data 0.001 (0.001) Batch 2.517 (3.565) Remain 07:23:54 loss: 0.2116 Lr: 0.02533
[2023-08-08 03:03:07,089 INFO misc.py line 115 22900] Train: [53/100][17/156] Data 0.001 (0.001) Batch 2.576 (3.494) Remain 07:15:02 loss: 0.2426 Lr: 0.02532
[2023-08-08 03:03:11,108 INFO misc.py line 115 22900] Train: [53/100][18/156] Data 0.001 (0.001) Batch 4.019 (3.529) Remain 07:19:20 loss: 0.3692 Lr: 0.02532
[2023-08-08 03:03:14,254 INFO misc.py line 115 22900] Train: [53/100][19/156] Data 0.001 (0.001) Batch 3.146 (3.505) Remain 07:16:18 loss: 0.4371 Lr: 0.02531
[2023-08-08 03:03:17,796 INFO misc.py line 115 22900] Train: [53/100][20/156] Data 0.001 (0.001) Batch 3.543 (3.507) Remain 07:16:31 loss: 0.1908 Lr: 0.02531
[2023-08-08 03:03:21,832 INFO misc.py line 115 22900] Train: [53/100][21/156] Data 0.001 (0.001) Batch 4.036 (3.537) Remain 07:20:07 loss: 0.1842 Lr: 0.02530
[2023-08-08 03:03:24,907 INFO misc.py line 115 22900] Train: [53/100][22/156] Data 0.001 (0.001) Batch 3.075 (3.512) Remain 07:17:02 loss: 0.1375 Lr: 0.02530
[2023-08-08 03:03:27,830 INFO misc.py line 115 22900] Train: [53/100][23/156] Data 0.001 (0.001) Batch 2.923 (3.483) Remain 07:13:18 loss: 0.1435 Lr: 0.02529
[2023-08-08 03:03:31,467 INFO misc.py line 115 22900] Train: [53/100][24/156] Data 0.001 (0.001) Batch 3.637 (3.490) Remain 07:14:10 loss: 0.1006 Lr: 0.02529
[2023-08-08 03:03:34,102 INFO misc.py line 115 22900] Train: [53/100][25/156] Data 0.001 (0.001) Batch 2.636 (3.451) Remain 07:09:16 loss: 0.3748 Lr: 0.02528
[2023-08-08 03:03:37,276 INFO misc.py line 115 22900] Train: [53/100][26/156] Data 0.001 (0.001) Batch 3.173 (3.439) Remain 07:07:43 loss: 0.1945 Lr: 0.02528
[2023-08-08 03:03:40,663 INFO misc.py line 115 22900] Train: [53/100][27/156] Data 0.001 (0.001) Batch 3.387 (3.437) Remain 07:07:23 loss: 0.1339 Lr: 0.02527
[2023-08-08 03:03:44,649 INFO misc.py line 115 22900] Train: [53/100][28/156] Data 0.001 (0.001) Batch 3.986 (3.459) Remain 07:10:03 loss: 0.2678 Lr: 0.02527
[2023-08-08 03:03:48,452 INFO misc.py line 115 22900] Train: [53/100][29/156] Data 0.001 (0.001) Batch 3.803 (3.472) Remain 07:11:39 loss: 0.1747 Lr: 0.02526
[2023-08-08 03:03:51,406 INFO misc.py line 115 22900] Train: [53/100][30/156] Data 0.001 (0.001) Batch 2.955 (3.453) Remain 07:09:12 loss: 0.3849 Lr: 0.02525
[2023-08-08 03:03:54,133 INFO misc.py line 115 22900] Train: [53/100][31/156] Data 0.001 (0.001) Batch 2.727 (3.427) Remain 07:05:55 loss: 0.1603 Lr: 0.02525
[2023-08-08 03:03:58,171 INFO misc.py line 115 22900] Train: [53/100][32/156] Data 0.001 (0.001) Batch 4.038 (3.448) Remain 07:08:29 loss: 0.7423 Lr: 0.02524
[2023-08-08 03:04:01,010 INFO misc.py line 115 22900] Train: [53/100][33/156] Data 0.001 (0.001) Batch 2.839 (3.428) Remain 07:05:54 loss: 0.2077 Lr: 0.02524
[2023-08-08 03:04:03,989 INFO misc.py line 115 22900] Train: [53/100][34/156] Data 0.001 (0.001) Batch 2.979 (3.413) Remain 07:04:03 loss: 0.2146 Lr: 0.02523
[2023-08-08 03:04:06,555 INFO misc.py line 115 22900] Train: [53/100][35/156] Data 0.001 (0.001) Batch 2.566 (3.387) Remain 07:00:42 loss: 0.1557 Lr: 0.02523
[2023-08-08 03:04:10,588 INFO misc.py line 115 22900] Train: [53/100][36/156] Data 0.001 (0.001) Batch 4.033 (3.406) Remain 07:03:05 loss: 0.2257 Lr: 0.02522
[2023-08-08 03:04:15,047 INFO misc.py line 115 22900] Train: [53/100][37/156] Data 0.001 (0.001) Batch 4.459 (3.437) Remain 07:06:52 loss: 0.2757 Lr: 0.02522
[2023-08-08 03:04:19,024 INFO misc.py line 115 22900] Train: [53/100][38/156] Data 0.001 (0.001) Batch 3.977 (3.453) Remain 07:08:43 loss: 0.2681 Lr: 0.02521
[2023-08-08 03:04:22,981 INFO misc.py line 115 22900] Train: [53/100][39/156] Data 0.001 (0.001) Batch 3.957 (3.467) Remain 07:10:24 loss: 0.5023 Lr: 0.02521
[2023-08-08 03:04:26,324 INFO misc.py line 115 22900] Train: [53/100][40/156] Data 0.001 (0.001) Batch 3.343 (3.463) Remain 07:09:56 loss: 0.2298 Lr: 0.02520
[2023-08-08 03:04:30,064 INFO misc.py line 115 22900] Train: [53/100][41/156] Data 0.001 (0.001) Batch 3.740 (3.471) Remain 07:10:46 loss: 0.1377 Lr: 0.02520
[2023-08-08 03:04:34,055 INFO misc.py line 115 22900] Train: [53/100][42/156] Data 0.001 (0.001) Batch 3.991 (3.484) Remain 07:12:22 loss: 0.5399 Lr: 0.02519
[2023-08-08 03:04:38,014 INFO misc.py line 115 22900] Train: [53/100][43/156] Data 0.001 (0.001) Batch 3.959 (3.496) Remain 07:13:47 loss: 0.2630 Lr: 0.02519
[2023-08-08 03:04:42,066 INFO misc.py line 115 22900] Train: [53/100][44/156] Data 0.001 (0.001) Batch 4.053 (3.510) Remain 07:15:25 loss: 0.5242 Lr: 0.02518
[2023-08-08 03:04:44,893 INFO misc.py line 115 22900] Train: [53/100][45/156] Data 0.001 (0.001) Batch 2.827 (3.493) Remain 07:13:20 loss: 0.2391 Lr: 0.02518
[2023-08-08 03:04:49,395 INFO misc.py line 115 22900] Train: [53/100][46/156] Data 0.001 (0.001) Batch 4.502 (3.517) Remain 07:16:11 loss: 0.3279 Lr: 0.02517
[2023-08-08 03:04:53,556 INFO misc.py line 115 22900] Train: [53/100][47/156] Data 0.001 (0.001) Batch 4.161 (3.531) Remain 07:17:57 loss: 0.2010 Lr: 0.02516
[2023-08-08 03:04:57,469 INFO misc.py line 115 22900] Train: [53/100][48/156] Data 0.001 (0.001) Batch 3.913 (3.540) Remain 07:18:56 loss: 0.2573 Lr: 0.02516
[2023-08-08 03:05:00,998 INFO misc.py line 115 22900] Train: [53/100][49/156] Data 0.001 (0.001) Batch 3.529 (3.540) Remain 07:18:51 loss: 0.3158 Lr: 0.02515
[2023-08-08 03:05:05,251 INFO misc.py line 115 22900] Train: [53/100][50/156] Data 0.001 (0.001) Batch 4.253 (3.555) Remain 07:20:40 loss: 0.4086 Lr: 0.02515
[2023-08-08 03:05:08,953 INFO misc.py line 115 22900] Train: [53/100][51/156] Data 0.001 (0.001) Batch 3.701 (3.558) Remain 07:20:59 loss: 0.1646 Lr: 0.02514
[2023-08-08 03:05:11,576 INFO misc.py line 115 22900] Train: [53/100][52/156] Data 0.001 (0.001) Batch 2.623 (3.539) Remain 07:18:34 loss: 0.2220 Lr: 0.02514
[2023-08-08 03:05:13,696 INFO misc.py line 115 22900] Train: [53/100][53/156] Data 0.001 (0.001) Batch 2.120 (3.510) Remain 07:15:00 loss: 0.1432 Lr: 0.02513
[2023-08-08 03:05:16,723 INFO misc.py line 115 22900] Train: [53/100][54/156] Data 0.001 (0.001) Batch 3.027 (3.501) Remain 07:13:46 loss: 0.1622 Lr: 0.02513
[2023-08-08 03:05:20,533 INFO misc.py line 115 22900] Train: [53/100][55/156] Data 0.001 (0.001) Batch 3.810 (3.507) Remain 07:14:26 loss: 0.2680 Lr: 0.02512
[2023-08-08 03:05:24,273 INFO misc.py line 115 22900] Train: [53/100][56/156] Data 0.001 (0.001) Batch 3.740 (3.511) Remain 07:14:55 loss: 0.2865 Lr: 0.02512
[2023-08-08 03:05:26,538 INFO misc.py line 115 22900] Train: [53/100][57/156] Data 0.001 (0.001) Batch 2.265 (3.488) Remain 07:12:00 loss: 0.1726 Lr: 0.02511
[2023-08-08 03:05:29,270 INFO misc.py line 115 22900] Train: [53/100][58/156] Data 0.001 (0.001) Batch 2.733 (3.474) Remain 07:10:15 loss: 0.2092 Lr: 0.02511
[2023-08-08 03:05:33,616 INFO misc.py line 115 22900] Train: [53/100][59/156] Data 0.001 (0.001) Batch 4.346 (3.490) Remain 07:12:07 loss: 0.3339 Lr: 0.02510
[2023-08-08 03:05:37,719 INFO misc.py line 115 22900] Train: [53/100][60/156] Data 0.001 (0.001) Batch 4.103 (3.501) Remain 07:13:23 loss: 0.3289 Lr: 0.02510
[2023-08-08 03:05:41,058 INFO misc.py line 115 22900] Train: [53/100][61/156] Data 0.001 (0.001) Batch 3.339 (3.498) Remain 07:12:59 loss: 0.2000 Lr: 0.02509
[2023-08-08 03:05:44,438 INFO misc.py line 115 22900] Train: [53/100][62/156] Data 0.001 (0.001) Batch 3.380 (3.496) Remain 07:12:41 loss: 0.2391 Lr: 0.02509
[2023-08-08 03:05:48,469 INFO misc.py line 115 22900] Train: [53/100][63/156] Data 0.001 (0.001) Batch 4.031 (3.505) Remain 07:13:43 loss: 0.4987 Lr: 0.02508
[2023-08-08 03:05:52,135 INFO misc.py line 115 22900] Train: [53/100][64/156] Data 0.001 (0.001) Batch 3.666 (3.508) Remain 07:14:00 loss: 0.3495 Lr: 0.02507
[2023-08-08 03:05:55,900 INFO misc.py line 115 22900] Train: [53/100][65/156] Data 0.001 (0.001) Batch 3.765 (3.512) Remain 07:14:27 loss: 0.3034 Lr: 0.02507
[2023-08-08 03:05:58,651 INFO misc.py line 115 22900] Train: [53/100][66/156] Data 0.001 (0.001) Batch 2.751 (3.500) Remain 07:12:54 loss: 0.1446 Lr: 0.02506
[2023-08-08 03:06:02,604 INFO misc.py line 115 22900] Train: [53/100][67/156] Data 0.001 (0.001) Batch 3.954 (3.507) Remain 07:13:43 loss: 0.1765 Lr: 0.02506
[2023-08-08 03:06:06,202 INFO misc.py line 115 22900] Train: [53/100][68/156] Data 0.001 (0.001) Batch 3.598 (3.508) Remain 07:13:50 loss: 0.1706 Lr: 0.02505
[2023-08-08 03:06:09,864 INFO misc.py line 115 22900] Train: [53/100][69/156] Data 0.001 (0.001) Batch 3.661 (3.510) Remain 07:14:03 loss: 0.2107 Lr: 0.02505
[2023-08-08 03:06:13,210 INFO misc.py line 115 22900] Train: [53/100][70/156] Data 0.001 (0.001) Batch 3.347 (3.508) Remain 07:13:42 loss: 0.1298 Lr: 0.02504
[2023-08-08 03:06:17,670 INFO misc.py line 115 22900] Train: [53/100][71/156] Data 0.001 (0.001) Batch 4.459 (3.522) Remain 07:15:22 loss: 0.2959 Lr: 0.02504
[2023-08-08 03:06:21,729 INFO misc.py line 115 22900] Train: [53/100][72/156] Data 0.001 (0.001) Batch 4.060 (3.530) Remain 07:16:16 loss: 0.1803 Lr: 0.02503
[2023-08-08 03:06:24,855 INFO misc.py line 115 22900] Train: [53/100][73/156] Data 0.001 (0.001) Batch 3.126 (3.524) Remain 07:15:30 loss: 0.3097 Lr: 0.02503
[2023-08-08 03:06:28,408 INFO misc.py line 115 22900] Train: [53/100][74/156] Data 0.001 (0.001) Batch 3.553 (3.524) Remain 07:15:30 loss: 0.2073 Lr: 0.02502
[2023-08-08 03:06:32,402 INFO misc.py line 115 22900] Train: [53/100][75/156] Data 0.001 (0.001) Batch 3.994 (3.531) Remain 07:16:14 loss: 0.3142 Lr: 0.02502
[2023-08-08 03:06:35,390 INFO misc.py line 115 22900] Train: [53/100][76/156] Data 0.001 (0.001) Batch 2.988 (3.524) Remain 07:15:16 loss: 0.2371 Lr: 0.02501
[2023-08-08 03:06:38,437 INFO misc.py line 115 22900] Train: [53/100][77/156] Data 0.001 (0.001) Batch 3.047 (3.517) Remain 07:14:24 loss: 0.0969 Lr: 0.02501
[2023-08-08 03:06:42,254 INFO misc.py line 115 22900] Train: [53/100][78/156] Data 0.001 (0.001) Batch 3.817 (3.521) Remain 07:14:51 loss: 0.3057 Lr: 0.02500
[2023-08-08 03:06:45,611 INFO misc.py line 115 22900] Train: [53/100][79/156] Data 0.001 (0.001) Batch 3.357 (3.519) Remain 07:14:31 loss: 0.1432 Lr: 0.02499
[2023-08-08 03:06:48,804 INFO misc.py line 115 22900] Train: [53/100][80/156] Data 0.001 (0.001) Batch 3.193 (3.515) Remain 07:13:56 loss: 0.1605 Lr: 0.02499
[2023-08-08 03:06:51,544 INFO misc.py line 115 22900] Train: [53/100][81/156] Data 0.001 (0.001) Batch 2.740 (3.505) Remain 07:12:39 loss: 0.2243 Lr: 0.02498
[2023-08-08 03:06:55,147 INFO misc.py line 115 22900] Train: [53/100][82/156] Data 0.001 (0.001) Batch 3.604 (3.506) Remain 07:12:45 loss: 0.2340 Lr: 0.02498
[2023-08-08 03:06:59,288 INFO misc.py line 115 22900] Train: [53/100][83/156] Data 0.001 (0.001) Batch 4.141 (3.514) Remain 07:13:40 loss: 0.3289 Lr: 0.02497
[2023-08-08 03:07:03,360 INFO misc.py line 115 22900] Train: [53/100][84/156] Data 0.001 (0.001) Batch 4.072 (3.521) Remain 07:14:28 loss: 0.2332 Lr: 0.02497
[2023-08-08 03:07:06,607 INFO misc.py line 115 22900] Train: [53/100][85/156] Data 0.001 (0.001) Batch 3.247 (3.517) Remain 07:13:59 loss: 0.1977 Lr: 0.02496
[2023-08-08 03:07:09,156 INFO misc.py line 115 22900] Train: [53/100][86/156] Data 0.001 (0.001) Batch 2.549 (3.506) Remain 07:12:29 loss: 0.2329 Lr: 0.02496
[2023-08-08 03:07:13,353 INFO misc.py line 115 22900] Train: [53/100][87/156] Data 0.001 (0.001) Batch 4.197 (3.514) Remain 07:13:27 loss: 0.3070 Lr: 0.02495
[2023-08-08 03:07:16,105 INFO misc.py line 115 22900] Train: [53/100][88/156] Data 0.001 (0.001) Batch 2.752 (3.505) Remain 07:12:17 loss: 0.1341 Lr: 0.02495
[2023-08-08 03:07:19,655 INFO misc.py line 115 22900] Train: [53/100][89/156] Data 0.001 (0.001) Batch 3.550 (3.506) Remain 07:12:17 loss: 0.1093 Lr: 0.02494
[2023-08-08 03:07:22,581 INFO misc.py line 115 22900] Train: [53/100][90/156] Data 0.001 (0.001) Batch 2.926 (3.499) Remain 07:11:25 loss: 0.1150 Lr: 0.02494
[2023-08-08 03:07:26,153 INFO misc.py line 115 22900] Train: [53/100][91/156] Data 0.001 (0.001) Batch 3.571 (3.500) Remain 07:11:27 loss: 0.2781 Lr: 0.02493
[2023-08-08 03:07:28,581 INFO misc.py line 115 22900] Train: [53/100][92/156] Data 0.001 (0.001) Batch 2.428 (3.488) Remain 07:09:55 loss: 0.0669 Lr: 0.02493
[2023-08-08 03:07:31,488 INFO misc.py line 115 22900] Train: [53/100][93/156] Data 0.001 (0.001) Batch 2.908 (3.481) Remain 07:09:03 loss: 0.2920 Lr: 0.02493
[2023-08-08 03:07:34,548 INFO misc.py line 115 22900] Train: [53/100][94/156] Data 0.001 (0.001) Batch 3.060 (3.477) Remain 07:08:26 loss: 0.2538 Lr: 0.02492
[2023-08-08 03:07:38,464 INFO misc.py line 115 22900] Train: [53/100][95/156] Data 0.001 (0.001) Batch 3.916 (3.481) Remain 07:08:58 loss: 0.3153 Lr: 0.02492
[2023-08-08 03:07:42,362 INFO misc.py line 115 22900] Train: [53/100][96/156] Data 0.001 (0.001) Batch 3.898 (3.486) Remain 07:09:27 loss: 0.1576 Lr: 0.02491
[2023-08-08 03:07:45,929 INFO misc.py line 115 22900] Train: [53/100][97/156] Data 0.001 (0.001) Batch 3.567 (3.487) Remain 07:09:30 loss: 0.3156 Lr: 0.02490
[2023-08-08 03:07:48,577 INFO misc.py line 115 22900] Train: [53/100][98/156] Data 0.001 (0.001) Batch 2.648 (3.478) Remain 07:08:21 loss: 0.1795 Lr: 0.02490
[2023-08-08 03:07:52,081 INFO misc.py line 115 22900] Train: [53/100][99/156] Data 0.001 (0.001) Batch 3.504 (3.478) Remain 07:08:20 loss: 0.1987 Lr: 0.02489
[2023-08-08 03:07:55,468 INFO misc.py line 115 22900] Train: [53/100][100/156] Data 0.001 (0.001) Batch 3.387 (3.477) Remain 07:08:09 loss: 0.1637 Lr: 0.02489
[2023-08-08 03:07:58,122 INFO misc.py line 115 22900] Train: [53/100][101/156] Data 0.001 (0.001) Batch 2.653 (3.469) Remain 07:07:04 loss: 0.1398 Lr: 0.02488
[2023-08-08 03:08:01,310 INFO misc.py line 115 22900] Train: [53/100][102/156] Data 0.001 (0.001) Batch 3.189 (3.466) Remain 07:06:40 loss: 0.2429 Lr: 0.02488
[2023-08-08 03:08:04,935 INFO misc.py line 115 22900] Train: [53/100][103/156] Data 0.001 (0.001) Batch 3.624 (3.468) Remain 07:06:48 loss: 0.2439 Lr: 0.02487
[2023-08-08 03:08:07,589 INFO misc.py line 115 22900] Train: [53/100][104/156] Data 0.001 (0.001) Batch 2.654 (3.460) Remain 07:05:45 loss: 0.1086 Lr: 0.02487
[2023-08-08 03:08:11,146 INFO misc.py line 115 22900] Train: [53/100][105/156] Data 0.001 (0.001) Batch 3.556 (3.461) Remain 07:05:48 loss: 0.2501 Lr: 0.02486
[2023-08-08 03:08:13,787 INFO misc.py line 115 22900] Train: [53/100][106/156] Data 0.001 (0.001) Batch 2.641 (3.453) Remain 07:04:46 loss: 0.1289 Lr: 0.02486
[2023-08-08 03:08:16,229 INFO misc.py line 115 22900] Train: [53/100][107/156] Data 0.001 (0.001) Batch 2.442 (3.443) Remain 07:03:31 loss: 0.0352 Lr: 0.02485
[2023-08-08 03:08:20,113 INFO misc.py line 115 22900] Train: [53/100][108/156] Data 0.001 (0.001) Batch 3.883 (3.447) Remain 07:03:59 loss: 0.4241 Lr: 0.02485
[2023-08-08 03:08:23,528 INFO misc.py line 115 22900] Train: [53/100][109/156] Data 0.001 (0.001) Batch 3.416 (3.447) Remain 07:03:53 loss: 0.2290 Lr: 0.02484
[2023-08-08 03:08:27,682 INFO misc.py line 115 22900] Train: [53/100][110/156] Data 0.001 (0.001) Batch 4.153 (3.453) Remain 07:04:38 loss: 0.5800 Lr: 0.02484
[2023-08-08 03:08:31,706 INFO misc.py line 115 22900] Train: [53/100][111/156] Data 0.001 (0.001) Batch 4.025 (3.459) Remain 07:05:14 loss: 0.2980 Lr: 0.02483
[2023-08-08 03:08:35,359 INFO misc.py line 115 22900] Train: [53/100][112/156] Data 0.001 (0.001) Batch 3.653 (3.460) Remain 07:05:23 loss: 0.2619 Lr: 0.02483
[2023-08-08 03:08:39,510 INFO misc.py line 115 22900] Train: [53/100][113/156] Data 0.001 (0.001) Batch 4.151 (3.467) Remain 07:06:06 loss: 0.1883 Lr: 0.02482
[2023-08-08 03:08:43,073 INFO misc.py line 115 22900] Train: [53/100][114/156] Data 0.001 (0.001) Batch 3.563 (3.468) Remain 07:06:09 loss: 0.2653 Lr: 0.02481
[2023-08-08 03:08:45,894 INFO misc.py line 115 22900] Train: [53/100][115/156] Data 0.001 (0.001) Batch 2.821 (3.462) Remain 07:05:23 loss: 0.1981 Lr: 0.02481
[2023-08-08 03:08:49,200 INFO misc.py line 115 22900] Train: [53/100][116/156] Data 0.001 (0.001) Batch 3.306 (3.460) Remain 07:05:10 loss: 0.1078 Lr: 0.02480
[2023-08-08 03:08:53,313 INFO misc.py line 115 22900] Train: [53/100][117/156] Data 0.001 (0.001) Batch 4.113 (3.466) Remain 07:05:48 loss: 0.2331 Lr: 0.02480
[2023-08-08 03:08:56,462 INFO misc.py line 115 22900] Train: [53/100][118/156] Data 0.001 (0.001) Batch 3.148 (3.463) Remain 07:05:25 loss: 0.3497 Lr: 0.02479
[2023-08-08 03:08:59,988 INFO misc.py line 115 22900] Train: [53/100][119/156] Data 0.001 (0.001) Batch 3.526 (3.464) Remain 07:05:25 loss: 0.2497 Lr: 0.02479
[2023-08-08 03:09:03,969 INFO misc.py line 115 22900] Train: [53/100][120/156] Data 0.001 (0.001) Batch 3.982 (3.468) Remain 07:05:54 loss: 0.2707 Lr: 0.02478
[2023-08-08 03:09:07,177 INFO misc.py line 115 22900] Train: [53/100][121/156] Data 0.001 (0.001) Batch 3.207 (3.466) Remain 07:05:34 loss: 0.1931 Lr: 0.02478
[2023-08-08 03:09:11,275 INFO misc.py line 115 22900] Train: [53/100][122/156] Data 0.001 (0.001) Batch 4.099 (3.471) Remain 07:06:10 loss: 0.3077 Lr: 0.02477
[2023-08-08 03:09:13,786 INFO misc.py line 115 22900] Train: [53/100][123/156] Data 0.001 (0.001) Batch 2.511 (3.463) Remain 07:05:08 loss: 0.1305 Lr: 0.02477
[2023-08-08 03:09:18,213 INFO misc.py line 115 22900] Train: [53/100][124/156] Data 0.001 (0.001) Batch 4.427 (3.471) Remain 07:06:03 loss: 0.3996 Lr: 0.02476
[2023-08-08 03:09:22,085 INFO misc.py line 115 22900] Train: [53/100][125/156] Data 0.001 (0.001) Batch 3.872 (3.475) Remain 07:06:24 loss: 0.2822 Lr: 0.02476
[2023-08-08 03:09:25,056 INFO misc.py line 115 22900] Train: [53/100][126/156] Data 0.001 (0.001) Batch 2.971 (3.471) Remain 07:05:50 loss: 0.0746 Lr: 0.02475
[2023-08-08 03:09:28,543 INFO misc.py line 115 22900] Train: [53/100][127/156] Data 0.001 (0.001) Batch 3.486 (3.471) Remain 07:05:47 loss: 0.2623 Lr: 0.02475
[2023-08-08 03:09:31,288 INFO misc.py line 115 22900] Train: [53/100][128/156] Data 0.001 (0.001) Batch 2.746 (3.465) Remain 07:05:01 loss: 0.1999 Lr: 0.02474
[2023-08-08 03:09:34,990 INFO misc.py line 115 22900] Train: [53/100][129/156] Data 0.001 (0.001) Batch 3.702 (3.467) Remain 07:05:12 loss: 0.2020 Lr: 0.02474
[2023-08-08 03:09:38,158 INFO misc.py line 115 22900] Train: [53/100][130/156] Data 0.001 (0.001) Batch 3.168 (3.464) Remain 07:04:51 loss: 0.1443 Lr: 0.02473
[2023-08-08 03:09:41,153 INFO misc.py line 115 22900] Train: [53/100][131/156] Data 0.001 (0.001) Batch 2.994 (3.461) Remain 07:04:20 loss: 0.2242 Lr: 0.02472
[2023-08-08 03:09:44,712 INFO misc.py line 115 22900] Train: [53/100][132/156] Data 0.001 (0.001) Batch 3.559 (3.462) Remain 07:04:23 loss: 0.4218 Lr: 0.02472
[2023-08-08 03:09:48,180 INFO misc.py line 115 22900] Train: [53/100][133/156] Data 0.001 (0.001) Batch 3.468 (3.462) Remain 07:04:19 loss: 0.3779 Lr: 0.02471
[2023-08-08 03:09:51,419 INFO misc.py line 115 22900] Train: [53/100][134/156] Data 0.001 (0.001) Batch 3.239 (3.460) Remain 07:04:03 loss: 0.2462 Lr: 0.02471
[2023-08-08 03:09:54,675 INFO misc.py line 115 22900] Train: [53/100][135/156] Data 0.001 (0.001) Batch 3.256 (3.458) Remain 07:03:49 loss: 0.3597 Lr: 0.02470
[2023-08-08 03:09:57,740 INFO misc.py line 115 22900] Train: [53/100][136/156] Data 0.001 (0.001) Batch 3.065 (3.455) Remain 07:03:23 loss: 0.2183 Lr: 0.02470
[2023-08-08 03:10:00,952 INFO misc.py line 115 22900] Train: [53/100][137/156] Data 0.001 (0.001) Batch 3.212 (3.454) Remain 07:03:07 loss: 0.5141 Lr: 0.02469
[2023-08-08 03:10:04,315 INFO misc.py line 115 22900] Train: [53/100][138/156] Data 0.001 (0.001) Batch 3.363 (3.453) Remain 07:02:58 loss: 0.1718 Lr: 0.02469
[2023-08-08 03:10:07,698 INFO misc.py line 115 22900] Train: [53/100][139/156] Data 0.001 (0.001) Batch 3.383 (3.452) Remain 07:02:51 loss: 0.4136 Lr: 0.02468
[2023-08-08 03:10:12,235 INFO misc.py line 115 22900] Train: [53/100][140/156] Data 0.001 (0.001) Batch 4.537 (3.460) Remain 07:03:46 loss: 0.3747 Lr: 0.02468
[2023-08-08 03:10:14,853 INFO misc.py line 115 22900] Train: [53/100][141/156] Data 0.001 (0.001) Batch 2.618 (3.454) Remain 07:02:57 loss: 0.2470 Lr: 0.02467
[2023-08-08 03:10:19,064 INFO misc.py line 115 22900] Train: [53/100][142/156] Data 0.001 (0.001) Batch 4.211 (3.460) Remain 07:03:34 loss: 0.3373 Lr: 0.02467
[2023-08-08 03:10:23,129 INFO misc.py line 115 22900] Train: [53/100][143/156] Data 0.001 (0.001) Batch 4.065 (3.464) Remain 07:04:02 loss: 0.1558 Lr: 0.02466
[2023-08-08 03:10:26,632 INFO misc.py line 115 22900] Train: [53/100][144/156] Data 0.001 (0.001) Batch 3.503 (3.464) Remain 07:04:01 loss: 0.3516 Lr: 0.02466
[2023-08-08 03:10:29,473 INFO misc.py line 115 22900] Train: [53/100][145/156] Data 0.001 (0.001) Batch 2.842 (3.460) Remain 07:03:25 loss: 0.1717 Lr: 0.02465
[2023-08-08 03:10:32,983 INFO misc.py line 115 22900] Train: [53/100][146/156] Data 0.001 (0.001) Batch 3.510 (3.460) Remain 07:03:24 loss: 0.2627 Lr: 0.02465
[2023-08-08 03:10:37,776 INFO misc.py line 115 22900] Train: [53/100][147/156] Data 0.001 (0.001) Batch 4.792 (3.469) Remain 07:04:29 loss: 0.3444 Lr: 0.02464
[2023-08-08 03:10:41,147 INFO misc.py line 115 22900] Train: [53/100][148/156] Data 0.001 (0.001) Batch 3.371 (3.469) Remain 07:04:20 loss: 0.3111 Lr: 0.02463
[2023-08-08 03:10:44,781 INFO misc.py line 115 22900] Train: [53/100][149/156] Data 0.001 (0.001) Batch 3.634 (3.470) Remain 07:04:25 loss: 0.3463 Lr: 0.02463
[2023-08-08 03:10:48,652 INFO misc.py line 115 22900] Train: [53/100][150/156] Data 0.001 (0.001) Batch 3.871 (3.473) Remain 07:04:42 loss: 0.2187 Lr: 0.02462
[2023-08-08 03:10:51,851 INFO misc.py line 115 22900] Train: [53/100][151/156] Data 0.001 (0.001) Batch 3.199 (3.471) Remain 07:04:25 loss: 0.3619 Lr: 0.02462
[2023-08-08 03:10:55,091 INFO misc.py line 115 22900] Train: [53/100][152/156] Data 0.001 (0.001) Batch 3.240 (3.469) Remain 07:04:10 loss: 0.2425 Lr: 0.02461
[2023-08-08 03:10:58,800 INFO misc.py line 115 22900] Train: [53/100][153/156] Data 0.001 (0.001) Batch 3.710 (3.471) Remain 07:04:18 loss: 0.2079 Lr: 0.02461
[2023-08-08 03:11:01,463 INFO misc.py line 115 22900] Train: [53/100][154/156] Data 0.001 (0.001) Batch 2.663 (3.465) Remain 07:03:35 loss: 0.3329 Lr: 0.02460
[2023-08-08 03:11:05,055 INFO misc.py line 115 22900] Train: [53/100][155/156] Data 0.001 (0.001) Batch 3.592 (3.466) Remain 07:03:38 loss: 0.3746 Lr: 0.02460
[2023-08-08 03:11:08,117 INFO misc.py line 115 22900] Train: [53/100][156/156] Data 0.001 (0.001) Batch 3.061 (3.464) Remain 07:03:15 loss: 0.1960 Lr: 0.02459
[2023-08-08 03:11:08,117 INFO misc.py line 129 22900] Train result: loss: 0.2526 
[2023-08-08 03:11:08,117 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 03:11:10,203 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.1217 
[2023-08-08 03:11:11,073 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.6035 
[2023-08-08 03:11:12,738 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.9630 
[2023-08-08 03:11:14,261 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.0145 
[2023-08-08 03:11:16,105 INFO evaluator.py line 122 22900] Test: [5/24] Loss 2.1483 
[2023-08-08 03:11:17,770 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.6460 
[2023-08-08 03:11:19,906 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.8938 
[2023-08-08 03:11:21,710 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.2231 
[2023-08-08 03:11:22,996 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.2127 
[2023-08-08 03:11:25,127 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.1919 
[2023-08-08 03:11:25,654 INFO evaluator.py line 122 22900] Test: [11/24] Loss 0.9547 
[2023-08-08 03:11:27,188 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.9250 
[2023-08-08 03:11:29,901 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.0772 
[2023-08-08 03:11:31,582 INFO evaluator.py line 122 22900] Test: [14/24] Loss 1.1376 
[2023-08-08 03:11:33,606 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.3878 
[2023-08-08 03:11:36,319 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.0495 
[2023-08-08 03:11:39,022 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.3015 
[2023-08-08 03:11:40,867 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.7348 
[2023-08-08 03:11:41,617 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.0790 
[2023-08-08 03:11:42,502 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.7925 
[2023-08-08 03:11:44,762 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3181 
[2023-08-08 03:11:46,727 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.9303 
[2023-08-08 03:11:48,574 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.5735 
[2023-08-08 03:11:50,510 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.8429 
[2023-08-08 03:11:50,557 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2404/0.3350/0.6863.
[2023-08-08 03:11:50,557 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6449/0.9508
[2023-08-08 03:11:50,557 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9456/0.9852
[2023-08-08 03:11:50,557 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1898/0.3337
[2023-08-08 03:11:50,557 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1579/0.3106
[2023-08-08 03:11:50,557 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.5720/0.8126
[2023-08-08 03:11:50,557 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.1917/0.3123
[2023-08-08 03:11:50,557 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5290/0.6322
[2023-08-08 03:11:50,557 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1597/0.1715
[2023-08-08 03:11:50,557 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1503/0.3864
[2023-08-08 03:11:50,557 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0392/0.0394
[2023-08-08 03:11:50,557 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 03:11:50,557 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0020/0.0020
[2023-08-08 03:11:50,557 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.2164/0.2788
[2023-08-08 03:11:50,557 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0594/0.0717
[2023-08-08 03:11:50,557 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0574/0.0631
[2023-08-08 03:11:50,557 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.3797/0.4297
[2023-08-08 03:11:50,557 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0955/0.1665
[2023-08-08 03:11:50,557 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.3468/0.6222
[2023-08-08 03:11:50,557 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 03:11:50,557 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0710/0.1315
[2023-08-08 03:11:50,558 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 03:11:50,558 INFO misc.py line 152 22900] Currently Best mIoU: 0.2562
[2023-08-08 03:11:50,558 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 03:11:55,861 INFO misc.py line 115 22900] Train: [54/100][1/156] Data 1.176 (1.176) Batch 4.499 (4.499) Remain 09:09:44 loss: 0.1709 Lr: 0.02459
[2023-08-08 03:11:59,645 INFO misc.py line 115 22900] Train: [54/100][2/156] Data 0.001 (0.001) Batch 3.784 (3.784) Remain 07:42:19 loss: 0.2116 Lr: 0.02458
[2023-08-08 03:12:03,713 INFO misc.py line 115 22900] Train: [54/100][3/156] Data 0.001 (0.001) Batch 4.068 (4.068) Remain 08:16:53 loss: 0.1424 Lr: 0.02458
[2023-08-08 03:12:06,897 INFO misc.py line 115 22900] Train: [54/100][4/156] Data 0.001 (0.001) Batch 3.184 (3.184) Remain 06:28:51 loss: 0.2860 Lr: 0.02457
[2023-08-08 03:12:09,753 INFO misc.py line 115 22900] Train: [54/100][5/156] Data 0.001 (0.001) Batch 2.855 (3.020) Remain 06:08:44 loss: 0.1908 Lr: 0.02457
[2023-08-08 03:12:13,191 INFO misc.py line 115 22900] Train: [54/100][6/156] Data 0.001 (0.001) Batch 3.438 (3.159) Remain 06:25:43 loss: 0.2829 Lr: 0.02456
[2023-08-08 03:12:16,577 INFO misc.py line 115 22900] Train: [54/100][7/156] Data 0.001 (0.001) Batch 3.386 (3.216) Remain 06:32:36 loss: 0.2411 Lr: 0.02456
[2023-08-08 03:12:20,551 INFO misc.py line 115 22900] Train: [54/100][8/156] Data 0.001 (0.001) Batch 3.974 (3.368) Remain 06:51:03 loss: 0.3120 Lr: 0.02455
[2023-08-08 03:12:24,598 INFO misc.py line 115 22900] Train: [54/100][9/156] Data 0.001 (0.001) Batch 4.047 (3.481) Remain 07:04:49 loss: 0.3693 Lr: 0.02454
[2023-08-08 03:12:27,721 INFO misc.py line 115 22900] Train: [54/100][10/156] Data 0.001 (0.001) Batch 3.123 (3.430) Remain 06:58:32 loss: 0.1168 Lr: 0.02454
[2023-08-08 03:12:31,349 INFO misc.py line 115 22900] Train: [54/100][11/156] Data 0.001 (0.001) Batch 3.628 (3.454) Remain 07:01:29 loss: 0.2030 Lr: 0.02453
[2023-08-08 03:12:34,163 INFO misc.py line 115 22900] Train: [54/100][12/156] Data 0.001 (0.001) Batch 2.814 (3.383) Remain 06:52:45 loss: 0.2886 Lr: 0.02453
[2023-08-08 03:12:37,542 INFO misc.py line 115 22900] Train: [54/100][13/156] Data 0.001 (0.001) Batch 3.379 (3.383) Remain 06:52:39 loss: 0.2306 Lr: 0.02452
[2023-08-08 03:12:40,466 INFO misc.py line 115 22900] Train: [54/100][14/156] Data 0.001 (0.001) Batch 2.923 (3.341) Remain 06:47:30 loss: 0.1312 Lr: 0.02452
[2023-08-08 03:12:43,739 INFO misc.py line 115 22900] Train: [54/100][15/156] Data 0.001 (0.001) Batch 3.273 (3.335) Remain 06:46:45 loss: 0.3092 Lr: 0.02451
[2023-08-08 03:12:47,844 INFO misc.py line 115 22900] Train: [54/100][16/156] Data 0.001 (0.001) Batch 4.106 (3.395) Remain 06:53:55 loss: 0.2089 Lr: 0.02451
[2023-08-08 03:12:51,753 INFO misc.py line 115 22900] Train: [54/100][17/156] Data 0.001 (0.001) Batch 3.909 (3.431) Remain 06:58:20 loss: 0.3497 Lr: 0.02450
[2023-08-08 03:12:55,098 INFO misc.py line 115 22900] Train: [54/100][18/156] Data 0.001 (0.001) Batch 3.345 (3.426) Remain 06:57:34 loss: 0.4712 Lr: 0.02450
[2023-08-08 03:12:58,673 INFO misc.py line 115 22900] Train: [54/100][19/156] Data 0.001 (0.001) Batch 3.575 (3.435) Remain 06:58:39 loss: 0.2131 Lr: 0.02449
[2023-08-08 03:13:02,722 INFO misc.py line 115 22900] Train: [54/100][20/156] Data 0.001 (0.001) Batch 4.049 (3.471) Remain 07:03:00 loss: 0.4130 Lr: 0.02449
[2023-08-08 03:13:05,110 INFO misc.py line 115 22900] Train: [54/100][21/156] Data 0.001 (0.001) Batch 2.388 (3.411) Remain 06:55:37 loss: 0.1514 Lr: 0.02448
[2023-08-08 03:13:09,205 INFO misc.py line 115 22900] Train: [54/100][22/156] Data 0.001 (0.001) Batch 4.095 (3.447) Remain 06:59:57 loss: 0.2289 Lr: 0.02448
[2023-08-08 03:13:11,700 INFO misc.py line 115 22900] Train: [54/100][23/156] Data 0.001 (0.001) Batch 2.495 (3.399) Remain 06:54:05 loss: 0.0693 Lr: 0.02447
[2023-08-08 03:13:14,999 INFO misc.py line 115 22900] Train: [54/100][24/156] Data 0.001 (0.001) Batch 3.299 (3.395) Remain 06:53:27 loss: 0.2844 Lr: 0.02447
[2023-08-08 03:13:18,230 INFO misc.py line 115 22900] Train: [54/100][25/156] Data 0.001 (0.001) Batch 3.231 (3.387) Remain 06:52:29 loss: 0.2882 Lr: 0.02446
[2023-08-08 03:13:22,749 INFO misc.py line 115 22900] Train: [54/100][26/156] Data 0.001 (0.001) Batch 4.519 (3.436) Remain 06:58:25 loss: 0.3551 Lr: 0.02445
[2023-08-08 03:13:26,358 INFO misc.py line 115 22900] Train: [54/100][27/156] Data 0.001 (0.001) Batch 3.609 (3.444) Remain 06:59:14 loss: 0.3034 Lr: 0.02445
[2023-08-08 03:13:30,406 INFO misc.py line 115 22900] Train: [54/100][28/156] Data 0.001 (0.001) Batch 4.049 (3.468) Remain 07:02:08 loss: 0.2863 Lr: 0.02444
[2023-08-08 03:13:34,009 INFO misc.py line 115 22900] Train: [54/100][29/156] Data 0.001 (0.001) Batch 3.603 (3.473) Remain 07:02:42 loss: 0.2146 Lr: 0.02444
[2023-08-08 03:13:36,575 INFO misc.py line 115 22900] Train: [54/100][30/156] Data 0.001 (0.001) Batch 2.566 (3.439) Remain 06:58:34 loss: 0.3193 Lr: 0.02443
[2023-08-08 03:13:40,310 INFO misc.py line 115 22900] Train: [54/100][31/156] Data 0.001 (0.001) Batch 3.734 (3.450) Remain 06:59:47 loss: 0.4362 Lr: 0.02443
[2023-08-08 03:13:43,074 INFO misc.py line 115 22900] Train: [54/100][32/156] Data 0.001 (0.001) Batch 2.765 (3.426) Remain 06:56:51 loss: 0.2226 Lr: 0.02442
[2023-08-08 03:13:46,158 INFO misc.py line 115 22900] Train: [54/100][33/156] Data 0.001 (0.001) Batch 3.084 (3.415) Remain 06:55:24 loss: 0.4182 Lr: 0.02442
[2023-08-08 03:13:50,264 INFO misc.py line 115 22900] Train: [54/100][34/156] Data 0.001 (0.001) Batch 4.106 (3.437) Remain 06:58:04 loss: 0.3657 Lr: 0.02441
[2023-08-08 03:13:53,681 INFO misc.py line 115 22900] Train: [54/100][35/156] Data 0.001 (0.001) Batch 3.417 (3.437) Remain 06:57:56 loss: 0.2848 Lr: 0.02441
[2023-08-08 03:13:56,434 INFO misc.py line 115 22900] Train: [54/100][36/156] Data 0.001 (0.001) Batch 2.752 (3.416) Remain 06:55:21 loss: 0.1412 Lr: 0.02440
[2023-08-08 03:13:59,783 INFO misc.py line 115 22900] Train: [54/100][37/156] Data 0.001 (0.001) Batch 3.349 (3.414) Remain 06:55:03 loss: 0.2857 Lr: 0.02440
[2023-08-08 03:14:03,749 INFO misc.py line 115 22900] Train: [54/100][38/156] Data 0.001 (0.001) Batch 3.965 (3.430) Remain 06:56:55 loss: 0.1704 Lr: 0.02439
[2023-08-08 03:14:07,013 INFO misc.py line 115 22900] Train: [54/100][39/156] Data 0.001 (0.001) Batch 3.264 (3.425) Remain 06:56:18 loss: 0.2961 Lr: 0.02439
[2023-08-08 03:14:11,016 INFO misc.py line 115 22900] Train: [54/100][40/156] Data 0.001 (0.001) Batch 4.004 (3.441) Remain 06:58:09 loss: 0.3187 Lr: 0.02438
[2023-08-08 03:14:14,365 INFO misc.py line 115 22900] Train: [54/100][41/156] Data 0.001 (0.001) Batch 3.348 (3.438) Remain 06:57:47 loss: 0.2986 Lr: 0.02437
[2023-08-08 03:14:18,101 INFO misc.py line 115 22900] Train: [54/100][42/156] Data 0.001 (0.001) Batch 3.736 (3.446) Remain 06:58:40 loss: 0.2521 Lr: 0.02437
[2023-08-08 03:14:20,823 INFO misc.py line 115 22900] Train: [54/100][43/156] Data 0.001 (0.001) Batch 2.722 (3.428) Remain 06:56:24 loss: 0.1179 Lr: 0.02436
[2023-08-08 03:14:24,173 INFO misc.py line 115 22900] Train: [54/100][44/156] Data 0.001 (0.001) Batch 3.349 (3.426) Remain 06:56:07 loss: 0.2854 Lr: 0.02436
[2023-08-08 03:14:28,230 INFO misc.py line 115 22900] Train: [54/100][45/156] Data 0.001 (0.001) Batch 4.057 (3.441) Remain 06:57:53 loss: 0.1408 Lr: 0.02435
[2023-08-08 03:14:31,558 INFO misc.py line 115 22900] Train: [54/100][46/156] Data 0.001 (0.001) Batch 3.328 (3.438) Remain 06:57:30 loss: 0.2729 Lr: 0.02435
[2023-08-08 03:14:35,595 INFO misc.py line 115 22900] Train: [54/100][47/156] Data 0.001 (0.001) Batch 4.038 (3.452) Remain 06:59:06 loss: 0.1789 Lr: 0.02434
[2023-08-08 03:14:38,930 INFO misc.py line 115 22900] Train: [54/100][48/156] Data 0.001 (0.001) Batch 3.335 (3.449) Remain 06:58:44 loss: 0.1980 Lr: 0.02434
[2023-08-08 03:14:41,158 INFO misc.py line 115 22900] Train: [54/100][49/156] Data 0.001 (0.001) Batch 2.228 (3.423) Remain 06:55:27 loss: 0.1226 Lr: 0.02433
[2023-08-08 03:14:45,214 INFO misc.py line 115 22900] Train: [54/100][50/156] Data 0.001 (0.001) Batch 4.056 (3.436) Remain 06:57:02 loss: 0.6467 Lr: 0.02433
[2023-08-08 03:14:49,639 INFO misc.py line 115 22900] Train: [54/100][51/156] Data 0.001 (0.001) Batch 4.425 (3.457) Remain 06:59:28 loss: 0.4278 Lr: 0.02432
[2023-08-08 03:14:53,652 INFO misc.py line 115 22900] Train: [54/100][52/156] Data 0.001 (0.001) Batch 4.012 (3.468) Remain 07:00:47 loss: 0.3870 Lr: 0.02432
[2023-08-08 03:14:56,856 INFO misc.py line 115 22900] Train: [54/100][53/156] Data 0.001 (0.001) Batch 3.204 (3.463) Remain 07:00:06 loss: 0.1061 Lr: 0.02431
[2023-08-08 03:15:01,307 INFO misc.py line 115 22900] Train: [54/100][54/156] Data 0.001 (0.001) Batch 4.451 (3.482) Remain 07:02:23 loss: 0.2893 Lr: 0.02431
[2023-08-08 03:15:04,641 INFO misc.py line 115 22900] Train: [54/100][55/156] Data 0.001 (0.001) Batch 3.334 (3.479) Remain 07:01:59 loss: 0.2394 Lr: 0.02430
[2023-08-08 03:15:08,605 INFO misc.py line 115 22900] Train: [54/100][56/156] Data 0.001 (0.001) Batch 3.964 (3.489) Remain 07:03:02 loss: 0.1361 Lr: 0.02430
[2023-08-08 03:15:12,154 INFO misc.py line 115 22900] Train: [54/100][57/156] Data 0.001 (0.001) Batch 3.550 (3.490) Remain 07:03:07 loss: 0.2085 Lr: 0.02429
[2023-08-08 03:15:15,325 INFO misc.py line 115 22900] Train: [54/100][58/156] Data 0.001 (0.001) Batch 3.171 (3.484) Remain 07:02:21 loss: 0.1375 Lr: 0.02428
[2023-08-08 03:15:18,713 INFO misc.py line 115 22900] Train: [54/100][59/156] Data 0.001 (0.001) Batch 3.387 (3.482) Remain 07:02:05 loss: 0.2947 Lr: 0.02428
[2023-08-08 03:15:22,741 INFO misc.py line 115 22900] Train: [54/100][60/156] Data 0.001 (0.001) Batch 4.029 (3.492) Remain 07:03:11 loss: 0.2734 Lr: 0.02427
[2023-08-08 03:15:26,204 INFO misc.py line 115 22900] Train: [54/100][61/156] Data 0.001 (0.001) Batch 3.462 (3.491) Remain 07:03:04 loss: 0.2772 Lr: 0.02427
[2023-08-08 03:15:29,573 INFO misc.py line 115 22900] Train: [54/100][62/156] Data 0.001 (0.001) Batch 3.369 (3.489) Remain 07:02:46 loss: 0.2991 Lr: 0.02426
[2023-08-08 03:15:32,225 INFO misc.py line 115 22900] Train: [54/100][63/156] Data 0.001 (0.001) Batch 2.653 (3.475) Remain 07:01:01 loss: 0.1612 Lr: 0.02426
[2023-08-08 03:15:34,665 INFO misc.py line 115 22900] Train: [54/100][64/156] Data 0.001 (0.001) Batch 2.439 (3.458) Remain 06:58:54 loss: 0.1388 Lr: 0.02425
[2023-08-08 03:15:37,869 INFO misc.py line 115 22900] Train: [54/100][65/156] Data 0.001 (0.001) Batch 3.204 (3.454) Remain 06:58:21 loss: 0.2307 Lr: 0.02425
[2023-08-08 03:15:41,202 INFO misc.py line 115 22900] Train: [54/100][66/156] Data 0.001 (0.001) Batch 3.332 (3.452) Remain 06:58:03 loss: 0.3188 Lr: 0.02424
[2023-08-08 03:15:43,055 INFO misc.py line 115 22900] Train: [54/100][67/156] Data 0.001 (0.001) Batch 1.854 (3.427) Remain 06:54:58 loss: 0.1489 Lr: 0.02424
[2023-08-08 03:15:46,357 INFO misc.py line 115 22900] Train: [54/100][68/156] Data 0.001 (0.001) Batch 3.302 (3.425) Remain 06:54:41 loss: 0.1369 Lr: 0.02423
[2023-08-08 03:15:49,242 INFO misc.py line 115 22900] Train: [54/100][69/156] Data 0.001 (0.001) Batch 2.885 (3.417) Remain 06:53:38 loss: 0.1104 Lr: 0.02423
[2023-08-08 03:15:53,012 INFO misc.py line 115 22900] Train: [54/100][70/156] Data 0.001 (0.001) Batch 3.770 (3.422) Remain 06:54:13 loss: 0.2639 Lr: 0.02422
[2023-08-08 03:15:55,875 INFO misc.py line 115 22900] Train: [54/100][71/156] Data 0.001 (0.001) Batch 2.863 (3.414) Remain 06:53:10 loss: 0.0883 Lr: 0.02422
[2023-08-08 03:15:59,904 INFO misc.py line 115 22900] Train: [54/100][72/156] Data 0.001 (0.001) Batch 4.030 (3.423) Remain 06:54:11 loss: 0.1935 Lr: 0.02421
[2023-08-08 03:16:03,565 INFO misc.py line 115 22900] Train: [54/100][73/156] Data 0.001 (0.001) Batch 3.661 (3.426) Remain 06:54:32 loss: 0.2336 Lr: 0.02421
[2023-08-08 03:16:07,015 INFO misc.py line 115 22900] Train: [54/100][74/156] Data 0.001 (0.001) Batch 3.449 (3.427) Remain 06:54:31 loss: 0.1988 Lr: 0.02420
[2023-08-08 03:16:10,350 INFO misc.py line 115 22900] Train: [54/100][75/156] Data 0.001 (0.001) Batch 3.335 (3.426) Remain 06:54:18 loss: 0.2027 Lr: 0.02419
[2023-08-08 03:16:14,755 INFO misc.py line 115 22900] Train: [54/100][76/156] Data 0.001 (0.001) Batch 4.405 (3.439) Remain 06:55:52 loss: 0.3987 Lr: 0.02419
[2023-08-08 03:16:18,183 INFO misc.py line 115 22900] Train: [54/100][77/156] Data 0.001 (0.001) Batch 3.428 (3.439) Remain 06:55:48 loss: 0.1326 Lr: 0.02418
[2023-08-08 03:16:22,505 INFO misc.py line 115 22900] Train: [54/100][78/156] Data 0.001 (0.001) Batch 4.322 (3.451) Remain 06:57:10 loss: 0.4460 Lr: 0.02418
[2023-08-08 03:16:25,903 INFO misc.py line 115 22900] Train: [54/100][79/156] Data 0.001 (0.001) Batch 3.398 (3.450) Remain 06:57:01 loss: 0.2034 Lr: 0.02417
[2023-08-08 03:16:29,998 INFO misc.py line 115 22900] Train: [54/100][80/156] Data 0.001 (0.001) Batch 4.095 (3.458) Remain 06:57:59 loss: 0.2533 Lr: 0.02417
[2023-08-08 03:16:33,373 INFO misc.py line 115 22900] Train: [54/100][81/156] Data 0.001 (0.001) Batch 3.375 (3.457) Remain 06:57:47 loss: 0.2726 Lr: 0.02416
[2023-08-08 03:16:36,801 INFO misc.py line 115 22900] Train: [54/100][82/156] Data 0.001 (0.001) Batch 3.428 (3.457) Remain 06:57:41 loss: 0.2255 Lr: 0.02416
[2023-08-08 03:16:40,344 INFO misc.py line 115 22900] Train: [54/100][83/156] Data 0.001 (0.001) Batch 3.543 (3.458) Remain 06:57:46 loss: 0.3245 Lr: 0.02415
[2023-08-08 03:16:44,780 INFO misc.py line 115 22900] Train: [54/100][84/156] Data 0.001 (0.001) Batch 4.436 (3.470) Remain 06:59:10 loss: 0.3488 Lr: 0.02415
[2023-08-08 03:16:48,142 INFO misc.py line 115 22900] Train: [54/100][85/156] Data 0.001 (0.001) Batch 3.362 (3.469) Remain 06:58:57 loss: 0.1836 Lr: 0.02414
[2023-08-08 03:16:51,379 INFO misc.py line 115 22900] Train: [54/100][86/156] Data 0.001 (0.001) Batch 3.237 (3.466) Remain 06:58:33 loss: 0.1850 Lr: 0.02414
[2023-08-08 03:16:54,083 INFO misc.py line 115 22900] Train: [54/100][87/156] Data 0.001 (0.001) Batch 2.704 (3.457) Remain 06:57:24 loss: 0.1308 Lr: 0.02413
[2023-08-08 03:16:58,063 INFO misc.py line 115 22900] Train: [54/100][88/156] Data 0.001 (0.001) Batch 3.980 (3.463) Remain 06:58:05 loss: 0.2612 Lr: 0.02413
[2023-08-08 03:17:01,554 INFO misc.py line 115 22900] Train: [54/100][89/156] Data 0.001 (0.001) Batch 3.492 (3.463) Remain 06:58:04 loss: 0.2450 Lr: 0.02412
[2023-08-08 03:17:04,771 INFO misc.py line 115 22900] Train: [54/100][90/156] Data 0.001 (0.001) Batch 3.217 (3.460) Remain 06:57:40 loss: 0.1708 Lr: 0.02412
[2023-08-08 03:17:08,729 INFO misc.py line 115 22900] Train: [54/100][91/156] Data 0.001 (0.001) Batch 3.958 (3.466) Remain 06:58:17 loss: 0.2793 Lr: 0.02411
[2023-08-08 03:17:12,177 INFO misc.py line 115 22900] Train: [54/100][92/156] Data 0.001 (0.001) Batch 3.448 (3.466) Remain 06:58:12 loss: 0.2119 Lr: 0.02410
[2023-08-08 03:17:14,763 INFO misc.py line 115 22900] Train: [54/100][93/156] Data 0.001 (0.001) Batch 2.586 (3.456) Remain 06:56:58 loss: 0.1316 Lr: 0.02410
[2023-08-08 03:17:18,979 INFO misc.py line 115 22900] Train: [54/100][94/156] Data 0.001 (0.001) Batch 4.216 (3.464) Remain 06:57:55 loss: 0.2806 Lr: 0.02409
[2023-08-08 03:17:22,982 INFO misc.py line 115 22900] Train: [54/100][95/156] Data 0.001 (0.001) Batch 4.003 (3.470) Remain 06:58:34 loss: 0.2867 Lr: 0.02409
[2023-08-08 03:17:27,028 INFO misc.py line 115 22900] Train: [54/100][96/156] Data 0.001 (0.001) Batch 4.046 (3.476) Remain 06:59:15 loss: 0.2125 Lr: 0.02408
[2023-08-08 03:17:29,806 INFO misc.py line 115 22900] Train: [54/100][97/156] Data 0.001 (0.001) Batch 2.779 (3.469) Remain 06:58:18 loss: 0.1314 Lr: 0.02408
[2023-08-08 03:17:33,772 INFO misc.py line 115 22900] Train: [54/100][98/156] Data 0.001 (0.001) Batch 3.966 (3.474) Remain 06:58:53 loss: 0.2345 Lr: 0.02407
[2023-08-08 03:17:37,613 INFO misc.py line 115 22900] Train: [54/100][99/156] Data 0.001 (0.001) Batch 3.841 (3.478) Remain 06:59:17 loss: 0.2305 Lr: 0.02407
[2023-08-08 03:17:41,216 INFO misc.py line 115 22900] Train: [54/100][100/156] Data 0.001 (0.001) Batch 3.603 (3.479) Remain 06:59:23 loss: 0.1305 Lr: 0.02406
[2023-08-08 03:17:45,134 INFO misc.py line 115 22900] Train: [54/100][101/156] Data 0.001 (0.001) Batch 3.918 (3.484) Remain 06:59:51 loss: 0.2779 Lr: 0.02406
[2023-08-08 03:17:48,565 INFO misc.py line 115 22900] Train: [54/100][102/156] Data 0.001 (0.001) Batch 3.431 (3.483) Remain 06:59:44 loss: 0.1726 Lr: 0.02405
[2023-08-08 03:17:52,157 INFO misc.py line 115 22900] Train: [54/100][103/156] Data 0.001 (0.001) Batch 3.592 (3.484) Remain 06:59:49 loss: 0.1636 Lr: 0.02405
[2023-08-08 03:17:55,323 INFO misc.py line 115 22900] Train: [54/100][104/156] Data 0.001 (0.001) Batch 3.165 (3.481) Remain 06:59:22 loss: 0.1361 Lr: 0.02404
[2023-08-08 03:17:59,203 INFO misc.py line 115 22900] Train: [54/100][105/156] Data 0.001 (0.001) Batch 3.881 (3.485) Remain 06:59:47 loss: 0.0859 Lr: 0.02404
[2023-08-08 03:18:01,893 INFO misc.py line 115 22900] Train: [54/100][106/156] Data 0.001 (0.001) Batch 2.689 (3.477) Remain 06:58:48 loss: 0.1203 Lr: 0.02403
[2023-08-08 03:18:05,193 INFO misc.py line 115 22900] Train: [54/100][107/156] Data 0.001 (0.001) Batch 3.301 (3.476) Remain 06:58:32 loss: 0.1391 Lr: 0.02403
[2023-08-08 03:18:07,926 INFO misc.py line 115 22900] Train: [54/100][108/156] Data 0.001 (0.001) Batch 2.733 (3.469) Remain 06:57:37 loss: 0.1202 Lr: 0.02402
[2023-08-08 03:18:10,647 INFO misc.py line 115 22900] Train: [54/100][109/156] Data 0.001 (0.001) Batch 2.721 (3.462) Remain 06:56:43 loss: 0.1373 Lr: 0.02401
[2023-08-08 03:18:13,635 INFO misc.py line 115 22900] Train: [54/100][110/156] Data 0.001 (0.001) Batch 2.989 (3.457) Remain 06:56:08 loss: 0.1700 Lr: 0.02401
[2023-08-08 03:18:15,833 INFO misc.py line 115 22900] Train: [54/100][111/156] Data 0.001 (0.001) Batch 2.198 (3.446) Remain 06:54:40 loss: 0.2474 Lr: 0.02400
[2023-08-08 03:18:19,817 INFO misc.py line 115 22900] Train: [54/100][112/156] Data 0.001 (0.001) Batch 3.984 (3.450) Remain 06:55:12 loss: 0.1787 Lr: 0.02400
[2023-08-08 03:18:23,071 INFO misc.py line 115 22900] Train: [54/100][113/156] Data 0.001 (0.001) Batch 3.253 (3.449) Remain 06:54:56 loss: 0.1747 Lr: 0.02399
[2023-08-08 03:18:25,934 INFO misc.py line 115 22900] Train: [54/100][114/156] Data 0.001 (0.001) Batch 2.863 (3.443) Remain 06:54:14 loss: 0.2763 Lr: 0.02399
[2023-08-08 03:18:29,245 INFO misc.py line 115 22900] Train: [54/100][115/156] Data 0.001 (0.001) Batch 3.311 (3.442) Remain 06:54:02 loss: 0.2912 Lr: 0.02398
[2023-08-08 03:18:31,889 INFO misc.py line 115 22900] Train: [54/100][116/156] Data 0.001 (0.001) Batch 2.644 (3.435) Remain 06:53:08 loss: 0.1104 Lr: 0.02398
[2023-08-08 03:18:35,398 INFO misc.py line 115 22900] Train: [54/100][117/156] Data 0.001 (0.001) Batch 3.509 (3.436) Remain 06:53:09 loss: 0.2574 Lr: 0.02397
[2023-08-08 03:18:38,984 INFO misc.py line 115 22900] Train: [54/100][118/156] Data 0.001 (0.001) Batch 3.586 (3.437) Remain 06:53:15 loss: 0.0629 Lr: 0.02397
[2023-08-08 03:18:43,744 INFO misc.py line 115 22900] Train: [54/100][119/156] Data 0.001 (0.001) Batch 4.761 (3.449) Remain 06:54:34 loss: 0.2845 Lr: 0.02396
[2023-08-08 03:18:45,965 INFO misc.py line 115 22900] Train: [54/100][120/156] Data 0.001 (0.001) Batch 2.221 (3.438) Remain 06:53:15 loss: 0.1562 Lr: 0.02396
[2023-08-08 03:18:49,990 INFO misc.py line 115 22900] Train: [54/100][121/156] Data 0.001 (0.001) Batch 4.024 (3.443) Remain 06:53:47 loss: 0.1477 Lr: 0.02395
[2023-08-08 03:18:52,790 INFO misc.py line 115 22900] Train: [54/100][122/156] Data 0.001 (0.001) Batch 2.800 (3.438) Remain 06:53:05 loss: 0.2951 Lr: 0.02395
[2023-08-08 03:18:56,436 INFO misc.py line 115 22900] Train: [54/100][123/156] Data 0.001 (0.001) Batch 3.646 (3.439) Remain 06:53:14 loss: 0.3546 Lr: 0.02394
[2023-08-08 03:19:00,175 INFO misc.py line 115 22900] Train: [54/100][124/156] Data 0.001 (0.001) Batch 3.739 (3.442) Remain 06:53:28 loss: 0.1334 Lr: 0.02394
[2023-08-08 03:19:03,749 INFO misc.py line 115 22900] Train: [54/100][125/156] Data 0.001 (0.001) Batch 3.575 (3.443) Remain 06:53:33 loss: 0.1870 Lr: 0.02393
[2023-08-08 03:19:07,506 INFO misc.py line 115 22900] Train: [54/100][126/156] Data 0.001 (0.001) Batch 3.756 (3.445) Remain 06:53:48 loss: 0.2337 Lr: 0.02392
[2023-08-08 03:19:10,704 INFO misc.py line 115 22900] Train: [54/100][127/156] Data 0.001 (0.001) Batch 3.198 (3.443) Remain 06:53:30 loss: 0.1641 Lr: 0.02392
[2023-08-08 03:19:13,379 INFO misc.py line 115 22900] Train: [54/100][128/156] Data 0.001 (0.001) Batch 2.675 (3.437) Remain 06:52:42 loss: 0.1400 Lr: 0.02391
[2023-08-08 03:19:17,640 INFO misc.py line 115 22900] Train: [54/100][129/156] Data 0.001 (0.001) Batch 4.261 (3.444) Remain 06:53:26 loss: 0.3555 Lr: 0.02391
[2023-08-08 03:19:20,146 INFO misc.py line 115 22900] Train: [54/100][130/156] Data 0.001 (0.001) Batch 2.506 (3.436) Remain 06:52:29 loss: 0.2284 Lr: 0.02390
[2023-08-08 03:19:24,092 INFO misc.py line 115 22900] Train: [54/100][131/156] Data 0.001 (0.001) Batch 3.947 (3.440) Remain 06:52:54 loss: 0.2739 Lr: 0.02390
[2023-08-08 03:19:28,290 INFO misc.py line 115 22900] Train: [54/100][132/156] Data 0.001 (0.001) Batch 4.198 (3.446) Remain 06:53:33 loss: 0.3314 Lr: 0.02389
[2023-08-08 03:19:31,599 INFO misc.py line 115 22900] Train: [54/100][133/156] Data 0.001 (0.001) Batch 3.309 (3.445) Remain 06:53:22 loss: 0.1548 Lr: 0.02389
[2023-08-08 03:19:35,427 INFO misc.py line 115 22900] Train: [54/100][134/156] Data 0.001 (0.001) Batch 3.828 (3.448) Remain 06:53:40 loss: 0.2791 Lr: 0.02388
[2023-08-08 03:19:39,336 INFO misc.py line 115 22900] Train: [54/100][135/156] Data 0.001 (0.001) Batch 3.910 (3.452) Remain 06:54:01 loss: 0.2439 Lr: 0.02388
[2023-08-08 03:19:42,029 INFO misc.py line 115 22900] Train: [54/100][136/156] Data 0.001 (0.001) Batch 2.693 (3.446) Remain 06:53:17 loss: 0.1634 Lr: 0.02387
[2023-08-08 03:19:45,545 INFO misc.py line 115 22900] Train: [54/100][137/156] Data 0.001 (0.001) Batch 3.516 (3.447) Remain 06:53:17 loss: 0.2274 Lr: 0.02387
[2023-08-08 03:19:49,139 INFO misc.py line 115 22900] Train: [54/100][138/156] Data 0.001 (0.001) Batch 3.594 (3.448) Remain 06:53:21 loss: 0.2127 Lr: 0.02386
[2023-08-08 03:19:52,443 INFO misc.py line 115 22900] Train: [54/100][139/156] Data 0.001 (0.001) Batch 3.305 (3.447) Remain 06:53:10 loss: 0.1834 Lr: 0.02386
[2023-08-08 03:19:54,755 INFO misc.py line 115 22900] Train: [54/100][140/156] Data 0.001 (0.001) Batch 2.311 (3.438) Remain 06:52:07 loss: 0.1411 Lr: 0.02385
[2023-08-08 03:19:58,134 INFO misc.py line 115 22900] Train: [54/100][141/156] Data 0.001 (0.001) Batch 3.380 (3.438) Remain 06:52:01 loss: 0.2501 Lr: 0.02385
[2023-08-08 03:20:02,355 INFO misc.py line 115 22900] Train: [54/100][142/156] Data 0.001 (0.001) Batch 4.221 (3.443) Remain 06:52:38 loss: 0.2636 Lr: 0.02384
[2023-08-08 03:20:05,500 INFO misc.py line 115 22900] Train: [54/100][143/156] Data 0.001 (0.001) Batch 3.145 (3.441) Remain 06:52:19 loss: 0.1484 Lr: 0.02383
[2023-08-08 03:20:08,714 INFO misc.py line 115 22900] Train: [54/100][144/156] Data 0.001 (0.001) Batch 3.213 (3.440) Remain 06:52:04 loss: 0.1293 Lr: 0.02383
[2023-08-08 03:20:13,499 INFO misc.py line 115 22900] Train: [54/100][145/156] Data 0.001 (0.001) Batch 4.785 (3.449) Remain 06:53:09 loss: 0.5595 Lr: 0.02382
[2023-08-08 03:20:17,200 INFO misc.py line 115 22900] Train: [54/100][146/156] Data 0.001 (0.001) Batch 3.701 (3.451) Remain 06:53:18 loss: 0.2833 Lr: 0.02382
[2023-08-08 03:20:21,297 INFO misc.py line 115 22900] Train: [54/100][147/156] Data 0.001 (0.001) Batch 4.097 (3.455) Remain 06:53:47 loss: 0.2304 Lr: 0.02381
[2023-08-08 03:20:24,901 INFO misc.py line 115 22900] Train: [54/100][148/156] Data 0.001 (0.001) Batch 3.604 (3.456) Remain 06:53:51 loss: 0.2183 Lr: 0.02381
[2023-08-08 03:20:28,545 INFO misc.py line 115 22900] Train: [54/100][149/156] Data 0.001 (0.001) Batch 3.643 (3.458) Remain 06:53:56 loss: 0.1901 Lr: 0.02380
[2023-08-08 03:20:32,121 INFO misc.py line 115 22900] Train: [54/100][150/156] Data 0.001 (0.001) Batch 3.576 (3.459) Remain 06:53:59 loss: 0.2109 Lr: 0.02380
[2023-08-08 03:20:35,016 INFO misc.py line 115 22900] Train: [54/100][151/156] Data 0.001 (0.001) Batch 2.896 (3.455) Remain 06:53:28 loss: 0.1460 Lr: 0.02379
[2023-08-08 03:20:38,387 INFO misc.py line 115 22900] Train: [54/100][152/156] Data 0.001 (0.001) Batch 3.370 (3.454) Remain 06:53:21 loss: 0.0757 Lr: 0.02379
[2023-08-08 03:20:40,489 INFO misc.py line 115 22900] Train: [54/100][153/156] Data 0.001 (0.001) Batch 2.103 (3.445) Remain 06:52:12 loss: 0.1473 Lr: 0.02378
[2023-08-08 03:20:44,119 INFO misc.py line 115 22900] Train: [54/100][154/156] Data 0.001 (0.001) Batch 3.630 (3.446) Remain 06:52:18 loss: 0.1257 Lr: 0.02378
[2023-08-08 03:20:47,378 INFO misc.py line 115 22900] Train: [54/100][155/156] Data 0.001 (0.001) Batch 3.259 (3.445) Remain 06:52:05 loss: 0.1080 Lr: 0.02377
[2023-08-08 03:20:51,847 INFO misc.py line 115 22900] Train: [54/100][156/156] Data 0.001 (0.001) Batch 4.468 (3.452) Remain 06:52:50 loss: 0.5715 Lr: 0.02377
[2023-08-08 03:20:51,847 INFO misc.py line 129 22900] Train result: loss: 0.2319 
[2023-08-08 03:20:51,847 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 03:20:53,953 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.1687 
[2023-08-08 03:20:54,825 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.4881 
[2023-08-08 03:20:56,491 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.8217 
[2023-08-08 03:20:58,012 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.3111 
[2023-08-08 03:20:59,856 INFO evaluator.py line 122 22900] Test: [5/24] Loss 2.0399 
[2023-08-08 03:21:01,520 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.4608 
[2023-08-08 03:21:03,658 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.9440 
[2023-08-08 03:21:05,463 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.1758 
[2023-08-08 03:21:06,747 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.5027 
[2023-08-08 03:21:08,876 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.2449 
[2023-08-08 03:21:09,402 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.2405 
[2023-08-08 03:21:10,934 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.6948 
[2023-08-08 03:21:13,644 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.1008 
[2023-08-08 03:21:15,323 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.9093 
[2023-08-08 03:21:17,344 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.4118 
[2023-08-08 03:21:20,056 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.1293 
[2023-08-08 03:21:22,762 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.3187 
[2023-08-08 03:21:24,607 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.7762 
[2023-08-08 03:21:25,354 INFO evaluator.py line 122 22900] Test: [19/24] Loss 0.9849 
[2023-08-08 03:21:26,239 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.6822 
[2023-08-08 03:21:28,499 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3436 
[2023-08-08 03:21:30,465 INFO evaluator.py line 122 22900] Test: [22/24] Loss 2.0012 
[2023-08-08 03:21:32,310 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.8057 
[2023-08-08 03:21:34,245 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.7691 
[2023-08-08 03:21:34,293 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2306/0.3327/0.6925.
[2023-08-08 03:21:34,293 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6650/0.9619
[2023-08-08 03:21:34,294 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9518/0.9927
[2023-08-08 03:21:34,294 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1936/0.5093
[2023-08-08 03:21:34,294 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1768/0.2956
[2023-08-08 03:21:34,294 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6114/0.6733
[2023-08-08 03:21:34,294 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.3009/0.4648
[2023-08-08 03:21:34,294 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.4907/0.7887
[2023-08-08 03:21:34,294 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1382/0.1420
[2023-08-08 03:21:34,294 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.2154/0.5484
[2023-08-08 03:21:34,294 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0302/0.0304
[2023-08-08 03:21:34,294 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 03:21:34,294 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0068/0.0070
[2023-08-08 03:21:34,294 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.0855/0.0998
[2023-08-08 03:21:34,294 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0245/0.0255
[2023-08-08 03:21:34,294 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0158/0.0172
[2023-08-08 03:21:34,294 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.1385/0.1394
[2023-08-08 03:21:34,294 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0995/0.1275
[2023-08-08 03:21:34,294 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.3721/0.6315
[2023-08-08 03:21:34,294 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 03:21:34,294 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0948/0.1994
[2023-08-08 03:21:34,294 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 03:21:34,294 INFO misc.py line 152 22900] Currently Best mIoU: 0.2562
[2023-08-08 03:21:34,294 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 03:21:40,012 INFO misc.py line 115 22900] Train: [55/100][1/156] Data 0.999 (0.999) Batch 4.950 (4.950) Remain 09:51:58 loss: 0.1165 Lr: 0.02376
[2023-08-08 03:21:43,457 INFO misc.py line 115 22900] Train: [55/100][2/156] Data 0.001 (0.001) Batch 3.445 (3.445) Remain 06:51:56 loss: 0.2664 Lr: 0.02376
[2023-08-08 03:21:46,639 INFO misc.py line 115 22900] Train: [55/100][3/156] Data 0.001 (0.001) Batch 3.182 (3.182) Remain 06:20:22 loss: 0.2139 Lr: 0.02375
[2023-08-08 03:21:49,497 INFO misc.py line 115 22900] Train: [55/100][4/156] Data 0.001 (0.001) Batch 2.858 (2.858) Remain 05:41:40 loss: 0.1333 Lr: 0.02374
[2023-08-08 03:21:52,283 INFO misc.py line 115 22900] Train: [55/100][5/156] Data 0.001 (0.001) Batch 2.786 (2.822) Remain 05:37:17 loss: 0.1837 Lr: 0.02374
[2023-08-08 03:21:56,012 INFO misc.py line 115 22900] Train: [55/100][6/156] Data 0.001 (0.001) Batch 3.729 (3.125) Remain 06:13:23 loss: 0.2521 Lr: 0.02373
[2023-08-08 03:21:59,844 INFO misc.py line 115 22900] Train: [55/100][7/156] Data 0.001 (0.001) Batch 3.832 (3.301) Remain 06:34:27 loss: 0.2926 Lr: 0.02373
[2023-08-08 03:22:02,978 INFO misc.py line 115 22900] Train: [55/100][8/156] Data 0.001 (0.001) Batch 3.134 (3.268) Remain 06:30:23 loss: 0.2566 Lr: 0.02372
[2023-08-08 03:22:07,656 INFO misc.py line 115 22900] Train: [55/100][9/156] Data 0.001 (0.001) Batch 4.679 (3.503) Remain 06:58:25 loss: 0.2632 Lr: 0.02372
[2023-08-08 03:22:09,902 INFO misc.py line 115 22900] Train: [55/100][10/156] Data 0.001 (0.001) Batch 2.246 (3.323) Remain 06:36:55 loss: 0.1406 Lr: 0.02371
[2023-08-08 03:22:13,202 INFO misc.py line 115 22900] Train: [55/100][11/156] Data 0.001 (0.001) Batch 3.300 (3.320) Remain 06:36:30 loss: 0.1539 Lr: 0.02371
[2023-08-08 03:22:16,513 INFO misc.py line 115 22900] Train: [55/100][12/156] Data 0.001 (0.001) Batch 3.312 (3.319) Remain 06:36:20 loss: 0.2765 Lr: 0.02370
[2023-08-08 03:22:20,566 INFO misc.py line 115 22900] Train: [55/100][13/156] Data 0.001 (0.001) Batch 4.052 (3.393) Remain 06:45:01 loss: 0.2973 Lr: 0.02370
[2023-08-08 03:22:23,323 INFO misc.py line 115 22900] Train: [55/100][14/156] Data 0.001 (0.001) Batch 2.757 (3.335) Remain 06:38:04 loss: 0.2427 Lr: 0.02369
[2023-08-08 03:22:26,153 INFO misc.py line 115 22900] Train: [55/100][15/156] Data 0.001 (0.001) Batch 2.830 (3.293) Remain 06:33:00 loss: 0.2380 Lr: 0.02369
[2023-08-08 03:22:28,308 INFO misc.py line 115 22900] Train: [55/100][16/156] Data 0.001 (0.001) Batch 2.155 (3.205) Remain 06:22:29 loss: 0.1988 Lr: 0.02368
[2023-08-08 03:22:31,533 INFO misc.py line 115 22900] Train: [55/100][17/156] Data 0.001 (0.001) Batch 3.226 (3.207) Remain 06:22:37 loss: 0.2480 Lr: 0.02368
[2023-08-08 03:22:34,246 INFO misc.py line 115 22900] Train: [55/100][18/156] Data 0.001 (0.001) Batch 2.713 (3.174) Remain 06:18:38 loss: 0.2031 Lr: 0.02367
[2023-08-08 03:22:37,271 INFO misc.py line 115 22900] Train: [55/100][19/156] Data 0.001 (0.001) Batch 3.025 (3.165) Remain 06:17:28 loss: 0.1601 Lr: 0.02367
[2023-08-08 03:22:40,871 INFO misc.py line 115 22900] Train: [55/100][20/156] Data 0.001 (0.001) Batch 3.600 (3.190) Remain 06:20:28 loss: 0.3827 Lr: 0.02366
[2023-08-08 03:22:45,430 INFO misc.py line 115 22900] Train: [55/100][21/156] Data 0.001 (0.001) Batch 4.559 (3.266) Remain 06:29:29 loss: 0.2814 Lr: 0.02365
[2023-08-08 03:22:47,234 INFO misc.py line 115 22900] Train: [55/100][22/156] Data 0.001 (0.001) Batch 1.804 (3.189) Remain 06:20:15 loss: 0.0763 Lr: 0.02365
[2023-08-08 03:22:50,470 INFO misc.py line 115 22900] Train: [55/100][23/156] Data 0.001 (0.001) Batch 3.236 (3.192) Remain 06:20:29 loss: 0.1879 Lr: 0.02364
[2023-08-08 03:22:52,663 INFO misc.py line 115 22900] Train: [55/100][24/156] Data 0.001 (0.001) Batch 2.193 (3.144) Remain 06:14:46 loss: 0.2399 Lr: 0.02364
[2023-08-08 03:22:56,096 INFO misc.py line 115 22900] Train: [55/100][25/156] Data 0.001 (0.001) Batch 3.433 (3.157) Remain 06:16:16 loss: 0.3402 Lr: 0.02363
[2023-08-08 03:22:59,707 INFO misc.py line 115 22900] Train: [55/100][26/156] Data 0.001 (0.001) Batch 3.611 (3.177) Remain 06:18:34 loss: 0.2018 Lr: 0.02363
[2023-08-08 03:23:02,316 INFO misc.py line 115 22900] Train: [55/100][27/156] Data 0.001 (0.001) Batch 2.608 (3.153) Remain 06:15:42 loss: 0.3340 Lr: 0.02362
[2023-08-08 03:23:05,318 INFO misc.py line 115 22900] Train: [55/100][28/156] Data 0.001 (0.001) Batch 3.002 (3.147) Remain 06:14:56 loss: 0.3261 Lr: 0.02362
[2023-08-08 03:23:09,083 INFO misc.py line 115 22900] Train: [55/100][29/156] Data 0.001 (0.001) Batch 3.765 (3.171) Remain 06:17:42 loss: 0.2316 Lr: 0.02361
[2023-08-08 03:23:12,336 INFO misc.py line 115 22900] Train: [55/100][30/156] Data 0.001 (0.001) Batch 3.253 (3.174) Remain 06:18:01 loss: 0.0976 Lr: 0.02361
[2023-08-08 03:23:16,439 INFO misc.py line 115 22900] Train: [55/100][31/156] Data 0.001 (0.001) Batch 4.103 (3.207) Remain 06:21:55 loss: 0.2452 Lr: 0.02360
[2023-08-08 03:23:20,360 INFO misc.py line 115 22900] Train: [55/100][32/156] Data 0.001 (0.001) Batch 3.921 (3.232) Remain 06:24:47 loss: 0.3040 Lr: 0.02360
[2023-08-08 03:23:23,334 INFO misc.py line 115 22900] Train: [55/100][33/156] Data 0.001 (0.001) Batch 2.975 (3.223) Remain 06:23:43 loss: 0.1646 Lr: 0.02359
[2023-08-08 03:23:26,728 INFO misc.py line 115 22900] Train: [55/100][34/156] Data 0.001 (0.001) Batch 3.393 (3.229) Remain 06:24:19 loss: 0.3098 Lr: 0.02359
[2023-08-08 03:23:29,729 INFO misc.py line 115 22900] Train: [55/100][35/156] Data 0.001 (0.001) Batch 3.001 (3.222) Remain 06:23:25 loss: 0.1261 Lr: 0.02358
[2023-08-08 03:23:33,796 INFO misc.py line 115 22900] Train: [55/100][36/156] Data 0.001 (0.001) Batch 4.067 (3.247) Remain 06:26:24 loss: 0.1827 Lr: 0.02358
[2023-08-08 03:23:36,808 INFO misc.py line 115 22900] Train: [55/100][37/156] Data 0.001 (0.001) Batch 3.013 (3.240) Remain 06:25:32 loss: 0.1655 Lr: 0.02357
[2023-08-08 03:23:40,037 INFO misc.py line 115 22900] Train: [55/100][38/156] Data 0.001 (0.001) Batch 3.228 (3.240) Remain 06:25:26 loss: 0.2316 Lr: 0.02356
[2023-08-08 03:23:43,330 INFO misc.py line 115 22900] Train: [55/100][39/156] Data 0.001 (0.001) Batch 3.293 (3.241) Remain 06:25:34 loss: 0.2500 Lr: 0.02356
[2023-08-08 03:23:46,700 INFO misc.py line 115 22900] Train: [55/100][40/156] Data 0.001 (0.001) Batch 3.369 (3.245) Remain 06:25:55 loss: 0.3503 Lr: 0.02355
[2023-08-08 03:23:50,727 INFO misc.py line 115 22900] Train: [55/100][41/156] Data 0.001 (0.001) Batch 4.028 (3.265) Remain 06:28:19 loss: 0.4195 Lr: 0.02355
[2023-08-08 03:23:54,716 INFO misc.py line 115 22900] Train: [55/100][42/156] Data 0.001 (0.001) Batch 3.988 (3.284) Remain 06:30:28 loss: 0.3446 Lr: 0.02354
[2023-08-08 03:23:58,413 INFO misc.py line 115 22900] Train: [55/100][43/156] Data 0.001 (0.001) Batch 3.697 (3.294) Remain 06:31:38 loss: 0.2787 Lr: 0.02354
[2023-08-08 03:24:02,052 INFO misc.py line 115 22900] Train: [55/100][44/156] Data 0.001 (0.001) Batch 3.639 (3.303) Remain 06:32:35 loss: 0.2599 Lr: 0.02353
[2023-08-08 03:24:05,395 INFO misc.py line 115 22900] Train: [55/100][45/156] Data 0.001 (0.001) Batch 3.343 (3.304) Remain 06:32:38 loss: 0.1853 Lr: 0.02353
[2023-08-08 03:24:10,259 INFO misc.py line 115 22900] Train: [55/100][46/156] Data 0.001 (0.001) Batch 4.864 (3.340) Remain 06:36:54 loss: 0.6592 Lr: 0.02352
[2023-08-08 03:24:14,374 INFO misc.py line 115 22900] Train: [55/100][47/156] Data 0.001 (0.001) Batch 4.116 (3.358) Remain 06:38:56 loss: 0.2891 Lr: 0.02352
[2023-08-08 03:24:18,485 INFO misc.py line 115 22900] Train: [55/100][48/156] Data 0.001 (0.001) Batch 4.111 (3.374) Remain 06:40:52 loss: 0.2585 Lr: 0.02351
[2023-08-08 03:24:21,684 INFO misc.py line 115 22900] Train: [55/100][49/156] Data 0.001 (0.001) Batch 3.199 (3.371) Remain 06:40:21 loss: 0.2888 Lr: 0.02351
[2023-08-08 03:24:24,777 INFO misc.py line 115 22900] Train: [55/100][50/156] Data 0.001 (0.001) Batch 3.093 (3.365) Remain 06:39:36 loss: 0.1788 Lr: 0.02350
[2023-08-08 03:24:28,486 INFO misc.py line 115 22900] Train: [55/100][51/156] Data 0.001 (0.001) Batch 3.709 (3.372) Remain 06:40:24 loss: 0.1722 Lr: 0.02350
[2023-08-08 03:24:31,616 INFO misc.py line 115 22900] Train: [55/100][52/156] Data 0.001 (0.001) Batch 3.130 (3.367) Remain 06:39:45 loss: 0.1338 Lr: 0.02349
[2023-08-08 03:24:35,632 INFO misc.py line 115 22900] Train: [55/100][53/156] Data 0.001 (0.001) Batch 4.016 (3.380) Remain 06:41:14 loss: 0.3520 Lr: 0.02349
[2023-08-08 03:24:38,902 INFO misc.py line 115 22900] Train: [55/100][54/156] Data 0.001 (0.001) Batch 3.270 (3.378) Remain 06:40:56 loss: 0.2699 Lr: 0.02348
[2023-08-08 03:24:42,958 INFO misc.py line 115 22900] Train: [55/100][55/156] Data 0.001 (0.001) Batch 4.056 (3.391) Remain 06:42:25 loss: 0.3272 Lr: 0.02347
[2023-08-08 03:24:47,419 INFO misc.py line 115 22900] Train: [55/100][56/156] Data 0.001 (0.001) Batch 4.461 (3.411) Remain 06:44:45 loss: 0.5754 Lr: 0.02347
[2023-08-08 03:24:50,314 INFO misc.py line 115 22900] Train: [55/100][57/156] Data 0.001 (0.001) Batch 2.895 (3.401) Remain 06:43:34 loss: 0.1485 Lr: 0.02346
[2023-08-08 03:24:53,100 INFO misc.py line 115 22900] Train: [55/100][58/156] Data 0.001 (0.001) Batch 2.786 (3.390) Remain 06:42:11 loss: 0.1459 Lr: 0.02346
[2023-08-08 03:24:57,119 INFO misc.py line 115 22900] Train: [55/100][59/156] Data 0.001 (0.001) Batch 4.019 (3.401) Remain 06:43:28 loss: 0.2035 Lr: 0.02345
[2023-08-08 03:25:00,648 INFO misc.py line 115 22900] Train: [55/100][60/156] Data 0.001 (0.001) Batch 3.529 (3.404) Remain 06:43:40 loss: 0.2364 Lr: 0.02345
[2023-08-08 03:25:04,725 INFO misc.py line 115 22900] Train: [55/100][61/156] Data 0.001 (0.001) Batch 4.076 (3.415) Remain 06:44:59 loss: 0.2958 Lr: 0.02344
[2023-08-08 03:25:07,991 INFO misc.py line 115 22900] Train: [55/100][62/156] Data 0.001 (0.001) Batch 3.266 (3.413) Remain 06:44:38 loss: 0.3544 Lr: 0.02344
[2023-08-08 03:25:11,171 INFO misc.py line 115 22900] Train: [55/100][63/156] Data 0.001 (0.001) Batch 3.180 (3.409) Remain 06:44:07 loss: 0.1809 Lr: 0.02343
[2023-08-08 03:25:14,586 INFO misc.py line 115 22900] Train: [55/100][64/156] Data 0.001 (0.001) Batch 3.414 (3.409) Remain 06:44:04 loss: 0.1727 Lr: 0.02343
[2023-08-08 03:25:18,666 INFO misc.py line 115 22900] Train: [55/100][65/156] Data 0.001 (0.001) Batch 4.080 (3.420) Remain 06:45:18 loss: 0.2432 Lr: 0.02342
[2023-08-08 03:25:22,446 INFO misc.py line 115 22900] Train: [55/100][66/156] Data 0.001 (0.001) Batch 3.780 (3.426) Remain 06:45:55 loss: 0.2417 Lr: 0.02342
[2023-08-08 03:25:24,975 INFO misc.py line 115 22900] Train: [55/100][67/156] Data 0.001 (0.001) Batch 2.528 (3.411) Remain 06:44:12 loss: 0.1491 Lr: 0.02341
[2023-08-08 03:25:27,658 INFO misc.py line 115 22900] Train: [55/100][68/156] Data 0.001 (0.001) Batch 2.683 (3.400) Remain 06:42:49 loss: 0.1280 Lr: 0.02341
[2023-08-08 03:25:32,051 INFO misc.py line 115 22900] Train: [55/100][69/156] Data 0.001 (0.001) Batch 4.393 (3.415) Remain 06:44:32 loss: 0.2117 Lr: 0.02340
[2023-08-08 03:25:36,134 INFO misc.py line 115 22900] Train: [55/100][70/156] Data 0.001 (0.001) Batch 4.083 (3.425) Remain 06:45:40 loss: 0.3195 Lr: 0.02340
[2023-08-08 03:25:39,138 INFO misc.py line 115 22900] Train: [55/100][71/156] Data 0.001 (0.001) Batch 3.004 (3.419) Remain 06:44:52 loss: 0.1276 Lr: 0.02339
[2023-08-08 03:25:42,867 INFO misc.py line 115 22900] Train: [55/100][72/156] Data 0.001 (0.001) Batch 3.729 (3.424) Remain 06:45:21 loss: 0.2161 Lr: 0.02339
[2023-08-08 03:25:44,712 INFO misc.py line 115 22900] Train: [55/100][73/156] Data 0.001 (0.001) Batch 1.845 (3.401) Remain 06:42:37 loss: 0.0690 Lr: 0.02338
[2023-08-08 03:25:47,707 INFO misc.py line 115 22900] Train: [55/100][74/156] Data 0.001 (0.001) Batch 2.995 (3.395) Remain 06:41:53 loss: 0.1707 Lr: 0.02337
[2023-08-08 03:25:52,347 INFO misc.py line 115 22900] Train: [55/100][75/156] Data 0.001 (0.001) Batch 4.640 (3.413) Remain 06:43:52 loss: 0.5098 Lr: 0.02337
[2023-08-08 03:25:55,839 INFO misc.py line 115 22900] Train: [55/100][76/156] Data 0.001 (0.001) Batch 3.491 (3.414) Remain 06:43:57 loss: 0.1629 Lr: 0.02336
[2023-08-08 03:25:59,793 INFO misc.py line 115 22900] Train: [55/100][77/156] Data 0.001 (0.001) Batch 3.954 (3.421) Remain 06:44:45 loss: 0.2812 Lr: 0.02336
[2023-08-08 03:26:02,463 INFO misc.py line 115 22900] Train: [55/100][78/156] Data 0.001 (0.001) Batch 2.670 (3.411) Remain 06:43:31 loss: 0.2183 Lr: 0.02335
[2023-08-08 03:26:06,133 INFO misc.py line 115 22900] Train: [55/100][79/156] Data 0.001 (0.001) Batch 3.671 (3.414) Remain 06:43:52 loss: 0.2717 Lr: 0.02335
[2023-08-08 03:26:09,557 INFO misc.py line 115 22900] Train: [55/100][80/156] Data 0.001 (0.001) Batch 3.424 (3.415) Remain 06:43:49 loss: 0.2133 Lr: 0.02334
[2023-08-08 03:26:12,136 INFO misc.py line 115 22900] Train: [55/100][81/156] Data 0.001 (0.001) Batch 2.579 (3.404) Remain 06:42:30 loss: 0.2201 Lr: 0.02334
[2023-08-08 03:26:15,263 INFO misc.py line 115 22900] Train: [55/100][82/156] Data 0.001 (0.001) Batch 3.127 (3.400) Remain 06:42:01 loss: 0.1898 Lr: 0.02333
[2023-08-08 03:26:19,263 INFO misc.py line 115 22900] Train: [55/100][83/156] Data 0.001 (0.001) Batch 4.000 (3.408) Remain 06:42:51 loss: 0.2797 Lr: 0.02333
[2023-08-08 03:26:22,696 INFO misc.py line 115 22900] Train: [55/100][84/156] Data 0.001 (0.001) Batch 3.432 (3.408) Remain 06:42:50 loss: 0.1818 Lr: 0.02332
[2023-08-08 03:26:25,372 INFO misc.py line 115 22900] Train: [55/100][85/156] Data 0.001 (0.001) Batch 2.676 (3.399) Remain 06:41:43 loss: 0.1881 Lr: 0.02332
[2023-08-08 03:26:27,812 INFO misc.py line 115 22900] Train: [55/100][86/156] Data 0.001 (0.001) Batch 2.440 (3.388) Remain 06:40:18 loss: 0.1407 Lr: 0.02331
[2023-08-08 03:26:31,902 INFO misc.py line 115 22900] Train: [55/100][87/156] Data 0.001 (0.001) Batch 4.090 (3.396) Remain 06:41:14 loss: 0.1521 Lr: 0.02331
[2023-08-08 03:26:34,990 INFO misc.py line 115 22900] Train: [55/100][88/156] Data 0.001 (0.001) Batch 3.087 (3.392) Remain 06:40:45 loss: 0.1329 Lr: 0.02330
[2023-08-08 03:26:39,129 INFO misc.py line 115 22900] Train: [55/100][89/156] Data 0.001 (0.001) Batch 4.139 (3.401) Remain 06:41:43 loss: 0.3079 Lr: 0.02330
[2023-08-08 03:26:42,711 INFO misc.py line 115 22900] Train: [55/100][90/156] Data 0.001 (0.001) Batch 3.582 (3.403) Remain 06:41:54 loss: 0.1886 Lr: 0.02329
[2023-08-08 03:26:47,035 INFO misc.py line 115 22900] Train: [55/100][91/156] Data 0.001 (0.001) Batch 4.323 (3.414) Remain 06:43:05 loss: 0.2902 Lr: 0.02328
[2023-08-08 03:26:50,892 INFO misc.py line 115 22900] Train: [55/100][92/156] Data 0.001 (0.001) Batch 3.857 (3.419) Remain 06:43:37 loss: 0.1295 Lr: 0.02328
[2023-08-08 03:26:54,095 INFO misc.py line 115 22900] Train: [55/100][93/156] Data 0.001 (0.001) Batch 3.204 (3.416) Remain 06:43:16 loss: 0.2371 Lr: 0.02327
[2023-08-08 03:26:56,447 INFO misc.py line 115 22900] Train: [55/100][94/156] Data 0.001 (0.001) Batch 2.351 (3.404) Remain 06:41:50 loss: 0.1869 Lr: 0.02327
[2023-08-08 03:27:00,164 INFO misc.py line 115 22900] Train: [55/100][95/156] Data 0.001 (0.001) Batch 3.717 (3.408) Remain 06:42:11 loss: 0.2012 Lr: 0.02326
[2023-08-08 03:27:03,016 INFO misc.py line 115 22900] Train: [55/100][96/156] Data 0.001 (0.001) Batch 2.852 (3.402) Remain 06:41:25 loss: 0.2433 Lr: 0.02326
[2023-08-08 03:27:06,638 INFO misc.py line 115 22900] Train: [55/100][97/156] Data 0.001 (0.001) Batch 3.622 (3.404) Remain 06:41:38 loss: 0.2361 Lr: 0.02325
[2023-08-08 03:27:09,944 INFO misc.py line 115 22900] Train: [55/100][98/156] Data 0.001 (0.001) Batch 3.306 (3.403) Remain 06:41:27 loss: 0.1774 Lr: 0.02325
[2023-08-08 03:27:13,698 INFO misc.py line 115 22900] Train: [55/100][99/156] Data 0.001 (0.001) Batch 3.754 (3.407) Remain 06:41:50 loss: 0.2499 Lr: 0.02324
[2023-08-08 03:27:17,914 INFO misc.py line 115 22900] Train: [55/100][100/156] Data 0.001 (0.001) Batch 4.216 (3.415) Remain 06:42:46 loss: 0.1980 Lr: 0.02324
[2023-08-08 03:27:21,356 INFO misc.py line 115 22900] Train: [55/100][101/156] Data 0.001 (0.001) Batch 3.442 (3.415) Remain 06:42:44 loss: 0.2077 Lr: 0.02323
[2023-08-08 03:27:25,126 INFO misc.py line 115 22900] Train: [55/100][102/156] Data 0.001 (0.001) Batch 3.770 (3.419) Remain 06:43:06 loss: 0.2409 Lr: 0.02323
[2023-08-08 03:27:28,147 INFO misc.py line 115 22900] Train: [55/100][103/156] Data 0.001 (0.001) Batch 3.021 (3.415) Remain 06:42:34 loss: 0.1449 Lr: 0.02322
[2023-08-08 03:27:30,682 INFO misc.py line 115 22900] Train: [55/100][104/156] Data 0.001 (0.001) Batch 2.535 (3.406) Remain 06:41:29 loss: 0.2045 Lr: 0.02322
[2023-08-08 03:27:34,128 INFO misc.py line 115 22900] Train: [55/100][105/156] Data 0.001 (0.001) Batch 3.446 (3.407) Remain 06:41:29 loss: 0.3046 Lr: 0.02321
[2023-08-08 03:27:37,688 INFO misc.py line 115 22900] Train: [55/100][106/156] Data 0.001 (0.001) Batch 3.560 (3.408) Remain 06:41:36 loss: 0.1586 Lr: 0.02321
[2023-08-08 03:27:41,236 INFO misc.py line 115 22900] Train: [55/100][107/156] Data 0.001 (0.001) Batch 3.548 (3.410) Remain 06:41:42 loss: 0.1689 Lr: 0.02320
[2023-08-08 03:27:45,422 INFO misc.py line 115 22900] Train: [55/100][108/156] Data 0.001 (0.001) Batch 4.186 (3.417) Remain 06:42:31 loss: 0.1781 Lr: 0.02319
[2023-08-08 03:27:48,695 INFO misc.py line 115 22900] Train: [55/100][109/156] Data 0.001 (0.001) Batch 3.273 (3.416) Remain 06:42:18 loss: 0.1351 Lr: 0.02319
[2023-08-08 03:27:51,861 INFO misc.py line 115 22900] Train: [55/100][110/156] Data 0.001 (0.001) Batch 3.167 (3.413) Remain 06:41:58 loss: 0.3151 Lr: 0.02318
[2023-08-08 03:27:55,346 INFO misc.py line 115 22900] Train: [55/100][111/156] Data 0.001 (0.001) Batch 3.485 (3.414) Remain 06:41:59 loss: 0.2534 Lr: 0.02318
[2023-08-08 03:27:59,887 INFO misc.py line 115 22900] Train: [55/100][112/156] Data 0.001 (0.001) Batch 4.541 (3.424) Remain 06:43:09 loss: 0.1769 Lr: 0.02317
[2023-08-08 03:28:03,965 INFO misc.py line 115 22900] Train: [55/100][113/156] Data 0.001 (0.001) Batch 4.077 (3.430) Remain 06:43:47 loss: 0.3765 Lr: 0.02317
[2023-08-08 03:28:07,428 INFO misc.py line 115 22900] Train: [55/100][114/156] Data 0.001 (0.001) Batch 3.463 (3.431) Remain 06:43:46 loss: 0.2616 Lr: 0.02316
[2023-08-08 03:28:11,253 INFO misc.py line 115 22900] Train: [55/100][115/156] Data 0.001 (0.001) Batch 3.825 (3.434) Remain 06:44:07 loss: 0.2435 Lr: 0.02316
[2023-08-08 03:28:14,744 INFO misc.py line 115 22900] Train: [55/100][116/156] Data 0.001 (0.001) Batch 3.491 (3.435) Remain 06:44:07 loss: 0.1806 Lr: 0.02315
[2023-08-08 03:28:18,646 INFO misc.py line 115 22900] Train: [55/100][117/156] Data 0.001 (0.001) Batch 3.902 (3.439) Remain 06:44:33 loss: 0.4493 Lr: 0.02315
[2023-08-08 03:28:21,837 INFO misc.py line 115 22900] Train: [55/100][118/156] Data 0.001 (0.001) Batch 3.191 (3.437) Remain 06:44:14 loss: 0.2044 Lr: 0.02314
[2023-08-08 03:28:24,762 INFO misc.py line 115 22900] Train: [55/100][119/156] Data 0.001 (0.001) Batch 2.924 (3.432) Remain 06:43:40 loss: 0.1570 Lr: 0.02314
[2023-08-08 03:28:28,217 INFO misc.py line 115 22900] Train: [55/100][120/156] Data 0.001 (0.001) Batch 3.456 (3.432) Remain 06:43:38 loss: 0.2043 Lr: 0.02313
[2023-08-08 03:28:32,193 INFO misc.py line 115 22900] Train: [55/100][121/156] Data 0.001 (0.001) Batch 3.975 (3.437) Remain 06:44:07 loss: 0.1630 Lr: 0.02313
[2023-08-08 03:28:35,538 INFO misc.py line 115 22900] Train: [55/100][122/156] Data 0.001 (0.001) Batch 3.345 (3.436) Remain 06:43:58 loss: 0.2650 Lr: 0.02312
[2023-08-08 03:28:39,494 INFO misc.py line 115 22900] Train: [55/100][123/156] Data 0.001 (0.001) Batch 3.957 (3.440) Remain 06:44:25 loss: 0.2291 Lr: 0.02312
[2023-08-08 03:28:43,547 INFO misc.py line 115 22900] Train: [55/100][124/156] Data 0.001 (0.001) Batch 4.053 (3.446) Remain 06:44:57 loss: 0.2804 Lr: 0.02311
[2023-08-08 03:28:46,767 INFO misc.py line 115 22900] Train: [55/100][125/156] Data 0.001 (0.001) Batch 3.221 (3.444) Remain 06:44:41 loss: 0.2829 Lr: 0.02310
[2023-08-08 03:28:49,950 INFO misc.py line 115 22900] Train: [55/100][126/156] Data 0.001 (0.001) Batch 3.183 (3.442) Remain 06:44:22 loss: 0.2682 Lr: 0.02310
[2023-08-08 03:28:53,666 INFO misc.py line 115 22900] Train: [55/100][127/156] Data 0.001 (0.001) Batch 3.715 (3.444) Remain 06:44:35 loss: 0.3260 Lr: 0.02309
[2023-08-08 03:28:56,982 INFO misc.py line 115 22900] Train: [55/100][128/156] Data 0.001 (0.001) Batch 3.317 (3.443) Remain 06:44:24 loss: 0.1622 Lr: 0.02309
[2023-08-08 03:29:01,011 INFO misc.py line 115 22900] Train: [55/100][129/156] Data 0.001 (0.001) Batch 4.028 (3.447) Remain 06:44:53 loss: 0.2040 Lr: 0.02308
[2023-08-08 03:29:04,339 INFO misc.py line 115 22900] Train: [55/100][130/156] Data 0.001 (0.001) Batch 3.328 (3.446) Remain 06:44:43 loss: 0.1656 Lr: 0.02308
[2023-08-08 03:29:07,481 INFO misc.py line 115 22900] Train: [55/100][131/156] Data 0.001 (0.001) Batch 3.142 (3.444) Remain 06:44:23 loss: 0.1807 Lr: 0.02307
[2023-08-08 03:29:11,093 INFO misc.py line 115 22900] Train: [55/100][132/156] Data 0.001 (0.001) Batch 3.612 (3.445) Remain 06:44:29 loss: 0.2088 Lr: 0.02307
[2023-08-08 03:29:14,272 INFO misc.py line 115 22900] Train: [55/100][133/156] Data 0.001 (0.001) Batch 3.179 (3.443) Remain 06:44:11 loss: 0.2658 Lr: 0.02306
[2023-08-08 03:29:17,463 INFO misc.py line 115 22900] Train: [55/100][134/156] Data 0.001 (0.001) Batch 3.191 (3.441) Remain 06:43:54 loss: 0.1724 Lr: 0.02306
[2023-08-08 03:29:20,104 INFO misc.py line 115 22900] Train: [55/100][135/156] Data 0.001 (0.001) Batch 2.641 (3.435) Remain 06:43:08 loss: 0.1374 Lr: 0.02305
[2023-08-08 03:29:23,456 INFO misc.py line 115 22900] Train: [55/100][136/156] Data 0.001 (0.001) Batch 3.352 (3.435) Remain 06:43:00 loss: 0.4050 Lr: 0.02305
[2023-08-08 03:29:27,890 INFO misc.py line 115 22900] Train: [55/100][137/156] Data 0.001 (0.001) Batch 4.434 (3.442) Remain 06:43:49 loss: 0.3851 Lr: 0.02304
[2023-08-08 03:29:31,590 INFO misc.py line 115 22900] Train: [55/100][138/156] Data 0.001 (0.001) Batch 3.700 (3.444) Remain 06:43:59 loss: 0.2112 Lr: 0.02304
[2023-08-08 03:29:34,873 INFO misc.py line 115 22900] Train: [55/100][139/156] Data 0.001 (0.001) Batch 3.283 (3.443) Remain 06:43:47 loss: 0.2146 Lr: 0.02303
[2023-08-08 03:29:37,084 INFO misc.py line 115 22900] Train: [55/100][140/156] Data 0.001 (0.001) Batch 2.211 (3.434) Remain 06:42:40 loss: 0.0343 Lr: 0.02303
[2023-08-08 03:29:41,189 INFO misc.py line 115 22900] Train: [55/100][141/156] Data 0.001 (0.001) Batch 4.105 (3.439) Remain 06:43:11 loss: 0.2453 Lr: 0.02302
[2023-08-08 03:29:44,543 INFO misc.py line 115 22900] Train: [55/100][142/156] Data 0.001 (0.001) Batch 3.354 (3.438) Remain 06:43:04 loss: 0.3934 Lr: 0.02302
[2023-08-08 03:29:48,524 INFO misc.py line 115 22900] Train: [55/100][143/156] Data 0.001 (0.001) Batch 3.981 (3.442) Remain 06:43:27 loss: 0.2139 Lr: 0.02301
[2023-08-08 03:29:51,783 INFO misc.py line 115 22900] Train: [55/100][144/156] Data 0.001 (0.001) Batch 3.258 (3.441) Remain 06:43:15 loss: 0.2327 Lr: 0.02300
[2023-08-08 03:29:55,648 INFO misc.py line 115 22900] Train: [55/100][145/156] Data 0.001 (0.001) Batch 3.865 (3.444) Remain 06:43:32 loss: 0.2604 Lr: 0.02300
[2023-08-08 03:29:58,528 INFO misc.py line 115 22900] Train: [55/100][146/156] Data 0.001 (0.001) Batch 2.880 (3.440) Remain 06:43:01 loss: 0.1802 Lr: 0.02299
[2023-08-08 03:30:02,564 INFO misc.py line 115 22900] Train: [55/100][147/156] Data 0.001 (0.001) Batch 4.036 (3.444) Remain 06:43:27 loss: 0.2993 Lr: 0.02299
[2023-08-08 03:30:06,308 INFO misc.py line 115 22900] Train: [55/100][148/156] Data 0.001 (0.001) Batch 3.744 (3.446) Remain 06:43:38 loss: 0.1134 Lr: 0.02298
[2023-08-08 03:30:09,418 INFO misc.py line 115 22900] Train: [55/100][149/156] Data 0.001 (0.001) Batch 3.110 (3.444) Remain 06:43:18 loss: 0.1068 Lr: 0.02298
[2023-08-08 03:30:12,116 INFO misc.py line 115 22900] Train: [55/100][150/156] Data 0.001 (0.001) Batch 2.698 (3.439) Remain 06:42:39 loss: 0.2329 Lr: 0.02297
[2023-08-08 03:30:16,586 INFO misc.py line 115 22900] Train: [55/100][151/156] Data 0.001 (0.001) Batch 4.470 (3.446) Remain 06:43:25 loss: 0.3070 Lr: 0.02297
[2023-08-08 03:30:20,760 INFO misc.py line 115 22900] Train: [55/100][152/156] Data 0.001 (0.001) Batch 4.174 (3.450) Remain 06:43:56 loss: 0.2367 Lr: 0.02296
[2023-08-08 03:30:23,820 INFO misc.py line 115 22900] Train: [55/100][153/156] Data 0.001 (0.001) Batch 3.060 (3.448) Remain 06:43:34 loss: 0.1453 Lr: 0.02296
[2023-08-08 03:30:27,829 INFO misc.py line 115 22900] Train: [55/100][154/156] Data 0.001 (0.001) Batch 4.009 (3.452) Remain 06:43:57 loss: 0.4964 Lr: 0.02295
[2023-08-08 03:30:32,113 INFO misc.py line 115 22900] Train: [55/100][155/156] Data 0.001 (0.001) Batch 4.283 (3.457) Remain 06:44:32 loss: 0.3649 Lr: 0.02295
[2023-08-08 03:30:35,737 INFO misc.py line 115 22900] Train: [55/100][156/156] Data 0.001 (0.001) Batch 3.625 (3.458) Remain 06:44:36 loss: 0.2594 Lr: 0.02294
[2023-08-08 03:30:35,737 INFO misc.py line 129 22900] Train result: loss: 0.2358 
[2023-08-08 03:30:35,738 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 03:30:37,835 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.1083 
[2023-08-08 03:30:38,704 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.5833 
[2023-08-08 03:30:40,368 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.9107 
[2023-08-08 03:30:41,890 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.2716 
[2023-08-08 03:30:43,734 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.6568 
[2023-08-08 03:30:45,397 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.5610 
[2023-08-08 03:30:47,533 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.7765 
[2023-08-08 03:30:49,336 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.0558 
[2023-08-08 03:30:50,621 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.3715 
[2023-08-08 03:30:52,749 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.4367 
[2023-08-08 03:30:53,275 INFO evaluator.py line 122 22900] Test: [11/24] Loss 0.7231 
[2023-08-08 03:30:54,807 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.8112 
[2023-08-08 03:30:57,517 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.0638 
[2023-08-08 03:30:59,198 INFO evaluator.py line 122 22900] Test: [14/24] Loss 1.1213 
[2023-08-08 03:31:01,219 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.3563 
[2023-08-08 03:31:03,928 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.0017 
[2023-08-08 03:31:06,634 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.3421 
[2023-08-08 03:31:08,481 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.3681 
[2023-08-08 03:31:09,231 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.0633 
[2023-08-08 03:31:10,117 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.8275 
[2023-08-08 03:31:12,378 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.2005 
[2023-08-08 03:31:14,343 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.5737 
[2023-08-08 03:31:16,192 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.5748 
[2023-08-08 03:31:18,128 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.8707 
[2023-08-08 03:31:18,175 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2320/0.3158/0.6932.
[2023-08-08 03:31:18,175 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6452/0.9642
[2023-08-08 03:31:18,175 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9498/0.9840
[2023-08-08 03:31:18,175 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1855/0.3355
[2023-08-08 03:31:18,175 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1387/0.2663
[2023-08-08 03:31:18,175 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6133/0.7269
[2023-08-08 03:31:18,175 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.3370/0.5885
[2023-08-08 03:31:18,175 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5152/0.7095
[2023-08-08 03:31:18,175 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1417/0.1541
[2023-08-08 03:31:18,175 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1392/0.2874
[2023-08-08 03:31:18,175 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0809/0.0815
[2023-08-08 03:31:18,175 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 03:31:18,175 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0176/0.0183
[2023-08-08 03:31:18,175 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1794/0.2231
[2023-08-08 03:31:18,175 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0378/0.0417
[2023-08-08 03:31:18,175 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0409/0.0551
[2023-08-08 03:31:18,175 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0076/0.0076
[2023-08-08 03:31:18,175 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0895/0.1098
[2023-08-08 03:31:18,175 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.4170/0.5309
[2023-08-08 03:31:18,175 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 03:31:18,175 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.1045/0.2306
[2023-08-08 03:31:18,175 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 03:31:18,175 INFO misc.py line 152 22900] Currently Best mIoU: 0.2562
[2023-08-08 03:31:18,175 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 03:31:24,636 INFO misc.py line 115 22900] Train: [56/100][1/156] Data 1.612 (1.612) Batch 5.642 (5.642) Remain 11:00:02 loss: 0.2009 Lr: 0.02294
[2023-08-08 03:31:28,791 INFO misc.py line 115 22900] Train: [56/100][2/156] Data 0.001 (0.001) Batch 4.155 (4.155) Remain 08:05:59 loss: 0.4339 Lr: 0.02293
[2023-08-08 03:31:31,430 INFO misc.py line 115 22900] Train: [56/100][3/156] Data 0.001 (0.001) Batch 2.639 (2.639) Remain 05:08:38 loss: 0.2315 Lr: 0.02293
[2023-08-08 03:31:35,310 INFO misc.py line 115 22900] Train: [56/100][4/156] Data 0.001 (0.001) Batch 3.881 (3.881) Remain 07:33:46 loss: 0.2134 Lr: 0.02292
[2023-08-08 03:31:40,334 INFO misc.py line 115 22900] Train: [56/100][5/156] Data 0.001 (0.001) Batch 5.024 (4.452) Remain 08:40:32 loss: 0.3415 Lr: 0.02291
[2023-08-08 03:31:44,083 INFO misc.py line 115 22900] Train: [56/100][6/156] Data 0.001 (0.001) Batch 3.749 (4.218) Remain 08:13:04 loss: 0.2106 Lr: 0.02291
[2023-08-08 03:31:48,114 INFO misc.py line 115 22900] Train: [56/100][7/156] Data 0.001 (0.001) Batch 4.030 (4.171) Remain 08:07:30 loss: 0.4092 Lr: 0.02290
[2023-08-08 03:31:52,081 INFO misc.py line 115 22900] Train: [56/100][8/156] Data 0.001 (0.001) Batch 3.967 (4.130) Remain 08:02:41 loss: 0.3265 Lr: 0.02290
[2023-08-08 03:31:54,735 INFO misc.py line 115 22900] Train: [56/100][9/156] Data 0.001 (0.001) Batch 2.653 (3.884) Remain 07:33:51 loss: 0.4204 Lr: 0.02289
[2023-08-08 03:31:59,106 INFO misc.py line 115 22900] Train: [56/100][10/156] Data 0.001 (0.001) Batch 4.371 (3.954) Remain 07:41:55 loss: 0.3031 Lr: 0.02289
[2023-08-08 03:32:02,802 INFO misc.py line 115 22900] Train: [56/100][11/156] Data 0.001 (0.001) Batch 3.696 (3.921) Remain 07:38:05 loss: 0.2126 Lr: 0.02288
[2023-08-08 03:32:06,322 INFO misc.py line 115 22900] Train: [56/100][12/156] Data 0.001 (0.001) Batch 3.520 (3.877) Remain 07:32:49 loss: 0.1849 Lr: 0.02288
[2023-08-08 03:32:09,333 INFO misc.py line 115 22900] Train: [56/100][13/156] Data 0.001 (0.001) Batch 3.011 (3.790) Remain 07:22:38 loss: 0.1242 Lr: 0.02287
[2023-08-08 03:32:13,072 INFO misc.py line 115 22900] Train: [56/100][14/156] Data 0.001 (0.001) Batch 3.739 (3.786) Remain 07:22:02 loss: 0.3420 Lr: 0.02287
[2023-08-08 03:32:16,419 INFO misc.py line 115 22900] Train: [56/100][15/156] Data 0.001 (0.001) Batch 3.348 (3.749) Remain 07:17:42 loss: 0.2267 Lr: 0.02286
[2023-08-08 03:32:19,789 INFO misc.py line 115 22900] Train: [56/100][16/156] Data 0.001 (0.001) Batch 3.369 (3.720) Remain 07:14:14 loss: 0.1822 Lr: 0.02286
[2023-08-08 03:32:22,927 INFO misc.py line 115 22900] Train: [56/100][17/156] Data 0.001 (0.001) Batch 3.138 (3.678) Remain 07:09:19 loss: 0.2245 Lr: 0.02285
[2023-08-08 03:32:26,278 INFO misc.py line 115 22900] Train: [56/100][18/156] Data 0.001 (0.001) Batch 3.351 (3.657) Remain 07:06:43 loss: 0.2145 Lr: 0.02285
[2023-08-08 03:32:28,693 INFO misc.py line 115 22900] Train: [56/100][19/156] Data 0.001 (0.001) Batch 2.415 (3.579) Remain 06:57:36 loss: 0.0934 Lr: 0.02284
[2023-08-08 03:32:31,485 INFO misc.py line 115 22900] Train: [56/100][20/156] Data 0.001 (0.001) Batch 2.793 (3.533) Remain 06:52:08 loss: 0.1604 Lr: 0.02284
[2023-08-08 03:32:34,794 INFO misc.py line 115 22900] Train: [56/100][21/156] Data 0.001 (0.001) Batch 3.309 (3.520) Remain 06:50:38 loss: 0.2650 Lr: 0.02283
[2023-08-08 03:32:38,461 INFO misc.py line 115 22900] Train: [56/100][22/156] Data 0.001 (0.001) Batch 3.667 (3.528) Remain 06:51:28 loss: 0.1156 Lr: 0.02282
[2023-08-08 03:32:42,191 INFO misc.py line 115 22900] Train: [56/100][23/156] Data 0.001 (0.001) Batch 3.730 (3.538) Remain 06:52:35 loss: 0.1295 Lr: 0.02282
[2023-08-08 03:32:45,479 INFO misc.py line 115 22900] Train: [56/100][24/156] Data 0.001 (0.001) Batch 3.288 (3.526) Remain 06:51:09 loss: 0.1757 Lr: 0.02281
[2023-08-08 03:32:48,909 INFO misc.py line 115 22900] Train: [56/100][25/156] Data 0.001 (0.001) Batch 3.429 (3.522) Remain 06:50:34 loss: 0.2096 Lr: 0.02281
[2023-08-08 03:32:51,774 INFO misc.py line 115 22900] Train: [56/100][26/156] Data 0.001 (0.001) Batch 2.865 (3.493) Remain 06:47:11 loss: 0.1207 Lr: 0.02280
[2023-08-08 03:32:55,820 INFO misc.py line 115 22900] Train: [56/100][27/156] Data 0.001 (0.001) Batch 4.046 (3.516) Remain 06:49:49 loss: 0.2742 Lr: 0.02280
[2023-08-08 03:32:59,926 INFO misc.py line 115 22900] Train: [56/100][28/156] Data 0.001 (0.001) Batch 4.106 (3.540) Remain 06:52:30 loss: 0.1278 Lr: 0.02279
[2023-08-08 03:33:03,223 INFO misc.py line 115 22900] Train: [56/100][29/156] Data 0.001 (0.001) Batch 3.297 (3.530) Remain 06:51:21 loss: 0.2069 Lr: 0.02279
[2023-08-08 03:33:06,752 INFO misc.py line 115 22900] Train: [56/100][30/156] Data 0.001 (0.001) Batch 3.529 (3.530) Remain 06:51:17 loss: 0.2425 Lr: 0.02278
[2023-08-08 03:33:10,746 INFO misc.py line 115 22900] Train: [56/100][31/156] Data 0.001 (0.001) Batch 3.994 (3.547) Remain 06:53:09 loss: 0.1055 Lr: 0.02278
[2023-08-08 03:33:13,970 INFO misc.py line 115 22900] Train: [56/100][32/156] Data 0.001 (0.001) Batch 3.224 (3.536) Remain 06:51:48 loss: 0.1165 Lr: 0.02277
[2023-08-08 03:33:17,577 INFO misc.py line 115 22900] Train: [56/100][33/156] Data 0.001 (0.001) Batch 3.606 (3.538) Remain 06:52:01 loss: 0.1783 Lr: 0.02277
[2023-08-08 03:33:21,780 INFO misc.py line 115 22900] Train: [56/100][34/156] Data 0.001 (0.001) Batch 4.204 (3.560) Remain 06:54:27 loss: 0.4081 Lr: 0.02276
[2023-08-08 03:33:25,052 INFO misc.py line 115 22900] Train: [56/100][35/156] Data 0.001 (0.001) Batch 3.272 (3.551) Remain 06:53:21 loss: 0.1132 Lr: 0.02276
[2023-08-08 03:33:28,273 INFO misc.py line 115 22900] Train: [56/100][36/156] Data 0.001 (0.001) Batch 3.220 (3.541) Remain 06:52:08 loss: 0.1981 Lr: 0.02275
[2023-08-08 03:33:32,416 INFO misc.py line 115 22900] Train: [56/100][37/156] Data 0.001 (0.001) Batch 4.143 (3.558) Remain 06:54:08 loss: 0.3734 Lr: 0.02275
[2023-08-08 03:33:35,646 INFO misc.py line 115 22900] Train: [56/100][38/156] Data 0.001 (0.001) Batch 3.230 (3.549) Remain 06:52:59 loss: 0.3045 Lr: 0.02274
[2023-08-08 03:33:38,521 INFO misc.py line 115 22900] Train: [56/100][39/156] Data 0.001 (0.001) Batch 2.875 (3.530) Remain 06:50:45 loss: 0.2093 Lr: 0.02274
[2023-08-08 03:33:42,007 INFO misc.py line 115 22900] Train: [56/100][40/156] Data 0.001 (0.001) Batch 3.486 (3.529) Remain 06:50:33 loss: 0.3443 Lr: 0.02273
[2023-08-08 03:33:46,044 INFO misc.py line 115 22900] Train: [56/100][41/156] Data 0.001 (0.001) Batch 4.037 (3.542) Remain 06:52:02 loss: 0.2280 Lr: 0.02272
[2023-08-08 03:33:49,652 INFO misc.py line 115 22900] Train: [56/100][42/156] Data 0.001 (0.001) Batch 3.608 (3.544) Remain 06:52:11 loss: 0.2344 Lr: 0.02272
[2023-08-08 03:33:52,263 INFO misc.py line 115 22900] Train: [56/100][43/156] Data 0.001 (0.001) Batch 2.611 (3.521) Remain 06:49:24 loss: 0.1422 Lr: 0.02271
[2023-08-08 03:33:55,767 INFO misc.py line 115 22900] Train: [56/100][44/156] Data 0.001 (0.001) Batch 3.504 (3.520) Remain 06:49:18 loss: 0.3891 Lr: 0.02271
[2023-08-08 03:33:59,614 INFO misc.py line 115 22900] Train: [56/100][45/156] Data 0.001 (0.001) Batch 3.848 (3.528) Remain 06:50:09 loss: 0.2678 Lr: 0.02270
[2023-08-08 03:34:01,634 INFO misc.py line 115 22900] Train: [56/100][46/156] Data 0.001 (0.001) Batch 2.019 (3.493) Remain 06:46:00 loss: 0.1448 Lr: 0.02270
[2023-08-08 03:34:04,943 INFO misc.py line 115 22900] Train: [56/100][47/156] Data 0.001 (0.001) Batch 3.309 (3.489) Remain 06:45:28 loss: 0.2440 Lr: 0.02269
[2023-08-08 03:34:08,362 INFO misc.py line 115 22900] Train: [56/100][48/156] Data 0.001 (0.001) Batch 3.419 (3.487) Remain 06:45:13 loss: 0.2445 Lr: 0.02269
[2023-08-08 03:34:11,785 INFO misc.py line 115 22900] Train: [56/100][49/156] Data 0.001 (0.001) Batch 3.423 (3.486) Remain 06:45:00 loss: 0.2840 Lr: 0.02268
[2023-08-08 03:34:15,386 INFO misc.py line 115 22900] Train: [56/100][50/156] Data 0.001 (0.001) Batch 3.601 (3.488) Remain 06:45:14 loss: 0.2223 Lr: 0.02268
[2023-08-08 03:34:18,684 INFO misc.py line 115 22900] Train: [56/100][51/156] Data 0.001 (0.001) Batch 3.297 (3.484) Remain 06:44:43 loss: 0.2333 Lr: 0.02267
[2023-08-08 03:34:21,214 INFO misc.py line 115 22900] Train: [56/100][52/156] Data 0.001 (0.001) Batch 2.531 (3.465) Remain 06:42:24 loss: 0.1982 Lr: 0.02267
[2023-08-08 03:34:25,220 INFO misc.py line 115 22900] Train: [56/100][53/156] Data 0.001 (0.001) Batch 4.005 (3.476) Remain 06:43:35 loss: 0.2712 Lr: 0.02266
[2023-08-08 03:34:29,233 INFO misc.py line 115 22900] Train: [56/100][54/156] Data 0.001 (0.001) Batch 4.014 (3.486) Remain 06:44:45 loss: 0.3387 Lr: 0.02266
[2023-08-08 03:34:32,401 INFO misc.py line 115 22900] Train: [56/100][55/156] Data 0.001 (0.001) Batch 3.168 (3.480) Remain 06:43:59 loss: 0.0988 Lr: 0.02265
[2023-08-08 03:34:35,266 INFO misc.py line 115 22900] Train: [56/100][56/156] Data 0.001 (0.001) Batch 2.865 (3.469) Remain 06:42:35 loss: 0.2022 Lr: 0.02265
[2023-08-08 03:34:39,257 INFO misc.py line 115 22900] Train: [56/100][57/156] Data 0.001 (0.001) Batch 3.991 (3.478) Remain 06:43:39 loss: 0.2729 Lr: 0.02264
[2023-08-08 03:34:41,225 INFO misc.py line 115 22900] Train: [56/100][58/156] Data 0.001 (0.001) Batch 1.968 (3.451) Remain 06:40:24 loss: 0.1263 Lr: 0.02263
[2023-08-08 03:34:43,509 INFO misc.py line 115 22900] Train: [56/100][59/156] Data 0.001 (0.001) Batch 2.283 (3.430) Remain 06:37:56 loss: 0.1438 Lr: 0.02263
[2023-08-08 03:34:47,445 INFO misc.py line 115 22900] Train: [56/100][60/156] Data 0.001 (0.001) Batch 3.937 (3.439) Remain 06:38:54 loss: 0.1936 Lr: 0.02262
[2023-08-08 03:34:51,465 INFO misc.py line 115 22900] Train: [56/100][61/156] Data 0.001 (0.001) Batch 4.020 (3.449) Remain 06:40:00 loss: 0.2213 Lr: 0.02262
[2023-08-08 03:34:55,209 INFO misc.py line 115 22900] Train: [56/100][62/156] Data 0.001 (0.001) Batch 3.744 (3.454) Remain 06:40:32 loss: 0.3060 Lr: 0.02261
[2023-08-08 03:34:59,335 INFO misc.py line 115 22900] Train: [56/100][63/156] Data 0.001 (0.001) Batch 4.126 (3.465) Remain 06:41:46 loss: 0.2363 Lr: 0.02261
[2023-08-08 03:35:02,669 INFO misc.py line 115 22900] Train: [56/100][64/156] Data 0.001 (0.001) Batch 3.334 (3.463) Remain 06:41:28 loss: 0.5863 Lr: 0.02260
[2023-08-08 03:35:05,330 INFO misc.py line 115 22900] Train: [56/100][65/156] Data 0.002 (0.001) Batch 2.661 (3.450) Remain 06:39:54 loss: 0.1980 Lr: 0.02260
[2023-08-08 03:35:09,467 INFO misc.py line 115 22900] Train: [56/100][66/156] Data 0.001 (0.001) Batch 4.137 (3.461) Remain 06:41:07 loss: 0.2226 Lr: 0.02259
[2023-08-08 03:35:12,576 INFO misc.py line 115 22900] Train: [56/100][67/156] Data 0.001 (0.001) Batch 3.109 (3.455) Remain 06:40:25 loss: 0.1764 Lr: 0.02259
[2023-08-08 03:35:15,455 INFO misc.py line 115 22900] Train: [56/100][68/156] Data 0.001 (0.001) Batch 2.879 (3.447) Remain 06:39:20 loss: 0.1603 Lr: 0.02258
[2023-08-08 03:35:18,942 INFO misc.py line 115 22900] Train: [56/100][69/156] Data 0.001 (0.001) Batch 3.487 (3.447) Remain 06:39:21 loss: 0.2630 Lr: 0.02258
[2023-08-08 03:35:22,439 INFO misc.py line 115 22900] Train: [56/100][70/156] Data 0.001 (0.001) Batch 3.496 (3.448) Remain 06:39:22 loss: 0.2193 Lr: 0.02257
[2023-08-08 03:35:26,490 INFO misc.py line 115 22900] Train: [56/100][71/156] Data 0.001 (0.001) Batch 4.051 (3.457) Remain 06:40:21 loss: 0.3892 Lr: 0.02257
[2023-08-08 03:35:30,234 INFO misc.py line 115 22900] Train: [56/100][72/156] Data 0.001 (0.001) Batch 3.744 (3.461) Remain 06:40:46 loss: 0.1659 Lr: 0.02256
[2023-08-08 03:35:32,878 INFO misc.py line 115 22900] Train: [56/100][73/156] Data 0.001 (0.001) Batch 2.644 (3.449) Remain 06:39:22 loss: 0.1006 Lr: 0.02256
[2023-08-08 03:35:35,816 INFO misc.py line 115 22900] Train: [56/100][74/156] Data 0.001 (0.001) Batch 2.938 (3.442) Remain 06:38:28 loss: 0.1724 Lr: 0.02255
[2023-08-08 03:35:39,520 INFO misc.py line 115 22900] Train: [56/100][75/156] Data 0.001 (0.001) Batch 3.705 (3.446) Remain 06:38:50 loss: 0.3045 Lr: 0.02255
[2023-08-08 03:35:43,305 INFO misc.py line 115 22900] Train: [56/100][76/156] Data 0.001 (0.001) Batch 3.784 (3.450) Remain 06:39:19 loss: 0.2079 Lr: 0.02254
[2023-08-08 03:35:46,184 INFO misc.py line 115 22900] Train: [56/100][77/156] Data 0.001 (0.001) Batch 2.879 (3.443) Remain 06:38:22 loss: 0.0968 Lr: 0.02253
[2023-08-08 03:35:49,368 INFO misc.py line 115 22900] Train: [56/100][78/156] Data 0.001 (0.001) Batch 3.184 (3.439) Remain 06:37:54 loss: 0.1745 Lr: 0.02253
[2023-08-08 03:35:53,019 INFO misc.py line 115 22900] Train: [56/100][79/156] Data 0.001 (0.001) Batch 3.651 (3.442) Remain 06:38:10 loss: 0.2400 Lr: 0.02252
[2023-08-08 03:35:57,865 INFO misc.py line 115 22900] Train: [56/100][80/156] Data 0.001 (0.001) Batch 4.846 (3.460) Remain 06:40:13 loss: 0.5333 Lr: 0.02252
[2023-08-08 03:36:01,445 INFO misc.py line 115 22900] Train: [56/100][81/156] Data 0.001 (0.001) Batch 3.580 (3.462) Remain 06:40:20 loss: 0.2059 Lr: 0.02251
[2023-08-08 03:36:05,532 INFO misc.py line 115 22900] Train: [56/100][82/156] Data 0.001 (0.001) Batch 4.087 (3.470) Remain 06:41:12 loss: 0.3198 Lr: 0.02251
[2023-08-08 03:36:08,361 INFO misc.py line 115 22900] Train: [56/100][83/156] Data 0.001 (0.001) Batch 2.829 (3.462) Remain 06:40:13 loss: 0.1009 Lr: 0.02250
[2023-08-08 03:36:11,989 INFO misc.py line 115 22900] Train: [56/100][84/156] Data 0.001 (0.001) Batch 3.628 (3.464) Remain 06:40:24 loss: 0.2087 Lr: 0.02250
[2023-08-08 03:36:14,574 INFO misc.py line 115 22900] Train: [56/100][85/156] Data 0.001 (0.001) Batch 2.586 (3.453) Remain 06:39:06 loss: 0.1382 Lr: 0.02249
[2023-08-08 03:36:17,719 INFO misc.py line 115 22900] Train: [56/100][86/156] Data 0.001 (0.001) Batch 3.144 (3.449) Remain 06:38:37 loss: 0.2083 Lr: 0.02249
[2023-08-08 03:36:20,632 INFO misc.py line 115 22900] Train: [56/100][87/156] Data 0.001 (0.001) Batch 2.913 (3.443) Remain 06:37:49 loss: 0.1127 Lr: 0.02248
[2023-08-08 03:36:24,578 INFO misc.py line 115 22900] Train: [56/100][88/156] Data 0.001 (0.001) Batch 3.947 (3.449) Remain 06:38:27 loss: 0.2531 Lr: 0.02248
[2023-08-08 03:36:28,590 INFO misc.py line 115 22900] Train: [56/100][89/156] Data 0.001 (0.001) Batch 4.011 (3.455) Remain 06:39:08 loss: 0.3914 Lr: 0.02247
[2023-08-08 03:36:31,228 INFO misc.py line 115 22900] Train: [56/100][90/156] Data 0.001 (0.001) Batch 2.638 (3.446) Remain 06:38:00 loss: 0.2290 Lr: 0.02247
[2023-08-08 03:36:33,592 INFO misc.py line 115 22900] Train: [56/100][91/156] Data 0.001 (0.001) Batch 2.364 (3.434) Remain 06:36:31 loss: 0.1071 Lr: 0.02246
[2023-08-08 03:36:37,645 INFO misc.py line 115 22900] Train: [56/100][92/156] Data 0.001 (0.001) Batch 4.054 (3.441) Remain 06:37:16 loss: 0.3377 Lr: 0.02246
[2023-08-08 03:36:41,675 INFO misc.py line 115 22900] Train: [56/100][93/156] Data 0.001 (0.001) Batch 4.029 (3.447) Remain 06:37:58 loss: 0.2314 Lr: 0.02245
[2023-08-08 03:36:44,794 INFO misc.py line 115 22900] Train: [56/100][94/156] Data 0.001 (0.001) Batch 3.120 (3.444) Remain 06:37:30 loss: 0.3462 Lr: 0.02245
[2023-08-08 03:36:48,204 INFO misc.py line 115 22900] Train: [56/100][95/156] Data 0.001 (0.001) Batch 3.410 (3.443) Remain 06:37:24 loss: 0.1594 Lr: 0.02244
[2023-08-08 03:36:51,585 INFO misc.py line 115 22900] Train: [56/100][96/156] Data 0.001 (0.001) Batch 3.380 (3.443) Remain 06:37:16 loss: 0.1308 Lr: 0.02243
[2023-08-08 03:36:55,468 INFO misc.py line 115 22900] Train: [56/100][97/156] Data 0.001 (0.001) Batch 3.883 (3.447) Remain 06:37:45 loss: 0.3029 Lr: 0.02243
[2023-08-08 03:37:00,003 INFO misc.py line 115 22900] Train: [56/100][98/156] Data 0.001 (0.001) Batch 4.535 (3.459) Remain 06:39:00 loss: 0.3217 Lr: 0.02242
[2023-08-08 03:37:02,653 INFO misc.py line 115 22900] Train: [56/100][99/156] Data 0.001 (0.001) Batch 2.649 (3.450) Remain 06:37:59 loss: 0.1035 Lr: 0.02242
[2023-08-08 03:37:05,598 INFO misc.py line 115 22900] Train: [56/100][100/156] Data 0.001 (0.001) Batch 2.946 (3.445) Remain 06:37:19 loss: 0.0933 Lr: 0.02241
[2023-08-08 03:37:08,388 INFO misc.py line 115 22900] Train: [56/100][101/156] Data 0.001 (0.001) Batch 2.789 (3.438) Remain 06:36:29 loss: 0.1189 Lr: 0.02241
[2023-08-08 03:37:11,700 INFO misc.py line 115 22900] Train: [56/100][102/156] Data 0.001 (0.001) Batch 3.312 (3.437) Remain 06:36:17 loss: 0.2489 Lr: 0.02240
[2023-08-08 03:37:15,791 INFO misc.py line 115 22900] Train: [56/100][103/156] Data 0.001 (0.001) Batch 4.091 (3.444) Remain 06:36:59 loss: 0.5006 Lr: 0.02240
[2023-08-08 03:37:19,843 INFO misc.py line 115 22900] Train: [56/100][104/156] Data 0.001 (0.001) Batch 4.052 (3.450) Remain 06:37:37 loss: 0.2124 Lr: 0.02239
[2023-08-08 03:37:22,055 INFO misc.py line 115 22900] Train: [56/100][105/156] Data 0.001 (0.001) Batch 2.212 (3.438) Remain 06:36:10 loss: 0.1075 Lr: 0.02239
[2023-08-08 03:37:25,251 INFO misc.py line 115 22900] Train: [56/100][106/156] Data 0.001 (0.001) Batch 3.196 (3.435) Remain 06:35:50 loss: 0.2351 Lr: 0.02238
[2023-08-08 03:37:28,763 INFO misc.py line 115 22900] Train: [56/100][107/156] Data 0.001 (0.001) Batch 3.512 (3.436) Remain 06:35:52 loss: 0.2846 Lr: 0.02238
[2023-08-08 03:37:32,586 INFO misc.py line 115 22900] Train: [56/100][108/156] Data 0.001 (0.001) Batch 3.823 (3.440) Remain 06:36:14 loss: 0.2043 Lr: 0.02237
[2023-08-08 03:37:36,623 INFO misc.py line 115 22900] Train: [56/100][109/156] Data 0.001 (0.001) Batch 4.037 (3.445) Remain 06:36:49 loss: 0.1422 Lr: 0.02237
[2023-08-08 03:37:39,928 INFO misc.py line 115 22900] Train: [56/100][110/156] Data 0.001 (0.001) Batch 3.305 (3.444) Remain 06:36:37 loss: 0.3364 Lr: 0.02236
[2023-08-08 03:37:43,254 INFO misc.py line 115 22900] Train: [56/100][111/156] Data 0.001 (0.001) Batch 3.326 (3.443) Remain 06:36:26 loss: 0.2466 Lr: 0.02236
[2023-08-08 03:37:47,498 INFO misc.py line 115 22900] Train: [56/100][112/156] Data 0.001 (0.001) Batch 4.244 (3.450) Remain 06:37:13 loss: 0.4808 Lr: 0.02235
[2023-08-08 03:37:51,562 INFO misc.py line 115 22900] Train: [56/100][113/156] Data 0.001 (0.001) Batch 4.064 (3.456) Remain 06:37:48 loss: 0.2054 Lr: 0.02234
[2023-08-08 03:37:54,611 INFO misc.py line 115 22900] Train: [56/100][114/156] Data 0.001 (0.001) Batch 3.049 (3.452) Remain 06:37:20 loss: 0.1881 Lr: 0.02234
[2023-08-08 03:37:58,400 INFO misc.py line 115 22900] Train: [56/100][115/156] Data 0.001 (0.001) Batch 3.789 (3.455) Remain 06:37:37 loss: 0.2439 Lr: 0.02233
[2023-08-08 03:38:01,802 INFO misc.py line 115 22900] Train: [56/100][116/156] Data 0.001 (0.001) Batch 3.402 (3.455) Remain 06:37:30 loss: 0.2507 Lr: 0.02233
[2023-08-08 03:38:05,297 INFO misc.py line 115 22900] Train: [56/100][117/156] Data 0.001 (0.001) Batch 3.495 (3.455) Remain 06:37:29 loss: 0.3558 Lr: 0.02232
[2023-08-08 03:38:08,678 INFO misc.py line 115 22900] Train: [56/100][118/156] Data 0.001 (0.001) Batch 3.382 (3.454) Remain 06:37:21 loss: 0.2743 Lr: 0.02232
[2023-08-08 03:38:12,174 INFO misc.py line 115 22900] Train: [56/100][119/156] Data 0.001 (0.001) Batch 3.496 (3.455) Remain 06:37:20 loss: 0.2410 Lr: 0.02231
[2023-08-08 03:38:15,797 INFO misc.py line 115 22900] Train: [56/100][120/156] Data 0.001 (0.001) Batch 3.623 (3.456) Remain 06:37:27 loss: 0.2206 Lr: 0.02231
[2023-08-08 03:38:18,088 INFO misc.py line 115 22900] Train: [56/100][121/156] Data 0.001 (0.001) Batch 2.291 (3.446) Remain 06:36:15 loss: 0.2002 Lr: 0.02230
[2023-08-08 03:38:21,993 INFO misc.py line 115 22900] Train: [56/100][122/156] Data 0.001 (0.001) Batch 3.905 (3.450) Remain 06:36:38 loss: 0.1538 Lr: 0.02230
[2023-08-08 03:38:25,182 INFO misc.py line 115 22900] Train: [56/100][123/156] Data 0.001 (0.001) Batch 3.190 (3.448) Remain 06:36:20 loss: 0.0848 Lr: 0.02229
[2023-08-08 03:38:27,797 INFO misc.py line 115 22900] Train: [56/100][124/156] Data 0.001 (0.001) Batch 2.615 (3.441) Remain 06:35:29 loss: 0.1377 Lr: 0.02229
[2023-08-08 03:38:32,118 INFO misc.py line 115 22900] Train: [56/100][125/156] Data 0.001 (0.001) Batch 4.321 (3.448) Remain 06:36:15 loss: 0.1916 Lr: 0.02228
[2023-08-08 03:38:34,911 INFO misc.py line 115 22900] Train: [56/100][126/156] Data 0.001 (0.001) Batch 2.793 (3.443) Remain 06:35:35 loss: 0.2232 Lr: 0.02228
[2023-08-08 03:38:38,268 INFO misc.py line 115 22900] Train: [56/100][127/156] Data 0.001 (0.001) Batch 3.357 (3.442) Remain 06:35:27 loss: 0.1447 Lr: 0.02227
[2023-08-08 03:38:42,051 INFO misc.py line 115 22900] Train: [56/100][128/156] Data 0.001 (0.001) Batch 3.783 (3.445) Remain 06:35:42 loss: 0.3424 Lr: 0.02227
[2023-08-08 03:38:45,472 INFO misc.py line 115 22900] Train: [56/100][129/156] Data 0.001 (0.001) Batch 3.421 (3.445) Remain 06:35:37 loss: 0.2957 Lr: 0.02226
[2023-08-08 03:38:48,858 INFO misc.py line 115 22900] Train: [56/100][130/156] Data 0.001 (0.001) Batch 3.386 (3.444) Remain 06:35:31 loss: 0.0897 Lr: 0.02226
[2023-08-08 03:38:52,893 INFO misc.py line 115 22900] Train: [56/100][131/156] Data 0.001 (0.001) Batch 4.035 (3.449) Remain 06:35:59 loss: 0.1689 Lr: 0.02225
[2023-08-08 03:38:55,906 INFO misc.py line 115 22900] Train: [56/100][132/156] Data 0.001 (0.001) Batch 3.014 (3.446) Remain 06:35:32 loss: 0.0716 Lr: 0.02224
[2023-08-08 03:38:59,096 INFO misc.py line 115 22900] Train: [56/100][133/156] Data 0.001 (0.001) Batch 3.189 (3.444) Remain 06:35:15 loss: 0.2130 Lr: 0.02224
[2023-08-08 03:39:02,591 INFO misc.py line 115 22900] Train: [56/100][134/156] Data 0.001 (0.001) Batch 3.495 (3.444) Remain 06:35:15 loss: 0.3157 Lr: 0.02223
[2023-08-08 03:39:06,139 INFO misc.py line 115 22900] Train: [56/100][135/156] Data 0.001 (0.001) Batch 3.548 (3.445) Remain 06:35:17 loss: 0.1651 Lr: 0.02223
[2023-08-08 03:39:10,261 INFO misc.py line 115 22900] Train: [56/100][136/156] Data 0.001 (0.001) Batch 4.122 (3.450) Remain 06:35:48 loss: 0.2863 Lr: 0.02222
[2023-08-08 03:39:13,957 INFO misc.py line 115 22900] Train: [56/100][137/156] Data 0.001 (0.001) Batch 3.696 (3.452) Remain 06:35:58 loss: 0.2360 Lr: 0.02222
[2023-08-08 03:39:17,900 INFO misc.py line 115 22900] Train: [56/100][138/156] Data 0.001 (0.001) Batch 3.943 (3.455) Remain 06:36:19 loss: 0.3914 Lr: 0.02221
[2023-08-08 03:39:21,121 INFO misc.py line 115 22900] Train: [56/100][139/156] Data 0.001 (0.001) Batch 3.221 (3.454) Remain 06:36:04 loss: 0.1322 Lr: 0.02221
[2023-08-08 03:39:24,640 INFO misc.py line 115 22900] Train: [56/100][140/156] Data 0.001 (0.001) Batch 3.519 (3.454) Remain 06:36:04 loss: 0.1424 Lr: 0.02220
[2023-08-08 03:39:27,457 INFO misc.py line 115 22900] Train: [56/100][141/156] Data 0.001 (0.001) Batch 2.817 (3.449) Remain 06:35:28 loss: 0.1792 Lr: 0.02220
[2023-08-08 03:39:30,988 INFO misc.py line 115 22900] Train: [56/100][142/156] Data 0.001 (0.001) Batch 3.531 (3.450) Remain 06:35:29 loss: 0.3125 Lr: 0.02219
[2023-08-08 03:39:35,444 INFO misc.py line 115 22900] Train: [56/100][143/156] Data 0.001 (0.001) Batch 4.456 (3.457) Remain 06:36:15 loss: 0.4218 Lr: 0.02219
[2023-08-08 03:39:38,285 INFO misc.py line 115 22900] Train: [56/100][144/156] Data 0.001 (0.001) Batch 2.840 (3.453) Remain 06:35:41 loss: 0.0931 Lr: 0.02218
[2023-08-08 03:39:42,349 INFO misc.py line 115 22900] Train: [56/100][145/156] Data 0.001 (0.001) Batch 4.064 (3.457) Remain 06:36:08 loss: 0.2006 Lr: 0.02218
[2023-08-08 03:39:45,916 INFO misc.py line 115 22900] Train: [56/100][146/156] Data 0.001 (0.001) Batch 3.567 (3.458) Remain 06:36:09 loss: 0.1032 Lr: 0.02217
[2023-08-08 03:39:49,816 INFO misc.py line 115 22900] Train: [56/100][147/156] Data 0.001 (0.001) Batch 3.900 (3.461) Remain 06:36:27 loss: 0.1574 Lr: 0.02217
[2023-08-08 03:39:53,905 INFO misc.py line 115 22900] Train: [56/100][148/156] Data 0.001 (0.001) Batch 4.089 (3.465) Remain 06:36:53 loss: 0.1878 Lr: 0.02216
[2023-08-08 03:39:57,457 INFO misc.py line 115 22900] Train: [56/100][149/156] Data 0.001 (0.001) Batch 3.552 (3.466) Remain 06:36:54 loss: 0.2680 Lr: 0.02216
[2023-08-08 03:40:00,460 INFO misc.py line 115 22900] Train: [56/100][150/156] Data 0.001 (0.001) Batch 3.003 (3.463) Remain 06:36:29 loss: 0.1889 Lr: 0.02215
[2023-08-08 03:40:04,198 INFO misc.py line 115 22900] Train: [56/100][151/156] Data 0.001 (0.001) Batch 3.738 (3.465) Remain 06:36:38 loss: 0.2361 Lr: 0.02214
[2023-08-08 03:40:07,629 INFO misc.py line 115 22900] Train: [56/100][152/156] Data 0.001 (0.001) Batch 3.430 (3.464) Remain 06:36:33 loss: 0.1351 Lr: 0.02214
[2023-08-08 03:40:09,658 INFO misc.py line 115 22900] Train: [56/100][153/156] Data 0.001 (0.001) Batch 2.029 (3.455) Remain 06:35:24 loss: 0.0906 Lr: 0.02213
[2023-08-08 03:40:13,148 INFO misc.py line 115 22900] Train: [56/100][154/156] Data 0.001 (0.001) Batch 3.490 (3.455) Remain 06:35:22 loss: 0.2486 Lr: 0.02213
[2023-08-08 03:40:17,770 INFO misc.py line 115 22900] Train: [56/100][155/156] Data 0.001 (0.001) Batch 4.623 (3.463) Remain 06:36:11 loss: 0.6040 Lr: 0.02212
[2023-08-08 03:40:20,524 INFO misc.py line 115 22900] Train: [56/100][156/156] Data 0.001 (0.001) Batch 2.754 (3.458) Remain 06:35:36 loss: 0.2572 Lr: 0.02212
[2023-08-08 03:40:20,524 INFO misc.py line 129 22900] Train result: loss: 0.2295 
[2023-08-08 03:40:20,525 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 03:40:22,615 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.1907 
[2023-08-08 03:40:23,485 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.5628 
[2023-08-08 03:40:25,149 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.7069 
[2023-08-08 03:40:26,670 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.3706 
[2023-08-08 03:40:28,516 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.6158 
[2023-08-08 03:40:30,179 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.6669 
[2023-08-08 03:40:32,316 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.9035 
[2023-08-08 03:40:34,121 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.1219 
[2023-08-08 03:40:35,405 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.5688 
[2023-08-08 03:40:37,535 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.2822 
[2023-08-08 03:40:38,061 INFO evaluator.py line 122 22900] Test: [11/24] Loss 0.9952 
[2023-08-08 03:40:39,593 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.7881 
[2023-08-08 03:40:42,302 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.1847 
[2023-08-08 03:40:43,981 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.8978 
[2023-08-08 03:40:46,003 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.3409 
[2023-08-08 03:40:48,710 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.2376 
[2023-08-08 03:40:51,415 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.5095 
[2023-08-08 03:40:53,259 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.5096 
[2023-08-08 03:40:54,008 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.1917 
[2023-08-08 03:40:54,891 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.7786 
[2023-08-08 03:40:57,152 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3011 
[2023-08-08 03:40:59,115 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.5999 
[2023-08-08 03:41:00,962 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.6300 
[2023-08-08 03:41:02,901 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.9624 
[2023-08-08 03:41:02,949 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2555/0.3513/0.6959.
[2023-08-08 03:41:02,949 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6426/0.9686
[2023-08-08 03:41:02,949 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9468/0.9914
[2023-08-08 03:41:02,949 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1641/0.4393
[2023-08-08 03:41:02,949 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1674/0.3786
[2023-08-08 03:41:02,949 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6077/0.6783
[2023-08-08 03:41:02,949 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.4260/0.6651
[2023-08-08 03:41:02,949 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5336/0.6986
[2023-08-08 03:41:02,949 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1677/0.1802
[2023-08-08 03:41:02,949 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1754/0.3887
[2023-08-08 03:41:02,949 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0355/0.0357
[2023-08-08 03:41:02,949 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 03:41:02,949 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0152/0.0155
[2023-08-08 03:41:02,949 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1792/0.2266
[2023-08-08 03:41:02,949 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0173/0.0178
[2023-08-08 03:41:02,949 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0132/0.0155
[2023-08-08 03:41:02,949 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.4275/0.4776
[2023-08-08 03:41:02,949 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.2101/0.3127
[2023-08-08 03:41:02,949 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.3019/0.4039
[2023-08-08 03:41:02,949 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 03:41:02,949 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0784/0.1313
[2023-08-08 03:41:02,950 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 03:41:02,950 INFO misc.py line 152 22900] Currently Best mIoU: 0.2562
[2023-08-08 03:41:02,950 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 03:41:09,440 INFO misc.py line 115 22900] Train: [57/100][1/156] Data 0.947 (0.947) Batch 5.695 (5.695) Remain 10:51:22 loss: 0.1558 Lr: 0.02211
[2023-08-08 03:41:13,955 INFO misc.py line 115 22900] Train: [57/100][2/156] Data 0.001 (0.001) Batch 4.515 (4.515) Remain 08:36:21 loss: 0.5867 Lr: 0.02211
[2023-08-08 03:41:17,573 INFO misc.py line 115 22900] Train: [57/100][3/156] Data 0.001 (0.001) Batch 3.618 (3.618) Remain 06:53:46 loss: 0.2154 Lr: 0.02210
[2023-08-08 03:41:20,428 INFO misc.py line 115 22900] Train: [57/100][4/156] Data 0.001 (0.001) Batch 2.854 (2.854) Remain 05:26:21 loss: 0.0999 Lr: 0.02210
[2023-08-08 03:41:23,931 INFO misc.py line 115 22900] Train: [57/100][5/156] Data 0.001 (0.001) Batch 3.503 (3.179) Remain 06:03:23 loss: 0.2067 Lr: 0.02209
[2023-08-08 03:41:27,355 INFO misc.py line 115 22900] Train: [57/100][6/156] Data 0.001 (0.001) Batch 3.424 (3.260) Remain 06:12:40 loss: 0.2349 Lr: 0.02209
[2023-08-08 03:41:30,819 INFO misc.py line 115 22900] Train: [57/100][7/156] Data 0.001 (0.001) Batch 3.464 (3.311) Remain 06:18:26 loss: 0.1251 Lr: 0.02208
[2023-08-08 03:41:35,018 INFO misc.py line 115 22900] Train: [57/100][8/156] Data 0.001 (0.001) Batch 4.199 (3.489) Remain 06:38:39 loss: 0.3876 Lr: 0.02208
[2023-08-08 03:41:39,071 INFO misc.py line 115 22900] Train: [57/100][9/156] Data 0.001 (0.001) Batch 4.054 (3.583) Remain 06:49:21 loss: 0.2820 Lr: 0.02207
[2023-08-08 03:41:43,154 INFO misc.py line 115 22900] Train: [57/100][10/156] Data 0.001 (0.001) Batch 4.083 (3.654) Remain 06:57:27 loss: 0.3342 Lr: 0.02207
[2023-08-08 03:41:46,426 INFO misc.py line 115 22900] Train: [57/100][11/156] Data 0.001 (0.001) Batch 3.272 (3.607) Remain 06:51:55 loss: 0.1102 Lr: 0.02206
[2023-08-08 03:41:49,695 INFO misc.py line 115 22900] Train: [57/100][12/156] Data 0.001 (0.001) Batch 3.268 (3.569) Remain 06:47:34 loss: 0.0869 Lr: 0.02206
[2023-08-08 03:41:52,581 INFO misc.py line 115 22900] Train: [57/100][13/156] Data 0.001 (0.001) Batch 2.886 (3.501) Remain 06:39:43 loss: 0.0914 Lr: 0.02205
[2023-08-08 03:41:55,862 INFO misc.py line 115 22900] Train: [57/100][14/156] Data 0.001 (0.001) Batch 3.281 (3.481) Remain 06:37:23 loss: 0.2877 Lr: 0.02204
[2023-08-08 03:41:59,263 INFO misc.py line 115 22900] Train: [57/100][15/156] Data 0.001 (0.001) Batch 3.401 (3.474) Remain 06:36:34 loss: 0.1772 Lr: 0.02204
[2023-08-08 03:42:01,861 INFO misc.py line 115 22900] Train: [57/100][16/156] Data 0.001 (0.001) Batch 2.598 (3.407) Remain 06:28:49 loss: 0.1256 Lr: 0.02203
[2023-08-08 03:42:05,946 INFO misc.py line 115 22900] Train: [57/100][17/156] Data 0.001 (0.001) Batch 4.085 (3.455) Remain 06:34:17 loss: 0.1759 Lr: 0.02203
[2023-08-08 03:42:09,960 INFO misc.py line 115 22900] Train: [57/100][18/156] Data 0.001 (0.001) Batch 4.014 (3.492) Remain 06:38:29 loss: 0.2310 Lr: 0.02202
[2023-08-08 03:42:12,635 INFO misc.py line 115 22900] Train: [57/100][19/156] Data 0.001 (0.001) Batch 2.675 (3.441) Remain 06:32:36 loss: 0.4042 Lr: 0.02202
[2023-08-08 03:42:14,853 INFO misc.py line 115 22900] Train: [57/100][20/156] Data 0.001 (0.001) Batch 2.217 (3.369) Remain 06:24:19 loss: 0.2819 Lr: 0.02201
[2023-08-08 03:42:18,343 INFO misc.py line 115 22900] Train: [57/100][21/156] Data 0.001 (0.001) Batch 3.490 (3.376) Remain 06:25:02 loss: 0.1503 Lr: 0.02201
[2023-08-08 03:42:21,880 INFO misc.py line 115 22900] Train: [57/100][22/156] Data 0.001 (0.001) Batch 3.537 (3.385) Remain 06:25:57 loss: 0.3025 Lr: 0.02200
[2023-08-08 03:42:25,640 INFO misc.py line 115 22900] Train: [57/100][23/156] Data 0.001 (0.001) Batch 3.760 (3.403) Remain 06:28:02 loss: 0.3297 Lr: 0.02200
[2023-08-08 03:42:29,997 INFO misc.py line 115 22900] Train: [57/100][24/156] Data 0.001 (0.001) Batch 4.357 (3.449) Remain 06:33:09 loss: 0.3503 Lr: 0.02199
[2023-08-08 03:42:32,820 INFO misc.py line 115 22900] Train: [57/100][25/156] Data 0.001 (0.001) Batch 2.824 (3.420) Remain 06:29:51 loss: 0.0806 Lr: 0.02199
[2023-08-08 03:42:36,171 INFO misc.py line 115 22900] Train: [57/100][26/156] Data 0.001 (0.001) Batch 3.350 (3.417) Remain 06:29:27 loss: 0.2480 Lr: 0.02198
[2023-08-08 03:42:37,683 INFO misc.py line 115 22900] Train: [57/100][27/156] Data 0.001 (0.001) Batch 1.513 (3.338) Remain 06:20:21 loss: 0.0571 Lr: 0.02198
[2023-08-08 03:42:41,629 INFO misc.py line 115 22900] Train: [57/100][28/156] Data 0.001 (0.001) Batch 3.945 (3.362) Remain 06:23:04 loss: 0.3044 Lr: 0.02197
[2023-08-08 03:42:44,231 INFO misc.py line 115 22900] Train: [57/100][29/156] Data 0.001 (0.001) Batch 2.602 (3.333) Remain 06:19:40 loss: 0.1774 Lr: 0.02197
[2023-08-08 03:42:47,595 INFO misc.py line 115 22900] Train: [57/100][30/156] Data 0.001 (0.001) Batch 3.364 (3.334) Remain 06:19:45 loss: 0.1780 Lr: 0.02196
[2023-08-08 03:42:51,647 INFO misc.py line 115 22900] Train: [57/100][31/156] Data 0.001 (0.001) Batch 4.052 (3.360) Remain 06:22:37 loss: 0.1967 Lr: 0.02196
[2023-08-08 03:42:54,506 INFO misc.py line 115 22900] Train: [57/100][32/156] Data 0.001 (0.001) Batch 2.859 (3.342) Remain 06:20:35 loss: 0.2276 Lr: 0.02195
[2023-08-08 03:42:58,613 INFO misc.py line 115 22900] Train: [57/100][33/156] Data 0.001 (0.001) Batch 4.107 (3.368) Remain 06:23:26 loss: 0.1832 Lr: 0.02194
[2023-08-08 03:43:02,207 INFO misc.py line 115 22900] Train: [57/100][34/156] Data 0.001 (0.001) Batch 3.593 (3.375) Remain 06:24:13 loss: 0.1982 Lr: 0.02194
[2023-08-08 03:43:06,236 INFO misc.py line 115 22900] Train: [57/100][35/156] Data 0.001 (0.001) Batch 4.029 (3.396) Remain 06:26:29 loss: 0.2861 Lr: 0.02193
[2023-08-08 03:43:10,682 INFO misc.py line 115 22900] Train: [57/100][36/156] Data 0.001 (0.001) Batch 4.446 (3.428) Remain 06:30:03 loss: 0.2246 Lr: 0.02193
[2023-08-08 03:43:14,515 INFO misc.py line 115 22900] Train: [57/100][37/156] Data 0.001 (0.001) Batch 3.834 (3.439) Remain 06:31:21 loss: 0.2247 Lr: 0.02192
[2023-08-08 03:43:17,836 INFO misc.py line 115 22900] Train: [57/100][38/156] Data 0.001 (0.001) Batch 3.321 (3.436) Remain 06:30:54 loss: 0.1287 Lr: 0.02192
[2023-08-08 03:43:21,016 INFO misc.py line 115 22900] Train: [57/100][39/156] Data 0.001 (0.001) Batch 3.179 (3.429) Remain 06:30:02 loss: 0.1339 Lr: 0.02191
[2023-08-08 03:43:25,035 INFO misc.py line 115 22900] Train: [57/100][40/156] Data 0.001 (0.001) Batch 4.020 (3.445) Remain 06:31:48 loss: 0.2316 Lr: 0.02191
[2023-08-08 03:43:28,674 INFO misc.py line 115 22900] Train: [57/100][41/156] Data 0.001 (0.001) Batch 3.639 (3.450) Remain 06:32:19 loss: 0.3318 Lr: 0.02190
[2023-08-08 03:43:32,370 INFO misc.py line 115 22900] Train: [57/100][42/156] Data 0.001 (0.001) Batch 3.696 (3.456) Remain 06:32:58 loss: 0.1458 Lr: 0.02190
[2023-08-08 03:43:36,035 INFO misc.py line 115 22900] Train: [57/100][43/156] Data 0.001 (0.001) Batch 3.666 (3.462) Remain 06:33:31 loss: 0.0993 Lr: 0.02189
[2023-08-08 03:43:37,403 INFO misc.py line 115 22900] Train: [57/100][44/156] Data 0.001 (0.001) Batch 1.368 (3.410) Remain 06:27:39 loss: 0.3156 Lr: 0.02189
[2023-08-08 03:43:41,565 INFO misc.py line 115 22900] Train: [57/100][45/156] Data 0.001 (0.001) Batch 4.162 (3.428) Remain 06:29:38 loss: 0.1739 Lr: 0.02188
[2023-08-08 03:43:45,092 INFO misc.py line 115 22900] Train: [57/100][46/156] Data 0.001 (0.001) Batch 3.527 (3.431) Remain 06:29:50 loss: 0.2165 Lr: 0.02188
[2023-08-08 03:43:47,621 INFO misc.py line 115 22900] Train: [57/100][47/156] Data 0.001 (0.001) Batch 2.529 (3.410) Remain 06:27:27 loss: 0.1553 Lr: 0.02187
[2023-08-08 03:43:51,215 INFO misc.py line 115 22900] Train: [57/100][48/156] Data 0.001 (0.001) Batch 3.594 (3.414) Remain 06:27:51 loss: 0.3139 Lr: 0.02187
[2023-08-08 03:43:54,586 INFO misc.py line 115 22900] Train: [57/100][49/156] Data 0.001 (0.001) Batch 3.371 (3.413) Remain 06:27:41 loss: 0.2551 Lr: 0.02186
[2023-08-08 03:43:56,930 INFO misc.py line 115 22900] Train: [57/100][50/156] Data 0.001 (0.001) Batch 2.344 (3.391) Remain 06:25:03 loss: 0.2353 Lr: 0.02186
[2023-08-08 03:43:59,947 INFO misc.py line 115 22900] Train: [57/100][51/156] Data 0.001 (0.001) Batch 3.017 (3.383) Remain 06:24:06 loss: 0.3743 Lr: 0.02185
[2023-08-08 03:44:03,263 INFO misc.py line 115 22900] Train: [57/100][52/156] Data 0.001 (0.001) Batch 3.315 (3.381) Remain 06:23:54 loss: 0.1600 Lr: 0.02184
[2023-08-08 03:44:06,974 INFO misc.py line 115 22900] Train: [57/100][53/156] Data 0.001 (0.001) Batch 3.712 (3.388) Remain 06:24:35 loss: 0.2778 Lr: 0.02184
[2023-08-08 03:44:10,583 INFO misc.py line 115 22900] Train: [57/100][54/156] Data 0.001 (0.001) Batch 3.609 (3.392) Remain 06:25:01 loss: 0.2551 Lr: 0.02183
[2023-08-08 03:44:13,623 INFO misc.py line 115 22900] Train: [57/100][55/156] Data 0.001 (0.001) Batch 3.040 (3.386) Remain 06:24:12 loss: 0.2079 Lr: 0.02183
[2023-08-08 03:44:17,085 INFO misc.py line 115 22900] Train: [57/100][56/156] Data 0.001 (0.001) Batch 3.462 (3.387) Remain 06:24:18 loss: 0.1620 Lr: 0.02182
[2023-08-08 03:44:19,800 INFO misc.py line 115 22900] Train: [57/100][57/156] Data 0.001 (0.001) Batch 2.715 (3.375) Remain 06:22:50 loss: 0.1558 Lr: 0.02182
[2023-08-08 03:44:22,669 INFO misc.py line 115 22900] Train: [57/100][58/156] Data 0.001 (0.001) Batch 2.869 (3.365) Remain 06:21:44 loss: 0.1144 Lr: 0.02181
[2023-08-08 03:44:26,588 INFO misc.py line 115 22900] Train: [57/100][59/156] Data 0.001 (0.001) Batch 3.920 (3.375) Remain 06:22:48 loss: 0.2290 Lr: 0.02181
[2023-08-08 03:44:30,544 INFO misc.py line 115 22900] Train: [57/100][60/156] Data 0.001 (0.001) Batch 3.956 (3.385) Remain 06:23:54 loss: 0.2172 Lr: 0.02180
[2023-08-08 03:44:33,274 INFO misc.py line 115 22900] Train: [57/100][61/156] Data 0.001 (0.001) Batch 2.730 (3.374) Remain 06:22:34 loss: 0.2275 Lr: 0.02180
[2023-08-08 03:44:37,534 INFO misc.py line 115 22900] Train: [57/100][62/156] Data 0.001 (0.001) Batch 4.260 (3.389) Remain 06:24:13 loss: 0.3772 Lr: 0.02179
[2023-08-08 03:44:40,602 INFO misc.py line 115 22900] Train: [57/100][63/156] Data 0.001 (0.001) Batch 3.067 (3.384) Remain 06:23:33 loss: 0.2279 Lr: 0.02179
[2023-08-08 03:44:44,609 INFO misc.py line 115 22900] Train: [57/100][64/156] Data 0.001 (0.001) Batch 4.007 (3.394) Remain 06:24:39 loss: 0.3650 Lr: 0.02178
[2023-08-08 03:44:47,871 INFO misc.py line 115 22900] Train: [57/100][65/156] Data 0.001 (0.001) Batch 3.263 (3.392) Remain 06:24:21 loss: 0.1452 Lr: 0.02178
[2023-08-08 03:44:51,153 INFO misc.py line 115 22900] Train: [57/100][66/156] Data 0.001 (0.001) Batch 3.281 (3.390) Remain 06:24:06 loss: 0.2880 Lr: 0.02177
[2023-08-08 03:44:53,959 INFO misc.py line 115 22900] Train: [57/100][67/156] Data 0.001 (0.001) Batch 2.806 (3.381) Remain 06:23:00 loss: 0.2172 Lr: 0.02177
[2023-08-08 03:44:56,710 INFO misc.py line 115 22900] Train: [57/100][68/156] Data 0.001 (0.001) Batch 2.751 (3.371) Remain 06:21:51 loss: 0.1180 Lr: 0.02176
[2023-08-08 03:44:59,500 INFO misc.py line 115 22900] Train: [57/100][69/156] Data 0.001 (0.001) Batch 2.790 (3.363) Remain 06:20:48 loss: 0.1436 Lr: 0.02176
[2023-08-08 03:45:03,515 INFO misc.py line 115 22900] Train: [57/100][70/156] Data 0.001 (0.001) Batch 4.015 (3.372) Remain 06:21:51 loss: 0.5136 Lr: 0.02175
[2023-08-08 03:45:06,605 INFO misc.py line 115 22900] Train: [57/100][71/156] Data 0.001 (0.001) Batch 3.090 (3.368) Remain 06:21:19 loss: 0.2067 Lr: 0.02175
[2023-08-08 03:45:09,573 INFO misc.py line 115 22900] Train: [57/100][72/156] Data 0.001 (0.001) Batch 2.969 (3.362) Remain 06:20:36 loss: 0.2232 Lr: 0.02174
[2023-08-08 03:45:12,884 INFO misc.py line 115 22900] Train: [57/100][73/156] Data 0.001 (0.001) Batch 3.310 (3.362) Remain 06:20:28 loss: 0.2301 Lr: 0.02173
[2023-08-08 03:45:16,705 INFO misc.py line 115 22900] Train: [57/100][74/156] Data 0.001 (0.001) Batch 3.821 (3.368) Remain 06:21:09 loss: 0.2260 Lr: 0.02173
[2023-08-08 03:45:20,741 INFO misc.py line 115 22900] Train: [57/100][75/156] Data 0.001 (0.001) Batch 4.036 (3.377) Remain 06:22:08 loss: 0.2417 Lr: 0.02172
[2023-08-08 03:45:24,442 INFO misc.py line 115 22900] Train: [57/100][76/156] Data 0.001 (0.001) Batch 3.701 (3.382) Remain 06:22:35 loss: 0.2828 Lr: 0.02172
[2023-08-08 03:45:25,972 INFO misc.py line 115 22900] Train: [57/100][77/156] Data 0.001 (0.001) Batch 1.531 (3.357) Remain 06:19:42 loss: 0.0436 Lr: 0.02171
[2023-08-08 03:45:28,640 INFO misc.py line 115 22900] Train: [57/100][78/156] Data 0.001 (0.001) Batch 2.668 (3.348) Remain 06:18:36 loss: 0.2029 Lr: 0.02171
[2023-08-08 03:45:32,619 INFO misc.py line 115 22900] Train: [57/100][79/156] Data 0.001 (0.001) Batch 3.978 (3.356) Remain 06:19:29 loss: 0.3352 Lr: 0.02170
[2023-08-08 03:45:36,445 INFO misc.py line 115 22900] Train: [57/100][80/156] Data 0.001 (0.001) Batch 3.827 (3.362) Remain 06:20:07 loss: 0.3329 Lr: 0.02170
[2023-08-08 03:45:39,586 INFO misc.py line 115 22900] Train: [57/100][81/156] Data 0.001 (0.001) Batch 3.141 (3.359) Remain 06:19:45 loss: 0.1713 Lr: 0.02169
[2023-08-08 03:45:42,714 INFO misc.py line 115 22900] Train: [57/100][82/156] Data 0.001 (0.001) Batch 3.127 (3.356) Remain 06:19:21 loss: 0.1088 Lr: 0.02169
[2023-08-08 03:45:46,248 INFO misc.py line 115 22900] Train: [57/100][83/156] Data 0.001 (0.001) Batch 3.534 (3.358) Remain 06:19:33 loss: 0.1620 Lr: 0.02168
[2023-08-08 03:45:48,256 INFO misc.py line 115 22900] Train: [57/100][84/156] Data 0.001 (0.001) Batch 2.008 (3.342) Remain 06:17:37 loss: 0.1232 Lr: 0.02168
[2023-08-08 03:45:52,549 INFO misc.py line 115 22900] Train: [57/100][85/156] Data 0.001 (0.001) Batch 4.293 (3.353) Remain 06:18:52 loss: 0.4447 Lr: 0.02167
[2023-08-08 03:45:55,529 INFO misc.py line 115 22900] Train: [57/100][86/156] Data 0.001 (0.001) Batch 2.980 (3.349) Remain 06:18:18 loss: 0.1645 Lr: 0.02167
[2023-08-08 03:45:59,511 INFO misc.py line 115 22900] Train: [57/100][87/156] Data 0.001 (0.001) Batch 3.981 (3.356) Remain 06:19:06 loss: 0.3932 Lr: 0.02166
[2023-08-08 03:46:03,573 INFO misc.py line 115 22900] Train: [57/100][88/156] Data 0.001 (0.001) Batch 4.062 (3.365) Remain 06:19:59 loss: 0.1441 Lr: 0.02166
[2023-08-08 03:46:07,242 INFO misc.py line 115 22900] Train: [57/100][89/156] Data 0.001 (0.001) Batch 3.669 (3.368) Remain 06:20:19 loss: 0.3380 Lr: 0.02165
[2023-08-08 03:46:11,342 INFO misc.py line 115 22900] Train: [57/100][90/156] Data 0.001 (0.001) Batch 4.100 (3.377) Remain 06:21:13 loss: 0.2566 Lr: 0.02165
[2023-08-08 03:46:14,557 INFO misc.py line 115 22900] Train: [57/100][91/156] Data 0.001 (0.001) Batch 3.215 (3.375) Remain 06:20:57 loss: 0.3037 Lr: 0.02164
[2023-08-08 03:46:17,827 INFO misc.py line 115 22900] Train: [57/100][92/156] Data 0.001 (0.001) Batch 3.271 (3.374) Remain 06:20:46 loss: 0.3551 Lr: 0.02163
[2023-08-08 03:46:21,915 INFO misc.py line 115 22900] Train: [57/100][93/156] Data 0.001 (0.001) Batch 4.087 (3.382) Remain 06:21:36 loss: 0.2469 Lr: 0.02163
[2023-08-08 03:46:25,664 INFO misc.py line 115 22900] Train: [57/100][94/156] Data 0.001 (0.001) Batch 3.749 (3.386) Remain 06:22:00 loss: 0.1313 Lr: 0.02162
[2023-08-08 03:46:29,252 INFO misc.py line 115 22900] Train: [57/100][95/156] Data 0.001 (0.001) Batch 3.588 (3.388) Remain 06:22:12 loss: 0.2129 Lr: 0.02162
[2023-08-08 03:46:32,908 INFO misc.py line 115 22900] Train: [57/100][96/156] Data 0.001 (0.001) Batch 3.656 (3.391) Remain 06:22:28 loss: 0.2426 Lr: 0.02161
[2023-08-08 03:46:36,557 INFO misc.py line 115 22900] Train: [57/100][97/156] Data 0.001 (0.001) Batch 3.649 (3.393) Remain 06:22:43 loss: 0.3161 Lr: 0.02161
[2023-08-08 03:46:39,761 INFO misc.py line 115 22900] Train: [57/100][98/156] Data 0.001 (0.001) Batch 3.203 (3.391) Remain 06:22:26 loss: 0.1535 Lr: 0.02160
[2023-08-08 03:46:42,686 INFO misc.py line 115 22900] Train: [57/100][99/156] Data 0.001 (0.001) Batch 2.926 (3.387) Remain 06:21:50 loss: 0.1973 Lr: 0.02160
[2023-08-08 03:46:46,754 INFO misc.py line 115 22900] Train: [57/100][100/156] Data 0.001 (0.001) Batch 4.067 (3.394) Remain 06:22:34 loss: 0.3247 Lr: 0.02159
[2023-08-08 03:46:50,373 INFO misc.py line 115 22900] Train: [57/100][101/156] Data 0.001 (0.001) Batch 3.620 (3.396) Remain 06:22:46 loss: 0.2229 Lr: 0.02159
[2023-08-08 03:46:53,430 INFO misc.py line 115 22900] Train: [57/100][102/156] Data 0.001 (0.001) Batch 3.056 (3.392) Remain 06:22:19 loss: 0.0616 Lr: 0.02158
[2023-08-08 03:46:57,007 INFO misc.py line 115 22900] Train: [57/100][103/156] Data 0.001 (0.001) Batch 3.578 (3.394) Remain 06:22:29 loss: 0.2523 Lr: 0.02158
[2023-08-08 03:46:59,111 INFO misc.py line 115 22900] Train: [57/100][104/156] Data 0.001 (0.001) Batch 2.104 (3.382) Remain 06:20:59 loss: 0.1382 Lr: 0.02157
[2023-08-08 03:47:03,175 INFO misc.py line 115 22900] Train: [57/100][105/156] Data 0.001 (0.001) Batch 4.063 (3.388) Remain 06:21:41 loss: 0.3690 Lr: 0.02157
[2023-08-08 03:47:07,292 INFO misc.py line 115 22900] Train: [57/100][106/156] Data 0.001 (0.001) Batch 4.118 (3.395) Remain 06:22:25 loss: 0.2493 Lr: 0.02156
[2023-08-08 03:47:11,209 INFO misc.py line 115 22900] Train: [57/100][107/156] Data 0.001 (0.001) Batch 3.917 (3.400) Remain 06:22:56 loss: 0.3607 Lr: 0.02156
[2023-08-08 03:47:14,656 INFO misc.py line 115 22900] Train: [57/100][108/156] Data 0.001 (0.001) Batch 3.446 (3.401) Remain 06:22:55 loss: 0.3025 Lr: 0.02155
[2023-08-08 03:47:17,079 INFO misc.py line 115 22900] Train: [57/100][109/156] Data 0.001 (0.001) Batch 2.423 (3.392) Remain 06:21:49 loss: 0.1964 Lr: 0.02155
[2023-08-08 03:47:20,899 INFO misc.py line 115 22900] Train: [57/100][110/156] Data 0.001 (0.001) Batch 3.820 (3.396) Remain 06:22:13 loss: 0.2421 Lr: 0.02154
[2023-08-08 03:47:23,810 INFO misc.py line 115 22900] Train: [57/100][111/156] Data 0.001 (0.001) Batch 2.911 (3.391) Remain 06:21:39 loss: 0.4265 Lr: 0.02153
[2023-08-08 03:47:28,078 INFO misc.py line 115 22900] Train: [57/100][112/156] Data 0.001 (0.001) Batch 4.268 (3.399) Remain 06:22:30 loss: 0.3363 Lr: 0.02153
[2023-08-08 03:47:31,273 INFO misc.py line 115 22900] Train: [57/100][113/156] Data 0.001 (0.001) Batch 3.195 (3.397) Remain 06:22:14 loss: 0.1917 Lr: 0.02152
[2023-08-08 03:47:35,256 INFO misc.py line 115 22900] Train: [57/100][114/156] Data 0.001 (0.001) Batch 3.982 (3.403) Remain 06:22:47 loss: 0.1036 Lr: 0.02152
[2023-08-08 03:47:38,865 INFO misc.py line 115 22900] Train: [57/100][115/156] Data 0.001 (0.001) Batch 3.610 (3.404) Remain 06:22:56 loss: 0.1972 Lr: 0.02151
[2023-08-08 03:47:41,754 INFO misc.py line 115 22900] Train: [57/100][116/156] Data 0.001 (0.001) Batch 2.889 (3.400) Remain 06:22:22 loss: 0.1822 Lr: 0.02151
[2023-08-08 03:47:45,326 INFO misc.py line 115 22900] Train: [57/100][117/156] Data 0.001 (0.001) Batch 3.572 (3.401) Remain 06:22:28 loss: 0.0814 Lr: 0.02150
[2023-08-08 03:47:48,137 INFO misc.py line 115 22900] Train: [57/100][118/156] Data 0.001 (0.001) Batch 2.811 (3.396) Remain 06:21:50 loss: 0.2588 Lr: 0.02150
[2023-08-08 03:47:51,237 INFO misc.py line 115 22900] Train: [57/100][119/156] Data 0.001 (0.001) Batch 3.100 (3.394) Remain 06:21:30 loss: 0.2325 Lr: 0.02149
[2023-08-08 03:47:54,134 INFO misc.py line 115 22900] Train: [57/100][120/156] Data 0.001 (0.001) Batch 2.897 (3.389) Remain 06:20:58 loss: 0.1087 Lr: 0.02149
[2023-08-08 03:47:58,758 INFO misc.py line 115 22900] Train: [57/100][121/156] Data 0.002 (0.001) Batch 4.623 (3.400) Remain 06:22:05 loss: 0.5921 Lr: 0.02148
[2023-08-08 03:48:02,018 INFO misc.py line 115 22900] Train: [57/100][122/156] Data 0.001 (0.001) Batch 3.260 (3.399) Remain 06:21:53 loss: 0.2077 Lr: 0.02148
[2023-08-08 03:48:06,013 INFO misc.py line 115 22900] Train: [57/100][123/156] Data 0.001 (0.001) Batch 3.996 (3.404) Remain 06:22:24 loss: 0.4110 Lr: 0.02147
[2023-08-08 03:48:08,919 INFO misc.py line 115 22900] Train: [57/100][124/156] Data 0.001 (0.001) Batch 2.906 (3.400) Remain 06:21:52 loss: 0.1356 Lr: 0.02147
[2023-08-08 03:48:12,634 INFO misc.py line 115 22900] Train: [57/100][125/156] Data 0.001 (0.001) Batch 3.715 (3.402) Remain 06:22:06 loss: 0.2008 Lr: 0.02146
[2023-08-08 03:48:16,089 INFO misc.py line 115 22900] Train: [57/100][126/156] Data 0.001 (0.001) Batch 3.455 (3.403) Remain 06:22:06 loss: 0.1064 Lr: 0.02146
[2023-08-08 03:48:20,690 INFO misc.py line 115 22900] Train: [57/100][127/156] Data 0.001 (0.001) Batch 4.601 (3.412) Remain 06:23:08 loss: 0.5496 Lr: 0.02145
[2023-08-08 03:48:24,388 INFO misc.py line 115 22900] Train: [57/100][128/156] Data 0.001 (0.001) Batch 3.698 (3.415) Remain 06:23:20 loss: 0.2700 Lr: 0.02145
[2023-08-08 03:48:27,906 INFO misc.py line 115 22900] Train: [57/100][129/156] Data 0.001 (0.001) Batch 3.518 (3.415) Remain 06:23:22 loss: 0.2307 Lr: 0.02144
[2023-08-08 03:48:31,093 INFO misc.py line 115 22900] Train: [57/100][130/156] Data 0.001 (0.001) Batch 3.187 (3.414) Remain 06:23:06 loss: 0.1544 Lr: 0.02144
[2023-08-08 03:48:34,799 INFO misc.py line 115 22900] Train: [57/100][131/156] Data 0.001 (0.001) Batch 3.706 (3.416) Remain 06:23:18 loss: 0.3369 Lr: 0.02143
[2023-08-08 03:48:38,806 INFO misc.py line 115 22900] Train: [57/100][132/156] Data 0.001 (0.001) Batch 4.007 (3.420) Remain 06:23:46 loss: 0.3707 Lr: 0.02142
[2023-08-08 03:48:41,060 INFO misc.py line 115 22900] Train: [57/100][133/156] Data 0.001 (0.001) Batch 2.254 (3.411) Remain 06:22:42 loss: 0.1505 Lr: 0.02142
[2023-08-08 03:48:44,377 INFO misc.py line 115 22900] Train: [57/100][134/156] Data 0.001 (0.001) Batch 3.317 (3.411) Remain 06:22:34 loss: 0.1622 Lr: 0.02141
[2023-08-08 03:48:48,011 INFO misc.py line 115 22900] Train: [57/100][135/156] Data 0.001 (0.001) Batch 3.633 (3.412) Remain 06:22:42 loss: 0.2800 Lr: 0.02141
[2023-08-08 03:48:51,605 INFO misc.py line 115 22900] Train: [57/100][136/156] Data 0.001 (0.001) Batch 3.594 (3.414) Remain 06:22:47 loss: 0.2581 Lr: 0.02140
[2023-08-08 03:48:55,264 INFO misc.py line 115 22900] Train: [57/100][137/156] Data 0.001 (0.001) Batch 3.659 (3.416) Remain 06:22:56 loss: 0.2246 Lr: 0.02140
[2023-08-08 03:48:59,467 INFO misc.py line 115 22900] Train: [57/100][138/156] Data 0.001 (0.001) Batch 4.203 (3.421) Remain 06:23:32 loss: 0.4669 Lr: 0.02139
[2023-08-08 03:49:03,494 INFO misc.py line 115 22900] Train: [57/100][139/156] Data 0.001 (0.001) Batch 4.027 (3.426) Remain 06:23:59 loss: 0.1258 Lr: 0.02139
[2023-08-08 03:49:07,424 INFO misc.py line 115 22900] Train: [57/100][140/156] Data 0.001 (0.001) Batch 3.930 (3.430) Remain 06:24:20 loss: 0.1738 Lr: 0.02138
[2023-08-08 03:49:11,480 INFO misc.py line 115 22900] Train: [57/100][141/156] Data 0.001 (0.001) Batch 4.056 (3.434) Remain 06:24:47 loss: 0.4027 Lr: 0.02138
[2023-08-08 03:49:15,532 INFO misc.py line 115 22900] Train: [57/100][142/156] Data 0.001 (0.001) Batch 4.052 (3.439) Remain 06:25:13 loss: 0.1218 Lr: 0.02137
[2023-08-08 03:49:19,597 INFO misc.py line 115 22900] Train: [57/100][143/156] Data 0.001 (0.001) Batch 4.066 (3.443) Remain 06:25:40 loss: 0.2809 Lr: 0.02137
[2023-08-08 03:49:22,786 INFO misc.py line 115 22900] Train: [57/100][144/156] Data 0.001 (0.001) Batch 3.188 (3.441) Remain 06:25:24 loss: 0.2139 Lr: 0.02136
[2023-08-08 03:49:26,395 INFO misc.py line 115 22900] Train: [57/100][145/156] Data 0.001 (0.001) Batch 3.609 (3.442) Remain 06:25:29 loss: 0.3347 Lr: 0.02136
[2023-08-08 03:49:29,834 INFO misc.py line 115 22900] Train: [57/100][146/156] Data 0.001 (0.001) Batch 3.440 (3.442) Remain 06:25:25 loss: 0.1894 Lr: 0.02135
[2023-08-08 03:49:33,416 INFO misc.py line 115 22900] Train: [57/100][147/156] Data 0.001 (0.001) Batch 3.582 (3.443) Remain 06:25:28 loss: 0.1480 Lr: 0.02135
[2023-08-08 03:49:37,222 INFO misc.py line 115 22900] Train: [57/100][148/156] Data 0.001 (0.001) Batch 3.806 (3.446) Remain 06:25:42 loss: 0.2014 Lr: 0.02134
[2023-08-08 03:49:40,059 INFO misc.py line 115 22900] Train: [57/100][149/156] Data 0.001 (0.001) Batch 2.838 (3.442) Remain 06:25:10 loss: 0.1348 Lr: 0.02134
[2023-08-08 03:49:43,243 INFO misc.py line 115 22900] Train: [57/100][150/156] Data 0.001 (0.001) Batch 3.184 (3.440) Remain 06:24:55 loss: 0.3629 Lr: 0.02133
[2023-08-08 03:49:45,878 INFO misc.py line 115 22900] Train: [57/100][151/156] Data 0.001 (0.001) Batch 2.635 (3.434) Remain 06:24:15 loss: 0.0470 Lr: 0.02133
[2023-08-08 03:49:47,742 INFO misc.py line 115 22900] Train: [57/100][152/156] Data 0.001 (0.001) Batch 1.864 (3.424) Remain 06:23:01 loss: 0.2254 Lr: 0.02132
[2023-08-08 03:49:51,021 INFO misc.py line 115 22900] Train: [57/100][153/156] Data 0.001 (0.001) Batch 3.280 (3.423) Remain 06:22:51 loss: 0.2876 Lr: 0.02131
[2023-08-08 03:49:54,608 INFO misc.py line 115 22900] Train: [57/100][154/156] Data 0.001 (0.001) Batch 3.587 (3.424) Remain 06:22:55 loss: 0.1231 Lr: 0.02131
[2023-08-08 03:49:59,040 INFO misc.py line 115 22900] Train: [57/100][155/156] Data 0.001 (0.001) Batch 4.431 (3.431) Remain 06:23:36 loss: 0.3482 Lr: 0.02130
[2023-08-08 03:50:02,723 INFO misc.py line 115 22900] Train: [57/100][156/156] Data 0.001 (0.001) Batch 3.683 (3.432) Remain 06:23:44 loss: 0.3927 Lr: 0.02130
[2023-08-08 03:50:02,724 INFO misc.py line 129 22900] Train result: loss: 0.2352 
[2023-08-08 03:50:02,724 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 03:50:04,847 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.3046 
[2023-08-08 03:50:05,716 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.6964 
[2023-08-08 03:50:07,382 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.6981 
[2023-08-08 03:50:08,901 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.4579 
[2023-08-08 03:50:10,746 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.7639 
[2023-08-08 03:50:12,411 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.8488 
[2023-08-08 03:50:14,549 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.6874 
[2023-08-08 03:50:16,357 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.2600 
[2023-08-08 03:50:17,641 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.6829 
[2023-08-08 03:50:19,772 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3746 
[2023-08-08 03:50:20,298 INFO evaluator.py line 122 22900] Test: [11/24] Loss 0.8655 
[2023-08-08 03:50:21,832 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.7828 
[2023-08-08 03:50:24,539 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.2929 
[2023-08-08 03:50:26,219 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.9038 
[2023-08-08 03:50:28,246 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.2911 
[2023-08-08 03:50:30,954 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.3387 
[2023-08-08 03:50:33,657 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.5800 
[2023-08-08 03:50:35,503 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.2981 
[2023-08-08 03:50:36,254 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.3016 
[2023-08-08 03:50:37,140 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.6989 
[2023-08-08 03:50:39,401 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.5098 
[2023-08-08 03:50:41,366 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.7374 
[2023-08-08 03:50:43,211 INFO evaluator.py line 122 22900] Test: [23/24] Loss 3.1944 
[2023-08-08 03:50:45,148 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.9298 
[2023-08-08 03:50:45,196 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2248/0.3088/0.6880.
[2023-08-08 03:50:45,196 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6439/0.9631
[2023-08-08 03:50:45,196 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9452/0.9903
[2023-08-08 03:50:45,196 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1365/0.3833
[2023-08-08 03:50:45,196 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1366/0.1900
[2023-08-08 03:50:45,196 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6130/0.6884
[2023-08-08 03:50:45,196 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.3771/0.5834
[2023-08-08 03:50:45,196 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5404/0.6564
[2023-08-08 03:50:45,196 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1713/0.2020
[2023-08-08 03:50:45,196 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1250/0.2514
[2023-08-08 03:50:45,196 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0209/0.0210
[2023-08-08 03:50:45,196 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 03:50:45,196 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0216/0.0221
[2023-08-08 03:50:45,196 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.2267/0.4362
[2023-08-08 03:50:45,196 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0616/0.0742
[2023-08-08 03:50:45,196 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0113/0.0127
[2023-08-08 03:50:45,196 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0019/0.0019
[2023-08-08 03:50:45,196 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0649/0.0734
[2023-08-08 03:50:45,196 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.3116/0.4568
[2023-08-08 03:50:45,196 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 03:50:45,196 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0866/0.1687
[2023-08-08 03:50:45,197 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 03:50:45,197 INFO misc.py line 152 22900] Currently Best mIoU: 0.2562
[2023-08-08 03:50:45,197 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 03:50:51,182 INFO misc.py line 115 22900] Train: [58/100][1/156] Data 1.431 (1.431) Batch 5.175 (5.175) Remain 09:38:30 loss: 0.1123 Lr: 0.02129
[2023-08-08 03:50:54,589 INFO misc.py line 115 22900] Train: [58/100][2/156] Data 0.001 (0.001) Batch 3.406 (3.406) Remain 06:20:43 loss: 0.3016 Lr: 0.02129
[2023-08-08 03:50:57,040 INFO misc.py line 115 22900] Train: [58/100][3/156] Data 0.001 (0.001) Batch 2.451 (2.451) Remain 04:33:53 loss: 0.0794 Lr: 0.02128
[2023-08-08 03:50:59,629 INFO misc.py line 115 22900] Train: [58/100][4/156] Data 0.007 (0.007) Batch 2.590 (2.590) Remain 04:49:20 loss: 0.2398 Lr: 0.02128
[2023-08-08 03:51:03,630 INFO misc.py line 115 22900] Train: [58/100][5/156] Data 0.001 (0.004) Batch 4.000 (3.295) Remain 06:08:05 loss: 0.2231 Lr: 0.02127
[2023-08-08 03:51:06,447 INFO misc.py line 115 22900] Train: [58/100][6/156] Data 0.001 (0.003) Batch 2.818 (3.136) Remain 05:50:16 loss: 0.1541 Lr: 0.02127
[2023-08-08 03:51:09,785 INFO misc.py line 115 22900] Train: [58/100][7/156] Data 0.001 (0.002) Batch 3.338 (3.186) Remain 05:55:51 loss: 0.2198 Lr: 0.02126
[2023-08-08 03:51:12,468 INFO misc.py line 115 22900] Train: [58/100][8/156] Data 0.001 (0.002) Batch 2.683 (3.086) Remain 05:44:33 loss: 0.4087 Lr: 0.02126
[2023-08-08 03:51:13,791 INFO misc.py line 115 22900] Train: [58/100][9/156] Data 0.001 (0.002) Batch 1.323 (2.792) Remain 05:11:42 loss: 0.1235 Lr: 0.02125
[2023-08-08 03:51:17,269 INFO misc.py line 115 22900] Train: [58/100][10/156] Data 0.001 (0.002) Batch 3.478 (2.890) Remain 05:22:36 loss: 0.1548 Lr: 0.02125
[2023-08-08 03:51:21,634 INFO misc.py line 115 22900] Train: [58/100][11/156] Data 0.001 (0.002) Batch 4.365 (3.074) Remain 05:43:08 loss: 0.3030 Lr: 0.02124
[2023-08-08 03:51:25,732 INFO misc.py line 115 22900] Train: [58/100][12/156] Data 0.001 (0.002) Batch 4.098 (3.188) Remain 05:55:46 loss: 0.3097 Lr: 0.02124
[2023-08-08 03:51:29,225 INFO misc.py line 115 22900] Train: [58/100][13/156] Data 0.001 (0.002) Batch 3.494 (3.219) Remain 05:59:08 loss: 0.4046 Lr: 0.02123
[2023-08-08 03:51:32,282 INFO misc.py line 115 22900] Train: [58/100][14/156] Data 0.001 (0.002) Batch 3.056 (3.204) Remain 05:57:26 loss: 0.0798 Lr: 0.02123
[2023-08-08 03:51:35,515 INFO misc.py line 115 22900] Train: [58/100][15/156] Data 0.001 (0.002) Batch 3.233 (3.206) Remain 05:57:39 loss: 0.2023 Lr: 0.02122
[2023-08-08 03:51:39,195 INFO misc.py line 115 22900] Train: [58/100][16/156] Data 0.001 (0.002) Batch 3.680 (3.243) Remain 06:01:40 loss: 0.1476 Lr: 0.02122
[2023-08-08 03:51:43,767 INFO misc.py line 115 22900] Train: [58/100][17/156] Data 0.001 (0.001) Batch 4.572 (3.338) Remain 06:12:12 loss: 0.3839 Lr: 0.02121
[2023-08-08 03:51:48,243 INFO misc.py line 115 22900] Train: [58/100][18/156] Data 0.001 (0.001) Batch 4.476 (3.414) Remain 06:20:36 loss: 0.3140 Lr: 0.02120
[2023-08-08 03:51:51,951 INFO misc.py line 115 22900] Train: [58/100][19/156] Data 0.001 (0.001) Batch 3.708 (3.432) Remain 06:22:36 loss: 0.1577 Lr: 0.02120
[2023-08-08 03:51:56,424 INFO misc.py line 115 22900] Train: [58/100][20/156] Data 0.001 (0.001) Batch 4.473 (3.493) Remain 06:29:22 loss: 0.4338 Lr: 0.02119
[2023-08-08 03:51:58,625 INFO misc.py line 115 22900] Train: [58/100][21/156] Data 0.001 (0.001) Batch 2.202 (3.421) Remain 06:21:19 loss: 0.2272 Lr: 0.02119
[2023-08-08 03:52:01,999 INFO misc.py line 115 22900] Train: [58/100][22/156] Data 0.001 (0.001) Batch 3.373 (3.419) Remain 06:20:58 loss: 0.2815 Lr: 0.02118
[2023-08-08 03:52:06,050 INFO misc.py line 115 22900] Train: [58/100][23/156] Data 0.001 (0.001) Batch 4.051 (3.451) Remain 06:24:26 loss: 0.2849 Lr: 0.02118
[2023-08-08 03:52:10,162 INFO misc.py line 115 22900] Train: [58/100][24/156] Data 0.001 (0.001) Batch 4.112 (3.482) Remain 06:27:53 loss: 0.5961 Lr: 0.02117
[2023-08-08 03:52:13,427 INFO misc.py line 115 22900] Train: [58/100][25/156] Data 0.001 (0.001) Batch 3.265 (3.472) Remain 06:26:44 loss: 0.1726 Lr: 0.02117
[2023-08-08 03:52:16,881 INFO misc.py line 115 22900] Train: [58/100][26/156] Data 0.001 (0.001) Batch 3.454 (3.471) Remain 06:26:35 loss: 0.1298 Lr: 0.02116
[2023-08-08 03:52:20,937 INFO misc.py line 115 22900] Train: [58/100][27/156] Data 0.001 (0.001) Batch 4.056 (3.496) Remain 06:29:14 loss: 0.3227 Lr: 0.02116
[2023-08-08 03:52:25,058 INFO misc.py line 115 22900] Train: [58/100][28/156] Data 0.001 (0.001) Batch 4.121 (3.521) Remain 06:31:58 loss: 0.2247 Lr: 0.02115
[2023-08-08 03:52:28,389 INFO misc.py line 115 22900] Train: [58/100][29/156] Data 0.001 (0.001) Batch 3.331 (3.513) Remain 06:31:06 loss: 0.1374 Lr: 0.02115
[2023-08-08 03:52:30,807 INFO misc.py line 115 22900] Train: [58/100][30/156] Data 0.001 (0.001) Batch 2.418 (3.473) Remain 06:26:31 loss: 0.1349 Lr: 0.02114
[2023-08-08 03:52:33,968 INFO misc.py line 115 22900] Train: [58/100][31/156] Data 0.001 (0.001) Batch 3.160 (3.462) Remain 06:25:13 loss: 0.2028 Lr: 0.02114
[2023-08-08 03:52:37,456 INFO misc.py line 115 22900] Train: [58/100][32/156] Data 0.001 (0.001) Batch 3.488 (3.463) Remain 06:25:16 loss: 0.1636 Lr: 0.02113
[2023-08-08 03:52:41,518 INFO misc.py line 115 22900] Train: [58/100][33/156] Data 0.001 (0.001) Batch 4.062 (3.483) Remain 06:27:26 loss: 0.3131 Lr: 0.02113
[2023-08-08 03:52:44,404 INFO misc.py line 115 22900] Train: [58/100][34/156] Data 0.001 (0.001) Batch 2.886 (3.463) Remain 06:25:14 loss: 0.2428 Lr: 0.02112
[2023-08-08 03:52:48,423 INFO misc.py line 115 22900] Train: [58/100][35/156] Data 0.001 (0.001) Batch 4.019 (3.481) Remain 06:27:06 loss: 0.4417 Lr: 0.02112
[2023-08-08 03:52:51,727 INFO misc.py line 115 22900] Train: [58/100][36/156] Data 0.001 (0.001) Batch 3.304 (3.475) Remain 06:26:27 loss: 0.1214 Lr: 0.02111
[2023-08-08 03:52:55,664 INFO misc.py line 115 22900] Train: [58/100][37/156] Data 0.001 (0.001) Batch 3.937 (3.489) Remain 06:27:54 loss: 0.3223 Lr: 0.02111
[2023-08-08 03:52:58,816 INFO misc.py line 115 22900] Train: [58/100][38/156] Data 0.001 (0.001) Batch 3.152 (3.479) Remain 06:26:47 loss: 0.2649 Lr: 0.02110
[2023-08-08 03:53:02,798 INFO misc.py line 115 22900] Train: [58/100][39/156] Data 0.001 (0.001) Batch 3.982 (3.493) Remain 06:28:16 loss: 0.3389 Lr: 0.02109
[2023-08-08 03:53:06,038 INFO misc.py line 115 22900] Train: [58/100][40/156] Data 0.001 (0.001) Batch 3.240 (3.486) Remain 06:27:27 loss: 0.2194 Lr: 0.02109
[2023-08-08 03:53:08,645 INFO misc.py line 115 22900] Train: [58/100][41/156] Data 0.001 (0.001) Batch 2.607 (3.463) Remain 06:24:49 loss: 0.1236 Lr: 0.02108
[2023-08-08 03:53:11,335 INFO misc.py line 115 22900] Train: [58/100][42/156] Data 0.001 (0.001) Batch 2.690 (3.443) Remain 06:22:34 loss: 0.1301 Lr: 0.02108
[2023-08-08 03:53:15,210 INFO misc.py line 115 22900] Train: [58/100][43/156] Data 0.001 (0.001) Batch 3.875 (3.454) Remain 06:23:42 loss: 0.3058 Lr: 0.02107
[2023-08-08 03:53:17,804 INFO misc.py line 115 22900] Train: [58/100][44/156] Data 0.001 (0.001) Batch 2.593 (3.433) Remain 06:21:19 loss: 0.1255 Lr: 0.02107
[2023-08-08 03:53:21,525 INFO misc.py line 115 22900] Train: [58/100][45/156] Data 0.001 (0.001) Batch 3.721 (3.440) Remain 06:22:01 loss: 0.2127 Lr: 0.02106
[2023-08-08 03:53:24,963 INFO misc.py line 115 22900] Train: [58/100][46/156] Data 0.001 (0.001) Batch 3.439 (3.440) Remain 06:21:57 loss: 0.2712 Lr: 0.02106
[2023-08-08 03:53:28,972 INFO misc.py line 115 22900] Train: [58/100][47/156] Data 0.001 (0.001) Batch 4.008 (3.453) Remain 06:23:20 loss: 0.2072 Lr: 0.02105
[2023-08-08 03:53:31,852 INFO misc.py line 115 22900] Train: [58/100][48/156] Data 0.001 (0.001) Batch 2.880 (3.440) Remain 06:21:52 loss: 0.2519 Lr: 0.02105
[2023-08-08 03:53:36,440 INFO misc.py line 115 22900] Train: [58/100][49/156] Data 0.001 (0.001) Batch 4.588 (3.465) Remain 06:24:34 loss: 0.3709 Lr: 0.02104
[2023-08-08 03:53:40,018 INFO misc.py line 115 22900] Train: [58/100][50/156] Data 0.001 (0.001) Batch 3.578 (3.468) Remain 06:24:47 loss: 0.3349 Lr: 0.02104
[2023-08-08 03:53:43,502 INFO misc.py line 115 22900] Train: [58/100][51/156] Data 0.001 (0.001) Batch 3.484 (3.468) Remain 06:24:46 loss: 0.2373 Lr: 0.02103
[2023-08-08 03:53:47,034 INFO misc.py line 115 22900] Train: [58/100][52/156] Data 0.001 (0.001) Batch 3.531 (3.469) Remain 06:24:51 loss: 0.1391 Lr: 0.02103
[2023-08-08 03:53:50,535 INFO misc.py line 115 22900] Train: [58/100][53/156] Data 0.001 (0.001) Batch 3.502 (3.470) Remain 06:24:52 loss: 0.1557 Lr: 0.02102
[2023-08-08 03:53:54,013 INFO misc.py line 115 22900] Train: [58/100][54/156] Data 0.001 (0.001) Batch 3.478 (3.470) Remain 06:24:49 loss: 0.3790 Lr: 0.02102
[2023-08-08 03:53:57,323 INFO misc.py line 115 22900] Train: [58/100][55/156] Data 0.001 (0.001) Batch 3.310 (3.467) Remain 06:24:25 loss: 0.1474 Lr: 0.02101
[2023-08-08 03:54:00,257 INFO misc.py line 115 22900] Train: [58/100][56/156] Data 0.001 (0.001) Batch 2.934 (3.457) Remain 06:23:15 loss: 0.1072 Lr: 0.02101
[2023-08-08 03:54:04,347 INFO misc.py line 115 22900] Train: [58/100][57/156] Data 0.001 (0.001) Batch 4.090 (3.469) Remain 06:24:30 loss: 0.1233 Lr: 0.02100
[2023-08-08 03:54:08,428 INFO misc.py line 115 22900] Train: [58/100][58/156] Data 0.001 (0.001) Batch 4.081 (3.480) Remain 06:25:40 loss: 0.1663 Lr: 0.02100
[2023-08-08 03:54:11,776 INFO misc.py line 115 22900] Train: [58/100][59/156] Data 0.001 (0.001) Batch 3.348 (3.477) Remain 06:25:21 loss: 0.3246 Lr: 0.02099
[2023-08-08 03:54:15,251 INFO misc.py line 115 22900] Train: [58/100][60/156] Data 0.001 (0.001) Batch 3.475 (3.477) Remain 06:25:17 loss: 0.1583 Lr: 0.02098
[2023-08-08 03:54:18,666 INFO misc.py line 115 22900] Train: [58/100][61/156] Data 0.001 (0.001) Batch 3.415 (3.476) Remain 06:25:07 loss: 0.1764 Lr: 0.02098
[2023-08-08 03:54:22,125 INFO misc.py line 115 22900] Train: [58/100][62/156] Data 0.001 (0.001) Batch 3.458 (3.476) Remain 06:25:01 loss: 0.2836 Lr: 0.02097
[2023-08-08 03:54:25,879 INFO misc.py line 115 22900] Train: [58/100][63/156] Data 0.001 (0.001) Batch 3.754 (3.481) Remain 06:25:28 loss: 0.1925 Lr: 0.02097
[2023-08-08 03:54:28,577 INFO misc.py line 115 22900] Train: [58/100][64/156] Data 0.001 (0.001) Batch 2.698 (3.468) Remain 06:24:00 loss: 0.1647 Lr: 0.02096
[2023-08-08 03:54:31,752 INFO misc.py line 115 22900] Train: [58/100][65/156] Data 0.001 (0.001) Batch 3.176 (3.463) Remain 06:23:25 loss: 0.1014 Lr: 0.02096
[2023-08-08 03:54:35,705 INFO misc.py line 115 22900] Train: [58/100][66/156] Data 0.001 (0.001) Batch 3.952 (3.471) Remain 06:24:13 loss: 0.1575 Lr: 0.02095
[2023-08-08 03:54:39,800 INFO misc.py line 115 22900] Train: [58/100][67/156] Data 0.001 (0.001) Batch 4.095 (3.481) Remain 06:25:14 loss: 0.3219 Lr: 0.02095
[2023-08-08 03:54:41,853 INFO misc.py line 115 22900] Train: [58/100][68/156] Data 0.001 (0.001) Batch 2.053 (3.459) Remain 06:22:45 loss: 0.2401 Lr: 0.02094
[2023-08-08 03:54:45,496 INFO misc.py line 115 22900] Train: [58/100][69/156] Data 0.001 (0.001) Batch 3.643 (3.461) Remain 06:23:00 loss: 0.4355 Lr: 0.02094
[2023-08-08 03:54:49,546 INFO misc.py line 115 22900] Train: [58/100][70/156] Data 0.001 (0.001) Batch 4.051 (3.470) Remain 06:23:55 loss: 0.6057 Lr: 0.02093
[2023-08-08 03:54:54,145 INFO misc.py line 115 22900] Train: [58/100][71/156] Data 0.001 (0.001) Batch 4.599 (3.487) Remain 06:25:42 loss: 0.4450 Lr: 0.02093
[2023-08-08 03:54:56,972 INFO misc.py line 115 22900] Train: [58/100][72/156] Data 0.001 (0.001) Batch 2.827 (3.477) Remain 06:24:35 loss: 0.1842 Lr: 0.02092
[2023-08-08 03:54:58,918 INFO misc.py line 115 22900] Train: [58/100][73/156] Data 0.001 (0.001) Batch 1.945 (3.455) Remain 06:22:06 loss: 0.2204 Lr: 0.02092
[2023-08-08 03:55:01,591 INFO misc.py line 115 22900] Train: [58/100][74/156] Data 0.001 (0.001) Batch 2.673 (3.444) Remain 06:20:49 loss: 0.2167 Lr: 0.02091
[2023-08-08 03:55:04,710 INFO misc.py line 115 22900] Train: [58/100][75/156] Data 0.001 (0.001) Batch 3.119 (3.440) Remain 06:20:16 loss: 0.2956 Lr: 0.02091
[2023-08-08 03:55:07,987 INFO misc.py line 115 22900] Train: [58/100][76/156] Data 0.001 (0.001) Batch 3.277 (3.438) Remain 06:19:58 loss: 0.2355 Lr: 0.02090
[2023-08-08 03:55:11,895 INFO misc.py line 115 22900] Train: [58/100][77/156] Data 0.001 (0.001) Batch 3.908 (3.444) Remain 06:20:37 loss: 0.1496 Lr: 0.02090
[2023-08-08 03:55:14,332 INFO misc.py line 115 22900] Train: [58/100][78/156] Data 0.001 (0.001) Batch 2.437 (3.431) Remain 06:19:04 loss: 0.2685 Lr: 0.02089
[2023-08-08 03:55:18,438 INFO misc.py line 115 22900] Train: [58/100][79/156] Data 0.001 (0.001) Batch 4.107 (3.439) Remain 06:20:00 loss: 0.1844 Lr: 0.02089
[2023-08-08 03:55:21,560 INFO misc.py line 115 22900] Train: [58/100][80/156] Data 0.001 (0.001) Batch 3.122 (3.435) Remain 06:19:29 loss: 0.2006 Lr: 0.02088
[2023-08-08 03:55:26,060 INFO misc.py line 115 22900] Train: [58/100][81/156] Data 0.001 (0.001) Batch 4.500 (3.449) Remain 06:20:56 loss: 0.3162 Lr: 0.02087
[2023-08-08 03:55:28,999 INFO misc.py line 115 22900] Train: [58/100][82/156] Data 0.001 (0.001) Batch 2.938 (3.443) Remain 06:20:10 loss: 0.1242 Lr: 0.02087
[2023-08-08 03:55:32,458 INFO misc.py line 115 22900] Train: [58/100][83/156] Data 0.001 (0.001) Batch 3.459 (3.443) Remain 06:20:08 loss: 0.2058 Lr: 0.02086
[2023-08-08 03:55:36,530 INFO misc.py line 115 22900] Train: [58/100][84/156] Data 0.001 (0.001) Batch 4.072 (3.450) Remain 06:20:56 loss: 0.2664 Lr: 0.02086
[2023-08-08 03:55:39,669 INFO misc.py line 115 22900] Train: [58/100][85/156] Data 0.001 (0.001) Batch 3.139 (3.447) Remain 06:20:27 loss: 0.2764 Lr: 0.02085
[2023-08-08 03:55:43,601 INFO misc.py line 115 22900] Train: [58/100][86/156] Data 0.001 (0.001) Batch 3.932 (3.453) Remain 06:21:02 loss: 0.1931 Lr: 0.02085
[2023-08-08 03:55:47,638 INFO misc.py line 115 22900] Train: [58/100][87/156] Data 0.001 (0.001) Batch 4.037 (3.460) Remain 06:21:45 loss: 0.1822 Lr: 0.02084
[2023-08-08 03:55:50,541 INFO misc.py line 115 22900] Train: [58/100][88/156] Data 0.001 (0.001) Batch 2.902 (3.453) Remain 06:20:58 loss: 0.1098 Lr: 0.02084
[2023-08-08 03:55:53,429 INFO misc.py line 115 22900] Train: [58/100][89/156] Data 0.001 (0.001) Batch 2.888 (3.446) Remain 06:20:11 loss: 0.2487 Lr: 0.02083
[2023-08-08 03:55:56,009 INFO misc.py line 115 22900] Train: [58/100][90/156] Data 0.001 (0.001) Batch 2.580 (3.436) Remain 06:19:02 loss: 0.2068 Lr: 0.02083
[2023-08-08 03:55:59,449 INFO misc.py line 115 22900] Train: [58/100][91/156] Data 0.001 (0.001) Batch 3.440 (3.436) Remain 06:18:59 loss: 0.2656 Lr: 0.02082
[2023-08-08 03:56:03,577 INFO misc.py line 115 22900] Train: [58/100][92/156] Data 0.001 (0.001) Batch 4.128 (3.444) Remain 06:19:47 loss: 0.3219 Lr: 0.02082
[2023-08-08 03:56:07,155 INFO misc.py line 115 22900] Train: [58/100][93/156] Data 0.001 (0.001) Batch 3.579 (3.446) Remain 06:19:53 loss: 0.1625 Lr: 0.02081
[2023-08-08 03:56:10,750 INFO misc.py line 115 22900] Train: [58/100][94/156] Data 0.001 (0.001) Batch 3.595 (3.447) Remain 06:20:00 loss: 0.1547 Lr: 0.02081
[2023-08-08 03:56:14,088 INFO misc.py line 115 22900] Train: [58/100][95/156] Data 0.001 (0.001) Batch 3.338 (3.446) Remain 06:19:49 loss: 0.2333 Lr: 0.02080
[2023-08-08 03:56:17,167 INFO misc.py line 115 22900] Train: [58/100][96/156] Data 0.001 (0.001) Batch 3.079 (3.442) Remain 06:19:19 loss: 0.1663 Lr: 0.02080
[2023-08-08 03:56:21,538 INFO misc.py line 115 22900] Train: [58/100][97/156] Data 0.001 (0.001) Batch 4.371 (3.452) Remain 06:20:21 loss: 0.2893 Lr: 0.02079
[2023-08-08 03:56:25,537 INFO misc.py line 115 22900] Train: [58/100][98/156] Data 0.001 (0.001) Batch 3.999 (3.458) Remain 06:20:56 loss: 0.6373 Lr: 0.02079
[2023-08-08 03:56:29,538 INFO misc.py line 115 22900] Train: [58/100][99/156] Data 0.001 (0.001) Batch 4.001 (3.464) Remain 06:21:30 loss: 0.2125 Lr: 0.02078
[2023-08-08 03:56:33,195 INFO misc.py line 115 22900] Train: [58/100][100/156] Data 0.001 (0.001) Batch 3.657 (3.466) Remain 06:21:40 loss: 0.0791 Lr: 0.02078
[2023-08-08 03:56:35,946 INFO misc.py line 115 22900] Train: [58/100][101/156] Data 0.001 (0.001) Batch 2.751 (3.458) Remain 06:20:48 loss: 0.1612 Lr: 0.02077
[2023-08-08 03:56:39,610 INFO misc.py line 115 22900] Train: [58/100][102/156] Data 0.001 (0.001) Batch 3.664 (3.460) Remain 06:20:58 loss: 0.2651 Lr: 0.02077
[2023-08-08 03:56:42,447 INFO misc.py line 115 22900] Train: [58/100][103/156] Data 0.001 (0.001) Batch 2.837 (3.454) Remain 06:20:14 loss: 0.1042 Lr: 0.02076
[2023-08-08 03:56:45,470 INFO misc.py line 115 22900] Train: [58/100][104/156] Data 0.001 (0.001) Batch 3.023 (3.450) Remain 06:19:42 loss: 0.1224 Lr: 0.02075
[2023-08-08 03:56:48,305 INFO misc.py line 115 22900] Train: [58/100][105/156] Data 0.001 (0.001) Batch 2.835 (3.444) Remain 06:18:59 loss: 0.1899 Lr: 0.02075
[2023-08-08 03:56:51,999 INFO misc.py line 115 22900] Train: [58/100][106/156] Data 0.001 (0.001) Batch 3.695 (3.446) Remain 06:19:11 loss: 0.1372 Lr: 0.02074
[2023-08-08 03:56:55,976 INFO misc.py line 115 22900] Train: [58/100][107/156] Data 0.001 (0.001) Batch 3.976 (3.451) Remain 06:19:42 loss: 0.2036 Lr: 0.02074
[2023-08-08 03:56:59,526 INFO misc.py line 115 22900] Train: [58/100][108/156] Data 0.001 (0.001) Batch 3.550 (3.452) Remain 06:19:44 loss: 0.1707 Lr: 0.02073
[2023-08-08 03:57:03,154 INFO misc.py line 115 22900] Train: [58/100][109/156] Data 0.001 (0.001) Batch 3.628 (3.454) Remain 06:19:52 loss: 0.2130 Lr: 0.02073
[2023-08-08 03:57:06,045 INFO misc.py line 115 22900] Train: [58/100][110/156] Data 0.001 (0.001) Batch 2.891 (3.449) Remain 06:19:14 loss: 0.1555 Lr: 0.02072
[2023-08-08 03:57:08,663 INFO misc.py line 115 22900] Train: [58/100][111/156] Data 0.001 (0.001) Batch 2.618 (3.441) Remain 06:18:19 loss: 0.1056 Lr: 0.02072
[2023-08-08 03:57:12,120 INFO misc.py line 115 22900] Train: [58/100][112/156] Data 0.001 (0.001) Batch 3.457 (3.441) Remain 06:18:17 loss: 0.1848 Lr: 0.02071
[2023-08-08 03:57:15,721 INFO misc.py line 115 22900] Train: [58/100][113/156] Data 0.001 (0.001) Batch 3.600 (3.443) Remain 06:18:23 loss: 0.1079 Lr: 0.02071
[2023-08-08 03:57:19,894 INFO misc.py line 115 22900] Train: [58/100][114/156] Data 0.001 (0.001) Batch 4.173 (3.449) Remain 06:19:03 loss: 0.3108 Lr: 0.02070
[2023-08-08 03:57:22,769 INFO misc.py line 115 22900] Train: [58/100][115/156] Data 0.001 (0.001) Batch 2.876 (3.444) Remain 06:18:26 loss: 0.2497 Lr: 0.02070
[2023-08-08 03:57:25,649 INFO misc.py line 115 22900] Train: [58/100][116/156] Data 0.001 (0.001) Batch 2.880 (3.439) Remain 06:17:50 loss: 0.2964 Lr: 0.02069
[2023-08-08 03:57:29,377 INFO misc.py line 115 22900] Train: [58/100][117/156] Data 0.001 (0.001) Batch 3.728 (3.442) Remain 06:18:03 loss: 0.1003 Lr: 0.02069
[2023-08-08 03:57:32,162 INFO misc.py line 115 22900] Train: [58/100][118/156] Data 0.001 (0.001) Batch 2.785 (3.436) Remain 06:17:22 loss: 0.0997 Lr: 0.02068
[2023-08-08 03:57:35,764 INFO misc.py line 115 22900] Train: [58/100][119/156] Data 0.001 (0.001) Batch 3.601 (3.437) Remain 06:17:28 loss: 0.2340 Lr: 0.02068
[2023-08-08 03:57:40,064 INFO misc.py line 115 22900] Train: [58/100][120/156] Data 0.001 (0.001) Batch 4.300 (3.445) Remain 06:18:13 loss: 0.3383 Lr: 0.02067
[2023-08-08 03:57:43,313 INFO misc.py line 115 22900] Train: [58/100][121/156] Data 0.001 (0.001) Batch 3.249 (3.443) Remain 06:17:58 loss: 0.2039 Lr: 0.02067
[2023-08-08 03:57:46,737 INFO misc.py line 115 22900] Train: [58/100][122/156] Data 0.001 (0.001) Batch 3.424 (3.443) Remain 06:17:54 loss: 0.1668 Lr: 0.02066
[2023-08-08 03:57:49,381 INFO misc.py line 115 22900] Train: [58/100][123/156] Data 0.001 (0.001) Batch 2.644 (3.436) Remain 06:17:07 loss: 0.3082 Lr: 0.02066
[2023-08-08 03:57:53,513 INFO misc.py line 115 22900] Train: [58/100][124/156] Data 0.001 (0.001) Batch 4.132 (3.442) Remain 06:17:41 loss: 0.2237 Lr: 0.02065
[2023-08-08 03:57:57,368 INFO misc.py line 115 22900] Train: [58/100][125/156] Data 0.001 (0.001) Batch 3.855 (3.445) Remain 06:18:00 loss: 0.2128 Lr: 0.02065
[2023-08-08 03:58:00,665 INFO misc.py line 115 22900] Train: [58/100][126/156] Data 0.001 (0.001) Batch 3.296 (3.444) Remain 06:17:49 loss: 0.1093 Lr: 0.02064
[2023-08-08 03:58:03,257 INFO misc.py line 115 22900] Train: [58/100][127/156] Data 0.001 (0.001) Batch 2.593 (3.437) Remain 06:17:00 loss: 0.2095 Lr: 0.02063
[2023-08-08 03:58:05,144 INFO misc.py line 115 22900] Train: [58/100][128/156] Data 0.001 (0.001) Batch 1.886 (3.425) Remain 06:15:35 loss: 0.0778 Lr: 0.02063
[2023-08-08 03:58:09,170 INFO misc.py line 115 22900] Train: [58/100][129/156] Data 0.001 (0.001) Batch 4.026 (3.430) Remain 06:16:03 loss: 0.3333 Lr: 0.02062
[2023-08-08 03:58:12,849 INFO misc.py line 115 22900] Train: [58/100][130/156] Data 0.001 (0.001) Batch 3.679 (3.432) Remain 06:16:12 loss: 0.3700 Lr: 0.02062
[2023-08-08 03:58:16,054 INFO misc.py line 115 22900] Train: [58/100][131/156] Data 0.001 (0.001) Batch 3.205 (3.430) Remain 06:15:57 loss: 0.1407 Lr: 0.02061
[2023-08-08 03:58:19,642 INFO misc.py line 115 22900] Train: [58/100][132/156] Data 0.001 (0.001) Batch 3.588 (3.431) Remain 06:16:02 loss: 0.1612 Lr: 0.02061
[2023-08-08 03:58:23,712 INFO misc.py line 115 22900] Train: [58/100][133/156] Data 0.001 (0.001) Batch 4.070 (3.436) Remain 06:16:31 loss: 0.1365 Lr: 0.02060
[2023-08-08 03:58:27,803 INFO misc.py line 115 22900] Train: [58/100][134/156] Data 0.001 (0.001) Batch 4.092 (3.441) Remain 06:17:00 loss: 0.2382 Lr: 0.02060
[2023-08-08 03:58:31,989 INFO misc.py line 115 22900] Train: [58/100][135/156] Data 0.001 (0.001) Batch 4.186 (3.447) Remain 06:17:34 loss: 0.2737 Lr: 0.02059
[2023-08-08 03:58:36,075 INFO misc.py line 115 22900] Train: [58/100][136/156] Data 0.001 (0.001) Batch 4.086 (3.451) Remain 06:18:02 loss: 0.3416 Lr: 0.02059
[2023-08-08 03:58:39,724 INFO misc.py line 115 22900] Train: [58/100][137/156] Data 0.001 (0.001) Batch 3.648 (3.453) Remain 06:18:08 loss: 0.1640 Lr: 0.02058
[2023-08-08 03:58:43,344 INFO misc.py line 115 22900] Train: [58/100][138/156] Data 0.001 (0.001) Batch 3.620 (3.454) Remain 06:18:13 loss: 0.2735 Lr: 0.02058
[2023-08-08 03:58:46,403 INFO misc.py line 115 22900] Train: [58/100][139/156] Data 0.001 (0.001) Batch 3.059 (3.451) Remain 06:17:50 loss: 0.2115 Lr: 0.02057
[2023-08-08 03:58:50,352 INFO misc.py line 115 22900] Train: [58/100][140/156] Data 0.001 (0.001) Batch 3.949 (3.455) Remain 06:18:11 loss: 0.2671 Lr: 0.02057
[2023-08-08 03:58:53,909 INFO misc.py line 115 22900] Train: [58/100][141/156] Data 0.001 (0.001) Batch 3.557 (3.456) Remain 06:18:12 loss: 0.1926 Lr: 0.02056
[2023-08-08 03:58:57,037 INFO misc.py line 115 22900] Train: [58/100][142/156] Data 0.001 (0.001) Batch 3.128 (3.453) Remain 06:17:53 loss: 0.1696 Lr: 0.02056
[2023-08-08 03:59:01,020 INFO misc.py line 115 22900] Train: [58/100][143/156] Data 0.001 (0.001) Batch 3.983 (3.457) Remain 06:18:15 loss: 0.1809 Lr: 0.02055
[2023-08-08 03:59:04,818 INFO misc.py line 115 22900] Train: [58/100][144/156] Data 0.001 (0.001) Batch 3.798 (3.459) Remain 06:18:27 loss: 0.2030 Lr: 0.02055
[2023-08-08 03:59:08,892 INFO misc.py line 115 22900] Train: [58/100][145/156] Data 0.001 (0.001) Batch 4.074 (3.464) Remain 06:18:52 loss: 0.1657 Lr: 0.02054
[2023-08-08 03:59:12,112 INFO misc.py line 115 22900] Train: [58/100][146/156] Data 0.001 (0.001) Batch 3.220 (3.462) Remain 06:18:37 loss: 0.1410 Lr: 0.02054
[2023-08-08 03:59:15,895 INFO misc.py line 115 22900] Train: [58/100][147/156] Data 0.001 (0.001) Batch 3.784 (3.464) Remain 06:18:49 loss: 0.1433 Lr: 0.02053
[2023-08-08 03:59:20,093 INFO misc.py line 115 22900] Train: [58/100][148/156] Data 0.001 (0.001) Batch 4.197 (3.469) Remain 06:19:18 loss: 0.4118 Lr: 0.02053
[2023-08-08 03:59:24,508 INFO misc.py line 115 22900] Train: [58/100][149/156] Data 0.001 (0.001) Batch 4.415 (3.476) Remain 06:19:57 loss: 0.3318 Lr: 0.02052
[2023-08-08 03:59:27,036 INFO misc.py line 115 22900] Train: [58/100][150/156] Data 0.001 (0.001) Batch 2.528 (3.469) Remain 06:19:12 loss: 0.1969 Lr: 0.02051
[2023-08-08 03:59:29,673 INFO misc.py line 115 22900] Train: [58/100][151/156] Data 0.001 (0.001) Batch 2.637 (3.464) Remain 06:18:31 loss: 0.2576 Lr: 0.02051
[2023-08-08 03:59:33,200 INFO misc.py line 115 22900] Train: [58/100][152/156] Data 0.001 (0.001) Batch 3.526 (3.464) Remain 06:18:31 loss: 0.2176 Lr: 0.02050
[2023-08-08 03:59:36,325 INFO misc.py line 115 22900] Train: [58/100][153/156] Data 0.001 (0.001) Batch 3.125 (3.462) Remain 06:18:12 loss: 0.1233 Lr: 0.02050
[2023-08-08 03:59:39,285 INFO misc.py line 115 22900] Train: [58/100][154/156] Data 0.001 (0.001) Batch 2.960 (3.459) Remain 06:17:47 loss: 0.1020 Lr: 0.02049
[2023-08-08 03:59:42,340 INFO misc.py line 115 22900] Train: [58/100][155/156] Data 0.001 (0.001) Batch 3.055 (3.456) Remain 06:17:26 loss: 0.1308 Lr: 0.02049
[2023-08-08 03:59:46,844 INFO misc.py line 115 22900] Train: [58/100][156/156] Data 0.001 (0.001) Batch 4.504 (3.463) Remain 06:18:08 loss: 0.4526 Lr: 0.02048
[2023-08-08 03:59:46,845 INFO misc.py line 129 22900] Train result: loss: 0.2275 
[2023-08-08 03:59:46,845 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 03:59:48,939 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.4356 
[2023-08-08 03:59:49,809 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.6277 
[2023-08-08 03:59:51,472 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.8386 
[2023-08-08 03:59:52,993 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.4104 
[2023-08-08 03:59:54,838 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.6131 
[2023-08-08 03:59:56,500 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.6187 
[2023-08-08 03:59:58,644 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.3578 
[2023-08-08 04:00:00,450 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.3474 
[2023-08-08 04:00:01,734 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.6560 
[2023-08-08 04:00:03,866 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.4339 
[2023-08-08 04:00:04,391 INFO evaluator.py line 122 22900] Test: [11/24] Loss 0.9346 
[2023-08-08 04:00:05,921 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.8087 
[2023-08-08 04:00:08,636 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.0505 
[2023-08-08 04:00:10,316 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.9076 
[2023-08-08 04:00:12,340 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.3099 
[2023-08-08 04:00:15,050 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.1380 
[2023-08-08 04:00:17,755 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.4587 
[2023-08-08 04:00:19,602 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.3650 
[2023-08-08 04:00:20,351 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.0325 
[2023-08-08 04:00:21,236 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.8868 
[2023-08-08 04:00:23,494 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3394 
[2023-08-08 04:00:25,457 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.6369 
[2023-08-08 04:00:27,303 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.9086 
[2023-08-08 04:00:29,239 INFO evaluator.py line 122 22900] Test: [24/24] Loss 2.1377 
[2023-08-08 04:00:29,288 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2226/0.3189/0.6848.
[2023-08-08 04:00:29,288 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6089/0.9451
[2023-08-08 04:00:29,288 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9234/0.9911
[2023-08-08 04:00:29,288 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1430/0.3197
[2023-08-08 04:00:29,288 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1177/0.1784
[2023-08-08 04:00:29,288 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6023/0.7156
[2023-08-08 04:00:29,288 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.2974/0.4006
[2023-08-08 04:00:29,288 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.4774/0.7354
[2023-08-08 04:00:29,288 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1976/0.2230
[2023-08-08 04:00:29,288 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1944/0.3098
[2023-08-08 04:00:29,288 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0209/0.0210
[2023-08-08 04:00:29,288 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 04:00:29,289 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0131/0.0133
[2023-08-08 04:00:29,289 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.2397/0.3774
[2023-08-08 04:00:29,289 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0400/0.0446
[2023-08-08 04:00:29,289 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0428/0.0469
[2023-08-08 04:00:29,289 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-08 04:00:29,289 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.1334/0.1944
[2023-08-08 04:00:29,289 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.3121/0.7233
[2023-08-08 04:00:29,289 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 04:00:29,289 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0875/0.1385
[2023-08-08 04:00:29,289 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 04:00:29,289 INFO misc.py line 152 22900] Currently Best mIoU: 0.2562
[2023-08-08 04:00:29,289 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 04:00:33,935 INFO misc.py line 115 22900] Train: [59/100][1/156] Data 0.837 (0.837) Batch 3.849 (3.849) Remain 07:00:13 loss: 0.1070 Lr: 0.02048
[2023-08-08 04:00:37,476 INFO misc.py line 115 22900] Train: [59/100][2/156] Data 0.001 (0.001) Batch 3.542 (3.542) Remain 06:26:39 loss: 0.1806 Lr: 0.02047
[2023-08-08 04:00:40,724 INFO misc.py line 115 22900] Train: [59/100][3/156] Data 0.001 (0.001) Batch 3.248 (3.248) Remain 05:54:29 loss: 0.1614 Lr: 0.02047
[2023-08-08 04:00:44,645 INFO misc.py line 115 22900] Train: [59/100][4/156] Data 0.001 (0.001) Batch 3.921 (3.921) Remain 07:07:55 loss: 0.1191 Lr: 0.02046
[2023-08-08 04:00:48,633 INFO misc.py line 115 22900] Train: [59/100][5/156] Data 0.001 (0.001) Batch 3.988 (3.955) Remain 07:11:30 loss: 0.2989 Lr: 0.02046
[2023-08-08 04:00:51,503 INFO misc.py line 115 22900] Train: [59/100][6/156] Data 0.001 (0.001) Batch 2.870 (3.593) Remain 06:32:00 loss: 0.2135 Lr: 0.02045
[2023-08-08 04:00:54,893 INFO misc.py line 115 22900] Train: [59/100][7/156] Data 0.001 (0.001) Batch 3.390 (3.542) Remain 06:26:23 loss: 0.2267 Lr: 0.02045
[2023-08-08 04:00:57,444 INFO misc.py line 115 22900] Train: [59/100][8/156] Data 0.001 (0.001) Batch 2.551 (3.344) Remain 06:04:43 loss: 0.2091 Lr: 0.02044
[2023-08-08 04:01:01,358 INFO misc.py line 115 22900] Train: [59/100][9/156] Data 0.001 (0.001) Batch 3.914 (3.439) Remain 06:15:01 loss: 0.2175 Lr: 0.02044
[2023-08-08 04:01:04,216 INFO misc.py line 115 22900] Train: [59/100][10/156] Data 0.001 (0.001) Batch 2.857 (3.356) Remain 06:05:54 loss: 0.0779 Lr: 0.02043
[2023-08-08 04:01:07,931 INFO misc.py line 115 22900] Train: [59/100][11/156] Data 0.001 (0.001) Batch 3.716 (3.401) Remain 06:10:45 loss: 0.2135 Lr: 0.02043
[2023-08-08 04:01:10,825 INFO misc.py line 115 22900] Train: [59/100][12/156] Data 0.001 (0.001) Batch 2.894 (3.344) Remain 06:04:33 loss: 0.1915 Lr: 0.02042
[2023-08-08 04:01:13,684 INFO misc.py line 115 22900] Train: [59/100][13/156] Data 0.001 (0.001) Batch 2.859 (3.296) Remain 05:59:12 loss: 0.1407 Lr: 0.02042
[2023-08-08 04:01:16,465 INFO misc.py line 115 22900] Train: [59/100][14/156] Data 0.001 (0.001) Batch 2.781 (3.249) Remain 05:54:02 loss: 0.1664 Lr: 0.02041
[2023-08-08 04:01:19,672 INFO misc.py line 115 22900] Train: [59/100][15/156] Data 0.001 (0.001) Batch 3.207 (3.246) Remain 05:53:36 loss: 0.2265 Lr: 0.02041
[2023-08-08 04:01:23,793 INFO misc.py line 115 22900] Train: [59/100][16/156] Data 0.001 (0.001) Batch 4.122 (3.313) Remain 06:00:53 loss: 0.1051 Lr: 0.02040
[2023-08-08 04:01:27,826 INFO misc.py line 115 22900] Train: [59/100][17/156] Data 0.001 (0.001) Batch 4.033 (3.364) Remain 06:06:26 loss: 0.5371 Lr: 0.02039
[2023-08-08 04:01:31,714 INFO misc.py line 115 22900] Train: [59/100][18/156] Data 0.001 (0.001) Batch 3.887 (3.399) Remain 06:10:10 loss: 0.2795 Lr: 0.02039
[2023-08-08 04:01:34,267 INFO misc.py line 115 22900] Train: [59/100][19/156] Data 0.001 (0.001) Batch 2.554 (3.346) Remain 06:04:22 loss: 0.1115 Lr: 0.02038
[2023-08-08 04:01:36,874 INFO misc.py line 115 22900] Train: [59/100][20/156] Data 0.001 (0.001) Batch 2.607 (3.303) Remain 05:59:34 loss: 0.1005 Lr: 0.02038
[2023-08-08 04:01:40,263 INFO misc.py line 115 22900] Train: [59/100][21/156] Data 0.001 (0.001) Batch 3.389 (3.308) Remain 06:00:02 loss: 0.2658 Lr: 0.02037
[2023-08-08 04:01:42,019 INFO misc.py line 115 22900] Train: [59/100][22/156] Data 0.001 (0.001) Batch 1.756 (3.226) Remain 05:51:06 loss: 0.1919 Lr: 0.02037
[2023-08-08 04:01:46,212 INFO misc.py line 115 22900] Train: [59/100][23/156] Data 0.001 (0.001) Batch 4.192 (3.274) Remain 05:56:18 loss: 0.3007 Lr: 0.02036
[2023-08-08 04:01:49,104 INFO misc.py line 115 22900] Train: [59/100][24/156] Data 0.001 (0.001) Batch 2.893 (3.256) Remain 05:54:16 loss: 0.2446 Lr: 0.02036
[2023-08-08 04:01:52,659 INFO misc.py line 115 22900] Train: [59/100][25/156] Data 0.001 (0.001) Batch 3.555 (3.270) Remain 05:55:41 loss: 0.3938 Lr: 0.02035
[2023-08-08 04:01:55,401 INFO misc.py line 115 22900] Train: [59/100][26/156] Data 0.001 (0.001) Batch 2.741 (3.247) Remain 05:53:08 loss: 0.2873 Lr: 0.02035
[2023-08-08 04:01:59,250 INFO misc.py line 115 22900] Train: [59/100][27/156] Data 0.001 (0.001) Batch 3.850 (3.272) Remain 05:55:49 loss: 0.2678 Lr: 0.02034
[2023-08-08 04:02:03,242 INFO misc.py line 115 22900] Train: [59/100][28/156] Data 0.001 (0.001) Batch 3.992 (3.301) Remain 05:58:53 loss: 0.3580 Lr: 0.02034
[2023-08-08 04:02:06,980 INFO misc.py line 115 22900] Train: [59/100][29/156] Data 0.001 (0.001) Batch 3.738 (3.318) Remain 06:00:40 loss: 0.1414 Lr: 0.02033
[2023-08-08 04:02:10,950 INFO misc.py line 115 22900] Train: [59/100][30/156] Data 0.001 (0.001) Batch 3.970 (3.342) Remain 06:03:14 loss: 0.1625 Lr: 0.02033
[2023-08-08 04:02:14,236 INFO misc.py line 115 22900] Train: [59/100][31/156] Data 0.001 (0.001) Batch 3.286 (3.340) Remain 06:02:58 loss: 0.2569 Lr: 0.02032
[2023-08-08 04:02:18,100 INFO misc.py line 115 22900] Train: [59/100][32/156] Data 0.001 (0.001) Batch 3.864 (3.358) Remain 06:04:52 loss: 0.1639 Lr: 0.02032
[2023-08-08 04:02:21,569 INFO misc.py line 115 22900] Train: [59/100][33/156] Data 0.001 (0.001) Batch 3.469 (3.361) Remain 06:05:13 loss: 0.2086 Lr: 0.02031
[2023-08-08 04:02:24,860 INFO misc.py line 115 22900] Train: [59/100][34/156] Data 0.001 (0.001) Batch 3.291 (3.359) Remain 06:04:55 loss: 0.1813 Lr: 0.02031
[2023-08-08 04:02:28,510 INFO misc.py line 115 22900] Train: [59/100][35/156] Data 0.001 (0.001) Batch 3.650 (3.368) Remain 06:05:51 loss: 0.2990 Lr: 0.02030
[2023-08-08 04:02:32,125 INFO misc.py line 115 22900] Train: [59/100][36/156] Data 0.001 (0.001) Batch 3.615 (3.376) Remain 06:06:36 loss: 0.2673 Lr: 0.02030
[2023-08-08 04:02:35,350 INFO misc.py line 115 22900] Train: [59/100][37/156] Data 0.001 (0.001) Batch 3.226 (3.371) Remain 06:06:04 loss: 0.1779 Lr: 0.02029
[2023-08-08 04:02:38,923 INFO misc.py line 115 22900] Train: [59/100][38/156] Data 0.001 (0.001) Batch 3.572 (3.377) Remain 06:06:38 loss: 0.3463 Lr: 0.02029
[2023-08-08 04:02:42,030 INFO misc.py line 115 22900] Train: [59/100][39/156] Data 0.001 (0.001) Batch 3.107 (3.370) Remain 06:05:46 loss: 0.1172 Lr: 0.02028
[2023-08-08 04:02:46,294 INFO misc.py line 115 22900] Train: [59/100][40/156] Data 0.001 (0.001) Batch 4.265 (3.394) Remain 06:08:20 loss: 0.3500 Lr: 0.02028
[2023-08-08 04:02:49,796 INFO misc.py line 115 22900] Train: [59/100][41/156] Data 0.001 (0.001) Batch 3.502 (3.397) Remain 06:08:35 loss: 0.0985 Lr: 0.02027
[2023-08-08 04:02:53,701 INFO misc.py line 115 22900] Train: [59/100][42/156] Data 0.001 (0.001) Batch 3.905 (3.410) Remain 06:09:56 loss: 0.2254 Lr: 0.02026
[2023-08-08 04:02:57,206 INFO misc.py line 115 22900] Train: [59/100][43/156] Data 0.001 (0.001) Batch 3.505 (3.412) Remain 06:10:08 loss: 0.3241 Lr: 0.02026
[2023-08-08 04:03:00,411 INFO misc.py line 115 22900] Train: [59/100][44/156] Data 0.001 (0.001) Batch 3.205 (3.407) Remain 06:09:32 loss: 0.1909 Lr: 0.02025
[2023-08-08 04:03:04,494 INFO misc.py line 115 22900] Train: [59/100][45/156] Data 0.001 (0.001) Batch 4.083 (3.423) Remain 06:11:13 loss: 0.2746 Lr: 0.02025
[2023-08-08 04:03:07,525 INFO misc.py line 115 22900] Train: [59/100][46/156] Data 0.001 (0.001) Batch 3.032 (3.414) Remain 06:10:11 loss: 0.1967 Lr: 0.02024
[2023-08-08 04:03:09,862 INFO misc.py line 115 22900] Train: [59/100][47/156] Data 0.001 (0.001) Batch 2.337 (3.390) Remain 06:07:28 loss: 0.2045 Lr: 0.02024
[2023-08-08 04:03:13,598 INFO misc.py line 115 22900] Train: [59/100][48/156] Data 0.001 (0.001) Batch 3.736 (3.397) Remain 06:08:15 loss: 0.1699 Lr: 0.02023
[2023-08-08 04:03:16,918 INFO misc.py line 115 22900] Train: [59/100][49/156] Data 0.001 (0.001) Batch 3.320 (3.396) Remain 06:08:01 loss: 0.1487 Lr: 0.02023
[2023-08-08 04:03:20,368 INFO misc.py line 115 22900] Train: [59/100][50/156] Data 0.001 (0.001) Batch 3.450 (3.397) Remain 06:08:05 loss: 0.2535 Lr: 0.02022
[2023-08-08 04:03:23,506 INFO misc.py line 115 22900] Train: [59/100][51/156] Data 0.001 (0.001) Batch 3.138 (3.391) Remain 06:07:26 loss: 0.3133 Lr: 0.02022
[2023-08-08 04:03:27,205 INFO misc.py line 115 22900] Train: [59/100][52/156] Data 0.001 (0.001) Batch 3.699 (3.398) Remain 06:08:04 loss: 0.1921 Lr: 0.02021
[2023-08-08 04:03:30,162 INFO misc.py line 115 22900] Train: [59/100][53/156] Data 0.001 (0.001) Batch 2.957 (3.389) Remain 06:07:03 loss: 0.1364 Lr: 0.02021
[2023-08-08 04:03:34,630 INFO misc.py line 115 22900] Train: [59/100][54/156] Data 0.001 (0.001) Batch 4.468 (3.410) Remain 06:09:17 loss: 0.3045 Lr: 0.02020
[2023-08-08 04:03:37,749 INFO misc.py line 115 22900] Train: [59/100][55/156] Data 0.001 (0.001) Batch 3.119 (3.404) Remain 06:08:37 loss: 0.2401 Lr: 0.02020
[2023-08-08 04:03:40,854 INFO misc.py line 115 22900] Train: [59/100][56/156] Data 0.001 (0.001) Batch 3.105 (3.399) Remain 06:07:57 loss: 0.1358 Lr: 0.02019
[2023-08-08 04:03:44,088 INFO misc.py line 115 22900] Train: [59/100][57/156] Data 0.001 (0.001) Batch 3.234 (3.396) Remain 06:07:34 loss: 0.2065 Lr: 0.02019
[2023-08-08 04:03:48,626 INFO misc.py line 115 22900] Train: [59/100][58/156] Data 0.001 (0.001) Batch 4.538 (3.416) Remain 06:09:46 loss: 0.2865 Lr: 0.02018
[2023-08-08 04:03:51,471 INFO misc.py line 115 22900] Train: [59/100][59/156] Data 0.001 (0.001) Batch 2.846 (3.406) Remain 06:08:36 loss: 0.2697 Lr: 0.02018
[2023-08-08 04:03:56,090 INFO misc.py line 115 22900] Train: [59/100][60/156] Data 0.001 (0.001) Batch 4.618 (3.427) Remain 06:10:51 loss: 0.3471 Lr: 0.02017
[2023-08-08 04:03:59,724 INFO misc.py line 115 22900] Train: [59/100][61/156] Data 0.001 (0.001) Batch 3.634 (3.431) Remain 06:11:10 loss: 0.1719 Lr: 0.02017
[2023-08-08 04:04:03,874 INFO misc.py line 115 22900] Train: [59/100][62/156] Data 0.001 (0.001) Batch 4.150 (3.443) Remain 06:12:26 loss: 0.3429 Lr: 0.02016
[2023-08-08 04:04:07,961 INFO misc.py line 115 22900] Train: [59/100][63/156] Data 0.001 (0.001) Batch 4.087 (3.454) Remain 06:13:32 loss: 0.3794 Lr: 0.02016
[2023-08-08 04:04:11,465 INFO misc.py line 115 22900] Train: [59/100][64/156] Data 0.001 (0.001) Batch 3.504 (3.455) Remain 06:13:34 loss: 0.3094 Lr: 0.02015
[2023-08-08 04:04:14,356 INFO misc.py line 115 22900] Train: [59/100][65/156] Data 0.001 (0.001) Batch 2.891 (3.446) Remain 06:12:32 loss: 0.1629 Lr: 0.02015
[2023-08-08 04:04:18,064 INFO misc.py line 115 22900] Train: [59/100][66/156] Data 0.001 (0.001) Batch 3.708 (3.450) Remain 06:12:55 loss: 0.2563 Lr: 0.02014
[2023-08-08 04:04:22,408 INFO misc.py line 115 22900] Train: [59/100][67/156] Data 0.001 (0.001) Batch 4.344 (3.464) Remain 06:14:22 loss: 0.5701 Lr: 0.02013
[2023-08-08 04:04:26,147 INFO misc.py line 115 22900] Train: [59/100][68/156] Data 0.001 (0.001) Batch 3.739 (3.468) Remain 06:14:46 loss: 0.2850 Lr: 0.02013
[2023-08-08 04:04:30,244 INFO misc.py line 115 22900] Train: [59/100][69/156] Data 0.001 (0.001) Batch 4.097 (3.478) Remain 06:15:45 loss: 0.3052 Lr: 0.02012
[2023-08-08 04:04:33,040 INFO misc.py line 115 22900] Train: [59/100][70/156] Data 0.001 (0.001) Batch 2.796 (3.467) Remain 06:14:35 loss: 0.1195 Lr: 0.02012
[2023-08-08 04:04:35,826 INFO misc.py line 115 22900] Train: [59/100][71/156] Data 0.001 (0.001) Batch 2.786 (3.457) Remain 06:13:27 loss: 0.1455 Lr: 0.02011
[2023-08-08 04:04:39,220 INFO misc.py line 115 22900] Train: [59/100][72/156] Data 0.001 (0.001) Batch 3.394 (3.456) Remain 06:13:17 loss: 0.0946 Lr: 0.02011
[2023-08-08 04:04:42,935 INFO misc.py line 115 22900] Train: [59/100][73/156] Data 0.001 (0.001) Batch 3.715 (3.460) Remain 06:13:38 loss: 0.2975 Lr: 0.02010
[2023-08-08 04:04:46,097 INFO misc.py line 115 22900] Train: [59/100][74/156] Data 0.001 (0.001) Batch 3.163 (3.456) Remain 06:13:07 loss: 0.2952 Lr: 0.02010
[2023-08-08 04:04:49,662 INFO misc.py line 115 22900] Train: [59/100][75/156] Data 0.001 (0.001) Batch 3.564 (3.457) Remain 06:13:13 loss: 0.1214 Lr: 0.02009
[2023-08-08 04:04:52,665 INFO misc.py line 115 22900] Train: [59/100][76/156] Data 0.001 (0.001) Batch 3.003 (3.451) Remain 06:12:30 loss: 0.0599 Lr: 0.02009
[2023-08-08 04:04:55,919 INFO misc.py line 115 22900] Train: [59/100][77/156] Data 0.001 (0.001) Batch 3.254 (3.449) Remain 06:12:09 loss: 0.1712 Lr: 0.02008
[2023-08-08 04:04:59,798 INFO misc.py line 115 22900] Train: [59/100][78/156] Data 0.001 (0.001) Batch 3.879 (3.454) Remain 06:12:43 loss: 0.3020 Lr: 0.02008
[2023-08-08 04:05:03,564 INFO misc.py line 115 22900] Train: [59/100][79/156] Data 0.001 (0.001) Batch 3.766 (3.458) Remain 06:13:06 loss: 0.2138 Lr: 0.02007
[2023-08-08 04:05:06,946 INFO misc.py line 115 22900] Train: [59/100][80/156] Data 0.001 (0.001) Batch 3.382 (3.457) Remain 06:12:56 loss: 0.2891 Lr: 0.02007
[2023-08-08 04:05:11,480 INFO misc.py line 115 22900] Train: [59/100][81/156] Data 0.001 (0.001) Batch 4.534 (3.471) Remain 06:14:22 loss: 0.2349 Lr: 0.02006
[2023-08-08 04:05:13,856 INFO misc.py line 115 22900] Train: [59/100][82/156] Data 0.001 (0.001) Batch 2.376 (3.457) Remain 06:12:49 loss: 0.0685 Lr: 0.02006
[2023-08-08 04:05:17,838 INFO misc.py line 115 22900] Train: [59/100][83/156] Data 0.001 (0.001) Batch 3.982 (3.464) Remain 06:13:28 loss: 0.3290 Lr: 0.02005
[2023-08-08 04:05:21,814 INFO misc.py line 115 22900] Train: [59/100][84/156] Data 0.001 (0.001) Batch 3.976 (3.470) Remain 06:14:05 loss: 0.2143 Lr: 0.02005
[2023-08-08 04:05:25,502 INFO misc.py line 115 22900] Train: [59/100][85/156] Data 0.001 (0.001) Batch 3.688 (3.473) Remain 06:14:19 loss: 0.3132 Lr: 0.02004
[2023-08-08 04:05:29,494 INFO misc.py line 115 22900] Train: [59/100][86/156] Data 0.001 (0.001) Batch 3.992 (3.479) Remain 06:14:56 loss: 0.2382 Lr: 0.02004
[2023-08-08 04:05:32,107 INFO misc.py line 115 22900] Train: [59/100][87/156] Data 0.001 (0.001) Batch 2.613 (3.469) Remain 06:13:46 loss: 0.0984 Lr: 0.02003
[2023-08-08 04:05:35,309 INFO misc.py line 115 22900] Train: [59/100][88/156] Data 0.001 (0.001) Batch 3.202 (3.466) Remain 06:13:22 loss: 0.1157 Lr: 0.02003
[2023-08-08 04:05:39,719 INFO misc.py line 115 22900] Train: [59/100][89/156] Data 0.001 (0.001) Batch 4.409 (3.477) Remain 06:14:29 loss: 0.3521 Lr: 0.02002
[2023-08-08 04:05:42,779 INFO misc.py line 115 22900] Train: [59/100][90/156] Data 0.001 (0.001) Batch 3.061 (3.472) Remain 06:13:55 loss: 0.1846 Lr: 0.02002
[2023-08-08 04:05:46,532 INFO misc.py line 115 22900] Train: [59/100][91/156] Data 0.001 (0.001) Batch 3.753 (3.475) Remain 06:14:12 loss: 0.2346 Lr: 0.02001
[2023-08-08 04:05:50,007 INFO misc.py line 115 22900] Train: [59/100][92/156] Data 0.001 (0.001) Batch 3.474 (3.475) Remain 06:14:09 loss: 0.2989 Lr: 0.02000
[2023-08-08 04:05:53,688 INFO misc.py line 115 22900] Train: [59/100][93/156] Data 0.001 (0.001) Batch 3.681 (3.477) Remain 06:14:20 loss: 0.2650 Lr: 0.02000
[2023-08-08 04:05:57,516 INFO misc.py line 115 22900] Train: [59/100][94/156] Data 0.001 (0.001) Batch 3.829 (3.481) Remain 06:14:41 loss: 0.1782 Lr: 0.01999
[2023-08-08 04:06:01,550 INFO misc.py line 115 22900] Train: [59/100][95/156] Data 0.001 (0.001) Batch 4.033 (3.487) Remain 06:15:17 loss: 0.1670 Lr: 0.01999
[2023-08-08 04:06:05,124 INFO misc.py line 115 22900] Train: [59/100][96/156] Data 0.001 (0.001) Batch 3.575 (3.488) Remain 06:15:19 loss: 0.1657 Lr: 0.01998
[2023-08-08 04:06:08,609 INFO misc.py line 115 22900] Train: [59/100][97/156] Data 0.001 (0.001) Batch 3.485 (3.488) Remain 06:15:15 loss: 0.1646 Lr: 0.01998
[2023-08-08 04:06:12,697 INFO misc.py line 115 22900] Train: [59/100][98/156] Data 0.001 (0.001) Batch 4.088 (3.494) Remain 06:15:53 loss: 0.1840 Lr: 0.01997
[2023-08-08 04:06:15,507 INFO misc.py line 115 22900] Train: [59/100][99/156] Data 0.001 (0.001) Batch 2.810 (3.487) Remain 06:15:03 loss: 0.0773 Lr: 0.01997
[2023-08-08 04:06:18,750 INFO misc.py line 115 22900] Train: [59/100][100/156] Data 0.001 (0.001) Batch 3.243 (3.485) Remain 06:14:43 loss: 0.1526 Lr: 0.01996
[2023-08-08 04:06:21,057 INFO misc.py line 115 22900] Train: [59/100][101/156] Data 0.001 (0.001) Batch 2.307 (3.473) Remain 06:13:22 loss: 0.1837 Lr: 0.01996
[2023-08-08 04:06:22,589 INFO misc.py line 115 22900] Train: [59/100][102/156] Data 0.001 (0.001) Batch 1.532 (3.453) Remain 06:11:13 loss: 0.0832 Lr: 0.01995
[2023-08-08 04:06:26,661 INFO misc.py line 115 22900] Train: [59/100][103/156] Data 0.001 (0.001) Batch 4.071 (3.459) Remain 06:11:49 loss: 0.1418 Lr: 0.01995
[2023-08-08 04:06:30,202 INFO misc.py line 115 22900] Train: [59/100][104/156] Data 0.001 (0.001) Batch 3.542 (3.460) Remain 06:11:51 loss: 0.1526 Lr: 0.01994
[2023-08-08 04:06:33,739 INFO misc.py line 115 22900] Train: [59/100][105/156] Data 0.001 (0.001) Batch 3.536 (3.461) Remain 06:11:52 loss: 0.2442 Lr: 0.01994
[2023-08-08 04:06:37,268 INFO misc.py line 115 22900] Train: [59/100][106/156] Data 0.001 (0.001) Batch 3.529 (3.462) Remain 06:11:53 loss: 0.1997 Lr: 0.01993
[2023-08-08 04:06:41,045 INFO misc.py line 115 22900] Train: [59/100][107/156] Data 0.001 (0.001) Batch 3.778 (3.465) Remain 06:12:09 loss: 0.1852 Lr: 0.01993
[2023-08-08 04:06:44,283 INFO misc.py line 115 22900] Train: [59/100][108/156] Data 0.001 (0.001) Batch 3.238 (3.462) Remain 06:11:52 loss: 0.2096 Lr: 0.01992
[2023-08-08 04:06:47,814 INFO misc.py line 115 22900] Train: [59/100][109/156] Data 0.001 (0.001) Batch 3.530 (3.463) Remain 06:11:52 loss: 0.1464 Lr: 0.01992
[2023-08-08 04:06:51,180 INFO misc.py line 115 22900] Train: [59/100][110/156] Data 0.001 (0.001) Batch 3.367 (3.462) Remain 06:11:43 loss: 0.1591 Lr: 0.01991
[2023-08-08 04:06:55,059 INFO misc.py line 115 22900] Train: [59/100][111/156] Data 0.001 (0.001) Batch 3.879 (3.466) Remain 06:12:04 loss: 0.4076 Lr: 0.01991
[2023-08-08 04:06:57,575 INFO misc.py line 115 22900] Train: [59/100][112/156] Data 0.001 (0.001) Batch 2.516 (3.457) Remain 06:11:05 loss: 0.1382 Lr: 0.01990
[2023-08-08 04:07:01,428 INFO misc.py line 115 22900] Train: [59/100][113/156] Data 0.001 (0.001) Batch 3.853 (3.461) Remain 06:11:25 loss: 0.1433 Lr: 0.01990
[2023-08-08 04:07:04,876 INFO misc.py line 115 22900] Train: [59/100][114/156] Data 0.001 (0.001) Batch 3.447 (3.461) Remain 06:11:20 loss: 0.1029 Lr: 0.01989
[2023-08-08 04:07:08,416 INFO misc.py line 115 22900] Train: [59/100][115/156] Data 0.001 (0.001) Batch 3.540 (3.462) Remain 06:11:21 loss: 0.2392 Lr: 0.01989
[2023-08-08 04:07:12,459 INFO misc.py line 115 22900] Train: [59/100][116/156] Data 0.001 (0.001) Batch 4.043 (3.467) Remain 06:11:51 loss: 0.5126 Lr: 0.01988
[2023-08-08 04:07:16,094 INFO misc.py line 115 22900] Train: [59/100][117/156] Data 0.001 (0.001) Batch 3.636 (3.468) Remain 06:11:57 loss: 0.2820 Lr: 0.01988
[2023-08-08 04:07:19,365 INFO misc.py line 115 22900] Train: [59/100][118/156] Data 0.001 (0.001) Batch 3.270 (3.466) Remain 06:11:43 loss: 0.2110 Lr: 0.01987
[2023-08-08 04:07:22,587 INFO misc.py line 115 22900] Train: [59/100][119/156] Data 0.001 (0.001) Batch 3.223 (3.464) Remain 06:11:26 loss: 0.1780 Lr: 0.01986
[2023-08-08 04:07:25,956 INFO misc.py line 115 22900] Train: [59/100][120/156] Data 0.001 (0.001) Batch 3.369 (3.464) Remain 06:11:17 loss: 0.1225 Lr: 0.01986
[2023-08-08 04:07:29,762 INFO misc.py line 115 22900] Train: [59/100][121/156] Data 0.001 (0.001) Batch 3.806 (3.466) Remain 06:11:32 loss: 0.2880 Lr: 0.01985
[2023-08-08 04:07:32,930 INFO misc.py line 115 22900] Train: [59/100][122/156] Data 0.001 (0.001) Batch 3.168 (3.464) Remain 06:11:12 loss: 0.2801 Lr: 0.01985
[2023-08-08 04:07:35,996 INFO misc.py line 115 22900] Train: [59/100][123/156] Data 0.001 (0.001) Batch 3.066 (3.461) Remain 06:10:48 loss: 0.1961 Lr: 0.01984
[2023-08-08 04:07:39,754 INFO misc.py line 115 22900] Train: [59/100][124/156] Data 0.001 (0.001) Batch 3.758 (3.463) Remain 06:11:00 loss: 0.1993 Lr: 0.01984
[2023-08-08 04:07:43,378 INFO misc.py line 115 22900] Train: [59/100][125/156] Data 0.001 (0.001) Batch 3.624 (3.464) Remain 06:11:05 loss: 0.2318 Lr: 0.01983
[2023-08-08 04:07:46,899 INFO misc.py line 115 22900] Train: [59/100][126/156] Data 0.001 (0.001) Batch 3.521 (3.465) Remain 06:11:05 loss: 0.2698 Lr: 0.01983
[2023-08-08 04:07:50,375 INFO misc.py line 115 22900] Train: [59/100][127/156] Data 0.001 (0.001) Batch 3.476 (3.465) Remain 06:11:02 loss: 0.2183 Lr: 0.01982
[2023-08-08 04:07:53,824 INFO misc.py line 115 22900] Train: [59/100][128/156] Data 0.001 (0.001) Batch 3.449 (3.465) Remain 06:10:57 loss: 0.0842 Lr: 0.01982
[2023-08-08 04:07:56,852 INFO misc.py line 115 22900] Train: [59/100][129/156] Data 0.001 (0.001) Batch 3.028 (3.461) Remain 06:10:32 loss: 0.1565 Lr: 0.01981
[2023-08-08 04:07:59,692 INFO misc.py line 115 22900] Train: [59/100][130/156] Data 0.001 (0.001) Batch 2.840 (3.456) Remain 06:09:57 loss: 0.0993 Lr: 0.01981
[2023-08-08 04:08:03,587 INFO misc.py line 115 22900] Train: [59/100][131/156] Data 0.001 (0.001) Batch 3.895 (3.460) Remain 06:10:15 loss: 0.1727 Lr: 0.01980
[2023-08-08 04:08:06,962 INFO misc.py line 115 22900] Train: [59/100][132/156] Data 0.001 (0.001) Batch 3.374 (3.459) Remain 06:10:08 loss: 0.3130 Lr: 0.01980
[2023-08-08 04:08:11,099 INFO misc.py line 115 22900] Train: [59/100][133/156] Data 0.001 (0.001) Batch 4.137 (3.464) Remain 06:10:38 loss: 0.2875 Lr: 0.01979
[2023-08-08 04:08:14,547 INFO misc.py line 115 22900] Train: [59/100][134/156] Data 0.001 (0.001) Batch 3.449 (3.464) Remain 06:10:33 loss: 0.3074 Lr: 0.01979
[2023-08-08 04:08:18,372 INFO misc.py line 115 22900] Train: [59/100][135/156] Data 0.001 (0.001) Batch 3.824 (3.467) Remain 06:10:47 loss: 0.2795 Lr: 0.01978
[2023-08-08 04:08:22,723 INFO misc.py line 115 22900] Train: [59/100][136/156] Data 0.001 (0.001) Batch 4.352 (3.474) Remain 06:11:27 loss: 0.2364 Lr: 0.01978
[2023-08-08 04:08:24,638 INFO misc.py line 115 22900] Train: [59/100][137/156] Data 0.001 (0.001) Batch 1.914 (3.462) Remain 06:10:08 loss: 0.2312 Lr: 0.01977
[2023-08-08 04:08:27,362 INFO misc.py line 115 22900] Train: [59/100][138/156] Data 0.001 (0.001) Batch 2.725 (3.457) Remain 06:09:30 loss: 0.2030 Lr: 0.01977
[2023-08-08 04:08:29,708 INFO misc.py line 115 22900] Train: [59/100][139/156] Data 0.001 (0.001) Batch 2.346 (3.448) Remain 06:08:34 loss: 0.2280 Lr: 0.01976
[2023-08-08 04:08:33,880 INFO misc.py line 115 22900] Train: [59/100][140/156] Data 0.001 (0.001) Batch 4.172 (3.454) Remain 06:09:05 loss: 0.3737 Lr: 0.01976
[2023-08-08 04:08:38,235 INFO misc.py line 115 22900] Train: [59/100][141/156] Data 0.001 (0.001) Batch 4.355 (3.460) Remain 06:09:43 loss: 0.2433 Lr: 0.01975
[2023-08-08 04:08:40,533 INFO misc.py line 115 22900] Train: [59/100][142/156] Data 0.001 (0.001) Batch 2.298 (3.452) Remain 06:08:46 loss: 0.1232 Lr: 0.01975
[2023-08-08 04:08:44,272 INFO misc.py line 115 22900] Train: [59/100][143/156] Data 0.001 (0.001) Batch 3.739 (3.454) Remain 06:08:56 loss: 0.1284 Lr: 0.01974
[2023-08-08 04:08:47,068 INFO misc.py line 115 22900] Train: [59/100][144/156] Data 0.001 (0.001) Batch 2.796 (3.449) Remain 06:08:22 loss: 0.0893 Lr: 0.01974
[2023-08-08 04:08:51,246 INFO misc.py line 115 22900] Train: [59/100][145/156] Data 0.001 (0.001) Batch 4.178 (3.454) Remain 06:08:52 loss: 0.0965 Lr: 0.01973
[2023-08-08 04:08:55,353 INFO misc.py line 115 22900] Train: [59/100][146/156] Data 0.001 (0.001) Batch 4.107 (3.459) Remain 06:09:17 loss: 0.2355 Lr: 0.01972
[2023-08-08 04:08:59,366 INFO misc.py line 115 22900] Train: [59/100][147/156] Data 0.001 (0.001) Batch 4.013 (3.463) Remain 06:09:39 loss: 0.2290 Lr: 0.01972
[2023-08-08 04:09:00,989 INFO misc.py line 115 22900] Train: [59/100][148/156] Data 0.001 (0.001) Batch 1.623 (3.450) Remain 06:08:14 loss: 0.0929 Lr: 0.01971
[2023-08-08 04:09:05,072 INFO misc.py line 115 22900] Train: [59/100][149/156] Data 0.001 (0.001) Batch 4.083 (3.454) Remain 06:08:38 loss: 0.2090 Lr: 0.01971
[2023-08-08 04:09:09,068 INFO misc.py line 115 22900] Train: [59/100][150/156] Data 0.001 (0.001) Batch 3.996 (3.458) Remain 06:08:58 loss: 0.3960 Lr: 0.01970
[2023-08-08 04:09:12,581 INFO misc.py line 115 22900] Train: [59/100][151/156] Data 0.001 (0.001) Batch 3.513 (3.458) Remain 06:08:57 loss: 0.3999 Lr: 0.01970
[2023-08-08 04:09:15,830 INFO misc.py line 115 22900] Train: [59/100][152/156] Data 0.001 (0.001) Batch 3.249 (3.457) Remain 06:08:45 loss: 0.1582 Lr: 0.01969
[2023-08-08 04:09:18,460 INFO misc.py line 115 22900] Train: [59/100][153/156] Data 0.001 (0.001) Batch 2.630 (3.452) Remain 06:08:06 loss: 0.0406 Lr: 0.01969
[2023-08-08 04:09:22,563 INFO misc.py line 115 22900] Train: [59/100][154/156] Data 0.001 (0.001) Batch 4.103 (3.456) Remain 06:08:30 loss: 0.2764 Lr: 0.01968
[2023-08-08 04:09:25,946 INFO misc.py line 115 22900] Train: [59/100][155/156] Data 0.001 (0.001) Batch 3.383 (3.455) Remain 06:08:24 loss: 0.1211 Lr: 0.01968
[2023-08-08 04:09:29,175 INFO misc.py line 115 22900] Train: [59/100][156/156] Data 0.001 (0.001) Batch 3.229 (3.454) Remain 06:08:11 loss: 0.1547 Lr: 0.01967
[2023-08-08 04:09:29,175 INFO misc.py line 129 22900] Train result: loss: 0.2180 
[2023-08-08 04:09:29,175 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 04:09:31,282 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.1484 
[2023-08-08 04:09:32,152 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.6743 
[2023-08-08 04:09:33,818 INFO evaluator.py line 122 22900] Test: [3/24] Loss 1.1315 
[2023-08-08 04:09:35,339 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.2597 
[2023-08-08 04:09:37,183 INFO evaluator.py line 122 22900] Test: [5/24] Loss 2.0615 
[2023-08-08 04:09:38,847 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.7030 
[2023-08-08 04:09:40,983 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.7386 
[2023-08-08 04:09:42,790 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.2977 
[2023-08-08 04:09:44,074 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.4759 
[2023-08-08 04:09:46,206 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3574 
[2023-08-08 04:09:46,731 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.3935 
[2023-08-08 04:09:48,263 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.9192 
[2023-08-08 04:09:50,972 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.3369 
[2023-08-08 04:09:52,652 INFO evaluator.py line 122 22900] Test: [14/24] Loss 1.2068 
[2023-08-08 04:09:54,676 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.4392 
[2023-08-08 04:09:57,384 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.4238 
[2023-08-08 04:10:00,090 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.6326 
[2023-08-08 04:10:01,938 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.7374 
[2023-08-08 04:10:02,685 INFO evaluator.py line 122 22900] Test: [19/24] Loss 0.7640 
[2023-08-08 04:10:03,569 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.8248 
[2023-08-08 04:10:05,833 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3464 
[2023-08-08 04:10:07,797 INFO evaluator.py line 122 22900] Test: [22/24] Loss 2.0708 
[2023-08-08 04:10:09,643 INFO evaluator.py line 122 22900] Test: [23/24] Loss 3.5074 
[2023-08-08 04:10:11,580 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.7781 
[2023-08-08 04:10:11,627 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2199/0.3150/0.6839.
[2023-08-08 04:10:11,627 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6509/0.9597
[2023-08-08 04:10:11,627 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9381/0.9931
[2023-08-08 04:10:11,627 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1459/0.4227
[2023-08-08 04:10:11,627 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1464/0.3829
[2023-08-08 04:10:11,627 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6360/0.7387
[2023-08-08 04:10:11,627 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.2630/0.3242
[2023-08-08 04:10:11,627 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.4666/0.6919
[2023-08-08 04:10:11,627 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1210/0.1257
[2023-08-08 04:10:11,627 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1912/0.4023
[2023-08-08 04:10:11,627 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0026/0.0026
[2023-08-08 04:10:11,627 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 04:10:11,627 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0100/0.0103
[2023-08-08 04:10:11,627 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1633/0.3104
[2023-08-08 04:10:11,627 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0642/0.0759
[2023-08-08 04:10:11,627 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0486/0.0562
[2023-08-08 04:10:11,627 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.1347/0.1398
[2023-08-08 04:10:11,627 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.1309/0.2204
[2023-08-08 04:10:11,627 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.1943/0.3015
[2023-08-08 04:10:11,627 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 04:10:11,627 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0906/0.1413
[2023-08-08 04:10:11,627 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 04:10:11,627 INFO misc.py line 152 22900] Currently Best mIoU: 0.2562
[2023-08-08 04:10:11,627 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 04:10:16,699 INFO misc.py line 115 22900] Train: [60/100][1/156] Data 1.058 (1.058) Batch 4.264 (4.264) Remain 07:34:28 loss: 0.1646 Lr: 0.01967
[2023-08-08 04:10:19,781 INFO misc.py line 115 22900] Train: [60/100][2/156] Data 0.001 (0.001) Batch 3.082 (3.082) Remain 05:28:26 loss: 0.2399 Lr: 0.01966
[2023-08-08 04:10:23,131 INFO misc.py line 115 22900] Train: [60/100][3/156] Data 0.001 (0.001) Batch 3.350 (3.350) Remain 05:56:55 loss: 0.0853 Lr: 0.01966
[2023-08-08 04:10:26,463 INFO misc.py line 115 22900] Train: [60/100][4/156] Data 0.001 (0.001) Batch 3.332 (3.332) Remain 05:55:00 loss: 0.3038 Lr: 0.01965
[2023-08-08 04:10:30,521 INFO misc.py line 115 22900] Train: [60/100][5/156] Data 0.001 (0.001) Batch 4.058 (3.695) Remain 06:33:36 loss: 0.2592 Lr: 0.01965
[2023-08-08 04:10:34,140 INFO misc.py line 115 22900] Train: [60/100][6/156] Data 0.001 (0.001) Batch 3.619 (3.670) Remain 06:30:50 loss: 0.2599 Lr: 0.01964
[2023-08-08 04:10:37,424 INFO misc.py line 115 22900] Train: [60/100][7/156] Data 0.001 (0.001) Batch 3.284 (3.573) Remain 06:20:30 loss: 0.1318 Lr: 0.01964
[2023-08-08 04:10:40,170 INFO misc.py line 115 22900] Train: [60/100][8/156] Data 0.001 (0.001) Batch 2.745 (3.408) Remain 06:02:49 loss: 0.1226 Lr: 0.01963
[2023-08-08 04:10:43,799 INFO misc.py line 115 22900] Train: [60/100][9/156] Data 0.001 (0.001) Batch 3.629 (3.445) Remain 06:06:41 loss: 0.2513 Lr: 0.01963
[2023-08-08 04:10:46,836 INFO misc.py line 115 22900] Train: [60/100][10/156] Data 0.001 (0.001) Batch 3.038 (3.387) Remain 06:00:26 loss: 0.1496 Lr: 0.01962
[2023-08-08 04:10:50,924 INFO misc.py line 115 22900] Train: [60/100][11/156] Data 0.001 (0.001) Batch 4.088 (3.474) Remain 06:09:42 loss: 0.2545 Lr: 0.01962
[2023-08-08 04:10:54,312 INFO misc.py line 115 22900] Train: [60/100][12/156] Data 0.001 (0.001) Batch 3.388 (3.465) Remain 06:08:38 loss: 0.3665 Lr: 0.01961
[2023-08-08 04:10:57,751 INFO misc.py line 115 22900] Train: [60/100][13/156] Data 0.001 (0.001) Batch 3.439 (3.462) Remain 06:08:17 loss: 0.1448 Lr: 0.01961
[2023-08-08 04:11:01,039 INFO misc.py line 115 22900] Train: [60/100][14/156] Data 0.001 (0.001) Batch 3.288 (3.446) Remain 06:06:33 loss: 0.2319 Lr: 0.01960
[2023-08-08 04:11:05,345 INFO misc.py line 115 22900] Train: [60/100][15/156] Data 0.001 (0.001) Batch 4.307 (3.518) Remain 06:14:07 loss: 0.4024 Lr: 0.01960
[2023-08-08 04:11:09,560 INFO misc.py line 115 22900] Train: [60/100][16/156] Data 0.001 (0.001) Batch 4.215 (3.572) Remain 06:19:46 loss: 0.2995 Lr: 0.01959
[2023-08-08 04:11:11,330 INFO misc.py line 115 22900] Train: [60/100][17/156] Data 0.001 (0.001) Batch 1.769 (3.443) Remain 06:06:01 loss: 0.0765 Lr: 0.01959
[2023-08-08 04:11:13,911 INFO misc.py line 115 22900] Train: [60/100][18/156] Data 0.001 (0.001) Batch 2.581 (3.385) Remain 05:59:51 loss: 0.1322 Lr: 0.01958
[2023-08-08 04:11:16,710 INFO misc.py line 115 22900] Train: [60/100][19/156] Data 0.001 (0.001) Batch 2.799 (3.349) Remain 05:55:54 loss: 0.0733 Lr: 0.01957
[2023-08-08 04:11:19,940 INFO misc.py line 115 22900] Train: [60/100][20/156] Data 0.001 (0.001) Batch 3.230 (3.342) Remain 05:55:06 loss: 0.2347 Lr: 0.01957
[2023-08-08 04:11:22,714 INFO misc.py line 115 22900] Train: [60/100][21/156] Data 0.001 (0.001) Batch 2.775 (3.310) Remain 05:51:42 loss: 0.0797 Lr: 0.01956
[2023-08-08 04:11:26,552 INFO misc.py line 115 22900] Train: [60/100][22/156] Data 0.001 (0.001) Batch 3.837 (3.338) Remain 05:54:36 loss: 0.0981 Lr: 0.01956
[2023-08-08 04:11:29,979 INFO misc.py line 115 22900] Train: [60/100][23/156] Data 0.001 (0.001) Batch 3.427 (3.342) Remain 05:55:01 loss: 0.2679 Lr: 0.01955
[2023-08-08 04:11:32,502 INFO misc.py line 115 22900] Train: [60/100][24/156] Data 0.001 (0.001) Batch 2.523 (3.303) Remain 05:50:49 loss: 0.1291 Lr: 0.01955
[2023-08-08 04:11:36,535 INFO misc.py line 115 22900] Train: [60/100][25/156] Data 0.001 (0.001) Batch 4.033 (3.337) Remain 05:54:17 loss: 0.5749 Lr: 0.01954
[2023-08-08 04:11:39,385 INFO misc.py line 115 22900] Train: [60/100][26/156] Data 0.001 (0.001) Batch 2.850 (3.315) Remain 05:51:59 loss: 0.1763 Lr: 0.01954
[2023-08-08 04:11:42,978 INFO misc.py line 115 22900] Train: [60/100][27/156] Data 0.001 (0.001) Batch 3.593 (3.327) Remain 05:53:09 loss: 0.1874 Lr: 0.01953
[2023-08-08 04:11:45,725 INFO misc.py line 115 22900] Train: [60/100][28/156] Data 0.001 (0.001) Batch 2.748 (3.304) Remain 05:50:38 loss: 0.2668 Lr: 0.01953
[2023-08-08 04:11:49,090 INFO misc.py line 115 22900] Train: [60/100][29/156] Data 0.001 (0.001) Batch 3.365 (3.306) Remain 05:50:50 loss: 0.1918 Lr: 0.01952
[2023-08-08 04:11:52,769 INFO misc.py line 115 22900] Train: [60/100][30/156] Data 0.001 (0.001) Batch 3.679 (3.320) Remain 05:52:14 loss: 0.2038 Lr: 0.01952
[2023-08-08 04:11:56,472 INFO misc.py line 115 22900] Train: [60/100][31/156] Data 0.001 (0.001) Batch 3.704 (3.334) Remain 05:53:38 loss: 0.2396 Lr: 0.01951
[2023-08-08 04:11:59,906 INFO misc.py line 115 22900] Train: [60/100][32/156] Data 0.001 (0.001) Batch 3.434 (3.337) Remain 05:53:57 loss: 0.1762 Lr: 0.01951
[2023-08-08 04:12:03,459 INFO misc.py line 115 22900] Train: [60/100][33/156] Data 0.001 (0.001) Batch 3.553 (3.344) Remain 05:54:39 loss: 0.2210 Lr: 0.01950
[2023-08-08 04:12:07,107 INFO misc.py line 115 22900] Train: [60/100][34/156] Data 0.001 (0.001) Batch 3.648 (3.354) Remain 05:55:38 loss: 0.2546 Lr: 0.01950
[2023-08-08 04:12:11,635 INFO misc.py line 115 22900] Train: [60/100][35/156] Data 0.001 (0.001) Batch 4.528 (3.391) Remain 05:59:28 loss: 0.2375 Lr: 0.01949
[2023-08-08 04:12:14,124 INFO misc.py line 115 22900] Train: [60/100][36/156] Data 0.001 (0.001) Batch 2.489 (3.363) Remain 05:56:31 loss: 0.1026 Lr: 0.01949
[2023-08-08 04:12:17,807 INFO misc.py line 115 22900] Train: [60/100][37/156] Data 0.001 (0.001) Batch 3.683 (3.373) Remain 05:57:27 loss: 0.2680 Lr: 0.01948
[2023-08-08 04:12:21,328 INFO misc.py line 115 22900] Train: [60/100][38/156] Data 0.001 (0.001) Batch 3.521 (3.377) Remain 05:57:51 loss: 0.1854 Lr: 0.01948
[2023-08-08 04:12:24,993 INFO misc.py line 115 22900] Train: [60/100][39/156] Data 0.001 (0.001) Batch 3.665 (3.385) Remain 05:58:38 loss: 0.2180 Lr: 0.01947
[2023-08-08 04:12:28,700 INFO misc.py line 115 22900] Train: [60/100][40/156] Data 0.001 (0.001) Batch 3.707 (3.394) Remain 05:59:30 loss: 0.1072 Lr: 0.01947
[2023-08-08 04:12:32,639 INFO misc.py line 115 22900] Train: [60/100][41/156] Data 0.001 (0.001) Batch 3.939 (3.408) Remain 06:00:58 loss: 0.1349 Lr: 0.01946
[2023-08-08 04:12:36,100 INFO misc.py line 115 22900] Train: [60/100][42/156] Data 0.001 (0.001) Batch 3.461 (3.409) Remain 06:01:03 loss: 0.1745 Lr: 0.01946
[2023-08-08 04:12:39,640 INFO misc.py line 115 22900] Train: [60/100][43/156] Data 0.001 (0.001) Batch 3.540 (3.413) Remain 06:01:21 loss: 0.1193 Lr: 0.01945
[2023-08-08 04:12:43,616 INFO misc.py line 115 22900] Train: [60/100][44/156] Data 0.001 (0.001) Batch 3.976 (3.426) Remain 06:02:44 loss: 0.2275 Lr: 0.01945
[2023-08-08 04:12:48,079 INFO misc.py line 115 22900] Train: [60/100][45/156] Data 0.001 (0.001) Batch 4.463 (3.451) Remain 06:05:18 loss: 0.2798 Lr: 0.01944
[2023-08-08 04:12:52,069 INFO misc.py line 115 22900] Train: [60/100][46/156] Data 0.001 (0.001) Batch 3.990 (3.464) Remain 06:06:34 loss: 0.1164 Lr: 0.01944
[2023-08-08 04:12:55,168 INFO misc.py line 115 22900] Train: [60/100][47/156] Data 0.001 (0.001) Batch 3.099 (3.455) Remain 06:05:38 loss: 0.2310 Lr: 0.01943
[2023-08-08 04:12:59,310 INFO misc.py line 115 22900] Train: [60/100][48/156] Data 0.001 (0.001) Batch 4.142 (3.471) Remain 06:07:11 loss: 0.1932 Lr: 0.01942
[2023-08-08 04:13:03,202 INFO misc.py line 115 22900] Train: [60/100][49/156] Data 0.001 (0.001) Batch 3.891 (3.480) Remain 06:08:06 loss: 0.4443 Lr: 0.01942
[2023-08-08 04:13:05,450 INFO misc.py line 115 22900] Train: [60/100][50/156] Data 0.001 (0.001) Batch 2.248 (3.454) Remain 06:05:16 loss: 0.0854 Lr: 0.01941
[2023-08-08 04:13:09,084 INFO misc.py line 115 22900] Train: [60/100][51/156] Data 0.001 (0.001) Batch 3.635 (3.457) Remain 06:05:36 loss: 0.1672 Lr: 0.01941
[2023-08-08 04:13:12,489 INFO misc.py line 115 22900] Train: [60/100][52/156] Data 0.001 (0.001) Batch 3.405 (3.456) Remain 06:05:26 loss: 0.2612 Lr: 0.01940
[2023-08-08 04:13:15,034 INFO misc.py line 115 22900] Train: [60/100][53/156] Data 0.001 (0.001) Batch 2.544 (3.438) Remain 06:03:27 loss: 0.1446 Lr: 0.01940
[2023-08-08 04:13:19,655 INFO misc.py line 115 22900] Train: [60/100][54/156] Data 0.001 (0.001) Batch 4.621 (3.461) Remain 06:05:51 loss: 0.3485 Lr: 0.01939
[2023-08-08 04:13:22,265 INFO misc.py line 115 22900] Train: [60/100][55/156] Data 0.001 (0.001) Batch 2.610 (3.445) Remain 06:04:03 loss: 0.1966 Lr: 0.01939
[2023-08-08 04:13:24,905 INFO misc.py line 115 22900] Train: [60/100][56/156] Data 0.001 (0.001) Batch 2.641 (3.430) Remain 06:02:24 loss: 0.1037 Lr: 0.01938
[2023-08-08 04:13:28,960 INFO misc.py line 115 22900] Train: [60/100][57/156] Data 0.001 (0.001) Batch 4.055 (3.441) Remain 06:03:34 loss: 0.3011 Lr: 0.01938
[2023-08-08 04:13:33,202 INFO misc.py line 115 22900] Train: [60/100][58/156] Data 0.001 (0.001) Batch 4.241 (3.456) Remain 06:05:03 loss: 0.2719 Lr: 0.01937
[2023-08-08 04:13:36,492 INFO misc.py line 115 22900] Train: [60/100][59/156] Data 0.001 (0.001) Batch 3.290 (3.453) Remain 06:04:40 loss: 0.4926 Lr: 0.01937
[2023-08-08 04:13:39,604 INFO misc.py line 115 22900] Train: [60/100][60/156] Data 0.001 (0.001) Batch 3.112 (3.447) Remain 06:03:59 loss: 0.0841 Lr: 0.01936
[2023-08-08 04:13:43,532 INFO misc.py line 115 22900] Train: [60/100][61/156] Data 0.001 (0.001) Batch 3.928 (3.455) Remain 06:04:48 loss: 0.2976 Lr: 0.01936
[2023-08-08 04:13:46,262 INFO misc.py line 115 22900] Train: [60/100][62/156] Data 0.001 (0.001) Batch 2.729 (3.443) Remain 06:03:27 loss: 0.1364 Lr: 0.01935
[2023-08-08 04:13:49,164 INFO misc.py line 115 22900] Train: [60/100][63/156] Data 0.001 (0.001) Batch 2.902 (3.434) Remain 06:02:26 loss: 0.0817 Lr: 0.01935
[2023-08-08 04:13:52,612 INFO misc.py line 115 22900] Train: [60/100][64/156] Data 0.001 (0.001) Batch 3.448 (3.434) Remain 06:02:24 loss: 0.2232 Lr: 0.01934
[2023-08-08 04:13:55,892 INFO misc.py line 115 22900] Train: [60/100][65/156] Data 0.001 (0.001) Batch 3.281 (3.432) Remain 06:02:05 loss: 0.1410 Lr: 0.01934
[2023-08-08 04:13:59,113 INFO misc.py line 115 22900] Train: [60/100][66/156] Data 0.001 (0.001) Batch 3.221 (3.428) Remain 06:01:41 loss: 0.1757 Lr: 0.01933
[2023-08-08 04:14:02,806 INFO misc.py line 115 22900] Train: [60/100][67/156] Data 0.001 (0.001) Batch 3.693 (3.432) Remain 06:02:03 loss: 0.2360 Lr: 0.01933
[2023-08-08 04:14:06,812 INFO misc.py line 115 22900] Train: [60/100][68/156] Data 0.001 (0.001) Batch 4.006 (3.441) Remain 06:02:56 loss: 0.4540 Lr: 0.01932
[2023-08-08 04:14:09,677 INFO misc.py line 115 22900] Train: [60/100][69/156] Data 0.001 (0.001) Batch 2.865 (3.433) Remain 06:01:57 loss: 0.1749 Lr: 0.01932
[2023-08-08 04:14:13,588 INFO misc.py line 115 22900] Train: [60/100][70/156] Data 0.001 (0.001) Batch 3.911 (3.440) Remain 06:02:39 loss: 0.2795 Lr: 0.01931
[2023-08-08 04:14:16,025 INFO misc.py line 115 22900] Train: [60/100][71/156] Data 0.001 (0.001) Batch 2.437 (3.425) Remain 06:01:02 loss: 0.1336 Lr: 0.01931
[2023-08-08 04:14:19,693 INFO misc.py line 115 22900] Train: [60/100][72/156] Data 0.001 (0.001) Batch 3.668 (3.428) Remain 06:01:21 loss: 0.1987 Lr: 0.01930
[2023-08-08 04:14:23,400 INFO misc.py line 115 22900] Train: [60/100][73/156] Data 0.001 (0.001) Batch 3.706 (3.432) Remain 06:01:43 loss: 0.2451 Lr: 0.01930
[2023-08-08 04:14:25,851 INFO misc.py line 115 22900] Train: [60/100][74/156] Data 0.001 (0.001) Batch 2.451 (3.419) Remain 06:00:12 loss: 0.0870 Lr: 0.01929
[2023-08-08 04:14:29,260 INFO misc.py line 115 22900] Train: [60/100][75/156] Data 0.001 (0.001) Batch 3.409 (3.418) Remain 06:00:08 loss: 0.2733 Lr: 0.01929
[2023-08-08 04:14:32,866 INFO misc.py line 115 22900] Train: [60/100][76/156] Data 0.001 (0.001) Batch 3.606 (3.421) Remain 06:00:20 loss: 0.2342 Lr: 0.01928
[2023-08-08 04:14:36,531 INFO misc.py line 115 22900] Train: [60/100][77/156] Data 0.001 (0.001) Batch 3.665 (3.424) Remain 06:00:38 loss: 0.1777 Lr: 0.01928
[2023-08-08 04:14:40,090 INFO misc.py line 115 22900] Train: [60/100][78/156] Data 0.001 (0.001) Batch 3.559 (3.426) Remain 06:00:46 loss: 0.1696 Lr: 0.01927
[2023-08-08 04:14:43,532 INFO misc.py line 115 22900] Train: [60/100][79/156] Data 0.001 (0.001) Batch 3.442 (3.426) Remain 06:00:44 loss: 0.1353 Lr: 0.01926
[2023-08-08 04:14:46,720 INFO misc.py line 115 22900] Train: [60/100][80/156] Data 0.001 (0.001) Batch 3.188 (3.423) Remain 06:00:21 loss: 0.1537 Lr: 0.01926
[2023-08-08 04:14:49,751 INFO misc.py line 115 22900] Train: [60/100][81/156] Data 0.001 (0.001) Batch 3.031 (3.418) Remain 05:59:45 loss: 0.2280 Lr: 0.01925
[2023-08-08 04:14:53,500 INFO misc.py line 115 22900] Train: [60/100][82/156] Data 0.001 (0.001) Batch 3.749 (3.422) Remain 06:00:08 loss: 0.2326 Lr: 0.01925
[2023-08-08 04:14:57,184 INFO misc.py line 115 22900] Train: [60/100][83/156] Data 0.001 (0.001) Batch 3.685 (3.426) Remain 06:00:26 loss: 0.1829 Lr: 0.01924
[2023-08-08 04:15:00,824 INFO misc.py line 115 22900] Train: [60/100][84/156] Data 0.001 (0.001) Batch 3.639 (3.428) Remain 06:00:39 loss: 0.2591 Lr: 0.01924
[2023-08-08 04:15:04,498 INFO misc.py line 115 22900] Train: [60/100][85/156] Data 0.001 (0.001) Batch 3.674 (3.431) Remain 06:00:54 loss: 0.1867 Lr: 0.01923
[2023-08-08 04:15:07,937 INFO misc.py line 115 22900] Train: [60/100][86/156] Data 0.001 (0.001) Batch 3.439 (3.431) Remain 06:00:52 loss: 0.2079 Lr: 0.01923
[2023-08-08 04:15:11,946 INFO misc.py line 115 22900] Train: [60/100][87/156] Data 0.001 (0.001) Batch 4.009 (3.438) Remain 06:01:32 loss: 0.2722 Lr: 0.01922
[2023-08-08 04:15:14,858 INFO misc.py line 115 22900] Train: [60/100][88/156] Data 0.001 (0.001) Batch 2.912 (3.432) Remain 06:00:49 loss: 0.1334 Lr: 0.01922
[2023-08-08 04:15:18,950 INFO misc.py line 115 22900] Train: [60/100][89/156] Data 0.001 (0.001) Batch 4.092 (3.440) Remain 06:01:34 loss: 0.4096 Lr: 0.01921
[2023-08-08 04:15:21,532 INFO misc.py line 115 22900] Train: [60/100][90/156] Data 0.001 (0.001) Batch 2.581 (3.430) Remain 06:00:28 loss: 0.1533 Lr: 0.01921
[2023-08-08 04:15:25,102 INFO misc.py line 115 22900] Train: [60/100][91/156] Data 0.001 (0.001) Batch 3.570 (3.431) Remain 06:00:35 loss: 0.4688 Lr: 0.01920
[2023-08-08 04:15:28,877 INFO misc.py line 115 22900] Train: [60/100][92/156] Data 0.001 (0.001) Batch 3.775 (3.435) Remain 06:00:56 loss: 0.2565 Lr: 0.01920
[2023-08-08 04:15:33,002 INFO misc.py line 115 22900] Train: [60/100][93/156] Data 0.001 (0.001) Batch 4.125 (3.443) Remain 06:01:41 loss: 0.3532 Lr: 0.01919
[2023-08-08 04:15:35,878 INFO misc.py line 115 22900] Train: [60/100][94/156] Data 0.001 (0.001) Batch 2.876 (3.437) Remain 06:00:58 loss: 0.1856 Lr: 0.01919
[2023-08-08 04:15:37,924 INFO misc.py line 115 22900] Train: [60/100][95/156] Data 0.001 (0.001) Batch 2.047 (3.422) Remain 05:59:19 loss: 0.0540 Lr: 0.01918
[2023-08-08 04:15:39,983 INFO misc.py line 115 22900] Train: [60/100][96/156] Data 0.001 (0.001) Batch 2.059 (3.407) Remain 05:57:44 loss: 0.2109 Lr: 0.01918
[2023-08-08 04:15:43,705 INFO misc.py line 115 22900] Train: [60/100][97/156] Data 0.001 (0.001) Batch 3.722 (3.410) Remain 05:58:01 loss: 0.4572 Lr: 0.01917
[2023-08-08 04:15:47,741 INFO misc.py line 115 22900] Train: [60/100][98/156] Data 0.001 (0.001) Batch 4.036 (3.417) Remain 05:58:39 loss: 0.1693 Lr: 0.01917
[2023-08-08 04:15:51,729 INFO misc.py line 115 22900] Train: [60/100][99/156] Data 0.001 (0.001) Batch 3.988 (3.423) Remain 05:59:14 loss: 0.2703 Lr: 0.01916
[2023-08-08 04:15:55,792 INFO misc.py line 115 22900] Train: [60/100][100/156] Data 0.001 (0.001) Batch 4.063 (3.429) Remain 05:59:52 loss: 0.2699 Lr: 0.01916
[2023-08-08 04:16:00,146 INFO misc.py line 115 22900] Train: [60/100][101/156] Data 0.001 (0.001) Batch 4.354 (3.439) Remain 06:00:48 loss: 0.2693 Lr: 0.01915
[2023-08-08 04:16:02,977 INFO misc.py line 115 22900] Train: [60/100][102/156] Data 0.001 (0.001) Batch 2.831 (3.433) Remain 06:00:05 loss: 0.3423 Lr: 0.01915
[2023-08-08 04:16:05,888 INFO misc.py line 115 22900] Train: [60/100][103/156] Data 0.001 (0.001) Batch 2.911 (3.428) Remain 05:59:29 loss: 0.0953 Lr: 0.01914
[2023-08-08 04:16:09,210 INFO misc.py line 115 22900] Train: [60/100][104/156] Data 0.001 (0.001) Batch 3.322 (3.427) Remain 05:59:19 loss: 0.1831 Lr: 0.01914
[2023-08-08 04:16:12,050 INFO misc.py line 115 22900] Train: [60/100][105/156] Data 0.001 (0.001) Batch 2.840 (3.421) Remain 05:58:40 loss: 0.1502 Lr: 0.01913
[2023-08-08 04:16:16,174 INFO misc.py line 115 22900] Train: [60/100][106/156] Data 0.004 (0.001) Batch 4.123 (3.428) Remain 05:59:19 loss: 0.1512 Lr: 0.01913
[2023-08-08 04:16:19,437 INFO misc.py line 115 22900] Train: [60/100][107/156] Data 0.001 (0.001) Batch 3.264 (3.426) Remain 05:59:06 loss: 0.1355 Lr: 0.01912
[2023-08-08 04:16:23,103 INFO misc.py line 115 22900] Train: [60/100][108/156] Data 0.001 (0.001) Batch 3.666 (3.428) Remain 05:59:17 loss: 0.2686 Lr: 0.01912
[2023-08-08 04:16:26,255 INFO misc.py line 115 22900] Train: [60/100][109/156] Data 0.001 (0.001) Batch 3.152 (3.426) Remain 05:58:57 loss: 0.2062 Lr: 0.01911
[2023-08-08 04:16:30,042 INFO misc.py line 115 22900] Train: [60/100][110/156] Data 0.001 (0.001) Batch 3.787 (3.429) Remain 05:59:15 loss: 0.1600 Lr: 0.01910
[2023-08-08 04:16:33,973 INFO misc.py line 115 22900] Train: [60/100][111/156] Data 0.001 (0.001) Batch 3.932 (3.434) Remain 05:59:40 loss: 0.2965 Lr: 0.01910
[2023-08-08 04:16:37,272 INFO misc.py line 115 22900] Train: [60/100][112/156] Data 0.001 (0.001) Batch 3.299 (3.432) Remain 05:59:29 loss: 0.1442 Lr: 0.01909
[2023-08-08 04:16:41,671 INFO misc.py line 115 22900] Train: [60/100][113/156] Data 0.001 (0.001) Batch 4.399 (3.441) Remain 06:00:21 loss: 0.3331 Lr: 0.01909
[2023-08-08 04:16:44,623 INFO misc.py line 115 22900] Train: [60/100][114/156] Data 0.001 (0.001) Batch 2.952 (3.437) Remain 05:59:50 loss: 0.1402 Lr: 0.01908
[2023-08-08 04:16:48,643 INFO misc.py line 115 22900] Train: [60/100][115/156] Data 0.001 (0.001) Batch 4.020 (3.442) Remain 06:00:19 loss: 0.3045 Lr: 0.01908
[2023-08-08 04:16:50,898 INFO misc.py line 115 22900] Train: [60/100][116/156] Data 0.001 (0.001) Batch 2.255 (3.432) Remain 05:59:10 loss: 0.1861 Lr: 0.01907
[2023-08-08 04:16:54,991 INFO misc.py line 115 22900] Train: [60/100][117/156] Data 0.001 (0.001) Batch 4.092 (3.437) Remain 05:59:43 loss: 0.5053 Lr: 0.01907
[2023-08-08 04:16:57,274 INFO misc.py line 115 22900] Train: [60/100][118/156] Data 0.001 (0.001) Batch 2.283 (3.427) Remain 05:58:36 loss: 0.1690 Lr: 0.01906
[2023-08-08 04:17:01,417 INFO misc.py line 115 22900] Train: [60/100][119/156] Data 0.001 (0.001) Batch 4.144 (3.434) Remain 05:59:12 loss: 0.1390 Lr: 0.01906
[2023-08-08 04:17:04,262 INFO misc.py line 115 22900] Train: [60/100][120/156] Data 0.001 (0.001) Batch 2.844 (3.428) Remain 05:58:37 loss: 0.0855 Lr: 0.01905
[2023-08-08 04:17:08,296 INFO misc.py line 115 22900] Train: [60/100][121/156] Data 0.001 (0.001) Batch 4.034 (3.434) Remain 05:59:05 loss: 0.3458 Lr: 0.01905
[2023-08-08 04:17:10,994 INFO misc.py line 115 22900] Train: [60/100][122/156] Data 0.001 (0.001) Batch 2.699 (3.427) Remain 05:58:23 loss: 0.1147 Lr: 0.01904
[2023-08-08 04:17:15,051 INFO misc.py line 115 22900] Train: [60/100][123/156] Data 0.001 (0.001) Batch 4.057 (3.433) Remain 05:58:53 loss: 0.2925 Lr: 0.01904
[2023-08-08 04:17:19,146 INFO misc.py line 115 22900] Train: [60/100][124/156] Data 0.002 (0.001) Batch 4.095 (3.438) Remain 05:59:24 loss: 0.2345 Lr: 0.01903
[2023-08-08 04:17:22,721 INFO misc.py line 115 22900] Train: [60/100][125/156] Data 0.001 (0.001) Batch 3.575 (3.439) Remain 05:59:27 loss: 0.2249 Lr: 0.01903
[2023-08-08 04:17:26,165 INFO misc.py line 115 22900] Train: [60/100][126/156] Data 0.001 (0.001) Batch 3.444 (3.439) Remain 05:59:24 loss: 0.2711 Lr: 0.01902
[2023-08-08 04:17:29,434 INFO misc.py line 115 22900] Train: [60/100][127/156] Data 0.001 (0.001) Batch 3.269 (3.438) Remain 05:59:12 loss: 0.1885 Lr: 0.01902
[2023-08-08 04:17:33,136 INFO misc.py line 115 22900] Train: [60/100][128/156] Data 0.001 (0.001) Batch 3.702 (3.440) Remain 05:59:22 loss: 0.2758 Lr: 0.01901
[2023-08-08 04:17:36,628 INFO misc.py line 115 22900] Train: [60/100][129/156] Data 0.001 (0.001) Batch 3.492 (3.440) Remain 05:59:21 loss: 0.1244 Lr: 0.01901
[2023-08-08 04:17:39,856 INFO misc.py line 115 22900] Train: [60/100][130/156] Data 0.001 (0.001) Batch 3.228 (3.439) Remain 05:59:07 loss: 0.1591 Lr: 0.01900
[2023-08-08 04:17:43,732 INFO misc.py line 115 22900] Train: [60/100][131/156] Data 0.001 (0.001) Batch 3.876 (3.442) Remain 05:59:25 loss: 0.2456 Lr: 0.01900
[2023-08-08 04:17:47,674 INFO misc.py line 115 22900] Train: [60/100][132/156] Data 0.001 (0.001) Batch 3.942 (3.446) Remain 05:59:46 loss: 0.3362 Lr: 0.01899
[2023-08-08 04:17:50,941 INFO misc.py line 115 22900] Train: [60/100][133/156] Data 0.001 (0.001) Batch 3.267 (3.445) Remain 05:59:34 loss: 0.1519 Lr: 0.01899
[2023-08-08 04:17:54,514 INFO misc.py line 115 22900] Train: [60/100][134/156] Data 0.001 (0.001) Batch 3.573 (3.446) Remain 05:59:36 loss: 0.2603 Lr: 0.01898
[2023-08-08 04:17:57,437 INFO misc.py line 115 22900] Train: [60/100][135/156] Data 0.001 (0.001) Batch 2.923 (3.442) Remain 05:59:08 loss: 0.2085 Lr: 0.01898
[2023-08-08 04:18:01,158 INFO misc.py line 115 22900] Train: [60/100][136/156] Data 0.001 (0.001) Batch 3.721 (3.444) Remain 05:59:18 loss: 0.1340 Lr: 0.01897
[2023-08-08 04:18:04,306 INFO misc.py line 115 22900] Train: [60/100][137/156] Data 0.001 (0.001) Batch 3.148 (3.442) Remain 05:59:00 loss: 0.2343 Lr: 0.01897
[2023-08-08 04:18:07,094 INFO misc.py line 115 22900] Train: [60/100][138/156] Data 0.001 (0.001) Batch 2.789 (3.437) Remain 05:58:27 loss: 0.1873 Lr: 0.01896
[2023-08-08 04:18:10,371 INFO misc.py line 115 22900] Train: [60/100][139/156] Data 0.001 (0.001) Batch 3.277 (3.436) Remain 05:58:16 loss: 0.2035 Lr: 0.01896
[2023-08-08 04:18:14,480 INFO misc.py line 115 22900] Train: [60/100][140/156] Data 0.001 (0.001) Batch 4.110 (3.441) Remain 05:58:43 loss: 0.1395 Lr: 0.01895
[2023-08-08 04:18:17,032 INFO misc.py line 115 22900] Train: [60/100][141/156] Data 0.001 (0.001) Batch 2.551 (3.434) Remain 05:58:00 loss: 0.1234 Lr: 0.01895
[2023-08-08 04:18:21,718 INFO misc.py line 115 22900] Train: [60/100][142/156] Data 0.001 (0.001) Batch 4.686 (3.443) Remain 05:58:52 loss: 0.2372 Lr: 0.01894
[2023-08-08 04:18:25,544 INFO misc.py line 115 22900] Train: [60/100][143/156] Data 0.001 (0.001) Batch 3.826 (3.446) Remain 05:59:06 loss: 0.1335 Lr: 0.01894
[2023-08-08 04:18:28,882 INFO misc.py line 115 22900] Train: [60/100][144/156] Data 0.001 (0.001) Batch 3.338 (3.445) Remain 05:58:58 loss: 0.0670 Lr: 0.01893
[2023-08-08 04:18:32,857 INFO misc.py line 115 22900] Train: [60/100][145/156] Data 0.001 (0.001) Batch 3.975 (3.449) Remain 05:59:18 loss: 0.4202 Lr: 0.01892
[2023-08-08 04:18:36,872 INFO misc.py line 115 22900] Train: [60/100][146/156] Data 0.001 (0.001) Batch 4.015 (3.453) Remain 05:59:39 loss: 0.1578 Lr: 0.01892
[2023-08-08 04:18:40,343 INFO misc.py line 115 22900] Train: [60/100][147/156] Data 0.001 (0.001) Batch 3.471 (3.453) Remain 05:59:36 loss: 0.2057 Lr: 0.01891
[2023-08-08 04:18:43,926 INFO misc.py line 115 22900] Train: [60/100][148/156] Data 0.001 (0.001) Batch 3.583 (3.454) Remain 05:59:39 loss: 0.1386 Lr: 0.01891
[2023-08-08 04:18:46,964 INFO misc.py line 115 22900] Train: [60/100][149/156] Data 0.001 (0.001) Batch 3.038 (3.451) Remain 05:59:17 loss: 0.1351 Lr: 0.01890
[2023-08-08 04:18:51,482 INFO misc.py line 115 22900] Train: [60/100][150/156] Data 0.001 (0.001) Batch 4.518 (3.458) Remain 05:59:59 loss: 0.1806 Lr: 0.01890
[2023-08-08 04:18:55,436 INFO misc.py line 115 22900] Train: [60/100][151/156] Data 0.001 (0.001) Batch 3.954 (3.462) Remain 06:00:17 loss: 0.2153 Lr: 0.01889
[2023-08-08 04:18:58,168 INFO misc.py line 115 22900] Train: [60/100][152/156] Data 0.001 (0.001) Batch 2.732 (3.457) Remain 05:59:43 loss: 0.1933 Lr: 0.01889
[2023-08-08 04:19:02,185 INFO misc.py line 115 22900] Train: [60/100][153/156] Data 0.001 (0.001) Batch 4.017 (3.460) Remain 06:00:03 loss: 0.3323 Lr: 0.01888
[2023-08-08 04:19:05,861 INFO misc.py line 115 22900] Train: [60/100][154/156] Data 0.001 (0.001) Batch 3.676 (3.462) Remain 06:00:08 loss: 0.1493 Lr: 0.01888
[2023-08-08 04:19:09,890 INFO misc.py line 115 22900] Train: [60/100][155/156] Data 0.001 (0.001) Batch 4.029 (3.466) Remain 06:00:28 loss: 0.4188 Lr: 0.01887
[2023-08-08 04:19:14,010 INFO misc.py line 115 22900] Train: [60/100][156/156] Data 0.001 (0.001) Batch 4.120 (3.470) Remain 06:00:51 loss: 0.1642 Lr: 0.01887
[2023-08-08 04:19:14,010 INFO misc.py line 129 22900] Train result: loss: 0.2153 
[2023-08-08 04:19:14,011 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 04:19:16,139 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.3644 
[2023-08-08 04:19:17,009 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.5728 
[2023-08-08 04:19:18,675 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.8162 
[2023-08-08 04:19:20,196 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.3682 
[2023-08-08 04:19:22,043 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.8816 
[2023-08-08 04:19:23,706 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.7204 
[2023-08-08 04:19:25,841 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.5230 
[2023-08-08 04:19:27,647 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.3878 
[2023-08-08 04:19:28,931 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.5708 
[2023-08-08 04:19:31,061 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.4149 
[2023-08-08 04:19:31,586 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.0446 
[2023-08-08 04:19:33,122 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.7970 
[2023-08-08 04:19:35,832 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.1672 
[2023-08-08 04:19:37,511 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.8870 
[2023-08-08 04:19:39,535 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.3568 
[2023-08-08 04:19:42,245 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.1806 
[2023-08-08 04:19:44,953 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.5640 
[2023-08-08 04:19:46,801 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.5845 
[2023-08-08 04:19:47,548 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.1118 
[2023-08-08 04:19:48,433 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.8267 
[2023-08-08 04:19:50,695 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.4395 
[2023-08-08 04:19:52,660 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.9252 
[2023-08-08 04:19:54,506 INFO evaluator.py line 122 22900] Test: [23/24] Loss 3.3515 
[2023-08-08 04:19:56,443 INFO evaluator.py line 122 22900] Test: [24/24] Loss 2.1027 
[2023-08-08 04:19:56,491 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2345/0.3239/0.6920.
[2023-08-08 04:19:56,492 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6159/0.9738
[2023-08-08 04:19:56,492 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9427/0.9923
[2023-08-08 04:19:56,492 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.2097/0.4092
[2023-08-08 04:19:56,492 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1704/0.3113
[2023-08-08 04:19:56,492 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6545/0.7078
[2023-08-08 04:19:56,492 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.4341/0.6595
[2023-08-08 04:19:56,492 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5157/0.6757
[2023-08-08 04:19:56,492 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1060/0.1116
[2023-08-08 04:19:56,492 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1570/0.3261
[2023-08-08 04:19:56,492 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0226/0.0227
[2023-08-08 04:19:56,492 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 04:19:56,492 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.1795/0.2215
[2023-08-08 04:19:56,492 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1350/0.2213
[2023-08-08 04:19:56,492 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0344/0.0391
[2023-08-08 04:19:56,492 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0219/0.0284
[2023-08-08 04:19:56,492 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0126/0.0130
[2023-08-08 04:19:56,492 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0271/0.0346
[2023-08-08 04:19:56,492 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.3440/0.5348
[2023-08-08 04:19:56,492 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 04:19:56,492 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.1065/0.1944
[2023-08-08 04:19:56,492 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 04:19:56,492 INFO misc.py line 152 22900] Currently Best mIoU: 0.2562
[2023-08-08 04:19:56,492 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 04:20:02,229 INFO misc.py line 115 22900] Train: [61/100][1/156] Data 0.799 (0.799) Batch 4.946 (4.946) Remain 08:34:17 loss: 0.2801 Lr: 0.01886
[2023-08-08 04:20:06,246 INFO misc.py line 115 22900] Train: [61/100][2/156] Data 0.001 (0.001) Batch 4.017 (4.017) Remain 06:57:38 loss: 0.4471 Lr: 0.01886
[2023-08-08 04:20:09,555 INFO misc.py line 115 22900] Train: [61/100][3/156] Data 0.001 (0.001) Batch 3.309 (3.309) Remain 05:43:57 loss: 0.2256 Lr: 0.01885
[2023-08-08 04:20:12,903 INFO misc.py line 115 22900] Train: [61/100][4/156] Data 0.001 (0.001) Batch 3.348 (3.348) Remain 05:48:00 loss: 0.4068 Lr: 0.01885
[2023-08-08 04:20:16,834 INFO misc.py line 115 22900] Train: [61/100][5/156] Data 0.001 (0.001) Batch 3.931 (3.640) Remain 06:18:13 loss: 0.2769 Lr: 0.01884
[2023-08-08 04:20:19,982 INFO misc.py line 115 22900] Train: [61/100][6/156] Data 0.001 (0.001) Batch 3.148 (3.476) Remain 06:01:08 loss: 0.1446 Lr: 0.01884
[2023-08-08 04:20:23,166 INFO misc.py line 115 22900] Train: [61/100][7/156] Data 0.001 (0.001) Batch 3.184 (3.403) Remain 05:53:29 loss: 0.2631 Lr: 0.01883
[2023-08-08 04:20:27,207 INFO misc.py line 115 22900] Train: [61/100][8/156] Data 0.001 (0.001) Batch 4.041 (3.531) Remain 06:06:42 loss: 0.1247 Lr: 0.01883
[2023-08-08 04:20:30,467 INFO misc.py line 115 22900] Train: [61/100][9/156] Data 0.001 (0.001) Batch 3.260 (3.485) Remain 06:01:57 loss: 0.1886 Lr: 0.01882
[2023-08-08 04:20:33,797 INFO misc.py line 115 22900] Train: [61/100][10/156] Data 0.001 (0.001) Batch 3.330 (3.463) Remain 05:59:35 loss: 0.1382 Lr: 0.01882
[2023-08-08 04:20:36,422 INFO misc.py line 115 22900] Train: [61/100][11/156] Data 0.001 (0.001) Batch 2.625 (3.358) Remain 05:48:39 loss: 0.1227 Lr: 0.01881
[2023-08-08 04:20:40,302 INFO misc.py line 115 22900] Train: [61/100][12/156] Data 0.001 (0.001) Batch 3.880 (3.416) Remain 05:54:37 loss: 0.2076 Lr: 0.01881
[2023-08-08 04:20:43,492 INFO misc.py line 115 22900] Train: [61/100][13/156] Data 0.001 (0.001) Batch 3.189 (3.394) Remain 05:52:12 loss: 0.2275 Lr: 0.01880
[2023-08-08 04:20:46,913 INFO misc.py line 115 22900] Train: [61/100][14/156] Data 0.001 (0.001) Batch 3.422 (3.396) Remain 05:52:25 loss: 0.2443 Lr: 0.01880
[2023-08-08 04:20:50,377 INFO misc.py line 115 22900] Train: [61/100][15/156] Data 0.001 (0.001) Batch 3.464 (3.402) Remain 05:52:56 loss: 0.1349 Lr: 0.01879
[2023-08-08 04:20:53,639 INFO misc.py line 115 22900] Train: [61/100][16/156] Data 0.001 (0.001) Batch 3.261 (3.391) Remain 05:51:46 loss: 0.2199 Lr: 0.01879
[2023-08-08 04:20:57,518 INFO misc.py line 115 22900] Train: [61/100][17/156] Data 0.001 (0.001) Batch 3.879 (3.426) Remain 05:55:19 loss: 0.2525 Lr: 0.01878
[2023-08-08 04:21:01,445 INFO misc.py line 115 22900] Train: [61/100][18/156] Data 0.001 (0.001) Batch 3.927 (3.459) Remain 05:58:44 loss: 0.1544 Lr: 0.01878
[2023-08-08 04:21:04,612 INFO misc.py line 115 22900] Train: [61/100][19/156] Data 0.001 (0.001) Batch 3.167 (3.441) Remain 05:56:46 loss: 0.2420 Lr: 0.01877
[2023-08-08 04:21:07,912 INFO misc.py line 115 22900] Train: [61/100][20/156] Data 0.001 (0.001) Batch 3.300 (3.433) Remain 05:55:51 loss: 0.0953 Lr: 0.01877
[2023-08-08 04:21:11,399 INFO misc.py line 115 22900] Train: [61/100][21/156] Data 0.001 (0.001) Batch 3.487 (3.436) Remain 05:56:07 loss: 0.2706 Lr: 0.01876
[2023-08-08 04:21:14,594 INFO misc.py line 115 22900] Train: [61/100][22/156] Data 0.001 (0.001) Batch 3.196 (3.423) Remain 05:54:45 loss: 0.2014 Lr: 0.01876
[2023-08-08 04:21:18,691 INFO misc.py line 115 22900] Train: [61/100][23/156] Data 0.001 (0.001) Batch 4.096 (3.457) Remain 05:58:10 loss: 0.1751 Lr: 0.01875
[2023-08-08 04:21:22,071 INFO misc.py line 115 22900] Train: [61/100][24/156] Data 0.001 (0.001) Batch 3.380 (3.453) Remain 05:57:44 loss: 0.2039 Lr: 0.01875
[2023-08-08 04:21:23,871 INFO misc.py line 115 22900] Train: [61/100][25/156] Data 0.001 (0.001) Batch 1.800 (3.378) Remain 05:49:54 loss: 0.2783 Lr: 0.01874
[2023-08-08 04:21:26,763 INFO misc.py line 115 22900] Train: [61/100][26/156] Data 0.001 (0.001) Batch 2.892 (3.357) Remain 05:47:39 loss: 0.2741 Lr: 0.01873
[2023-08-08 04:21:31,005 INFO misc.py line 115 22900] Train: [61/100][27/156] Data 0.001 (0.001) Batch 4.242 (3.394) Remain 05:51:25 loss: 0.3488 Lr: 0.01873
[2023-08-08 04:21:34,060 INFO misc.py line 115 22900] Train: [61/100][28/156] Data 0.001 (0.001) Batch 3.055 (3.380) Remain 05:49:57 loss: 0.1250 Lr: 0.01872
[2023-08-08 04:21:37,336 INFO misc.py line 115 22900] Train: [61/100][29/156] Data 0.001 (0.001) Batch 3.276 (3.376) Remain 05:49:29 loss: 0.1274 Lr: 0.01872
[2023-08-08 04:21:40,931 INFO misc.py line 115 22900] Train: [61/100][30/156] Data 0.001 (0.001) Batch 3.595 (3.384) Remain 05:50:16 loss: 0.2021 Lr: 0.01871
[2023-08-08 04:21:44,545 INFO misc.py line 115 22900] Train: [61/100][31/156] Data 0.001 (0.001) Batch 3.614 (3.393) Remain 05:51:04 loss: 0.2685 Lr: 0.01871
[2023-08-08 04:21:48,666 INFO misc.py line 115 22900] Train: [61/100][32/156] Data 0.001 (0.001) Batch 4.120 (3.418) Remain 05:53:36 loss: 0.3293 Lr: 0.01870
[2023-08-08 04:21:51,230 INFO misc.py line 115 22900] Train: [61/100][33/156] Data 0.001 (0.001) Batch 2.564 (3.389) Remain 05:50:36 loss: 0.1908 Lr: 0.01870
[2023-08-08 04:21:54,587 INFO misc.py line 115 22900] Train: [61/100][34/156] Data 0.001 (0.001) Batch 3.357 (3.388) Remain 05:50:26 loss: 0.1315 Lr: 0.01869
[2023-08-08 04:21:57,837 INFO misc.py line 115 22900] Train: [61/100][35/156] Data 0.001 (0.001) Batch 3.250 (3.384) Remain 05:49:56 loss: 0.2937 Lr: 0.01869
[2023-08-08 04:22:01,731 INFO misc.py line 115 22900] Train: [61/100][36/156] Data 0.001 (0.001) Batch 3.894 (3.399) Remain 05:51:29 loss: 0.3306 Lr: 0.01868
[2023-08-08 04:22:05,055 INFO misc.py line 115 22900] Train: [61/100][37/156] Data 0.001 (0.001) Batch 3.325 (3.397) Remain 05:51:12 loss: 0.1472 Lr: 0.01868
[2023-08-08 04:22:08,682 INFO misc.py line 115 22900] Train: [61/100][38/156] Data 0.001 (0.001) Batch 3.627 (3.404) Remain 05:51:49 loss: 0.1822 Lr: 0.01867
[2023-08-08 04:22:13,272 INFO misc.py line 115 22900] Train: [61/100][39/156] Data 0.001 (0.001) Batch 4.590 (3.437) Remain 05:55:10 loss: 0.2261 Lr: 0.01867
[2023-08-08 04:22:16,120 INFO misc.py line 115 22900] Train: [61/100][40/156] Data 0.001 (0.001) Batch 2.848 (3.421) Remain 05:53:28 loss: 0.1168 Lr: 0.01866
[2023-08-08 04:22:19,363 INFO misc.py line 115 22900] Train: [61/100][41/156] Data 0.001 (0.001) Batch 3.243 (3.416) Remain 05:52:55 loss: 0.1261 Lr: 0.01866
[2023-08-08 04:22:22,013 INFO misc.py line 115 22900] Train: [61/100][42/156] Data 0.001 (0.001) Batch 2.650 (3.396) Remain 05:50:50 loss: 0.1464 Lr: 0.01865
[2023-08-08 04:22:26,121 INFO misc.py line 115 22900] Train: [61/100][43/156] Data 0.001 (0.001) Batch 4.108 (3.414) Remain 05:52:37 loss: 0.2723 Lr: 0.01865
[2023-08-08 04:22:30,085 INFO misc.py line 115 22900] Train: [61/100][44/156] Data 0.001 (0.001) Batch 3.963 (3.428) Remain 05:53:57 loss: 0.2798 Lr: 0.01864
[2023-08-08 04:22:34,277 INFO misc.py line 115 22900] Train: [61/100][45/156] Data 0.001 (0.001) Batch 4.192 (3.446) Remain 05:55:46 loss: 0.1563 Lr: 0.01864
[2023-08-08 04:22:37,571 INFO misc.py line 115 22900] Train: [61/100][46/156] Data 0.001 (0.001) Batch 3.294 (3.442) Remain 05:55:21 loss: 0.3136 Lr: 0.01863
[2023-08-08 04:22:41,660 INFO misc.py line 115 22900] Train: [61/100][47/156] Data 0.001 (0.001) Batch 4.089 (3.457) Remain 05:56:48 loss: 0.3022 Lr: 0.01863
[2023-08-08 04:22:45,327 INFO misc.py line 115 22900] Train: [61/100][48/156] Data 0.001 (0.001) Batch 3.667 (3.462) Remain 05:57:14 loss: 0.1310 Lr: 0.01862
[2023-08-08 04:22:48,758 INFO misc.py line 115 22900] Train: [61/100][49/156] Data 0.001 (0.001) Batch 3.431 (3.461) Remain 05:57:06 loss: 0.1644 Lr: 0.01862
[2023-08-08 04:22:52,446 INFO misc.py line 115 22900] Train: [61/100][50/156] Data 0.001 (0.001) Batch 3.687 (3.466) Remain 05:57:33 loss: 0.2728 Lr: 0.01861
[2023-08-08 04:22:56,573 INFO misc.py line 115 22900] Train: [61/100][51/156] Data 0.001 (0.001) Batch 4.127 (3.480) Remain 05:58:54 loss: 0.2988 Lr: 0.01861
[2023-08-08 04:22:59,987 INFO misc.py line 115 22900] Train: [61/100][52/156] Data 0.001 (0.001) Batch 3.414 (3.478) Remain 05:58:43 loss: 0.2423 Lr: 0.01860
[2023-08-08 04:23:02,229 INFO misc.py line 115 22900] Train: [61/100][53/156] Data 0.001 (0.001) Batch 2.242 (3.453) Remain 05:56:06 loss: 0.0881 Lr: 0.01860
[2023-08-08 04:23:04,860 INFO misc.py line 115 22900] Train: [61/100][54/156] Data 0.001 (0.001) Batch 2.631 (3.437) Remain 05:54:23 loss: 0.1666 Lr: 0.01859
[2023-08-08 04:23:08,819 INFO misc.py line 115 22900] Train: [61/100][55/156] Data 0.001 (0.001) Batch 3.958 (3.447) Remain 05:55:22 loss: 0.2683 Lr: 0.01859
[2023-08-08 04:23:12,163 INFO misc.py line 115 22900] Train: [61/100][56/156] Data 0.001 (0.001) Batch 3.344 (3.445) Remain 05:55:06 loss: 0.0756 Lr: 0.01858
[2023-08-08 04:23:15,168 INFO misc.py line 115 22900] Train: [61/100][57/156] Data 0.001 (0.001) Batch 3.005 (3.437) Remain 05:54:12 loss: 0.1904 Lr: 0.01858
[2023-08-08 04:23:18,453 INFO misc.py line 115 22900] Train: [61/100][58/156] Data 0.001 (0.001) Batch 3.284 (3.435) Remain 05:53:52 loss: 0.2559 Lr: 0.01857
[2023-08-08 04:23:22,989 INFO misc.py line 115 22900] Train: [61/100][59/156] Data 0.001 (0.001) Batch 4.537 (3.454) Remain 05:55:50 loss: 0.3824 Lr: 0.01857
[2023-08-08 04:23:26,121 INFO misc.py line 115 22900] Train: [61/100][60/156] Data 0.001 (0.001) Batch 3.131 (3.449) Remain 05:55:11 loss: 0.2273 Lr: 0.01856
[2023-08-08 04:23:29,359 INFO misc.py line 115 22900] Train: [61/100][61/156] Data 0.001 (0.001) Batch 3.238 (3.445) Remain 05:54:46 loss: 0.1621 Lr: 0.01856
[2023-08-08 04:23:33,330 INFO misc.py line 115 22900] Train: [61/100][62/156] Data 0.001 (0.001) Batch 3.971 (3.454) Remain 05:55:37 loss: 0.2976 Lr: 0.01855
[2023-08-08 04:23:37,718 INFO misc.py line 115 22900] Train: [61/100][63/156] Data 0.001 (0.001) Batch 4.388 (3.469) Remain 05:57:10 loss: 0.2598 Lr: 0.01855
[2023-08-08 04:23:40,751 INFO misc.py line 115 22900] Train: [61/100][64/156] Data 0.001 (0.001) Batch 3.033 (3.462) Remain 05:56:22 loss: 0.0999 Lr: 0.01854
[2023-08-08 04:23:44,151 INFO misc.py line 115 22900] Train: [61/100][65/156] Data 0.001 (0.001) Batch 3.400 (3.461) Remain 05:56:13 loss: 0.2187 Lr: 0.01853
[2023-08-08 04:23:47,764 INFO misc.py line 115 22900] Train: [61/100][66/156] Data 0.001 (0.001) Batch 3.613 (3.464) Remain 05:56:24 loss: 0.2249 Lr: 0.01853
[2023-08-08 04:23:50,820 INFO misc.py line 115 22900] Train: [61/100][67/156] Data 0.001 (0.001) Batch 3.056 (3.457) Remain 05:55:41 loss: 0.1711 Lr: 0.01852
[2023-08-08 04:23:54,546 INFO misc.py line 115 22900] Train: [61/100][68/156] Data 0.001 (0.001) Batch 3.726 (3.461) Remain 05:56:03 loss: 0.2389 Lr: 0.01852
[2023-08-08 04:23:57,187 INFO misc.py line 115 22900] Train: [61/100][69/156] Data 0.001 (0.001) Batch 2.641 (3.449) Remain 05:54:43 loss: 0.4103 Lr: 0.01851
[2023-08-08 04:24:00,772 INFO misc.py line 115 22900] Train: [61/100][70/156] Data 0.001 (0.001) Batch 3.585 (3.451) Remain 05:54:52 loss: 0.1815 Lr: 0.01851
[2023-08-08 04:24:04,834 INFO misc.py line 115 22900] Train: [61/100][71/156] Data 0.001 (0.001) Batch 4.063 (3.460) Remain 05:55:44 loss: 0.4435 Lr: 0.01850
[2023-08-08 04:24:08,143 INFO misc.py line 115 22900] Train: [61/100][72/156] Data 0.001 (0.001) Batch 3.309 (3.458) Remain 05:55:27 loss: 0.1767 Lr: 0.01850
[2023-08-08 04:24:12,282 INFO misc.py line 115 22900] Train: [61/100][73/156] Data 0.001 (0.001) Batch 4.139 (3.468) Remain 05:56:24 loss: 0.1342 Lr: 0.01849
[2023-08-08 04:24:15,791 INFO misc.py line 115 22900] Train: [61/100][74/156] Data 0.001 (0.001) Batch 3.509 (3.468) Remain 05:56:24 loss: 0.1151 Lr: 0.01849
[2023-08-08 04:24:19,123 INFO misc.py line 115 22900] Train: [61/100][75/156] Data 0.001 (0.001) Batch 3.332 (3.466) Remain 05:56:09 loss: 0.1029 Lr: 0.01848
[2023-08-08 04:24:22,608 INFO misc.py line 115 22900] Train: [61/100][76/156] Data 0.001 (0.001) Batch 3.485 (3.466) Remain 05:56:07 loss: 0.2015 Lr: 0.01848
[2023-08-08 04:24:26,480 INFO misc.py line 115 22900] Train: [61/100][77/156] Data 0.001 (0.001) Batch 3.872 (3.472) Remain 05:56:37 loss: 0.2606 Lr: 0.01847
[2023-08-08 04:24:30,579 INFO misc.py line 115 22900] Train: [61/100][78/156] Data 0.001 (0.001) Batch 4.099 (3.480) Remain 05:57:25 loss: 0.1792 Lr: 0.01847
[2023-08-08 04:24:34,018 INFO misc.py line 115 22900] Train: [61/100][79/156] Data 0.001 (0.001) Batch 3.439 (3.480) Remain 05:57:18 loss: 0.3244 Lr: 0.01846
[2023-08-08 04:24:38,096 INFO misc.py line 115 22900] Train: [61/100][80/156] Data 0.001 (0.001) Batch 4.078 (3.488) Remain 05:58:03 loss: 0.3245 Lr: 0.01846
[2023-08-08 04:24:40,917 INFO misc.py line 115 22900] Train: [61/100][81/156] Data 0.001 (0.001) Batch 2.821 (3.479) Remain 05:57:07 loss: 0.2092 Lr: 0.01845
[2023-08-08 04:24:43,806 INFO misc.py line 115 22900] Train: [61/100][82/156] Data 0.001 (0.001) Batch 2.889 (3.472) Remain 05:56:17 loss: 0.1084 Lr: 0.01845
[2023-08-08 04:24:46,729 INFO misc.py line 115 22900] Train: [61/100][83/156] Data 0.001 (0.001) Batch 2.923 (3.465) Remain 05:55:32 loss: 0.1166 Lr: 0.01844
[2023-08-08 04:24:50,779 INFO misc.py line 115 22900] Train: [61/100][84/156] Data 0.001 (0.001) Batch 4.050 (3.472) Remain 05:56:13 loss: 0.3860 Lr: 0.01844
[2023-08-08 04:24:54,879 INFO misc.py line 115 22900] Train: [61/100][85/156] Data 0.001 (0.001) Batch 4.100 (3.480) Remain 05:56:56 loss: 0.1850 Lr: 0.01843
[2023-08-08 04:24:58,191 INFO misc.py line 115 22900] Train: [61/100][86/156] Data 0.001 (0.001) Batch 3.312 (3.478) Remain 05:56:40 loss: 0.2016 Lr: 0.01843
[2023-08-08 04:25:01,477 INFO misc.py line 115 22900] Train: [61/100][87/156] Data 0.001 (0.001) Batch 3.286 (3.475) Remain 05:56:23 loss: 0.1986 Lr: 0.01842
[2023-08-08 04:25:05,366 INFO misc.py line 115 22900] Train: [61/100][88/156] Data 0.001 (0.001) Batch 3.890 (3.480) Remain 05:56:49 loss: 0.4154 Lr: 0.01842
[2023-08-08 04:25:07,211 INFO misc.py line 115 22900] Train: [61/100][89/156] Data 0.001 (0.001) Batch 1.845 (3.461) Remain 05:54:49 loss: 0.1225 Lr: 0.01841
[2023-08-08 04:25:10,413 INFO misc.py line 115 22900] Train: [61/100][90/156] Data 0.001 (0.001) Batch 3.202 (3.458) Remain 05:54:27 loss: 0.0932 Lr: 0.01841
[2023-08-08 04:25:14,503 INFO misc.py line 115 22900] Train: [61/100][91/156] Data 0.001 (0.001) Batch 4.090 (3.465) Remain 05:55:08 loss: 0.2575 Lr: 0.01840
[2023-08-08 04:25:17,938 INFO misc.py line 115 22900] Train: [61/100][92/156] Data 0.001 (0.001) Batch 3.435 (3.465) Remain 05:55:02 loss: 0.2994 Lr: 0.01840
[2023-08-08 04:25:22,086 INFO misc.py line 115 22900] Train: [61/100][93/156] Data 0.001 (0.001) Batch 4.148 (3.473) Remain 05:55:45 loss: 0.2419 Lr: 0.01839
[2023-08-08 04:25:24,943 INFO misc.py line 115 22900] Train: [61/100][94/156] Data 0.001 (0.001) Batch 2.857 (3.466) Remain 05:55:00 loss: 0.1553 Lr: 0.01839
[2023-08-08 04:25:28,209 INFO misc.py line 115 22900] Train: [61/100][95/156] Data 0.001 (0.001) Batch 3.265 (3.464) Remain 05:54:43 loss: 0.1438 Lr: 0.01838
[2023-08-08 04:25:31,694 INFO misc.py line 115 22900] Train: [61/100][96/156] Data 0.001 (0.001) Batch 3.485 (3.464) Remain 05:54:41 loss: 0.2499 Lr: 0.01838
[2023-08-08 04:25:35,130 INFO misc.py line 115 22900] Train: [61/100][97/156] Data 0.001 (0.001) Batch 3.436 (3.464) Remain 05:54:36 loss: 0.2167 Lr: 0.01837
[2023-08-08 04:25:39,195 INFO misc.py line 115 22900] Train: [61/100][98/156] Data 0.001 (0.001) Batch 4.066 (3.470) Remain 05:55:12 loss: 0.1390 Lr: 0.01837
[2023-08-08 04:25:43,115 INFO misc.py line 115 22900] Train: [61/100][99/156] Data 0.001 (0.001) Batch 3.920 (3.475) Remain 05:55:37 loss: 0.2511 Lr: 0.01836
[2023-08-08 04:25:46,086 INFO misc.py line 115 22900] Train: [61/100][100/156] Data 0.001 (0.001) Batch 2.971 (3.469) Remain 05:55:02 loss: 0.1818 Lr: 0.01836
[2023-08-08 04:25:49,253 INFO misc.py line 115 22900] Train: [61/100][101/156] Data 0.001 (0.001) Batch 3.167 (3.466) Remain 05:54:39 loss: 0.0566 Lr: 0.01835
[2023-08-08 04:25:53,360 INFO misc.py line 115 22900] Train: [61/100][102/156] Data 0.001 (0.001) Batch 4.107 (3.473) Remain 05:55:15 loss: 0.1693 Lr: 0.01835
[2023-08-08 04:25:56,877 INFO misc.py line 115 22900] Train: [61/100][103/156] Data 0.001 (0.001) Batch 3.517 (3.473) Remain 05:55:15 loss: 0.2266 Lr: 0.01834
[2023-08-08 04:26:00,246 INFO misc.py line 115 22900] Train: [61/100][104/156] Data 0.001 (0.001) Batch 3.369 (3.472) Remain 05:55:05 loss: 0.3148 Lr: 0.01834
[2023-08-08 04:26:03,162 INFO misc.py line 115 22900] Train: [61/100][105/156] Data 0.001 (0.001) Batch 2.916 (3.467) Remain 05:54:28 loss: 0.1624 Lr: 0.01833
[2023-08-08 04:26:06,326 INFO misc.py line 115 22900] Train: [61/100][106/156] Data 0.001 (0.001) Batch 3.164 (3.464) Remain 05:54:06 loss: 0.0977 Lr: 0.01833
[2023-08-08 04:26:08,680 INFO misc.py line 115 22900] Train: [61/100][107/156] Data 0.001 (0.001) Batch 2.354 (3.453) Remain 05:52:58 loss: 0.2339 Lr: 0.01832
[2023-08-08 04:26:12,228 INFO misc.py line 115 22900] Train: [61/100][108/156] Data 0.001 (0.001) Batch 3.549 (3.454) Remain 05:53:00 loss: 0.1061 Lr: 0.01832
[2023-08-08 04:26:15,369 INFO misc.py line 115 22900] Train: [61/100][109/156] Data 0.001 (0.001) Batch 3.141 (3.451) Remain 05:52:38 loss: 0.3093 Lr: 0.01831
[2023-08-08 04:26:19,833 INFO misc.py line 115 22900] Train: [61/100][110/156] Data 0.001 (0.001) Batch 4.464 (3.461) Remain 05:53:33 loss: 0.3051 Lr: 0.01830
[2023-08-08 04:26:23,531 INFO misc.py line 115 22900] Train: [61/100][111/156] Data 0.001 (0.001) Batch 3.697 (3.463) Remain 05:53:43 loss: 0.3147 Lr: 0.01830
[2023-08-08 04:26:27,534 INFO misc.py line 115 22900] Train: [61/100][112/156] Data 0.001 (0.001) Batch 4.003 (3.468) Remain 05:54:10 loss: 0.3880 Lr: 0.01829
[2023-08-08 04:26:31,586 INFO misc.py line 115 22900] Train: [61/100][113/156] Data 0.001 (0.001) Batch 4.052 (3.473) Remain 05:54:39 loss: 0.1228 Lr: 0.01829
[2023-08-08 04:26:34,434 INFO misc.py line 115 22900] Train: [61/100][114/156] Data 0.001 (0.001) Batch 2.847 (3.467) Remain 05:54:01 loss: 0.2577 Lr: 0.01828
[2023-08-08 04:26:38,405 INFO misc.py line 115 22900] Train: [61/100][115/156] Data 0.001 (0.001) Batch 3.971 (3.472) Remain 05:54:25 loss: 0.2568 Lr: 0.01828
[2023-08-08 04:26:41,476 INFO misc.py line 115 22900] Train: [61/100][116/156] Data 0.001 (0.001) Batch 3.071 (3.468) Remain 05:54:00 loss: 0.1742 Lr: 0.01827
[2023-08-08 04:26:44,747 INFO misc.py line 115 22900] Train: [61/100][117/156] Data 0.001 (0.001) Batch 3.271 (3.467) Remain 05:53:45 loss: 0.1023 Lr: 0.01827
[2023-08-08 04:26:48,153 INFO misc.py line 115 22900] Train: [61/100][118/156] Data 0.001 (0.001) Batch 3.406 (3.466) Remain 05:53:39 loss: 0.2176 Lr: 0.01826
[2023-08-08 04:26:52,153 INFO misc.py line 115 22900] Train: [61/100][119/156] Data 0.001 (0.001) Batch 4.000 (3.471) Remain 05:54:03 loss: 0.2437 Lr: 0.01826
[2023-08-08 04:26:55,965 INFO misc.py line 115 22900] Train: [61/100][120/156] Data 0.001 (0.001) Batch 3.812 (3.474) Remain 05:54:18 loss: 0.1367 Lr: 0.01825
[2023-08-08 04:26:59,934 INFO misc.py line 115 22900] Train: [61/100][121/156] Data 0.001 (0.001) Batch 3.968 (3.478) Remain 05:54:40 loss: 0.2735 Lr: 0.01825
[2023-08-08 04:27:03,639 INFO misc.py line 115 22900] Train: [61/100][122/156] Data 0.001 (0.001) Batch 3.706 (3.480) Remain 05:54:48 loss: 0.1027 Lr: 0.01824
[2023-08-08 04:27:05,621 INFO misc.py line 115 22900] Train: [61/100][123/156] Data 0.001 (0.001) Batch 1.982 (3.467) Remain 05:53:28 loss: 0.2202 Lr: 0.01824
[2023-08-08 04:27:09,577 INFO misc.py line 115 22900] Train: [61/100][124/156] Data 0.001 (0.001) Batch 3.956 (3.471) Remain 05:53:50 loss: 0.2016 Lr: 0.01823
[2023-08-08 04:27:13,586 INFO misc.py line 115 22900] Train: [61/100][125/156] Data 0.001 (0.001) Batch 4.009 (3.476) Remain 05:54:13 loss: 0.2179 Lr: 0.01823
[2023-08-08 04:27:17,240 INFO misc.py line 115 22900] Train: [61/100][126/156] Data 0.001 (0.001) Batch 3.654 (3.477) Remain 05:54:19 loss: 0.0883 Lr: 0.01822
[2023-08-08 04:27:20,640 INFO misc.py line 115 22900] Train: [61/100][127/156] Data 0.001 (0.001) Batch 3.400 (3.476) Remain 05:54:11 loss: 0.2097 Lr: 0.01822
[2023-08-08 04:27:24,896 INFO misc.py line 115 22900] Train: [61/100][128/156] Data 0.001 (0.001) Batch 4.256 (3.483) Remain 05:54:46 loss: 0.2341 Lr: 0.01821
[2023-08-08 04:27:28,072 INFO misc.py line 115 22900] Train: [61/100][129/156] Data 0.001 (0.001) Batch 3.177 (3.480) Remain 05:54:28 loss: 0.1420 Lr: 0.01821
[2023-08-08 04:27:30,817 INFO misc.py line 115 22900] Train: [61/100][130/156] Data 0.001 (0.001) Batch 2.745 (3.475) Remain 05:53:49 loss: 0.1326 Lr: 0.01820
[2023-08-08 04:27:34,291 INFO misc.py line 115 22900] Train: [61/100][131/156] Data 0.001 (0.001) Batch 3.474 (3.475) Remain 05:53:45 loss: 0.1324 Lr: 0.01820
[2023-08-08 04:27:37,788 INFO misc.py line 115 22900] Train: [61/100][132/156] Data 0.001 (0.001) Batch 3.497 (3.475) Remain 05:53:43 loss: 0.2051 Lr: 0.01819
[2023-08-08 04:27:41,793 INFO misc.py line 115 22900] Train: [61/100][133/156] Data 0.001 (0.001) Batch 4.005 (3.479) Remain 05:54:04 loss: 0.3715 Lr: 0.01819
[2023-08-08 04:27:45,854 INFO misc.py line 115 22900] Train: [61/100][134/156] Data 0.001 (0.001) Batch 4.061 (3.483) Remain 05:54:28 loss: 0.5713 Lr: 0.01818
[2023-08-08 04:27:49,977 INFO misc.py line 115 22900] Train: [61/100][135/156] Data 0.001 (0.001) Batch 4.122 (3.488) Remain 05:54:54 loss: 0.2222 Lr: 0.01818
[2023-08-08 04:27:53,998 INFO misc.py line 115 22900] Train: [61/100][136/156] Data 0.001 (0.001) Batch 4.021 (3.492) Remain 05:55:15 loss: 0.2132 Lr: 0.01817
[2023-08-08 04:27:56,896 INFO misc.py line 115 22900] Train: [61/100][137/156] Data 0.001 (0.001) Batch 2.898 (3.488) Remain 05:54:44 loss: 0.2962 Lr: 0.01817
[2023-08-08 04:28:00,659 INFO misc.py line 115 22900] Train: [61/100][138/156] Data 0.001 (0.001) Batch 3.763 (3.490) Remain 05:54:53 loss: 0.2620 Lr: 0.01816
[2023-08-08 04:28:05,475 INFO misc.py line 115 22900] Train: [61/100][139/156] Data 0.001 (0.001) Batch 4.816 (3.499) Remain 05:55:49 loss: 0.2288 Lr: 0.01816
[2023-08-08 04:28:08,034 INFO misc.py line 115 22900] Train: [61/100][140/156] Data 0.001 (0.001) Batch 2.559 (3.493) Remain 05:55:04 loss: 0.1630 Lr: 0.01815
[2023-08-08 04:28:10,367 INFO misc.py line 115 22900] Train: [61/100][141/156] Data 0.001 (0.001) Batch 2.333 (3.484) Remain 05:54:09 loss: 0.0904 Lr: 0.01815
[2023-08-08 04:28:14,449 INFO misc.py line 115 22900] Train: [61/100][142/156] Data 0.001 (0.001) Batch 4.082 (3.488) Remain 05:54:32 loss: 0.2770 Lr: 0.01814
[2023-08-08 04:28:17,382 INFO misc.py line 115 22900] Train: [61/100][143/156] Data 0.001 (0.001) Batch 2.933 (3.484) Remain 05:54:04 loss: 0.1758 Lr: 0.01814
[2023-08-08 04:28:21,416 INFO misc.py line 115 22900] Train: [61/100][144/156] Data 0.001 (0.001) Batch 4.034 (3.488) Remain 05:54:25 loss: 0.2192 Lr: 0.01813
[2023-08-08 04:28:25,300 INFO misc.py line 115 22900] Train: [61/100][145/156] Data 0.001 (0.001) Batch 3.883 (3.491) Remain 05:54:38 loss: 0.1275 Lr: 0.01813
[2023-08-08 04:28:28,552 INFO misc.py line 115 22900] Train: [61/100][146/156] Data 0.001 (0.001) Batch 3.252 (3.489) Remain 05:54:24 loss: 0.2952 Lr: 0.01812
[2023-08-08 04:28:32,989 INFO misc.py line 115 22900] Train: [61/100][147/156] Data 0.001 (0.001) Batch 4.437 (3.496) Remain 05:55:01 loss: 0.2397 Lr: 0.01812
[2023-08-08 04:28:35,689 INFO misc.py line 115 22900] Train: [61/100][148/156] Data 0.001 (0.001) Batch 2.701 (3.491) Remain 05:54:24 loss: 0.1716 Lr: 0.01811
[2023-08-08 04:28:39,024 INFO misc.py line 115 22900] Train: [61/100][149/156] Data 0.001 (0.001) Batch 3.335 (3.490) Remain 05:54:14 loss: 0.1956 Lr: 0.01811
[2023-08-08 04:28:41,966 INFO misc.py line 115 22900] Train: [61/100][150/156] Data 0.001 (0.001) Batch 2.942 (3.486) Remain 05:53:48 loss: 0.1717 Lr: 0.01810
[2023-08-08 04:28:45,953 INFO misc.py line 115 22900] Train: [61/100][151/156] Data 0.001 (0.001) Batch 3.987 (3.489) Remain 05:54:05 loss: 0.2837 Lr: 0.01810
[2023-08-08 04:28:48,642 INFO misc.py line 115 22900] Train: [61/100][152/156] Data 0.001 (0.001) Batch 2.690 (3.484) Remain 05:53:29 loss: 0.3061 Lr: 0.01809
[2023-08-08 04:28:51,654 INFO misc.py line 115 22900] Train: [61/100][153/156] Data 0.001 (0.001) Batch 3.011 (3.481) Remain 05:53:06 loss: 0.1823 Lr: 0.01809
[2023-08-08 04:28:54,446 INFO misc.py line 115 22900] Train: [61/100][154/156] Data 0.001 (0.001) Batch 2.793 (3.476) Remain 05:52:35 loss: 0.0997 Lr: 0.01808
[2023-08-08 04:28:57,335 INFO misc.py line 115 22900] Train: [61/100][155/156] Data 0.001 (0.001) Batch 2.889 (3.472) Remain 05:52:08 loss: 0.1017 Lr: 0.01808
[2023-08-08 04:29:00,487 INFO misc.py line 115 22900] Train: [61/100][156/156] Data 0.001 (0.001) Batch 3.152 (3.470) Remain 05:51:52 loss: 0.1863 Lr: 0.01807
[2023-08-08 04:29:00,488 INFO misc.py line 129 22900] Train result: loss: 0.2160 
[2023-08-08 04:29:00,488 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 04:29:02,605 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.0188 
[2023-08-08 04:29:03,475 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.4805 
[2023-08-08 04:29:05,136 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.6433 
[2023-08-08 04:29:06,657 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.3058 
[2023-08-08 04:29:08,500 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.3846 
[2023-08-08 04:29:10,166 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.5807 
[2023-08-08 04:29:12,304 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.4689 
[2023-08-08 04:29:14,108 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.1544 
[2023-08-08 04:29:15,393 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.3776 
[2023-08-08 04:29:17,524 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.1778 
[2023-08-08 04:29:18,049 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.5107 
[2023-08-08 04:29:19,583 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.8194 
[2023-08-08 04:29:22,293 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.1855 
[2023-08-08 04:29:23,971 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.9365 
[2023-08-08 04:29:25,994 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.3785 
[2023-08-08 04:29:28,704 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.1431 
[2023-08-08 04:29:31,412 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.5578 
[2023-08-08 04:29:33,258 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.0734 
[2023-08-08 04:29:34,006 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.0663 
[2023-08-08 04:29:34,892 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.7981 
[2023-08-08 04:29:37,154 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.2777 
[2023-08-08 04:29:39,117 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.3209 
[2023-08-08 04:29:40,964 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.9400 
[2023-08-08 04:29:42,900 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.6219 
[2023-08-08 04:29:42,948 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2650/0.3565/0.7067.
[2023-08-08 04:29:42,948 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6576/0.9395
[2023-08-08 04:29:42,948 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9460/0.9912
[2023-08-08 04:29:42,948 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1679/0.4276
[2023-08-08 04:29:42,948 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1788/0.2415
[2023-08-08 04:29:42,948 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6459/0.7357
[2023-08-08 04:29:42,948 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.3368/0.4376
[2023-08-08 04:29:42,948 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5849/0.7114
[2023-08-08 04:29:42,948 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.3036/0.3657
[2023-08-08 04:29:42,948 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.2436/0.4504
[2023-08-08 04:29:42,948 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0100/0.0101
[2023-08-08 04:29:42,948 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 04:29:42,948 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0103/0.0108
[2023-08-08 04:29:42,948 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.3650/0.5470
[2023-08-08 04:29:42,948 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0283/0.0311
[2023-08-08 04:29:42,948 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0390/0.0522
[2023-08-08 04:29:42,948 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.1497/0.1502
[2023-08-08 04:29:42,948 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.1422/0.2530
[2023-08-08 04:29:42,948 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.3966/0.5191
[2023-08-08 04:29:42,948 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 04:29:42,948 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0942/0.2561
[2023-08-08 04:29:42,949 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 04:29:42,949 INFO misc.py line 150 22900] Best validation mIoU updated to: 0.2650
[2023-08-08 04:29:42,949 INFO misc.py line 152 22900] Currently Best mIoU: 0.2650
[2023-08-08 04:29:42,949 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 04:29:47,713 INFO misc.py line 115 22900] Train: [62/100][1/156] Data 0.735 (0.735) Batch 3.670 (3.670) Remain 06:12:05 loss: 0.1114 Lr: 0.01807
[2023-08-08 04:29:50,447 INFO misc.py line 115 22900] Train: [62/100][2/156] Data 0.001 (0.001) Batch 2.733 (2.733) Remain 04:37:04 loss: 0.1619 Lr: 0.01806
[2023-08-08 04:29:53,773 INFO misc.py line 115 22900] Train: [62/100][3/156] Data 0.001 (0.001) Batch 3.327 (3.327) Remain 05:37:10 loss: 0.2830 Lr: 0.01806
[2023-08-08 04:29:58,285 INFO misc.py line 115 22900] Train: [62/100][4/156] Data 0.001 (0.001) Batch 4.511 (4.511) Remain 07:37:08 loss: 0.1769 Lr: 0.01805
[2023-08-08 04:30:00,464 INFO misc.py line 115 22900] Train: [62/100][5/156] Data 0.001 (0.001) Batch 2.180 (3.345) Remain 05:38:56 loss: 0.0780 Lr: 0.01804
[2023-08-08 04:30:04,965 INFO misc.py line 115 22900] Train: [62/100][6/156] Data 0.001 (0.001) Batch 4.501 (3.730) Remain 06:17:53 loss: 0.3062 Lr: 0.01804
[2023-08-08 04:30:08,274 INFO misc.py line 115 22900] Train: [62/100][7/156] Data 0.001 (0.001) Batch 3.310 (3.625) Remain 06:07:10 loss: 0.1793 Lr: 0.01803
[2023-08-08 04:30:11,860 INFO misc.py line 115 22900] Train: [62/100][8/156] Data 0.001 (0.001) Batch 3.586 (3.617) Remain 06:06:19 loss: 0.1962 Lr: 0.01803
[2023-08-08 04:30:15,212 INFO misc.py line 115 22900] Train: [62/100][9/156] Data 0.001 (0.001) Batch 3.351 (3.573) Remain 06:01:46 loss: 0.1344 Lr: 0.01802
[2023-08-08 04:30:18,971 INFO misc.py line 115 22900] Train: [62/100][10/156] Data 0.001 (0.001) Batch 3.759 (3.600) Remain 06:04:24 loss: 0.1214 Lr: 0.01802
[2023-08-08 04:30:22,012 INFO misc.py line 115 22900] Train: [62/100][11/156] Data 0.001 (0.001) Batch 3.042 (3.530) Remain 05:57:16 loss: 0.1608 Lr: 0.01801
[2023-08-08 04:30:25,236 INFO misc.py line 115 22900] Train: [62/100][12/156] Data 0.001 (0.001) Batch 3.223 (3.496) Remain 05:53:46 loss: 0.1886 Lr: 0.01801
[2023-08-08 04:30:29,138 INFO misc.py line 115 22900] Train: [62/100][13/156] Data 0.001 (0.001) Batch 3.902 (3.536) Remain 05:57:49 loss: 0.2443 Lr: 0.01800
[2023-08-08 04:30:31,957 INFO misc.py line 115 22900] Train: [62/100][14/156] Data 0.001 (0.001) Batch 2.819 (3.471) Remain 05:51:10 loss: 0.1055 Lr: 0.01800
[2023-08-08 04:30:36,057 INFO misc.py line 115 22900] Train: [62/100][15/156] Data 0.001 (0.001) Batch 4.100 (3.524) Remain 05:56:25 loss: 0.2178 Lr: 0.01799
[2023-08-08 04:30:40,141 INFO misc.py line 115 22900] Train: [62/100][16/156] Data 0.001 (0.001) Batch 4.083 (3.567) Remain 06:00:42 loss: 0.2463 Lr: 0.01799
[2023-08-08 04:30:44,178 INFO misc.py line 115 22900] Train: [62/100][17/156] Data 0.001 (0.001) Batch 4.037 (3.600) Remain 06:04:03 loss: 0.1050 Lr: 0.01798
[2023-08-08 04:30:46,794 INFO misc.py line 115 22900] Train: [62/100][18/156] Data 0.001 (0.001) Batch 2.617 (3.535) Remain 05:57:21 loss: 0.1611 Lr: 0.01798
[2023-08-08 04:30:49,668 INFO misc.py line 115 22900] Train: [62/100][19/156] Data 0.001 (0.001) Batch 2.874 (3.493) Remain 05:53:07 loss: 0.0839 Lr: 0.01797
[2023-08-08 04:30:53,720 INFO misc.py line 115 22900] Train: [62/100][20/156] Data 0.001 (0.001) Batch 4.052 (3.526) Remain 05:56:23 loss: 0.3349 Lr: 0.01797
[2023-08-08 04:30:57,053 INFO misc.py line 115 22900] Train: [62/100][21/156] Data 0.001 (0.001) Batch 3.333 (3.516) Remain 05:55:14 loss: 0.2240 Lr: 0.01796
[2023-08-08 04:31:00,532 INFO misc.py line 115 22900] Train: [62/100][22/156] Data 0.001 (0.001) Batch 3.480 (3.514) Remain 05:54:59 loss: 0.1355 Lr: 0.01796
[2023-08-08 04:31:04,622 INFO misc.py line 115 22900] Train: [62/100][23/156] Data 0.001 (0.001) Batch 4.089 (3.542) Remain 05:57:50 loss: 0.3733 Lr: 0.01795
[2023-08-08 04:31:08,413 INFO misc.py line 115 22900] Train: [62/100][24/156] Data 0.001 (0.001) Batch 3.791 (3.554) Remain 05:58:58 loss: 0.1977 Lr: 0.01795
[2023-08-08 04:31:11,648 INFO misc.py line 115 22900] Train: [62/100][25/156] Data 0.001 (0.001) Batch 3.235 (3.540) Remain 05:57:27 loss: 0.2214 Lr: 0.01794
[2023-08-08 04:31:15,718 INFO misc.py line 115 22900] Train: [62/100][26/156] Data 0.001 (0.001) Batch 4.070 (3.563) Remain 05:59:43 loss: 0.2092 Lr: 0.01794
[2023-08-08 04:31:19,048 INFO misc.py line 115 22900] Train: [62/100][27/156] Data 0.001 (0.001) Batch 3.330 (3.553) Remain 05:58:41 loss: 0.1574 Lr: 0.01793
[2023-08-08 04:31:21,690 INFO misc.py line 115 22900] Train: [62/100][28/156] Data 0.001 (0.001) Batch 2.642 (3.517) Remain 05:54:57 loss: 0.1020 Lr: 0.01793
[2023-08-08 04:31:25,677 INFO misc.py line 115 22900] Train: [62/100][29/156] Data 0.001 (0.001) Batch 3.986 (3.535) Remain 05:56:42 loss: 0.1654 Lr: 0.01792
[2023-08-08 04:31:28,928 INFO misc.py line 115 22900] Train: [62/100][30/156] Data 0.001 (0.001) Batch 3.251 (3.524) Remain 05:55:35 loss: 0.2283 Lr: 0.01792
[2023-08-08 04:31:32,300 INFO misc.py line 115 22900] Train: [62/100][31/156] Data 0.001 (0.001) Batch 3.372 (3.519) Remain 05:54:59 loss: 0.1295 Lr: 0.01791
[2023-08-08 04:31:36,199 INFO misc.py line 115 22900] Train: [62/100][32/156] Data 0.001 (0.001) Batch 3.899 (3.532) Remain 05:56:15 loss: 0.3296 Lr: 0.01791
[2023-08-08 04:31:39,705 INFO misc.py line 115 22900] Train: [62/100][33/156] Data 0.001 (0.001) Batch 3.506 (3.531) Remain 05:56:06 loss: 0.1418 Lr: 0.01790
[2023-08-08 04:31:42,768 INFO misc.py line 115 22900] Train: [62/100][34/156] Data 0.001 (0.001) Batch 3.064 (3.516) Remain 05:54:31 loss: 0.2487 Lr: 0.01790
[2023-08-08 04:31:46,027 INFO misc.py line 115 22900] Train: [62/100][35/156] Data 0.001 (0.001) Batch 3.258 (3.508) Remain 05:53:39 loss: 0.1325 Lr: 0.01789
[2023-08-08 04:31:49,659 INFO misc.py line 115 22900] Train: [62/100][36/156] Data 0.001 (0.001) Batch 3.632 (3.512) Remain 05:53:58 loss: 0.3211 Lr: 0.01789
[2023-08-08 04:31:53,213 INFO misc.py line 115 22900] Train: [62/100][37/156] Data 0.001 (0.001) Batch 3.554 (3.513) Remain 05:54:02 loss: 0.1316 Lr: 0.01788
[2023-08-08 04:31:56,461 INFO misc.py line 115 22900] Train: [62/100][38/156] Data 0.001 (0.001) Batch 3.247 (3.505) Remain 05:53:13 loss: 0.2050 Lr: 0.01788
[2023-08-08 04:31:59,974 INFO misc.py line 115 22900] Train: [62/100][39/156] Data 0.001 (0.001) Batch 3.513 (3.506) Remain 05:53:11 loss: 0.1123 Lr: 0.01787
[2023-08-08 04:32:04,238 INFO misc.py line 115 22900] Train: [62/100][40/156] Data 0.001 (0.001) Batch 4.264 (3.526) Remain 05:55:11 loss: 0.3399 Lr: 0.01787
[2023-08-08 04:32:07,763 INFO misc.py line 115 22900] Train: [62/100][41/156] Data 0.001 (0.001) Batch 3.526 (3.526) Remain 05:55:07 loss: 0.4358 Lr: 0.01786
[2023-08-08 04:32:11,491 INFO misc.py line 115 22900] Train: [62/100][42/156] Data 0.001 (0.001) Batch 3.728 (3.531) Remain 05:55:35 loss: 0.1559 Lr: 0.01786
[2023-08-08 04:32:15,537 INFO misc.py line 115 22900] Train: [62/100][43/156] Data 0.001 (0.001) Batch 4.046 (3.544) Remain 05:56:49 loss: 0.3816 Lr: 0.01785
[2023-08-08 04:32:18,633 INFO misc.py line 115 22900] Train: [62/100][44/156] Data 0.001 (0.001) Batch 3.096 (3.533) Remain 05:55:40 loss: 0.2619 Lr: 0.01785
[2023-08-08 04:32:22,121 INFO misc.py line 115 22900] Train: [62/100][45/156] Data 0.001 (0.001) Batch 3.488 (3.532) Remain 05:55:30 loss: 0.1958 Lr: 0.01784
[2023-08-08 04:32:26,181 INFO misc.py line 115 22900] Train: [62/100][46/156] Data 0.001 (0.001) Batch 4.060 (3.544) Remain 05:56:40 loss: 0.5072 Lr: 0.01784
[2023-08-08 04:32:29,713 INFO misc.py line 115 22900] Train: [62/100][47/156] Data 0.001 (0.001) Batch 3.531 (3.544) Remain 05:56:35 loss: 0.2032 Lr: 0.01783
[2023-08-08 04:32:33,750 INFO misc.py line 115 22900] Train: [62/100][48/156] Data 0.001 (0.001) Batch 4.037 (3.555) Remain 05:57:38 loss: 0.3685 Lr: 0.01783
[2023-08-08 04:32:36,885 INFO misc.py line 115 22900] Train: [62/100][49/156] Data 0.001 (0.001) Batch 3.135 (3.546) Remain 05:56:39 loss: 0.1717 Lr: 0.01782
[2023-08-08 04:32:39,721 INFO misc.py line 115 22900] Train: [62/100][50/156] Data 0.001 (0.001) Batch 2.836 (3.531) Remain 05:55:04 loss: 0.0608 Lr: 0.01782
[2023-08-08 04:32:43,452 INFO misc.py line 115 22900] Train: [62/100][51/156] Data 0.001 (0.001) Batch 3.731 (3.535) Remain 05:55:26 loss: 0.2510 Lr: 0.01781
[2023-08-08 04:32:46,879 INFO misc.py line 115 22900] Train: [62/100][52/156] Data 0.001 (0.001) Batch 3.427 (3.533) Remain 05:55:09 loss: 0.3298 Lr: 0.01781
[2023-08-08 04:32:50,151 INFO misc.py line 115 22900] Train: [62/100][53/156] Data 0.001 (0.001) Batch 3.272 (3.528) Remain 05:54:34 loss: 0.2638 Lr: 0.01780
[2023-08-08 04:32:52,769 INFO misc.py line 115 22900] Train: [62/100][54/156] Data 0.001 (0.001) Batch 2.619 (3.510) Remain 05:52:43 loss: 0.1421 Lr: 0.01780
[2023-08-08 04:32:56,687 INFO misc.py line 115 22900] Train: [62/100][55/156] Data 0.001 (0.001) Batch 3.918 (3.518) Remain 05:53:27 loss: 0.1366 Lr: 0.01779
[2023-08-08 04:32:59,824 INFO misc.py line 115 22900] Train: [62/100][56/156] Data 0.001 (0.001) Batch 3.137 (3.510) Remain 05:52:40 loss: 0.0809 Lr: 0.01779
[2023-08-08 04:33:03,431 INFO misc.py line 115 22900] Train: [62/100][57/156] Data 0.001 (0.001) Batch 3.607 (3.512) Remain 05:52:47 loss: 0.2642 Lr: 0.01778
[2023-08-08 04:33:07,564 INFO misc.py line 115 22900] Train: [62/100][58/156] Data 0.001 (0.001) Batch 4.132 (3.523) Remain 05:53:52 loss: 0.4956 Lr: 0.01778
[2023-08-08 04:33:11,023 INFO misc.py line 115 22900] Train: [62/100][59/156] Data 0.001 (0.001) Batch 3.460 (3.522) Remain 05:53:41 loss: 0.3041 Lr: 0.01777
[2023-08-08 04:33:15,017 INFO misc.py line 115 22900] Train: [62/100][60/156] Data 0.001 (0.001) Batch 3.994 (3.531) Remain 05:54:28 loss: 0.3948 Lr: 0.01777
[2023-08-08 04:33:18,184 INFO misc.py line 115 22900] Train: [62/100][61/156] Data 0.001 (0.001) Batch 3.167 (3.524) Remain 05:53:46 loss: 0.1637 Lr: 0.01776
[2023-08-08 04:33:21,382 INFO misc.py line 115 22900] Train: [62/100][62/156] Data 0.001 (0.001) Batch 3.198 (3.519) Remain 05:53:10 loss: 0.2824 Lr: 0.01776
[2023-08-08 04:33:25,117 INFO misc.py line 115 22900] Train: [62/100][63/156] Data 0.001 (0.001) Batch 3.735 (3.522) Remain 05:53:28 loss: 0.1239 Lr: 0.01775
[2023-08-08 04:33:28,947 INFO misc.py line 115 22900] Train: [62/100][64/156] Data 0.001 (0.001) Batch 3.829 (3.527) Remain 05:53:55 loss: 0.1151 Lr: 0.01775
[2023-08-08 04:33:32,529 INFO misc.py line 115 22900] Train: [62/100][65/156] Data 0.001 (0.001) Batch 3.582 (3.528) Remain 05:53:56 loss: 0.1599 Lr: 0.01774
[2023-08-08 04:33:36,580 INFO misc.py line 115 22900] Train: [62/100][66/156] Data 0.001 (0.001) Batch 4.051 (3.537) Remain 05:54:43 loss: 0.2383 Lr: 0.01773
[2023-08-08 04:33:40,648 INFO misc.py line 115 22900] Train: [62/100][67/156] Data 0.001 (0.001) Batch 4.069 (3.545) Remain 05:55:29 loss: 0.2550 Lr: 0.01773
[2023-08-08 04:33:44,114 INFO misc.py line 115 22900] Train: [62/100][68/156] Data 0.001 (0.001) Batch 3.466 (3.544) Remain 05:55:18 loss: 0.2408 Lr: 0.01772
[2023-08-08 04:33:47,308 INFO misc.py line 115 22900] Train: [62/100][69/156] Data 0.001 (0.001) Batch 3.193 (3.538) Remain 05:54:43 loss: 0.1545 Lr: 0.01772
[2023-08-08 04:33:50,859 INFO misc.py line 115 22900] Train: [62/100][70/156] Data 0.001 (0.001) Batch 3.552 (3.539) Remain 05:54:41 loss: 0.1869 Lr: 0.01771
[2023-08-08 04:33:53,538 INFO misc.py line 115 22900] Train: [62/100][71/156] Data 0.001 (0.001) Batch 2.679 (3.526) Remain 05:53:21 loss: 0.1275 Lr: 0.01771
[2023-08-08 04:33:57,268 INFO misc.py line 115 22900] Train: [62/100][72/156] Data 0.001 (0.001) Batch 3.730 (3.529) Remain 05:53:35 loss: 0.1443 Lr: 0.01770
[2023-08-08 04:34:00,746 INFO misc.py line 115 22900] Train: [62/100][73/156] Data 0.001 (0.001) Batch 3.478 (3.528) Remain 05:53:27 loss: 0.2102 Lr: 0.01770
[2023-08-08 04:34:04,358 INFO misc.py line 115 22900] Train: [62/100][74/156] Data 0.001 (0.001) Batch 3.611 (3.529) Remain 05:53:31 loss: 0.1850 Lr: 0.01769
[2023-08-08 04:34:07,043 INFO misc.py line 115 22900] Train: [62/100][75/156] Data 0.001 (0.001) Batch 2.685 (3.518) Remain 05:52:17 loss: 0.2913 Lr: 0.01769
[2023-08-08 04:34:10,432 INFO misc.py line 115 22900] Train: [62/100][76/156] Data 0.001 (0.001) Batch 3.389 (3.516) Remain 05:52:03 loss: 0.2366 Lr: 0.01768
[2023-08-08 04:34:13,365 INFO misc.py line 115 22900] Train: [62/100][77/156] Data 0.001 (0.001) Batch 2.933 (3.508) Remain 05:51:12 loss: 0.1333 Lr: 0.01768
[2023-08-08 04:34:16,189 INFO misc.py line 115 22900] Train: [62/100][78/156] Data 0.001 (0.001) Batch 2.824 (3.499) Remain 05:50:14 loss: 0.0916 Lr: 0.01767
[2023-08-08 04:34:19,749 INFO misc.py line 115 22900] Train: [62/100][79/156] Data 0.001 (0.001) Batch 3.560 (3.500) Remain 05:50:15 loss: 0.2196 Lr: 0.01767
[2023-08-08 04:34:23,214 INFO misc.py line 115 22900] Train: [62/100][80/156] Data 0.001 (0.001) Batch 3.465 (3.499) Remain 05:50:09 loss: 0.1691 Lr: 0.01766
[2023-08-08 04:34:27,574 INFO misc.py line 115 22900] Train: [62/100][81/156] Data 0.001 (0.001) Batch 4.360 (3.510) Remain 05:51:12 loss: 0.2657 Lr: 0.01766
[2023-08-08 04:34:30,698 INFO misc.py line 115 22900] Train: [62/100][82/156] Data 0.001 (0.001) Batch 3.124 (3.505) Remain 05:50:39 loss: 0.3008 Lr: 0.01765
[2023-08-08 04:34:33,589 INFO misc.py line 115 22900] Train: [62/100][83/156] Data 0.001 (0.001) Batch 2.891 (3.498) Remain 05:49:49 loss: 0.3634 Lr: 0.01765
[2023-08-08 04:34:37,649 INFO misc.py line 115 22900] Train: [62/100][84/156] Data 0.001 (0.001) Batch 4.060 (3.505) Remain 05:50:27 loss: 0.2255 Lr: 0.01764
[2023-08-08 04:34:39,609 INFO misc.py line 115 22900] Train: [62/100][85/156] Data 0.001 (0.001) Batch 1.960 (3.486) Remain 05:48:31 loss: 0.1717 Lr: 0.01764
[2023-08-08 04:34:43,138 INFO misc.py line 115 22900] Train: [62/100][86/156] Data 0.001 (0.001) Batch 3.529 (3.486) Remain 05:48:30 loss: 0.2475 Lr: 0.01763
[2023-08-08 04:34:47,267 INFO misc.py line 115 22900] Train: [62/100][87/156] Data 0.001 (0.001) Batch 4.129 (3.494) Remain 05:49:13 loss: 0.2923 Lr: 0.01763
[2023-08-08 04:34:51,214 INFO misc.py line 115 22900] Train: [62/100][88/156] Data 0.001 (0.001) Batch 3.947 (3.499) Remain 05:49:41 loss: 0.2762 Lr: 0.01762
[2023-08-08 04:34:54,479 INFO misc.py line 115 22900] Train: [62/100][89/156] Data 0.001 (0.001) Batch 3.265 (3.497) Remain 05:49:21 loss: 0.4103 Lr: 0.01762
[2023-08-08 04:34:57,828 INFO misc.py line 115 22900] Train: [62/100][90/156] Data 0.001 (0.001) Batch 3.349 (3.495) Remain 05:49:08 loss: 0.4905 Lr: 0.01761
[2023-08-08 04:35:01,403 INFO misc.py line 115 22900] Train: [62/100][91/156] Data 0.001 (0.001) Batch 3.575 (3.496) Remain 05:49:10 loss: 0.1374 Lr: 0.01761
[2023-08-08 04:35:05,455 INFO misc.py line 115 22900] Train: [62/100][92/156] Data 0.001 (0.001) Batch 4.052 (3.502) Remain 05:49:44 loss: 0.1800 Lr: 0.01760
[2023-08-08 04:35:09,391 INFO misc.py line 115 22900] Train: [62/100][93/156] Data 0.001 (0.001) Batch 3.936 (3.507) Remain 05:50:09 loss: 0.2766 Lr: 0.01760
[2023-08-08 04:35:12,387 INFO misc.py line 115 22900] Train: [62/100][94/156] Data 0.001 (0.001) Batch 2.996 (3.501) Remain 05:49:32 loss: 0.1136 Lr: 0.01759
[2023-08-08 04:35:16,657 INFO misc.py line 115 22900] Train: [62/100][95/156] Data 0.001 (0.001) Batch 4.270 (3.510) Remain 05:50:19 loss: 0.2902 Lr: 0.01759
[2023-08-08 04:35:20,223 INFO misc.py line 115 22900] Train: [62/100][96/156] Data 0.001 (0.001) Batch 3.566 (3.510) Remain 05:50:19 loss: 0.2177 Lr: 0.01758
[2023-08-08 04:35:23,276 INFO misc.py line 115 22900] Train: [62/100][97/156] Data 0.001 (0.001) Batch 3.053 (3.505) Remain 05:49:46 loss: 0.2794 Lr: 0.01758
[2023-08-08 04:35:25,848 INFO misc.py line 115 22900] Train: [62/100][98/156] Data 0.001 (0.001) Batch 2.572 (3.496) Remain 05:48:44 loss: 0.0912 Lr: 0.01757
[2023-08-08 04:35:30,089 INFO misc.py line 115 22900] Train: [62/100][99/156] Data 0.001 (0.001) Batch 4.241 (3.503) Remain 05:49:27 loss: 0.2806 Lr: 0.01757
[2023-08-08 04:35:33,562 INFO misc.py line 115 22900] Train: [62/100][100/156] Data 0.001 (0.001) Batch 3.473 (3.503) Remain 05:49:21 loss: 0.2213 Lr: 0.01756
[2023-08-08 04:35:36,805 INFO misc.py line 115 22900] Train: [62/100][101/156] Data 0.001 (0.001) Batch 3.243 (3.500) Remain 05:49:02 loss: 0.2658 Lr: 0.01756
[2023-08-08 04:35:39,367 INFO misc.py line 115 22900] Train: [62/100][102/156] Data 0.001 (0.001) Batch 2.562 (3.491) Remain 05:48:02 loss: 0.1540 Lr: 0.01755
[2023-08-08 04:35:42,529 INFO misc.py line 115 22900] Train: [62/100][103/156] Data 0.001 (0.001) Batch 3.162 (3.488) Remain 05:47:39 loss: 0.2223 Lr: 0.01755
[2023-08-08 04:35:45,802 INFO misc.py line 115 22900] Train: [62/100][104/156] Data 0.001 (0.001) Batch 3.273 (3.485) Remain 05:47:22 loss: 0.2493 Lr: 0.01754
[2023-08-08 04:35:49,217 INFO misc.py line 115 22900] Train: [62/100][105/156] Data 0.001 (0.001) Batch 3.415 (3.485) Remain 05:47:15 loss: 0.1567 Lr: 0.01754
[2023-08-08 04:35:53,220 INFO misc.py line 115 22900] Train: [62/100][106/156] Data 0.001 (0.001) Batch 4.003 (3.490) Remain 05:47:41 loss: 0.2815 Lr: 0.01753
[2023-08-08 04:35:56,478 INFO misc.py line 115 22900] Train: [62/100][107/156] Data 0.001 (0.001) Batch 3.257 (3.488) Remain 05:47:25 loss: 0.1630 Lr: 0.01753
[2023-08-08 04:36:00,804 INFO misc.py line 115 22900] Train: [62/100][108/156] Data 0.001 (0.001) Batch 4.326 (3.496) Remain 05:48:09 loss: 0.3941 Lr: 0.01752
[2023-08-08 04:36:05,274 INFO misc.py line 115 22900] Train: [62/100][109/156] Data 0.001 (0.001) Batch 4.471 (3.505) Remain 05:49:00 loss: 0.3501 Lr: 0.01752
[2023-08-08 04:36:08,937 INFO misc.py line 115 22900] Train: [62/100][110/156] Data 0.001 (0.001) Batch 3.663 (3.506) Remain 05:49:06 loss: 0.1560 Lr: 0.01751
[2023-08-08 04:36:12,189 INFO misc.py line 115 22900] Train: [62/100][111/156] Data 0.001 (0.001) Batch 3.251 (3.504) Remain 05:48:48 loss: 0.2146 Lr: 0.01751
[2023-08-08 04:36:14,997 INFO misc.py line 115 22900] Train: [62/100][112/156] Data 0.001 (0.001) Batch 2.809 (3.497) Remain 05:48:06 loss: 0.1536 Lr: 0.01750
[2023-08-08 04:36:18,263 INFO misc.py line 115 22900] Train: [62/100][113/156] Data 0.002 (0.001) Batch 3.266 (3.495) Remain 05:47:50 loss: 0.1398 Lr: 0.01750
[2023-08-08 04:36:22,487 INFO misc.py line 115 22900] Train: [62/100][114/156] Data 0.001 (0.001) Batch 4.224 (3.502) Remain 05:48:26 loss: 0.1803 Lr: 0.01749
[2023-08-08 04:36:26,407 INFO misc.py line 115 22900] Train: [62/100][115/156] Data 0.001 (0.001) Batch 3.920 (3.506) Remain 05:48:45 loss: 0.2325 Lr: 0.01749
[2023-08-08 04:36:30,046 INFO misc.py line 115 22900] Train: [62/100][116/156] Data 0.001 (0.001) Batch 3.639 (3.507) Remain 05:48:48 loss: 0.2399 Lr: 0.01748
[2023-08-08 04:36:32,098 INFO misc.py line 115 22900] Train: [62/100][117/156] Data 0.001 (0.001) Batch 2.052 (3.494) Remain 05:47:29 loss: 0.1653 Lr: 0.01748
[2023-08-08 04:36:36,143 INFO misc.py line 115 22900] Train: [62/100][118/156] Data 0.001 (0.001) Batch 4.044 (3.499) Remain 05:47:54 loss: 0.2675 Lr: 0.01747
[2023-08-08 04:36:39,980 INFO misc.py line 115 22900] Train: [62/100][119/156] Data 0.001 (0.001) Batch 3.837 (3.502) Remain 05:48:08 loss: 0.2477 Lr: 0.01747
[2023-08-08 04:36:42,828 INFO misc.py line 115 22900] Train: [62/100][120/156] Data 0.001 (0.001) Batch 2.848 (3.496) Remain 05:47:31 loss: 0.1056 Lr: 0.01746
[2023-08-08 04:36:45,662 INFO misc.py line 115 22900] Train: [62/100][121/156] Data 0.001 (0.001) Batch 2.834 (3.491) Remain 05:46:54 loss: 0.1185 Lr: 0.01746
[2023-08-08 04:36:49,009 INFO misc.py line 115 22900] Train: [62/100][122/156] Data 0.001 (0.001) Batch 3.347 (3.489) Remain 05:46:43 loss: 0.3172 Lr: 0.01745
[2023-08-08 04:36:53,040 INFO misc.py line 115 22900] Train: [62/100][123/156] Data 0.001 (0.001) Batch 4.032 (3.494) Remain 05:47:07 loss: 0.2653 Lr: 0.01745
[2023-08-08 04:36:56,012 INFO misc.py line 115 22900] Train: [62/100][124/156] Data 0.001 (0.001) Batch 2.972 (3.490) Remain 05:46:37 loss: 0.1876 Lr: 0.01744
[2023-08-08 04:36:59,010 INFO misc.py line 115 22900] Train: [62/100][125/156] Data 0.001 (0.001) Batch 2.999 (3.486) Remain 05:46:10 loss: 0.1916 Lr: 0.01744
[2023-08-08 04:37:03,104 INFO misc.py line 115 22900] Train: [62/100][126/156] Data 0.001 (0.001) Batch 4.093 (3.490) Remain 05:46:36 loss: 0.2759 Lr: 0.01743
[2023-08-08 04:37:07,473 INFO misc.py line 115 22900] Train: [62/100][127/156] Data 0.001 (0.001) Batch 4.369 (3.498) Remain 05:47:15 loss: 0.2807 Lr: 0.01743
[2023-08-08 04:37:09,933 INFO misc.py line 115 22900] Train: [62/100][128/156] Data 0.001 (0.001) Batch 2.460 (3.489) Remain 05:46:22 loss: 0.1227 Lr: 0.01742
[2023-08-08 04:37:12,810 INFO misc.py line 115 22900] Train: [62/100][129/156] Data 0.001 (0.001) Batch 2.876 (3.484) Remain 05:45:49 loss: 0.1355 Lr: 0.01742
[2023-08-08 04:37:16,416 INFO misc.py line 115 22900] Train: [62/100][130/156] Data 0.001 (0.001) Batch 3.606 (3.485) Remain 05:45:51 loss: 0.1591 Lr: 0.01741
[2023-08-08 04:37:20,202 INFO misc.py line 115 22900] Train: [62/100][131/156] Data 0.001 (0.001) Batch 3.786 (3.488) Remain 05:46:02 loss: 0.1245 Lr: 0.01741
[2023-08-08 04:37:23,861 INFO misc.py line 115 22900] Train: [62/100][132/156] Data 0.001 (0.001) Batch 3.660 (3.489) Remain 05:46:06 loss: 0.1529 Lr: 0.01740
[2023-08-08 04:37:28,074 INFO misc.py line 115 22900] Train: [62/100][133/156] Data 0.001 (0.001) Batch 4.213 (3.495) Remain 05:46:36 loss: 0.1729 Lr: 0.01740
[2023-08-08 04:37:30,793 INFO misc.py line 115 22900] Train: [62/100][134/156] Data 0.001 (0.001) Batch 2.718 (3.489) Remain 05:45:57 loss: 0.1260 Lr: 0.01739
[2023-08-08 04:37:33,584 INFO misc.py line 115 22900] Train: [62/100][135/156] Data 0.001 (0.001) Batch 2.791 (3.483) Remain 05:45:22 loss: 0.1029 Lr: 0.01739
[2023-08-08 04:37:36,844 INFO misc.py line 115 22900] Train: [62/100][136/156] Data 0.001 (0.001) Batch 3.261 (3.482) Remain 05:45:09 loss: 0.2315 Lr: 0.01738
[2023-08-08 04:37:40,851 INFO misc.py line 115 22900] Train: [62/100][137/156] Data 0.001 (0.001) Batch 4.007 (3.486) Remain 05:45:29 loss: 0.2539 Lr: 0.01738
[2023-08-08 04:37:43,105 INFO misc.py line 115 22900] Train: [62/100][138/156] Data 0.001 (0.001) Batch 2.254 (3.477) Remain 05:44:31 loss: 0.1961 Lr: 0.01737
[2023-08-08 04:37:46,603 INFO misc.py line 115 22900] Train: [62/100][139/156] Data 0.001 (0.001) Batch 3.498 (3.477) Remain 05:44:28 loss: 0.1757 Lr: 0.01737
[2023-08-08 04:37:49,637 INFO misc.py line 115 22900] Train: [62/100][140/156] Data 0.001 (0.001) Batch 3.034 (3.473) Remain 05:44:06 loss: 0.2227 Lr: 0.01736
[2023-08-08 04:37:52,297 INFO misc.py line 115 22900] Train: [62/100][141/156] Data 0.001 (0.001) Batch 2.660 (3.468) Remain 05:43:27 loss: 0.1816 Lr: 0.01736
[2023-08-08 04:37:55,958 INFO misc.py line 115 22900] Train: [62/100][142/156] Data 0.001 (0.001) Batch 3.661 (3.469) Remain 05:43:32 loss: 0.0880 Lr: 0.01735
[2023-08-08 04:37:58,212 INFO misc.py line 115 22900] Train: [62/100][143/156] Data 0.001 (0.001) Batch 2.254 (3.460) Remain 05:42:37 loss: 0.1239 Lr: 0.01735
[2023-08-08 04:38:01,534 INFO misc.py line 115 22900] Train: [62/100][144/156] Data 0.001 (0.001) Batch 3.321 (3.459) Remain 05:42:28 loss: 0.0922 Lr: 0.01734
[2023-08-08 04:38:05,266 INFO misc.py line 115 22900] Train: [62/100][145/156] Data 0.001 (0.001) Batch 3.732 (3.461) Remain 05:42:36 loss: 0.2436 Lr: 0.01734
[2023-08-08 04:38:08,405 INFO misc.py line 115 22900] Train: [62/100][146/156] Data 0.001 (0.001) Batch 3.139 (3.459) Remain 05:42:19 loss: 0.1200 Lr: 0.01733
[2023-08-08 04:38:13,315 INFO misc.py line 115 22900] Train: [62/100][147/156] Data 0.001 (0.001) Batch 4.910 (3.469) Remain 05:43:15 loss: 0.4629 Lr: 0.01733
[2023-08-08 04:38:17,242 INFO misc.py line 115 22900] Train: [62/100][148/156] Data 0.001 (0.001) Batch 3.928 (3.472) Remain 05:43:30 loss: 0.1267 Lr: 0.01732
[2023-08-08 04:38:19,922 INFO misc.py line 115 22900] Train: [62/100][149/156] Data 0.001 (0.001) Batch 2.680 (3.467) Remain 05:42:55 loss: 0.1373 Lr: 0.01732
[2023-08-08 04:38:22,283 INFO misc.py line 115 22900] Train: [62/100][150/156] Data 0.001 (0.001) Batch 2.361 (3.459) Remain 05:42:07 loss: 0.1337 Lr: 0.01731
[2023-08-08 04:38:25,794 INFO misc.py line 115 22900] Train: [62/100][151/156] Data 0.001 (0.001) Batch 3.511 (3.460) Remain 05:42:05 loss: 0.1934 Lr: 0.01731
[2023-08-08 04:38:29,473 INFO misc.py line 115 22900] Train: [62/100][152/156] Data 0.001 (0.001) Batch 3.679 (3.461) Remain 05:42:11 loss: 0.2937 Lr: 0.01730
[2023-08-08 04:38:32,960 INFO misc.py line 115 22900] Train: [62/100][153/156] Data 0.001 (0.001) Batch 3.487 (3.461) Remain 05:42:08 loss: 0.3426 Lr: 0.01730
[2023-08-08 04:38:36,148 INFO misc.py line 115 22900] Train: [62/100][154/156] Data 0.001 (0.001) Batch 3.188 (3.459) Remain 05:41:54 loss: 0.1951 Lr: 0.01729
[2023-08-08 04:38:39,297 INFO misc.py line 115 22900] Train: [62/100][155/156] Data 0.001 (0.001) Batch 3.149 (3.457) Remain 05:41:38 loss: 0.0605 Lr: 0.01728
[2023-08-08 04:38:43,372 INFO misc.py line 115 22900] Train: [62/100][156/156] Data 0.001 (0.001) Batch 4.075 (3.461) Remain 05:41:59 loss: 0.3204 Lr: 0.01728
[2023-08-08 04:38:43,372 INFO misc.py line 129 22900] Train result: loss: 0.2131 
[2023-08-08 04:38:43,372 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 04:38:45,489 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.2167 
[2023-08-08 04:38:46,357 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.3772 
[2023-08-08 04:38:48,019 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.7696 
[2023-08-08 04:38:49,541 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.3757 
[2023-08-08 04:38:51,386 INFO evaluator.py line 122 22900] Test: [5/24] Loss 2.2439 
[2023-08-08 04:38:53,047 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.7407 
[2023-08-08 04:38:55,183 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.0171 
[2023-08-08 04:38:56,985 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.2141 
[2023-08-08 04:38:58,268 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.5995 
[2023-08-08 04:39:00,400 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.2966 
[2023-08-08 04:39:00,926 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.1219 
[2023-08-08 04:39:02,459 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.7313 
[2023-08-08 04:39:05,171 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.2719 
[2023-08-08 04:39:06,853 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.9058 
[2023-08-08 04:39:08,876 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.4364 
[2023-08-08 04:39:11,586 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.2436 
[2023-08-08 04:39:14,291 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.6039 
[2023-08-08 04:39:16,138 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.6111 
[2023-08-08 04:39:16,889 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.0708 
[2023-08-08 04:39:17,774 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.6488 
[2023-08-08 04:39:20,035 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3396 
[2023-08-08 04:39:21,999 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.8253 
[2023-08-08 04:39:23,845 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.8635 
[2023-08-08 04:39:25,782 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.7963 
[2023-08-08 04:39:25,831 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2396/0.3375/0.6951.
[2023-08-08 04:39:25,831 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6515/0.9453
[2023-08-08 04:39:25,831 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9460/0.9903
[2023-08-08 04:39:25,831 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1838/0.5016
[2023-08-08 04:39:25,831 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1648/0.3281
[2023-08-08 04:39:25,831 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6304/0.6839
[2023-08-08 04:39:25,831 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.4234/0.7029
[2023-08-08 04:39:25,831 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5128/0.6355
[2023-08-08 04:39:25,831 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.2032/0.2401
[2023-08-08 04:39:25,831 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1883/0.4352
[2023-08-08 04:39:25,831 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0261/0.0262
[2023-08-08 04:39:25,831 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 04:39:25,831 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0082/0.0084
[2023-08-08 04:39:25,831 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1641/0.2438
[2023-08-08 04:39:25,831 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0512/0.0578
[2023-08-08 04:39:25,831 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0250/0.0319
[2023-08-08 04:39:25,831 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0565/0.0583
[2023-08-08 04:39:25,831 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0563/0.0969
[2023-08-08 04:39:25,831 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.3864/0.5484
[2023-08-08 04:39:25,831 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 04:39:25,831 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.1141/0.2152
[2023-08-08 04:39:25,831 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 04:39:25,832 INFO misc.py line 152 22900] Currently Best mIoU: 0.2650
[2023-08-08 04:39:25,832 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 04:39:31,558 INFO misc.py line 115 22900] Train: [63/100][1/156] Data 1.308 (1.308) Batch 4.954 (4.954) Remain 08:09:22 loss: 0.1053 Lr: 0.01727
[2023-08-08 04:39:35,392 INFO misc.py line 115 22900] Train: [63/100][2/156] Data 0.001 (0.001) Batch 3.833 (3.833) Remain 06:18:36 loss: 0.2003 Lr: 0.01727
[2023-08-08 04:39:39,305 INFO misc.py line 115 22900] Train: [63/100][3/156] Data 0.001 (0.001) Batch 3.913 (3.913) Remain 06:26:26 loss: 0.1723 Lr: 0.01726
[2023-08-08 04:39:42,468 INFO misc.py line 115 22900] Train: [63/100][4/156] Data 0.001 (0.001) Batch 3.163 (3.163) Remain 05:12:19 loss: 0.0875 Lr: 0.01726
[2023-08-08 04:39:45,221 INFO misc.py line 115 22900] Train: [63/100][5/156] Data 0.001 (0.001) Batch 2.752 (2.958) Remain 04:51:59 loss: 0.1011 Lr: 0.01725
[2023-08-08 04:39:48,556 INFO misc.py line 115 22900] Train: [63/100][6/156] Data 0.001 (0.001) Batch 3.335 (3.084) Remain 05:04:21 loss: 0.1804 Lr: 0.01725
[2023-08-08 04:39:52,063 INFO misc.py line 115 22900] Train: [63/100][7/156] Data 0.001 (0.001) Batch 3.507 (3.190) Remain 05:14:45 loss: 0.2327 Lr: 0.01724
[2023-08-08 04:39:56,182 INFO misc.py line 115 22900] Train: [63/100][8/156] Data 0.001 (0.001) Batch 4.119 (3.375) Remain 05:33:02 loss: 0.3903 Lr: 0.01724
[2023-08-08 04:40:00,187 INFO misc.py line 115 22900] Train: [63/100][9/156] Data 0.001 (0.001) Batch 4.005 (3.480) Remain 05:43:19 loss: 0.3310 Lr: 0.01723
[2023-08-08 04:40:03,827 INFO misc.py line 115 22900] Train: [63/100][10/156] Data 0.001 (0.001) Batch 3.640 (3.503) Remain 05:45:31 loss: 0.1282 Lr: 0.01723
[2023-08-08 04:40:08,112 INFO misc.py line 115 22900] Train: [63/100][11/156] Data 0.001 (0.001) Batch 4.285 (3.601) Remain 05:55:06 loss: 0.3606 Lr: 0.01722
[2023-08-08 04:40:10,739 INFO misc.py line 115 22900] Train: [63/100][12/156] Data 0.001 (0.001) Batch 2.627 (3.493) Remain 05:44:22 loss: 0.1849 Lr: 0.01722
[2023-08-08 04:40:14,883 INFO misc.py line 115 22900] Train: [63/100][13/156] Data 0.001 (0.001) Batch 4.144 (3.558) Remain 05:50:44 loss: 0.1857 Lr: 0.01721
[2023-08-08 04:40:18,808 INFO misc.py line 115 22900] Train: [63/100][14/156] Data 0.001 (0.001) Batch 3.926 (3.591) Remain 05:53:58 loss: 0.2279 Lr: 0.01721
[2023-08-08 04:40:21,712 INFO misc.py line 115 22900] Train: [63/100][15/156] Data 0.001 (0.001) Batch 2.904 (3.534) Remain 05:48:16 loss: 0.1488 Lr: 0.01720
[2023-08-08 04:40:25,491 INFO misc.py line 115 22900] Train: [63/100][16/156] Data 0.001 (0.001) Batch 3.779 (3.553) Remain 05:50:03 loss: 0.1365 Lr: 0.01720
[2023-08-08 04:40:29,920 INFO misc.py line 115 22900] Train: [63/100][17/156] Data 0.001 (0.001) Batch 4.429 (3.615) Remain 05:56:10 loss: 0.1821 Lr: 0.01719
[2023-08-08 04:40:33,218 INFO misc.py line 115 22900] Train: [63/100][18/156] Data 0.001 (0.001) Batch 3.298 (3.594) Remain 05:54:01 loss: 0.1130 Lr: 0.01719
[2023-08-08 04:40:35,479 INFO misc.py line 115 22900] Train: [63/100][19/156] Data 0.001 (0.001) Batch 2.261 (3.511) Remain 05:45:45 loss: 0.1200 Lr: 0.01718
[2023-08-08 04:40:39,059 INFO misc.py line 115 22900] Train: [63/100][20/156] Data 0.001 (0.001) Batch 3.580 (3.515) Remain 05:46:06 loss: 0.3368 Lr: 0.01718
[2023-08-08 04:40:42,715 INFO misc.py line 115 22900] Train: [63/100][21/156] Data 0.001 (0.001) Batch 3.656 (3.523) Remain 05:46:49 loss: 0.1866 Lr: 0.01717
[2023-08-08 04:40:46,119 INFO misc.py line 115 22900] Train: [63/100][22/156] Data 0.001 (0.001) Batch 3.404 (3.517) Remain 05:46:08 loss: 0.2264 Lr: 0.01717
[2023-08-08 04:40:49,470 INFO misc.py line 115 22900] Train: [63/100][23/156] Data 0.001 (0.001) Batch 3.351 (3.508) Remain 05:45:16 loss: 0.2042 Lr: 0.01716
[2023-08-08 04:40:52,770 INFO misc.py line 115 22900] Train: [63/100][24/156] Data 0.001 (0.001) Batch 3.300 (3.498) Remain 05:44:14 loss: 0.1984 Lr: 0.01716
[2023-08-08 04:40:56,868 INFO misc.py line 115 22900] Train: [63/100][25/156] Data 0.001 (0.001) Batch 4.099 (3.526) Remain 05:46:51 loss: 0.3211 Lr: 0.01715
[2023-08-08 04:41:00,759 INFO misc.py line 115 22900] Train: [63/100][26/156] Data 0.001 (0.001) Batch 3.891 (3.541) Remain 05:48:21 loss: 0.1284 Lr: 0.01715
[2023-08-08 04:41:04,830 INFO misc.py line 115 22900] Train: [63/100][27/156] Data 0.001 (0.001) Batch 4.071 (3.564) Remain 05:50:28 loss: 0.1779 Lr: 0.01714
[2023-08-08 04:41:08,789 INFO misc.py line 115 22900] Train: [63/100][28/156] Data 0.001 (0.001) Batch 3.959 (3.579) Remain 05:51:58 loss: 0.3041 Lr: 0.01714
[2023-08-08 04:41:11,602 INFO misc.py line 115 22900] Train: [63/100][29/156] Data 0.001 (0.001) Batch 2.813 (3.550) Remain 05:49:00 loss: 0.0900 Lr: 0.01713
[2023-08-08 04:41:14,939 INFO misc.py line 115 22900] Train: [63/100][30/156] Data 0.001 (0.001) Batch 3.337 (3.542) Remain 05:48:10 loss: 0.2882 Lr: 0.01713
[2023-08-08 04:41:18,079 INFO misc.py line 115 22900] Train: [63/100][31/156] Data 0.001 (0.001) Batch 3.139 (3.528) Remain 05:46:42 loss: 0.0594 Lr: 0.01712
[2023-08-08 04:41:22,246 INFO misc.py line 115 22900] Train: [63/100][32/156] Data 0.001 (0.001) Batch 4.168 (3.550) Remain 05:48:49 loss: 0.1305 Lr: 0.01712
[2023-08-08 04:41:26,264 INFO misc.py line 115 22900] Train: [63/100][33/156] Data 0.001 (0.001) Batch 4.018 (3.565) Remain 05:50:17 loss: 0.1204 Lr: 0.01711
[2023-08-08 04:41:30,271 INFO misc.py line 115 22900] Train: [63/100][34/156] Data 0.001 (0.001) Batch 4.007 (3.580) Remain 05:51:37 loss: 0.1490 Lr: 0.01711
[2023-08-08 04:41:32,954 INFO misc.py line 115 22900] Train: [63/100][35/156] Data 0.001 (0.001) Batch 2.683 (3.552) Remain 05:48:49 loss: 0.1046 Lr: 0.01710
[2023-08-08 04:41:37,059 INFO misc.py line 115 22900] Train: [63/100][36/156] Data 0.001 (0.001) Batch 4.105 (3.568) Remain 05:50:24 loss: 0.2408 Lr: 0.01710
[2023-08-08 04:41:41,141 INFO misc.py line 115 22900] Train: [63/100][37/156] Data 0.001 (0.001) Batch 4.082 (3.583) Remain 05:51:49 loss: 0.1286 Lr: 0.01709
[2023-08-08 04:41:44,835 INFO misc.py line 115 22900] Train: [63/100][38/156] Data 0.001 (0.001) Batch 3.694 (3.587) Remain 05:52:04 loss: 0.1017 Lr: 0.01709
[2023-08-08 04:41:48,188 INFO misc.py line 115 22900] Train: [63/100][39/156] Data 0.001 (0.001) Batch 3.353 (3.580) Remain 05:51:23 loss: 0.2953 Lr: 0.01708
[2023-08-08 04:41:51,353 INFO misc.py line 115 22900] Train: [63/100][40/156] Data 0.001 (0.001) Batch 3.166 (3.569) Remain 05:50:13 loss: 0.3196 Lr: 0.01708
[2023-08-08 04:41:55,381 INFO misc.py line 115 22900] Train: [63/100][41/156] Data 0.001 (0.001) Batch 4.028 (3.581) Remain 05:51:20 loss: 0.3132 Lr: 0.01707
[2023-08-08 04:41:59,219 INFO misc.py line 115 22900] Train: [63/100][42/156] Data 0.001 (0.001) Batch 3.838 (3.588) Remain 05:51:56 loss: 0.2442 Lr: 0.01707
[2023-08-08 04:42:03,458 INFO misc.py line 115 22900] Train: [63/100][43/156] Data 0.001 (0.001) Batch 4.239 (3.604) Remain 05:53:28 loss: 0.3198 Lr: 0.01706
[2023-08-08 04:42:07,373 INFO misc.py line 115 22900] Train: [63/100][44/156] Data 0.001 (0.001) Batch 3.915 (3.611) Remain 05:54:09 loss: 0.2235 Lr: 0.01706
[2023-08-08 04:42:11,499 INFO misc.py line 115 22900] Train: [63/100][45/156] Data 0.001 (0.001) Batch 4.126 (3.624) Remain 05:55:17 loss: 0.2210 Lr: 0.01705
[2023-08-08 04:42:14,702 INFO misc.py line 115 22900] Train: [63/100][46/156] Data 0.001 (0.001) Batch 3.203 (3.614) Remain 05:54:16 loss: 0.2317 Lr: 0.01705
[2023-08-08 04:42:17,575 INFO misc.py line 115 22900] Train: [63/100][47/156] Data 0.001 (0.001) Batch 2.872 (3.597) Remain 05:52:34 loss: 0.0547 Lr: 0.01704
[2023-08-08 04:42:20,484 INFO misc.py line 115 22900] Train: [63/100][48/156] Data 0.001 (0.001) Batch 2.909 (3.582) Remain 05:51:00 loss: 0.1239 Lr: 0.01704
[2023-08-08 04:42:23,552 INFO misc.py line 115 22900] Train: [63/100][49/156] Data 0.001 (0.001) Batch 3.068 (3.571) Remain 05:49:51 loss: 0.4684 Lr: 0.01703
[2023-08-08 04:42:27,075 INFO misc.py line 115 22900] Train: [63/100][50/156] Data 0.001 (0.001) Batch 3.523 (3.570) Remain 05:49:41 loss: 0.0587 Lr: 0.01703
[2023-08-08 04:42:30,234 INFO misc.py line 115 22900] Train: [63/100][51/156] Data 0.001 (0.001) Batch 3.159 (3.561) Remain 05:48:48 loss: 0.1526 Lr: 0.01702
[2023-08-08 04:42:33,168 INFO misc.py line 115 22900] Train: [63/100][52/156] Data 0.001 (0.001) Batch 2.934 (3.548) Remain 05:47:29 loss: 0.2585 Lr: 0.01702
[2023-08-08 04:42:36,040 INFO misc.py line 115 22900] Train: [63/100][53/156] Data 0.001 (0.001) Batch 2.872 (3.535) Remain 05:46:06 loss: 0.1191 Lr: 0.01701
[2023-08-08 04:42:40,558 INFO misc.py line 115 22900] Train: [63/100][54/156] Data 0.001 (0.001) Batch 4.518 (3.554) Remain 05:47:55 loss: 0.2871 Lr: 0.01701
[2023-08-08 04:42:43,868 INFO misc.py line 115 22900] Train: [63/100][55/156] Data 0.001 (0.001) Batch 3.311 (3.549) Remain 05:47:24 loss: 0.1743 Lr: 0.01700
[2023-08-08 04:42:46,474 INFO misc.py line 115 22900] Train: [63/100][56/156] Data 0.001 (0.001) Batch 2.605 (3.531) Remain 05:45:36 loss: 0.0973 Lr: 0.01700
[2023-08-08 04:42:50,028 INFO misc.py line 115 22900] Train: [63/100][57/156] Data 0.001 (0.001) Batch 3.554 (3.532) Remain 05:45:35 loss: 0.2020 Lr: 0.01699
[2023-08-08 04:42:52,748 INFO misc.py line 115 22900] Train: [63/100][58/156] Data 0.001 (0.001) Batch 2.720 (3.517) Remain 05:44:05 loss: 0.2332 Lr: 0.01699
[2023-08-08 04:42:55,461 INFO misc.py line 115 22900] Train: [63/100][59/156] Data 0.001 (0.001) Batch 2.714 (3.503) Remain 05:42:37 loss: 0.1126 Lr: 0.01698
[2023-08-08 04:42:59,639 INFO misc.py line 115 22900] Train: [63/100][60/156] Data 0.001 (0.001) Batch 4.177 (3.515) Remain 05:43:43 loss: 0.1668 Lr: 0.01698
[2023-08-08 04:43:03,909 INFO misc.py line 115 22900] Train: [63/100][61/156] Data 0.001 (0.001) Batch 4.270 (3.528) Remain 05:44:56 loss: 0.1885 Lr: 0.01697
[2023-08-08 04:43:08,000 INFO misc.py line 115 22900] Train: [63/100][62/156] Data 0.001 (0.001) Batch 4.091 (3.537) Remain 05:45:49 loss: 0.3178 Lr: 0.01697
[2023-08-08 04:43:12,535 INFO misc.py line 115 22900] Train: [63/100][63/156] Data 0.001 (0.001) Batch 4.535 (3.554) Remain 05:47:23 loss: 0.2240 Lr: 0.01696
[2023-08-08 04:43:16,543 INFO misc.py line 115 22900] Train: [63/100][64/156] Data 0.001 (0.001) Batch 4.008 (3.561) Remain 05:48:03 loss: 0.2134 Lr: 0.01696
[2023-08-08 04:43:18,789 INFO misc.py line 115 22900] Train: [63/100][65/156] Data 0.001 (0.001) Batch 2.246 (3.540) Remain 05:45:55 loss: 0.1168 Lr: 0.01695
[2023-08-08 04:43:22,375 INFO misc.py line 115 22900] Train: [63/100][66/156] Data 0.001 (0.001) Batch 3.586 (3.541) Remain 05:45:56 loss: 0.2066 Lr: 0.01695
[2023-08-08 04:43:26,457 INFO misc.py line 115 22900] Train: [63/100][67/156] Data 0.001 (0.001) Batch 4.082 (3.549) Remain 05:46:42 loss: 0.1544 Lr: 0.01694
[2023-08-08 04:43:29,509 INFO misc.py line 115 22900] Train: [63/100][68/156] Data 0.001 (0.001) Batch 3.053 (3.542) Remain 05:45:53 loss: 0.2463 Lr: 0.01694
[2023-08-08 04:43:33,130 INFO misc.py line 115 22900] Train: [63/100][69/156] Data 0.001 (0.001) Batch 3.621 (3.543) Remain 05:45:57 loss: 0.1967 Lr: 0.01693
[2023-08-08 04:43:36,523 INFO misc.py line 115 22900] Train: [63/100][70/156] Data 0.001 (0.001) Batch 3.393 (3.541) Remain 05:45:40 loss: 0.1858 Lr: 0.01693
[2023-08-08 04:43:40,471 INFO misc.py line 115 22900] Train: [63/100][71/156] Data 0.001 (0.001) Batch 3.948 (3.547) Remain 05:46:12 loss: 0.1575 Lr: 0.01692
[2023-08-08 04:43:43,957 INFO misc.py line 115 22900] Train: [63/100][72/156] Data 0.001 (0.001) Batch 3.487 (3.546) Remain 05:46:03 loss: 0.0826 Lr: 0.01692
[2023-08-08 04:43:47,314 INFO misc.py line 115 22900] Train: [63/100][73/156] Data 0.001 (0.001) Batch 3.357 (3.543) Remain 05:45:44 loss: 0.0978 Lr: 0.01691
[2023-08-08 04:43:49,942 INFO misc.py line 115 22900] Train: [63/100][74/156] Data 0.001 (0.001) Batch 2.628 (3.530) Remain 05:44:25 loss: 0.1758 Lr: 0.01691
[2023-08-08 04:43:53,163 INFO misc.py line 115 22900] Train: [63/100][75/156] Data 0.001 (0.001) Batch 3.220 (3.526) Remain 05:43:56 loss: 0.2584 Lr: 0.01690
[2023-08-08 04:43:55,896 INFO misc.py line 115 22900] Train: [63/100][76/156] Data 0.001 (0.001) Batch 2.733 (3.515) Remain 05:42:49 loss: 0.0902 Lr: 0.01690
[2023-08-08 04:43:59,984 INFO misc.py line 115 22900] Train: [63/100][77/156] Data 0.001 (0.001) Batch 4.087 (3.523) Remain 05:43:31 loss: 0.2122 Lr: 0.01689
[2023-08-08 04:44:03,231 INFO misc.py line 115 22900] Train: [63/100][78/156] Data 0.001 (0.001) Batch 3.248 (3.519) Remain 05:43:06 loss: 0.1043 Lr: 0.01689
[2023-08-08 04:44:06,298 INFO misc.py line 115 22900] Train: [63/100][79/156] Data 0.001 (0.001) Batch 3.067 (3.513) Remain 05:42:27 loss: 0.1394 Lr: 0.01688
[2023-08-08 04:44:09,401 INFO misc.py line 115 22900] Train: [63/100][80/156] Data 0.001 (0.001) Batch 3.102 (3.508) Remain 05:41:53 loss: 0.2026 Lr: 0.01688
[2023-08-08 04:44:11,892 INFO misc.py line 115 22900] Train: [63/100][81/156] Data 0.001 (0.001) Batch 2.491 (3.495) Remain 05:40:33 loss: 0.1281 Lr: 0.01687
[2023-08-08 04:44:15,913 INFO misc.py line 115 22900] Train: [63/100][82/156] Data 0.001 (0.001) Batch 4.021 (3.501) Remain 05:41:08 loss: 0.3549 Lr: 0.01687
[2023-08-08 04:44:19,600 INFO misc.py line 115 22900] Train: [63/100][83/156] Data 0.001 (0.001) Batch 3.686 (3.504) Remain 05:41:19 loss: 0.2063 Lr: 0.01686
[2023-08-08 04:44:23,002 INFO misc.py line 115 22900] Train: [63/100][84/156] Data 0.001 (0.001) Batch 3.402 (3.502) Remain 05:41:08 loss: 0.2530 Lr: 0.01686
[2023-08-08 04:44:26,421 INFO misc.py line 115 22900] Train: [63/100][85/156] Data 0.001 (0.001) Batch 3.419 (3.501) Remain 05:40:58 loss: 0.2141 Lr: 0.01685
[2023-08-08 04:44:30,477 INFO misc.py line 115 22900] Train: [63/100][86/156] Data 0.001 (0.001) Batch 4.056 (3.508) Remain 05:41:34 loss: 0.2521 Lr: 0.01685
[2023-08-08 04:44:34,002 INFO misc.py line 115 22900] Train: [63/100][87/156] Data 0.001 (0.001) Batch 3.525 (3.508) Remain 05:41:31 loss: 0.2408 Lr: 0.01684
[2023-08-08 04:44:37,591 INFO misc.py line 115 22900] Train: [63/100][88/156] Data 0.001 (0.001) Batch 3.589 (3.509) Remain 05:41:33 loss: 0.1579 Lr: 0.01684
[2023-08-08 04:44:41,252 INFO misc.py line 115 22900] Train: [63/100][89/156] Data 0.001 (0.001) Batch 3.661 (3.511) Remain 05:41:40 loss: 0.2857 Lr: 0.01683
[2023-08-08 04:44:43,927 INFO misc.py line 115 22900] Train: [63/100][90/156] Data 0.002 (0.001) Batch 2.675 (3.501) Remain 05:40:41 loss: 0.3610 Lr: 0.01683
[2023-08-08 04:44:47,092 INFO misc.py line 115 22900] Train: [63/100][91/156] Data 0.001 (0.001) Batch 3.165 (3.498) Remain 05:40:15 loss: 0.0853 Lr: 0.01682
[2023-08-08 04:44:49,956 INFO misc.py line 115 22900] Train: [63/100][92/156] Data 0.001 (0.001) Batch 2.864 (3.490) Remain 05:39:30 loss: 0.2534 Lr: 0.01682
[2023-08-08 04:44:52,426 INFO misc.py line 115 22900] Train: [63/100][93/156] Data 0.001 (0.001) Batch 2.471 (3.479) Remain 05:38:20 loss: 0.2009 Lr: 0.01681
[2023-08-08 04:44:55,912 INFO misc.py line 115 22900] Train: [63/100][94/156] Data 0.001 (0.001) Batch 3.486 (3.479) Remain 05:38:17 loss: 0.1515 Lr: 0.01681
[2023-08-08 04:44:59,455 INFO misc.py line 115 22900] Train: [63/100][95/156] Data 0.001 (0.001) Batch 3.543 (3.480) Remain 05:38:18 loss: 0.2106 Lr: 0.01680
[2023-08-08 04:45:03,451 INFO misc.py line 115 22900] Train: [63/100][96/156] Data 0.001 (0.001) Batch 3.997 (3.485) Remain 05:38:47 loss: 0.1053 Lr: 0.01680
[2023-08-08 04:45:06,918 INFO misc.py line 115 22900] Train: [63/100][97/156] Data 0.001 (0.001) Batch 3.466 (3.485) Remain 05:38:42 loss: 0.1854 Lr: 0.01679
[2023-08-08 04:45:10,224 INFO misc.py line 115 22900] Train: [63/100][98/156] Data 0.001 (0.001) Batch 3.306 (3.483) Remain 05:38:27 loss: 0.2877 Lr: 0.01679
[2023-08-08 04:45:13,108 INFO misc.py line 115 22900] Train: [63/100][99/156] Data 0.001 (0.001) Batch 2.884 (3.477) Remain 05:37:48 loss: 0.3252 Lr: 0.01678
[2023-08-08 04:45:16,169 INFO misc.py line 115 22900] Train: [63/100][100/156] Data 0.001 (0.001) Batch 3.062 (3.473) Remain 05:37:19 loss: 0.1618 Lr: 0.01678
[2023-08-08 04:45:19,746 INFO misc.py line 115 22900] Train: [63/100][101/156] Data 0.001 (0.001) Batch 3.576 (3.474) Remain 05:37:22 loss: 0.2622 Lr: 0.01677
[2023-08-08 04:45:22,542 INFO misc.py line 115 22900] Train: [63/100][102/156] Data 0.001 (0.001) Batch 2.796 (3.467) Remain 05:36:38 loss: 0.1153 Lr: 0.01677
[2023-08-08 04:45:26,274 INFO misc.py line 115 22900] Train: [63/100][103/156] Data 0.001 (0.001) Batch 3.732 (3.470) Remain 05:36:50 loss: 0.1959 Lr: 0.01676
[2023-08-08 04:45:29,716 INFO misc.py line 115 22900] Train: [63/100][104/156] Data 0.001 (0.001) Batch 3.442 (3.469) Remain 05:36:45 loss: 0.2460 Lr: 0.01676
[2023-08-08 04:45:33,862 INFO misc.py line 115 22900] Train: [63/100][105/156] Data 0.001 (0.001) Batch 4.146 (3.476) Remain 05:37:21 loss: 0.1402 Lr: 0.01675
[2023-08-08 04:45:38,186 INFO misc.py line 115 22900] Train: [63/100][106/156] Data 0.001 (0.001) Batch 4.324 (3.484) Remain 05:38:05 loss: 0.3634 Lr: 0.01675
[2023-08-08 04:45:41,740 INFO misc.py line 115 22900] Train: [63/100][107/156] Data 0.001 (0.001) Batch 3.553 (3.485) Remain 05:38:05 loss: 0.2794 Lr: 0.01674
[2023-08-08 04:45:44,138 INFO misc.py line 115 22900] Train: [63/100][108/156] Data 0.001 (0.001) Batch 2.399 (3.475) Remain 05:37:02 loss: 0.1115 Lr: 0.01674
[2023-08-08 04:45:47,108 INFO misc.py line 115 22900] Train: [63/100][109/156] Data 0.001 (0.001) Batch 2.970 (3.470) Remain 05:36:30 loss: 0.1321 Lr: 0.01673
[2023-08-08 04:45:50,567 INFO misc.py line 115 22900] Train: [63/100][110/156] Data 0.001 (0.001) Batch 3.459 (3.470) Remain 05:36:26 loss: 0.1402 Lr: 0.01673
[2023-08-08 04:45:53,520 INFO misc.py line 115 22900] Train: [63/100][111/156] Data 0.001 (0.001) Batch 2.953 (3.465) Remain 05:35:55 loss: 0.1401 Lr: 0.01672
[2023-08-08 04:45:57,732 INFO misc.py line 115 22900] Train: [63/100][112/156] Data 0.001 (0.001) Batch 4.211 (3.472) Remain 05:36:32 loss: 0.3542 Lr: 0.01672
[2023-08-08 04:46:01,940 INFO misc.py line 115 22900] Train: [63/100][113/156] Data 0.001 (0.001) Batch 4.208 (3.478) Remain 05:37:07 loss: 0.2765 Lr: 0.01671
[2023-08-08 04:46:05,843 INFO misc.py line 115 22900] Train: [63/100][114/156] Data 0.001 (0.001) Batch 3.903 (3.482) Remain 05:37:26 loss: 0.1495 Lr: 0.01671
[2023-08-08 04:46:09,831 INFO misc.py line 115 22900] Train: [63/100][115/156] Data 0.001 (0.001) Batch 3.988 (3.487) Remain 05:37:48 loss: 0.2506 Lr: 0.01670
[2023-08-08 04:46:11,743 INFO misc.py line 115 22900] Train: [63/100][116/156] Data 0.001 (0.001) Batch 1.912 (3.473) Remain 05:36:24 loss: 0.0660 Lr: 0.01670
[2023-08-08 04:46:15,469 INFO misc.py line 115 22900] Train: [63/100][117/156] Data 0.001 (0.001) Batch 3.726 (3.475) Remain 05:36:33 loss: 0.2048 Lr: 0.01669
[2023-08-08 04:46:19,177 INFO misc.py line 115 22900] Train: [63/100][118/156] Data 0.001 (0.001) Batch 3.708 (3.477) Remain 05:36:42 loss: 0.2097 Lr: 0.01669
[2023-08-08 04:46:23,093 INFO misc.py line 115 22900] Train: [63/100][119/156] Data 0.001 (0.001) Batch 3.916 (3.481) Remain 05:37:00 loss: 0.1393 Lr: 0.01668
[2023-08-08 04:46:26,810 INFO misc.py line 115 22900] Train: [63/100][120/156] Data 0.001 (0.001) Batch 3.717 (3.483) Remain 05:37:08 loss: 0.1506 Lr: 0.01668
[2023-08-08 04:46:30,367 INFO misc.py line 115 22900] Train: [63/100][121/156] Data 0.001 (0.001) Batch 3.557 (3.484) Remain 05:37:09 loss: 0.2044 Lr: 0.01667
[2023-08-08 04:46:32,990 INFO misc.py line 115 22900] Train: [63/100][122/156] Data 0.001 (0.001) Batch 2.623 (3.476) Remain 05:36:23 loss: 0.0897 Lr: 0.01667
[2023-08-08 04:46:36,321 INFO misc.py line 115 22900] Train: [63/100][123/156] Data 0.001 (0.001) Batch 3.331 (3.475) Remain 05:36:13 loss: 0.1154 Lr: 0.01666
[2023-08-08 04:46:39,376 INFO misc.py line 115 22900] Train: [63/100][124/156] Data 0.001 (0.001) Batch 3.054 (3.472) Remain 05:35:49 loss: 0.3426 Lr: 0.01666
[2023-08-08 04:46:42,116 INFO misc.py line 115 22900] Train: [63/100][125/156] Data 0.001 (0.001) Batch 2.740 (3.466) Remain 05:35:11 loss: 0.2499 Lr: 0.01665
[2023-08-08 04:46:46,211 INFO misc.py line 115 22900] Train: [63/100][126/156] Data 0.001 (0.001) Batch 4.096 (3.471) Remain 05:35:37 loss: 0.1008 Lr: 0.01665
[2023-08-08 04:46:49,689 INFO misc.py line 115 22900] Train: [63/100][127/156] Data 0.001 (0.001) Batch 3.478 (3.471) Remain 05:35:34 loss: 0.2365 Lr: 0.01664
[2023-08-08 04:46:52,560 INFO misc.py line 115 22900] Train: [63/100][128/156] Data 0.001 (0.001) Batch 2.871 (3.466) Remain 05:35:03 loss: 0.2468 Lr: 0.01664
[2023-08-08 04:46:55,716 INFO misc.py line 115 22900] Train: [63/100][129/156] Data 0.001 (0.001) Batch 3.155 (3.464) Remain 05:34:45 loss: 0.1931 Lr: 0.01663
[2023-08-08 04:47:00,032 INFO misc.py line 115 22900] Train: [63/100][130/156] Data 0.001 (0.001) Batch 4.316 (3.470) Remain 05:35:20 loss: 0.3050 Lr: 0.01663
[2023-08-08 04:47:03,768 INFO misc.py line 115 22900] Train: [63/100][131/156] Data 0.001 (0.001) Batch 3.736 (3.472) Remain 05:35:29 loss: 0.3349 Lr: 0.01662
[2023-08-08 04:47:07,924 INFO misc.py line 115 22900] Train: [63/100][132/156] Data 0.001 (0.001) Batch 4.155 (3.478) Remain 05:35:56 loss: 0.2233 Lr: 0.01662
[2023-08-08 04:47:10,502 INFO misc.py line 115 22900] Train: [63/100][133/156] Data 0.001 (0.001) Batch 2.578 (3.471) Remain 05:35:12 loss: 0.1241 Lr: 0.01661
[2023-08-08 04:47:13,804 INFO misc.py line 115 22900] Train: [63/100][134/156] Data 0.001 (0.001) Batch 3.302 (3.469) Remain 05:35:02 loss: 0.1861 Lr: 0.01661
[2023-08-08 04:47:18,068 INFO misc.py line 115 22900] Train: [63/100][135/156] Data 0.001 (0.001) Batch 4.264 (3.475) Remain 05:35:33 loss: 0.2640 Lr: 0.01660
[2023-08-08 04:47:21,612 INFO misc.py line 115 22900] Train: [63/100][136/156] Data 0.001 (0.001) Batch 3.545 (3.476) Remain 05:35:32 loss: 0.2674 Lr: 0.01660
[2023-08-08 04:47:25,093 INFO misc.py line 115 22900] Train: [63/100][137/156] Data 0.001 (0.001) Batch 3.480 (3.476) Remain 05:35:29 loss: 0.2445 Lr: 0.01659
[2023-08-08 04:47:28,811 INFO misc.py line 115 22900] Train: [63/100][138/156] Data 0.001 (0.001) Batch 3.718 (3.478) Remain 05:35:36 loss: 0.1572 Lr: 0.01659
[2023-08-08 04:47:32,649 INFO misc.py line 115 22900] Train: [63/100][139/156] Data 0.001 (0.001) Batch 3.838 (3.480) Remain 05:35:48 loss: 0.2611 Lr: 0.01658
[2023-08-08 04:47:36,087 INFO misc.py line 115 22900] Train: [63/100][140/156] Data 0.001 (0.001) Batch 3.437 (3.480) Remain 05:35:43 loss: 0.1510 Lr: 0.01658
[2023-08-08 04:47:39,814 INFO misc.py line 115 22900] Train: [63/100][141/156] Data 0.001 (0.001) Batch 3.727 (3.482) Remain 05:35:50 loss: 0.1713 Lr: 0.01657
[2023-08-08 04:47:42,341 INFO misc.py line 115 22900] Train: [63/100][142/156] Data 0.001 (0.001) Batch 2.527 (3.475) Remain 05:35:06 loss: 0.1173 Lr: 0.01657
[2023-08-08 04:47:44,208 INFO misc.py line 115 22900] Train: [63/100][143/156] Data 0.001 (0.001) Batch 1.867 (3.464) Remain 05:33:56 loss: 0.0682 Lr: 0.01656
[2023-08-08 04:47:47,746 INFO misc.py line 115 22900] Train: [63/100][144/156] Data 0.001 (0.001) Batch 3.538 (3.464) Remain 05:33:56 loss: 0.2311 Lr: 0.01656
[2023-08-08 04:47:50,986 INFO misc.py line 115 22900] Train: [63/100][145/156] Data 0.001 (0.001) Batch 3.240 (3.463) Remain 05:33:43 loss: 0.1790 Lr: 0.01655
[2023-08-08 04:47:54,200 INFO misc.py line 115 22900] Train: [63/100][146/156] Data 0.001 (0.001) Batch 3.214 (3.461) Remain 05:33:30 loss: 0.1401 Lr: 0.01655
[2023-08-08 04:47:57,037 INFO misc.py line 115 22900] Train: [63/100][147/156] Data 0.001 (0.001) Batch 2.837 (3.456) Remain 05:33:01 loss: 0.0602 Lr: 0.01654
[2023-08-08 04:48:01,089 INFO misc.py line 115 22900] Train: [63/100][148/156] Data 0.001 (0.001) Batch 4.052 (3.461) Remain 05:33:22 loss: 0.1527 Lr: 0.01654
[2023-08-08 04:48:05,127 INFO misc.py line 115 22900] Train: [63/100][149/156] Data 0.001 (0.001) Batch 4.038 (3.465) Remain 05:33:41 loss: 0.2151 Lr: 0.01653
[2023-08-08 04:48:08,368 INFO misc.py line 115 22900] Train: [63/100][150/156] Data 0.001 (0.001) Batch 3.241 (3.463) Remain 05:33:29 loss: 0.1604 Lr: 0.01653
[2023-08-08 04:48:12,194 INFO misc.py line 115 22900] Train: [63/100][151/156] Data 0.001 (0.001) Batch 3.826 (3.465) Remain 05:33:39 loss: 0.2751 Lr: 0.01652
[2023-08-08 04:48:15,244 INFO misc.py line 115 22900] Train: [63/100][152/156] Data 0.001 (0.001) Batch 3.050 (3.463) Remain 05:33:20 loss: 0.1682 Lr: 0.01652
[2023-08-08 04:48:18,846 INFO misc.py line 115 22900] Train: [63/100][153/156] Data 0.001 (0.001) Batch 3.602 (3.464) Remain 05:33:22 loss: 0.2479 Lr: 0.01651
[2023-08-08 04:48:21,479 INFO misc.py line 115 22900] Train: [63/100][154/156] Data 0.001 (0.001) Batch 2.633 (3.458) Remain 05:32:47 loss: 0.2419 Lr: 0.01651
[2023-08-08 04:48:25,049 INFO misc.py line 115 22900] Train: [63/100][155/156] Data 0.001 (0.001) Batch 3.571 (3.459) Remain 05:32:47 loss: 0.1155 Lr: 0.01650
[2023-08-08 04:48:28,232 INFO misc.py line 115 22900] Train: [63/100][156/156] Data 0.001 (0.001) Batch 3.182 (3.457) Remain 05:32:33 loss: 0.1441 Lr: 0.01650
[2023-08-08 04:48:28,232 INFO misc.py line 129 22900] Train result: loss: 0.1966 
[2023-08-08 04:48:28,232 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 04:48:30,381 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.1279 
[2023-08-08 04:48:31,250 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.4608 
[2023-08-08 04:48:32,914 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.5373 
[2023-08-08 04:48:34,437 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.2060 
[2023-08-08 04:48:36,283 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.9281 
[2023-08-08 04:48:37,947 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.6839 
[2023-08-08 04:48:40,086 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.8610 
[2023-08-08 04:48:41,890 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.2234 
[2023-08-08 04:48:43,173 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.4896 
[2023-08-08 04:48:45,303 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3397 
[2023-08-08 04:48:45,844 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.1531 
[2023-08-08 04:48:47,377 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.7029 
[2023-08-08 04:48:50,090 INFO evaluator.py line 122 22900] Test: [13/24] Loss 0.8976 
[2023-08-08 04:48:51,771 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.7179 
[2023-08-08 04:48:53,795 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.3666 
[2023-08-08 04:48:56,503 INFO evaluator.py line 122 22900] Test: [16/24] Loss 0.9681 
[2023-08-08 04:48:59,207 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.3247 
[2023-08-08 04:49:01,056 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.6446 
[2023-08-08 04:49:01,807 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.0840 
[2023-08-08 04:49:02,692 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.6807 
[2023-08-08 04:49:04,952 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3972 
[2023-08-08 04:49:06,919 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.9319 
[2023-08-08 04:49:08,763 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.6488 
[2023-08-08 04:49:10,697 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.8892 
[2023-08-08 04:49:10,744 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2577/0.3558/0.7051.
[2023-08-08 04:49:10,744 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6669/0.9493
[2023-08-08 04:49:10,744 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9470/0.9884
[2023-08-08 04:49:10,744 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1848/0.4232
[2023-08-08 04:49:10,744 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1769/0.2008
[2023-08-08 04:49:10,744 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6654/0.7638
[2023-08-08 04:49:10,744 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.3987/0.5672
[2023-08-08 04:49:10,744 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5382/0.6212
[2023-08-08 04:49:10,744 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.2001/0.2257
[2023-08-08 04:49:10,744 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1980/0.4681
[2023-08-08 04:49:10,744 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0890/0.0899
[2023-08-08 04:49:10,744 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 04:49:10,744 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0511/0.0571
[2023-08-08 04:49:10,744 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.2140/0.5824
[2023-08-08 04:49:10,744 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0934/0.1105
[2023-08-08 04:49:10,744 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0165/0.0259
[2023-08-08 04:49:10,744 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.1393/0.1618
[2023-08-08 04:49:10,744 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0221/0.0253
[2023-08-08 04:49:10,744 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.4582/0.6537
[2023-08-08 04:49:10,744 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 04:49:10,744 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0951/0.2021
[2023-08-08 04:49:10,745 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 04:49:10,745 INFO misc.py line 152 22900] Currently Best mIoU: 0.2650
[2023-08-08 04:49:10,745 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 04:49:16,913 INFO misc.py line 115 22900] Train: [64/100][1/156] Data 1.576 (1.576) Batch 5.367 (5.367) Remain 08:36:14 loss: 0.2987 Lr: 0.01649
[2023-08-08 04:49:20,804 INFO misc.py line 115 22900] Train: [64/100][2/156] Data 0.001 (0.001) Batch 3.891 (3.891) Remain 06:14:12 loss: 0.1867 Lr: 0.01649
[2023-08-08 04:49:24,822 INFO misc.py line 115 22900] Train: [64/100][3/156] Data 0.001 (0.001) Batch 4.018 (4.018) Remain 06:26:19 loss: 0.1990 Lr: 0.01648
[2023-08-08 04:49:29,128 INFO misc.py line 115 22900] Train: [64/100][4/156] Data 0.001 (0.001) Batch 4.306 (4.306) Remain 06:53:54 loss: 0.3123 Lr: 0.01648
[2023-08-08 04:49:31,840 INFO misc.py line 115 22900] Train: [64/100][5/156] Data 0.001 (0.001) Batch 2.713 (3.509) Remain 05:37:16 loss: 0.2260 Lr: 0.01647
[2023-08-08 04:49:35,384 INFO misc.py line 115 22900] Train: [64/100][6/156] Data 0.001 (0.001) Batch 3.544 (3.521) Remain 05:38:19 loss: 0.1859 Lr: 0.01647
[2023-08-08 04:49:39,004 INFO misc.py line 115 22900] Train: [64/100][7/156] Data 0.001 (0.001) Batch 3.620 (3.545) Remain 05:40:39 loss: 0.2455 Lr: 0.01646
[2023-08-08 04:49:42,723 INFO misc.py line 115 22900] Train: [64/100][8/156] Data 0.001 (0.001) Batch 3.719 (3.580) Remain 05:43:55 loss: 0.2368 Lr: 0.01646
[2023-08-08 04:49:46,095 INFO misc.py line 115 22900] Train: [64/100][9/156] Data 0.001 (0.001) Batch 3.372 (3.545) Remain 05:40:32 loss: 0.2689 Lr: 0.01645
[2023-08-08 04:49:48,747 INFO misc.py line 115 22900] Train: [64/100][10/156] Data 0.001 (0.001) Batch 2.653 (3.418) Remain 05:28:13 loss: 0.2522 Lr: 0.01645
[2023-08-08 04:49:51,870 INFO misc.py line 115 22900] Train: [64/100][11/156] Data 0.001 (0.001) Batch 3.123 (3.381) Remain 05:24:38 loss: 0.3521 Lr: 0.01644
[2023-08-08 04:49:56,447 INFO misc.py line 115 22900] Train: [64/100][12/156] Data 0.001 (0.001) Batch 4.577 (3.514) Remain 05:37:19 loss: 0.1932 Lr: 0.01644
[2023-08-08 04:49:59,398 INFO misc.py line 115 22900] Train: [64/100][13/156] Data 0.001 (0.001) Batch 2.951 (3.458) Remain 05:31:52 loss: 0.1833 Lr: 0.01643
[2023-08-08 04:50:03,471 INFO misc.py line 115 22900] Train: [64/100][14/156] Data 0.001 (0.001) Batch 4.073 (3.514) Remain 05:37:10 loss: 0.2777 Lr: 0.01643
[2023-08-08 04:50:06,389 INFO misc.py line 115 22900] Train: [64/100][15/156] Data 0.001 (0.001) Batch 2.918 (3.464) Remain 05:32:21 loss: 0.0788 Lr: 0.01642
[2023-08-08 04:50:08,930 INFO misc.py line 115 22900] Train: [64/100][16/156] Data 0.001 (0.001) Batch 2.542 (3.393) Remain 05:25:29 loss: 0.1936 Lr: 0.01642
[2023-08-08 04:50:11,640 INFO misc.py line 115 22900] Train: [64/100][17/156] Data 0.001 (0.001) Batch 2.710 (3.344) Remain 05:20:45 loss: 0.1153 Lr: 0.01641
[2023-08-08 04:50:15,612 INFO misc.py line 115 22900] Train: [64/100][18/156] Data 0.001 (0.001) Batch 3.972 (3.386) Remain 05:24:42 loss: 0.3088 Lr: 0.01641
[2023-08-08 04:50:19,599 INFO misc.py line 115 22900] Train: [64/100][19/156] Data 0.001 (0.001) Batch 3.987 (3.424) Remain 05:28:15 loss: 0.5158 Lr: 0.01640
[2023-08-08 04:50:24,061 INFO misc.py line 115 22900] Train: [64/100][20/156] Data 0.001 (0.001) Batch 4.463 (3.485) Remain 05:34:03 loss: 0.2071 Lr: 0.01640
[2023-08-08 04:50:27,015 INFO misc.py line 115 22900] Train: [64/100][21/156] Data 0.001 (0.001) Batch 2.954 (3.455) Remain 05:31:10 loss: 0.1401 Lr: 0.01639
[2023-08-08 04:50:30,884 INFO misc.py line 115 22900] Train: [64/100][22/156] Data 0.001 (0.001) Batch 3.869 (3.477) Remain 05:33:12 loss: 0.1141 Lr: 0.01639
[2023-08-08 04:50:35,174 INFO misc.py line 115 22900] Train: [64/100][23/156] Data 0.001 (0.001) Batch 4.290 (3.518) Remain 05:37:02 loss: 0.1655 Lr: 0.01638
[2023-08-08 04:50:38,334 INFO misc.py line 115 22900] Train: [64/100][24/156] Data 0.001 (0.001) Batch 3.160 (3.501) Remain 05:35:21 loss: 0.1255 Lr: 0.01638
[2023-08-08 04:50:42,882 INFO misc.py line 115 22900] Train: [64/100][25/156] Data 0.001 (0.001) Batch 4.548 (3.548) Remain 05:39:51 loss: 0.3203 Lr: 0.01637
[2023-08-08 04:50:45,267 INFO misc.py line 115 22900] Train: [64/100][26/156] Data 0.001 (0.001) Batch 2.385 (3.498) Remain 05:34:57 loss: 0.0768 Lr: 0.01637
[2023-08-08 04:50:49,378 INFO misc.py line 115 22900] Train: [64/100][27/156] Data 0.001 (0.001) Batch 4.112 (3.523) Remain 05:37:20 loss: 0.2697 Lr: 0.01636
[2023-08-08 04:50:52,479 INFO misc.py line 115 22900] Train: [64/100][28/156] Data 0.001 (0.001) Batch 3.100 (3.506) Remain 05:35:39 loss: 0.3590 Lr: 0.01636
[2023-08-08 04:50:56,022 INFO misc.py line 115 22900] Train: [64/100][29/156] Data 0.001 (0.001) Batch 3.543 (3.508) Remain 05:35:44 loss: 0.2474 Lr: 0.01635
[2023-08-08 04:50:59,789 INFO misc.py line 115 22900] Train: [64/100][30/156] Data 0.001 (0.001) Batch 3.768 (3.517) Remain 05:36:36 loss: 0.1403 Lr: 0.01635
[2023-08-08 04:51:03,320 INFO misc.py line 115 22900] Train: [64/100][31/156] Data 0.001 (0.001) Batch 3.531 (3.518) Remain 05:36:35 loss: 0.0868 Lr: 0.01634
[2023-08-08 04:51:06,888 INFO misc.py line 115 22900] Train: [64/100][32/156] Data 0.001 (0.001) Batch 3.568 (3.520) Remain 05:36:41 loss: 0.1914 Lr: 0.01634
[2023-08-08 04:51:10,206 INFO misc.py line 115 22900] Train: [64/100][33/156] Data 0.001 (0.001) Batch 3.318 (3.513) Remain 05:35:59 loss: 0.2712 Lr: 0.01633
[2023-08-08 04:51:12,819 INFO misc.py line 115 22900] Train: [64/100][34/156] Data 0.001 (0.001) Batch 2.613 (3.484) Remain 05:33:09 loss: 0.1187 Lr: 0.01633
[2023-08-08 04:51:15,457 INFO misc.py line 115 22900] Train: [64/100][35/156] Data 0.001 (0.001) Batch 2.638 (3.457) Remain 05:30:34 loss: 0.1154 Lr: 0.01632
[2023-08-08 04:51:18,393 INFO misc.py line 115 22900] Train: [64/100][36/156] Data 0.001 (0.001) Batch 2.936 (3.442) Remain 05:29:00 loss: 0.2433 Lr: 0.01632
[2023-08-08 04:51:21,747 INFO misc.py line 115 22900] Train: [64/100][37/156] Data 0.001 (0.001) Batch 3.354 (3.439) Remain 05:28:42 loss: 0.1302 Lr: 0.01631
[2023-08-08 04:51:24,186 INFO misc.py line 115 22900] Train: [64/100][38/156] Data 0.001 (0.001) Batch 2.439 (3.410) Remain 05:25:55 loss: 0.0872 Lr: 0.01631
[2023-08-08 04:51:26,959 INFO misc.py line 115 22900] Train: [64/100][39/156] Data 0.001 (0.001) Batch 2.773 (3.393) Remain 05:24:10 loss: 0.1797 Lr: 0.01630
[2023-08-08 04:51:30,508 INFO misc.py line 115 22900] Train: [64/100][40/156] Data 0.001 (0.001) Batch 3.549 (3.397) Remain 05:24:31 loss: 0.1916 Lr: 0.01630
[2023-08-08 04:51:33,782 INFO misc.py line 115 22900] Train: [64/100][41/156] Data 0.001 (0.001) Batch 3.274 (3.394) Remain 05:24:09 loss: 0.0914 Lr: 0.01629
[2023-08-08 04:51:37,874 INFO misc.py line 115 22900] Train: [64/100][42/156] Data 0.001 (0.001) Batch 4.092 (3.412) Remain 05:25:48 loss: 0.3096 Lr: 0.01629
[2023-08-08 04:51:41,528 INFO misc.py line 115 22900] Train: [64/100][43/156] Data 0.001 (0.001) Batch 3.654 (3.418) Remain 05:26:19 loss: 0.2151 Lr: 0.01628
[2023-08-08 04:51:44,983 INFO misc.py line 115 22900] Train: [64/100][44/156] Data 0.001 (0.001) Batch 3.455 (3.419) Remain 05:26:21 loss: 0.1518 Lr: 0.01628
[2023-08-08 04:51:49,074 INFO misc.py line 115 22900] Train: [64/100][45/156] Data 0.001 (0.001) Batch 4.091 (3.435) Remain 05:27:49 loss: 0.1418 Lr: 0.01627
[2023-08-08 04:51:52,605 INFO misc.py line 115 22900] Train: [64/100][46/156] Data 0.001 (0.001) Batch 3.531 (3.437) Remain 05:27:59 loss: 0.2533 Lr: 0.01627
[2023-08-08 04:51:56,517 INFO misc.py line 115 22900] Train: [64/100][47/156] Data 0.001 (0.001) Batch 3.912 (3.448) Remain 05:28:57 loss: 0.2000 Lr: 0.01626
[2023-08-08 04:52:00,210 INFO misc.py line 115 22900] Train: [64/100][48/156] Data 0.001 (0.001) Batch 3.693 (3.453) Remain 05:29:25 loss: 0.1845 Lr: 0.01626
[2023-08-08 04:52:01,518 INFO misc.py line 115 22900] Train: [64/100][49/156] Data 0.001 (0.001) Batch 1.308 (3.406) Remain 05:24:55 loss: 0.4738 Lr: 0.01625
[2023-08-08 04:52:05,518 INFO misc.py line 115 22900] Train: [64/100][50/156] Data 0.001 (0.001) Batch 4.000 (3.419) Remain 05:26:03 loss: 0.2647 Lr: 0.01625
[2023-08-08 04:52:09,884 INFO misc.py line 115 22900] Train: [64/100][51/156] Data 0.001 (0.001) Batch 4.366 (3.439) Remain 05:27:53 loss: 0.3179 Lr: 0.01624
[2023-08-08 04:52:13,186 INFO misc.py line 115 22900] Train: [64/100][52/156] Data 0.001 (0.001) Batch 3.302 (3.436) Remain 05:27:33 loss: 0.1675 Lr: 0.01624
[2023-08-08 04:52:16,963 INFO misc.py line 115 22900] Train: [64/100][53/156] Data 0.001 (0.001) Batch 3.777 (3.443) Remain 05:28:09 loss: 0.1922 Lr: 0.01623
[2023-08-08 04:52:20,676 INFO misc.py line 115 22900] Train: [64/100][54/156] Data 0.001 (0.001) Batch 3.713 (3.448) Remain 05:28:36 loss: 0.1372 Lr: 0.01623
[2023-08-08 04:52:24,581 INFO misc.py line 115 22900] Train: [64/100][55/156] Data 0.001 (0.001) Batch 3.905 (3.457) Remain 05:29:23 loss: 0.1937 Lr: 0.01622
[2023-08-08 04:52:28,817 INFO misc.py line 115 22900] Train: [64/100][56/156] Data 0.001 (0.001) Batch 4.236 (3.472) Remain 05:30:43 loss: 0.3102 Lr: 0.01622
[2023-08-08 04:52:32,083 INFO misc.py line 115 22900] Train: [64/100][57/156] Data 0.001 (0.001) Batch 3.266 (3.468) Remain 05:30:18 loss: 0.2010 Lr: 0.01621
[2023-08-08 04:52:35,641 INFO misc.py line 115 22900] Train: [64/100][58/156] Data 0.001 (0.001) Batch 3.558 (3.469) Remain 05:30:24 loss: 0.1489 Lr: 0.01621
[2023-08-08 04:52:38,553 INFO misc.py line 115 22900] Train: [64/100][59/156] Data 0.001 (0.001) Batch 2.911 (3.459) Remain 05:29:23 loss: 0.1920 Lr: 0.01620
[2023-08-08 04:52:41,709 INFO misc.py line 115 22900] Train: [64/100][60/156] Data 0.001 (0.001) Batch 3.156 (3.454) Remain 05:28:50 loss: 0.1724 Lr: 0.01620
[2023-08-08 04:52:44,893 INFO misc.py line 115 22900] Train: [64/100][61/156] Data 0.001 (0.001) Batch 3.184 (3.450) Remain 05:28:20 loss: 0.2363 Lr: 0.01619
[2023-08-08 04:52:48,012 INFO misc.py line 115 22900] Train: [64/100][62/156] Data 0.001 (0.001) Batch 3.119 (3.444) Remain 05:27:44 loss: 0.2627 Lr: 0.01619
[2023-08-08 04:52:50,637 INFO misc.py line 115 22900] Train: [64/100][63/156] Data 0.001 (0.001) Batch 2.625 (3.430) Remain 05:26:23 loss: 0.0816 Lr: 0.01618
[2023-08-08 04:52:55,074 INFO misc.py line 115 22900] Train: [64/100][64/156] Data 0.001 (0.001) Batch 4.436 (3.447) Remain 05:27:54 loss: 0.3964 Lr: 0.01618
[2023-08-08 04:52:59,015 INFO misc.py line 115 22900] Train: [64/100][65/156] Data 0.001 (0.001) Batch 3.941 (3.455) Remain 05:28:36 loss: 0.2742 Lr: 0.01617
[2023-08-08 04:53:01,915 INFO misc.py line 115 22900] Train: [64/100][66/156] Data 0.016 (0.001) Batch 2.900 (3.446) Remain 05:27:42 loss: 0.2674 Lr: 0.01617
[2023-08-08 04:53:05,695 INFO misc.py line 115 22900] Train: [64/100][67/156] Data 0.001 (0.001) Batch 3.780 (3.451) Remain 05:28:08 loss: 0.3969 Lr: 0.01616
[2023-08-08 04:53:08,533 INFO misc.py line 115 22900] Train: [64/100][68/156] Data 0.001 (0.001) Batch 2.839 (3.442) Remain 05:27:11 loss: 0.1516 Lr: 0.01616
[2023-08-08 04:53:12,536 INFO misc.py line 115 22900] Train: [64/100][69/156] Data 0.001 (0.001) Batch 4.003 (3.450) Remain 05:27:56 loss: 0.2687 Lr: 0.01616
[2023-08-08 04:53:16,039 INFO misc.py line 115 22900] Train: [64/100][70/156] Data 0.001 (0.001) Batch 3.503 (3.451) Remain 05:27:57 loss: 0.1979 Lr: 0.01615
[2023-08-08 04:53:18,944 INFO misc.py line 115 22900] Train: [64/100][71/156] Data 0.001 (0.001) Batch 2.904 (3.443) Remain 05:27:08 loss: 0.1960 Lr: 0.01615
[2023-08-08 04:53:21,614 INFO misc.py line 115 22900] Train: [64/100][72/156] Data 0.001 (0.001) Batch 2.670 (3.432) Remain 05:26:01 loss: 0.0953 Lr: 0.01614
[2023-08-08 04:53:25,425 INFO misc.py line 115 22900] Train: [64/100][73/156] Data 0.001 (0.001) Batch 3.811 (3.437) Remain 05:26:28 loss: 0.3625 Lr: 0.01614
[2023-08-08 04:53:28,013 INFO misc.py line 115 22900] Train: [64/100][74/156] Data 0.001 (0.001) Batch 2.587 (3.425) Remain 05:25:16 loss: 0.1782 Lr: 0.01613
[2023-08-08 04:53:31,706 INFO misc.py line 115 22900] Train: [64/100][75/156] Data 0.001 (0.001) Batch 3.693 (3.429) Remain 05:25:34 loss: 0.2419 Lr: 0.01613
[2023-08-08 04:53:35,095 INFO misc.py line 115 22900] Train: [64/100][76/156] Data 0.001 (0.001) Batch 3.389 (3.428) Remain 05:25:28 loss: 0.3111 Lr: 0.01612
[2023-08-08 04:53:37,693 INFO misc.py line 115 22900] Train: [64/100][77/156] Data 0.001 (0.001) Batch 2.598 (3.417) Remain 05:24:20 loss: 0.1649 Lr: 0.01612
[2023-08-08 04:53:40,279 INFO misc.py line 115 22900] Train: [64/100][78/156] Data 0.002 (0.001) Batch 2.586 (3.406) Remain 05:23:14 loss: 0.2289 Lr: 0.01611
[2023-08-08 04:53:44,397 INFO misc.py line 115 22900] Train: [64/100][79/156] Data 0.001 (0.001) Batch 4.118 (3.415) Remain 05:24:04 loss: 0.2718 Lr: 0.01611
[2023-08-08 04:53:46,714 INFO misc.py line 115 22900] Train: [64/100][80/156] Data 0.001 (0.001) Batch 2.317 (3.401) Remain 05:22:39 loss: 0.1775 Lr: 0.01610
[2023-08-08 04:53:50,301 INFO misc.py line 115 22900] Train: [64/100][81/156] Data 0.001 (0.001) Batch 3.587 (3.404) Remain 05:22:49 loss: 0.2491 Lr: 0.01610
[2023-08-08 04:53:54,395 INFO misc.py line 115 22900] Train: [64/100][82/156] Data 0.001 (0.001) Batch 4.093 (3.412) Remain 05:23:36 loss: 0.3058 Lr: 0.01609
[2023-08-08 04:53:57,958 INFO misc.py line 115 22900] Train: [64/100][83/156] Data 0.001 (0.001) Batch 3.563 (3.414) Remain 05:23:43 loss: 0.1968 Lr: 0.01609
[2023-08-08 04:54:01,608 INFO misc.py line 115 22900] Train: [64/100][84/156] Data 0.001 (0.001) Batch 3.650 (3.417) Remain 05:23:56 loss: 0.1455 Lr: 0.01608
[2023-08-08 04:54:05,148 INFO misc.py line 115 22900] Train: [64/100][85/156] Data 0.001 (0.001) Batch 3.541 (3.419) Remain 05:24:01 loss: 0.1257 Lr: 0.01608
[2023-08-08 04:54:09,149 INFO misc.py line 115 22900] Train: [64/100][86/156] Data 0.001 (0.001) Batch 4.001 (3.426) Remain 05:24:38 loss: 0.3572 Lr: 0.01607
[2023-08-08 04:54:11,956 INFO misc.py line 115 22900] Train: [64/100][87/156] Data 0.001 (0.001) Batch 2.806 (3.418) Remain 05:23:52 loss: 0.1468 Lr: 0.01607
[2023-08-08 04:54:15,935 INFO misc.py line 115 22900] Train: [64/100][88/156] Data 0.001 (0.001) Batch 3.980 (3.425) Remain 05:24:26 loss: 0.3054 Lr: 0.01606
[2023-08-08 04:54:19,986 INFO misc.py line 115 22900] Train: [64/100][89/156] Data 0.001 (0.001) Batch 4.051 (3.432) Remain 05:25:04 loss: 0.2741 Lr: 0.01606
[2023-08-08 04:54:23,187 INFO misc.py line 115 22900] Train: [64/100][90/156] Data 0.001 (0.001) Batch 3.200 (3.429) Remain 05:24:46 loss: 0.1070 Lr: 0.01605
[2023-08-08 04:54:26,749 INFO misc.py line 115 22900] Train: [64/100][91/156] Data 0.001 (0.001) Batch 3.563 (3.431) Remain 05:24:51 loss: 0.2414 Lr: 0.01605
[2023-08-08 04:54:29,917 INFO misc.py line 115 22900] Train: [64/100][92/156] Data 0.001 (0.001) Batch 3.167 (3.428) Remain 05:24:31 loss: 0.2729 Lr: 0.01604
[2023-08-08 04:54:34,437 INFO misc.py line 115 22900] Train: [64/100][93/156] Data 0.001 (0.001) Batch 4.521 (3.440) Remain 05:25:36 loss: 0.2244 Lr: 0.01604
[2023-08-08 04:54:37,930 INFO misc.py line 115 22900] Train: [64/100][94/156] Data 0.001 (0.001) Batch 3.492 (3.441) Remain 05:25:36 loss: 0.0965 Lr: 0.01603
[2023-08-08 04:54:42,207 INFO misc.py line 115 22900] Train: [64/100][95/156] Data 0.001 (0.001) Batch 4.277 (3.450) Remain 05:26:24 loss: 0.3907 Lr: 0.01603
[2023-08-08 04:54:44,956 INFO misc.py line 115 22900] Train: [64/100][96/156] Data 0.001 (0.001) Batch 2.749 (3.442) Remain 05:25:38 loss: 0.1505 Lr: 0.01602
[2023-08-08 04:54:48,421 INFO misc.py line 115 22900] Train: [64/100][97/156] Data 0.001 (0.001) Batch 3.465 (3.443) Remain 05:25:36 loss: 0.2656 Lr: 0.01602
[2023-08-08 04:54:52,515 INFO misc.py line 115 22900] Train: [64/100][98/156] Data 0.001 (0.001) Batch 4.094 (3.449) Remain 05:26:11 loss: 0.1679 Lr: 0.01601
[2023-08-08 04:54:55,833 INFO misc.py line 115 22900] Train: [64/100][99/156] Data 0.001 (0.001) Batch 3.318 (3.448) Remain 05:26:00 loss: 0.1519 Lr: 0.01601
[2023-08-08 04:54:58,882 INFO misc.py line 115 22900] Train: [64/100][100/156] Data 0.001 (0.001) Batch 3.049 (3.444) Remain 05:25:33 loss: 0.2228 Lr: 0.01600
[2023-08-08 04:55:02,984 INFO misc.py line 115 22900] Train: [64/100][101/156] Data 0.001 (0.001) Batch 4.102 (3.451) Remain 05:26:08 loss: 0.2188 Lr: 0.01600
[2023-08-08 04:55:07,218 INFO misc.py line 115 22900] Train: [64/100][102/156] Data 0.001 (0.001) Batch 4.234 (3.459) Remain 05:26:49 loss: 0.2862 Lr: 0.01599
[2023-08-08 04:55:10,403 INFO misc.py line 115 22900] Train: [64/100][103/156] Data 0.001 (0.001) Batch 3.185 (3.456) Remain 05:26:30 loss: 0.2341 Lr: 0.01599
[2023-08-08 04:55:13,402 INFO misc.py line 115 22900] Train: [64/100][104/156] Data 0.001 (0.001) Batch 2.999 (3.451) Remain 05:26:01 loss: 0.2488 Lr: 0.01598
[2023-08-08 04:55:17,102 INFO misc.py line 115 22900] Train: [64/100][105/156] Data 0.001 (0.001) Batch 3.700 (3.454) Remain 05:26:12 loss: 0.2626 Lr: 0.01598
[2023-08-08 04:55:19,780 INFO misc.py line 115 22900] Train: [64/100][106/156] Data 0.001 (0.001) Batch 2.678 (3.446) Remain 05:25:26 loss: 0.1300 Lr: 0.01597
[2023-08-08 04:55:22,781 INFO misc.py line 115 22900] Train: [64/100][107/156] Data 0.001 (0.001) Batch 3.001 (3.442) Remain 05:24:58 loss: 0.1153 Lr: 0.01597
[2023-08-08 04:55:26,535 INFO misc.py line 115 22900] Train: [64/100][108/156] Data 0.001 (0.001) Batch 3.755 (3.445) Remain 05:25:11 loss: 0.1230 Lr: 0.01596
[2023-08-08 04:55:29,879 INFO misc.py line 115 22900] Train: [64/100][109/156] Data 0.001 (0.001) Batch 3.343 (3.444) Remain 05:25:02 loss: 0.3122 Lr: 0.01596
[2023-08-08 04:55:33,417 INFO misc.py line 115 22900] Train: [64/100][110/156] Data 0.001 (0.001) Batch 3.539 (3.445) Remain 05:25:04 loss: 0.1486 Lr: 0.01595
[2023-08-08 04:55:36,821 INFO misc.py line 115 22900] Train: [64/100][111/156] Data 0.001 (0.001) Batch 3.404 (3.444) Remain 05:24:58 loss: 0.0933 Lr: 0.01595
[2023-08-08 04:55:40,205 INFO misc.py line 115 22900] Train: [64/100][112/156] Data 0.001 (0.001) Batch 3.384 (3.444) Remain 05:24:52 loss: 0.2899 Lr: 0.01594
[2023-08-08 04:55:43,520 INFO misc.py line 115 22900] Train: [64/100][113/156] Data 0.001 (0.001) Batch 3.316 (3.443) Remain 05:24:42 loss: 0.1905 Lr: 0.01594
[2023-08-08 04:55:47,781 INFO misc.py line 115 22900] Train: [64/100][114/156] Data 0.001 (0.001) Batch 4.261 (3.450) Remain 05:25:20 loss: 0.2179 Lr: 0.01593
[2023-08-08 04:55:51,194 INFO misc.py line 115 22900] Train: [64/100][115/156] Data 0.001 (0.001) Batch 3.413 (3.450) Remain 05:25:15 loss: 0.2435 Lr: 0.01593
[2023-08-08 04:55:55,625 INFO misc.py line 115 22900] Train: [64/100][116/156] Data 0.001 (0.001) Batch 4.431 (3.458) Remain 05:26:00 loss: 0.2540 Lr: 0.01592
[2023-08-08 04:55:59,235 INFO misc.py line 115 22900] Train: [64/100][117/156] Data 0.001 (0.001) Batch 3.610 (3.460) Remain 05:26:04 loss: 0.2228 Lr: 0.01592
[2023-08-08 04:56:03,289 INFO misc.py line 115 22900] Train: [64/100][118/156] Data 0.001 (0.001) Batch 4.054 (3.465) Remain 05:26:30 loss: 0.2795 Lr: 0.01591
[2023-08-08 04:56:06,532 INFO misc.py line 115 22900] Train: [64/100][119/156] Data 0.001 (0.001) Batch 3.243 (3.463) Remain 05:26:16 loss: 0.3302 Lr: 0.01591
[2023-08-08 04:56:10,085 INFO misc.py line 115 22900] Train: [64/100][120/156] Data 0.001 (0.001) Batch 3.553 (3.464) Remain 05:26:17 loss: 0.1625 Lr: 0.01590
[2023-08-08 04:56:13,792 INFO misc.py line 115 22900] Train: [64/100][121/156] Data 0.001 (0.001) Batch 3.706 (3.466) Remain 05:26:25 loss: 0.2442 Lr: 0.01590
[2023-08-08 04:56:17,714 INFO misc.py line 115 22900] Train: [64/100][122/156] Data 0.001 (0.001) Batch 3.923 (3.470) Remain 05:26:43 loss: 0.1310 Lr: 0.01589
[2023-08-08 04:56:20,979 INFO misc.py line 115 22900] Train: [64/100][123/156] Data 0.001 (0.001) Batch 3.265 (3.468) Remain 05:26:30 loss: 0.1186 Lr: 0.01589
[2023-08-08 04:56:24,521 INFO misc.py line 115 22900] Train: [64/100][124/156] Data 0.001 (0.001) Batch 3.542 (3.469) Remain 05:26:30 loss: 0.1733 Lr: 0.01588
[2023-08-08 04:56:27,470 INFO misc.py line 115 22900] Train: [64/100][125/156] Data 0.001 (0.001) Batch 2.949 (3.464) Remain 05:26:03 loss: 0.0751 Lr: 0.01588
[2023-08-08 04:56:31,289 INFO misc.py line 115 22900] Train: [64/100][126/156] Data 0.001 (0.001) Batch 3.819 (3.467) Remain 05:26:15 loss: 0.1194 Lr: 0.01587
[2023-08-08 04:56:34,498 INFO misc.py line 115 22900] Train: [64/100][127/156] Data 0.001 (0.001) Batch 3.209 (3.465) Remain 05:26:00 loss: 0.1701 Lr: 0.01587
[2023-08-08 04:56:37,845 INFO misc.py line 115 22900] Train: [64/100][128/156] Data 0.001 (0.001) Batch 3.347 (3.464) Remain 05:25:51 loss: 0.1641 Lr: 0.01586
[2023-08-08 04:56:40,889 INFO misc.py line 115 22900] Train: [64/100][129/156] Data 0.001 (0.001) Batch 3.044 (3.461) Remain 05:25:29 loss: 0.1002 Lr: 0.01586
[2023-08-08 04:56:43,519 INFO misc.py line 115 22900] Train: [64/100][130/156] Data 0.001 (0.001) Batch 2.630 (3.454) Remain 05:24:49 loss: 0.1450 Lr: 0.01585
[2023-08-08 04:56:46,505 INFO misc.py line 115 22900] Train: [64/100][131/156] Data 0.001 (0.001) Batch 2.986 (3.451) Remain 05:24:25 loss: 0.1089 Lr: 0.01585
[2023-08-08 04:56:50,317 INFO misc.py line 115 22900] Train: [64/100][132/156] Data 0.001 (0.001) Batch 3.812 (3.453) Remain 05:24:37 loss: 0.2283 Lr: 0.01584
[2023-08-08 04:56:53,620 INFO misc.py line 115 22900] Train: [64/100][133/156] Data 0.001 (0.001) Batch 3.303 (3.452) Remain 05:24:27 loss: 0.2352 Lr: 0.01584
[2023-08-08 04:56:58,034 INFO misc.py line 115 22900] Train: [64/100][134/156] Data 0.001 (0.001) Batch 4.413 (3.460) Remain 05:25:05 loss: 0.6346 Lr: 0.01583
[2023-08-08 04:57:00,527 INFO misc.py line 115 22900] Train: [64/100][135/156] Data 0.001 (0.001) Batch 2.493 (3.452) Remain 05:24:20 loss: 0.1371 Lr: 0.01583
[2023-08-08 04:57:02,823 INFO misc.py line 115 22900] Train: [64/100][136/156] Data 0.001 (0.001) Batch 2.296 (3.444) Remain 05:23:28 loss: 0.0708 Lr: 0.01582
[2023-08-08 04:57:06,166 INFO misc.py line 115 22900] Train: [64/100][137/156] Data 0.001 (0.001) Batch 3.344 (3.443) Remain 05:23:20 loss: 0.1434 Lr: 0.01582
[2023-08-08 04:57:09,998 INFO misc.py line 115 22900] Train: [64/100][138/156] Data 0.001 (0.001) Batch 3.832 (3.446) Remain 05:23:33 loss: 0.2991 Lr: 0.01581
[2023-08-08 04:57:14,130 INFO misc.py line 115 22900] Train: [64/100][139/156] Data 0.001 (0.001) Batch 4.133 (3.451) Remain 05:23:58 loss: 0.1025 Lr: 0.01581
[2023-08-08 04:57:18,145 INFO misc.py line 115 22900] Train: [64/100][140/156] Data 0.001 (0.001) Batch 4.014 (3.455) Remain 05:24:18 loss: 0.2320 Lr: 0.01580
[2023-08-08 04:57:20,797 INFO misc.py line 115 22900] Train: [64/100][141/156] Data 0.001 (0.001) Batch 2.652 (3.449) Remain 05:23:41 loss: 0.0748 Lr: 0.01580
[2023-08-08 04:57:24,875 INFO misc.py line 115 22900] Train: [64/100][142/156] Data 0.001 (0.001) Batch 4.078 (3.454) Remain 05:24:03 loss: 0.1871 Lr: 0.01579
[2023-08-08 04:57:27,948 INFO misc.py line 115 22900] Train: [64/100][143/156] Data 0.001 (0.001) Batch 3.073 (3.451) Remain 05:23:45 loss: 0.1115 Lr: 0.01579
[2023-08-08 04:57:32,038 INFO misc.py line 115 22900] Train: [64/100][144/156] Data 0.001 (0.001) Batch 4.090 (3.455) Remain 05:24:07 loss: 0.2453 Lr: 0.01578
[2023-08-08 04:57:35,883 INFO misc.py line 115 22900] Train: [64/100][145/156] Data 0.001 (0.001) Batch 3.845 (3.458) Remain 05:24:19 loss: 0.2676 Lr: 0.01578
[2023-08-08 04:57:39,289 INFO misc.py line 115 22900] Train: [64/100][146/156] Data 0.001 (0.001) Batch 3.406 (3.458) Remain 05:24:13 loss: 0.2039 Lr: 0.01577
[2023-08-08 04:57:42,488 INFO misc.py line 115 22900] Train: [64/100][147/156] Data 0.001 (0.001) Batch 3.199 (3.456) Remain 05:24:00 loss: 0.1747 Lr: 0.01577
[2023-08-08 04:57:45,122 INFO misc.py line 115 22900] Train: [64/100][148/156] Data 0.001 (0.001) Batch 2.635 (3.450) Remain 05:23:24 loss: 0.1381 Lr: 0.01576
[2023-08-08 04:57:49,331 INFO misc.py line 115 22900] Train: [64/100][149/156] Data 0.001 (0.001) Batch 4.209 (3.456) Remain 05:23:50 loss: 0.1853 Lr: 0.01576
[2023-08-08 04:57:52,553 INFO misc.py line 115 22900] Train: [64/100][150/156] Data 0.001 (0.001) Batch 3.222 (3.454) Remain 05:23:38 loss: 0.3076 Lr: 0.01575
[2023-08-08 04:57:55,947 INFO misc.py line 115 22900] Train: [64/100][151/156] Data 0.001 (0.001) Batch 3.394 (3.454) Remain 05:23:32 loss: 0.3090 Lr: 0.01575
[2023-08-08 04:57:58,975 INFO misc.py line 115 22900] Train: [64/100][152/156] Data 0.001 (0.001) Batch 3.028 (3.451) Remain 05:23:12 loss: 0.0856 Lr: 0.01575
[2023-08-08 04:58:01,551 INFO misc.py line 115 22900] Train: [64/100][153/156] Data 0.001 (0.001) Batch 2.576 (3.445) Remain 05:22:36 loss: 0.1099 Lr: 0.01574
[2023-08-08 04:58:04,081 INFO misc.py line 115 22900] Train: [64/100][154/156] Data 0.001 (0.001) Batch 2.529 (3.439) Remain 05:21:59 loss: 0.1458 Lr: 0.01574
[2023-08-08 04:58:07,602 INFO misc.py line 115 22900] Train: [64/100][155/156] Data 0.001 (0.001) Batch 3.521 (3.439) Remain 05:21:58 loss: 0.2122 Lr: 0.01573
[2023-08-08 04:58:10,557 INFO misc.py line 115 22900] Train: [64/100][156/156] Data 0.001 (0.001) Batch 2.954 (3.436) Remain 05:21:37 loss: 0.1371 Lr: 0.01573
[2023-08-08 04:58:10,557 INFO misc.py line 129 22900] Train result: loss: 0.2102 
[2023-08-08 04:58:10,558 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 04:58:12,645 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.2700 
[2023-08-08 04:58:13,517 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.4547 
[2023-08-08 04:58:15,182 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.7157 
[2023-08-08 04:58:16,703 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.2812 
[2023-08-08 04:58:18,547 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.8773 
[2023-08-08 04:58:20,207 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.6003 
[2023-08-08 04:58:22,348 INFO evaluator.py line 122 22900] Test: [7/24] Loss 2.9946 
[2023-08-08 04:58:24,152 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.3229 
[2023-08-08 04:58:25,435 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.4504 
[2023-08-08 04:58:27,564 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3164 
[2023-08-08 04:58:28,091 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.1143 
[2023-08-08 04:58:29,623 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.7854 
[2023-08-08 04:58:32,335 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.0700 
[2023-08-08 04:58:34,014 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.9176 
[2023-08-08 04:58:36,037 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.2863 
[2023-08-08 04:58:38,748 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.1425 
[2023-08-08 04:58:41,455 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.5001 
[2023-08-08 04:58:43,302 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.5017 
[2023-08-08 04:58:44,051 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.3482 
[2023-08-08 04:58:44,937 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.8080 
[2023-08-08 04:58:47,197 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3411 
[2023-08-08 04:58:49,164 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.7100 
[2023-08-08 04:58:51,010 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.6474 
[2023-08-08 04:58:52,945 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.8016 
[2023-08-08 04:58:52,991 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2378/0.3291/0.7007.
[2023-08-08 04:58:52,991 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6450/0.9643
[2023-08-08 04:58:52,991 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9490/0.9916
[2023-08-08 04:58:52,991 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1763/0.4018
[2023-08-08 04:58:52,991 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1823/0.2882
[2023-08-08 04:58:52,991 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6803/0.7622
[2023-08-08 04:58:52,991 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.3816/0.5553
[2023-08-08 04:58:52,991 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5515/0.7291
[2023-08-08 04:58:52,991 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1592/0.1751
[2023-08-08 04:58:52,991 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1775/0.3812
[2023-08-08 04:58:52,991 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0194/0.0196
[2023-08-08 04:58:52,991 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 04:58:52,991 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0237/0.0258
[2023-08-08 04:58:52,991 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.2103/0.2944
[2023-08-08 04:58:52,991 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0474/0.0567
[2023-08-08 04:58:52,991 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0167/0.0293
[2023-08-08 04:58:52,991 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-08 04:58:52,991 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0815/0.1037
[2023-08-08 04:58:52,992 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.3518/0.6064
[2023-08-08 04:58:52,992 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 04:58:52,992 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.1026/0.1975
[2023-08-08 04:58:52,992 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 04:58:52,992 INFO misc.py line 152 22900] Currently Best mIoU: 0.2650
[2023-08-08 04:58:52,992 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 04:58:59,351 INFO misc.py line 115 22900] Train: [65/100][1/156] Data 1.639 (1.639) Batch 5.586 (5.586) Remain 08:42:45 loss: 0.1595 Lr: 0.01572
[2023-08-08 04:59:02,746 INFO misc.py line 115 22900] Train: [65/100][2/156] Data 0.001 (0.001) Batch 3.395 (3.395) Remain 05:17:41 loss: 0.2623 Lr: 0.01572
[2023-08-08 04:59:06,048 INFO misc.py line 115 22900] Train: [65/100][3/156] Data 0.001 (0.001) Batch 3.302 (3.302) Remain 05:08:54 loss: 0.2393 Lr: 0.01571
[2023-08-08 04:59:08,579 INFO misc.py line 115 22900] Train: [65/100][4/156] Data 0.001 (0.001) Batch 2.531 (2.531) Remain 03:56:45 loss: 0.0530 Lr: 0.01571
[2023-08-08 04:59:13,246 INFO misc.py line 115 22900] Train: [65/100][5/156] Data 0.001 (0.001) Batch 4.667 (3.599) Remain 05:36:33 loss: 0.3547 Lr: 0.01570
[2023-08-08 04:59:15,859 INFO misc.py line 115 22900] Train: [65/100][6/156] Data 0.001 (0.001) Batch 2.613 (3.270) Remain 05:05:46 loss: 0.1692 Lr: 0.01570
[2023-08-08 04:59:20,274 INFO misc.py line 115 22900] Train: [65/100][7/156] Data 0.001 (0.001) Batch 4.415 (3.556) Remain 05:32:28 loss: 0.2249 Lr: 0.01569
[2023-08-08 04:59:22,987 INFO misc.py line 115 22900] Train: [65/100][8/156] Data 0.001 (0.001) Batch 2.713 (3.388) Remain 05:16:38 loss: 0.1744 Lr: 0.01569
[2023-08-08 04:59:26,346 INFO misc.py line 115 22900] Train: [65/100][9/156] Data 0.001 (0.001) Batch 3.359 (3.383) Remain 05:16:08 loss: 0.3142 Lr: 0.01568
[2023-08-08 04:59:30,333 INFO misc.py line 115 22900] Train: [65/100][10/156] Data 0.001 (0.001) Batch 3.987 (3.469) Remain 05:24:08 loss: 0.1473 Lr: 0.01568
[2023-08-08 04:59:32,532 INFO misc.py line 115 22900] Train: [65/100][11/156] Data 0.001 (0.001) Batch 2.199 (3.311) Remain 05:09:15 loss: 0.1774 Lr: 0.01567
[2023-08-08 04:59:36,694 INFO misc.py line 115 22900] Train: [65/100][12/156] Data 0.001 (0.001) Batch 4.162 (3.405) Remain 05:18:02 loss: 0.2465 Lr: 0.01567
[2023-08-08 04:59:39,565 INFO misc.py line 115 22900] Train: [65/100][13/156] Data 0.001 (0.001) Batch 2.871 (3.352) Remain 05:12:59 loss: 0.1128 Lr: 0.01566
[2023-08-08 04:59:43,117 INFO misc.py line 115 22900] Train: [65/100][14/156] Data 0.001 (0.001) Batch 3.552 (3.370) Remain 05:14:38 loss: 0.1841 Lr: 0.01566
[2023-08-08 04:59:46,777 INFO misc.py line 115 22900] Train: [65/100][15/156] Data 0.001 (0.001) Batch 3.660 (3.394) Remain 05:16:50 loss: 0.2695 Lr: 0.01565
[2023-08-08 04:59:49,885 INFO misc.py line 115 22900] Train: [65/100][16/156] Data 0.001 (0.001) Batch 3.108 (3.372) Remain 05:14:43 loss: 0.1365 Lr: 0.01565
[2023-08-08 04:59:53,370 INFO misc.py line 115 22900] Train: [65/100][17/156] Data 0.001 (0.001) Batch 3.485 (3.380) Remain 05:15:25 loss: 0.1141 Lr: 0.01564
[2023-08-08 04:59:57,659 INFO misc.py line 115 22900] Train: [65/100][18/156] Data 0.001 (0.001) Batch 4.289 (3.441) Remain 05:21:01 loss: 0.3406 Lr: 0.01564
[2023-08-08 05:00:01,529 INFO misc.py line 115 22900] Train: [65/100][19/156] Data 0.001 (0.001) Batch 3.870 (3.468) Remain 05:23:27 loss: 0.2570 Lr: 0.01563
[2023-08-08 05:00:04,617 INFO misc.py line 115 22900] Train: [65/100][20/156] Data 0.001 (0.001) Batch 3.089 (3.445) Remain 05:21:19 loss: 0.2095 Lr: 0.01563
[2023-08-08 05:00:06,618 INFO misc.py line 115 22900] Train: [65/100][21/156] Data 0.001 (0.001) Batch 2.001 (3.365) Remain 05:13:47 loss: 0.0285 Lr: 0.01562
[2023-08-08 05:00:09,828 INFO misc.py line 115 22900] Train: [65/100][22/156] Data 0.001 (0.001) Batch 3.209 (3.357) Remain 05:12:58 loss: 0.2342 Lr: 0.01562
[2023-08-08 05:00:12,166 INFO misc.py line 115 22900] Train: [65/100][23/156] Data 0.001 (0.001) Batch 2.338 (3.306) Remain 05:08:09 loss: 0.1102 Lr: 0.01561
[2023-08-08 05:00:15,972 INFO misc.py line 115 22900] Train: [65/100][24/156] Data 0.001 (0.001) Batch 3.806 (3.330) Remain 05:10:19 loss: 0.2239 Lr: 0.01561
[2023-08-08 05:00:19,379 INFO misc.py line 115 22900] Train: [65/100][25/156] Data 0.001 (0.001) Batch 3.407 (3.333) Remain 05:10:35 loss: 0.2053 Lr: 0.01560
[2023-08-08 05:00:22,828 INFO misc.py line 115 22900] Train: [65/100][26/156] Data 0.001 (0.001) Batch 3.450 (3.338) Remain 05:11:00 loss: 0.2742 Lr: 0.01560
[2023-08-08 05:00:26,807 INFO misc.py line 115 22900] Train: [65/100][27/156] Data 0.001 (0.001) Batch 3.978 (3.365) Remain 05:13:26 loss: 0.1813 Lr: 0.01559
[2023-08-08 05:00:30,821 INFO misc.py line 115 22900] Train: [65/100][28/156] Data 0.001 (0.001) Batch 4.015 (3.391) Remain 05:15:48 loss: 0.3218 Lr: 0.01559
[2023-08-08 05:00:34,251 INFO misc.py line 115 22900] Train: [65/100][29/156] Data 0.001 (0.001) Batch 3.430 (3.392) Remain 05:15:53 loss: 0.1655 Lr: 0.01558
[2023-08-08 05:00:36,706 INFO misc.py line 115 22900] Train: [65/100][30/156] Data 0.001 (0.001) Batch 2.455 (3.358) Remain 05:12:36 loss: 0.2870 Lr: 0.01558
[2023-08-08 05:00:40,110 INFO misc.py line 115 22900] Train: [65/100][31/156] Data 0.001 (0.001) Batch 3.403 (3.359) Remain 05:12:41 loss: 0.1435 Lr: 0.01557
[2023-08-08 05:00:42,647 INFO misc.py line 115 22900] Train: [65/100][32/156] Data 0.001 (0.001) Batch 2.538 (3.331) Remain 05:10:00 loss: 0.3233 Lr: 0.01557
[2023-08-08 05:00:46,086 INFO misc.py line 115 22900] Train: [65/100][33/156] Data 0.001 (0.001) Batch 3.439 (3.335) Remain 05:10:17 loss: 0.3581 Lr: 0.01556
[2023-08-08 05:00:50,288 INFO misc.py line 115 22900] Train: [65/100][34/156] Data 0.001 (0.001) Batch 4.202 (3.363) Remain 05:12:49 loss: 0.1626 Lr: 0.01556
[2023-08-08 05:00:53,233 INFO misc.py line 115 22900] Train: [65/100][35/156] Data 0.001 (0.001) Batch 2.945 (3.350) Remain 05:11:33 loss: 0.1331 Lr: 0.01555
[2023-08-08 05:00:57,354 INFO misc.py line 115 22900] Train: [65/100][36/156] Data 0.001 (0.001) Batch 4.121 (3.373) Remain 05:13:40 loss: 0.2870 Lr: 0.01555
[2023-08-08 05:01:00,881 INFO misc.py line 115 22900] Train: [65/100][37/156] Data 0.001 (0.001) Batch 3.528 (3.377) Remain 05:14:02 loss: 0.1254 Lr: 0.01554
[2023-08-08 05:01:03,952 INFO misc.py line 115 22900] Train: [65/100][38/156] Data 0.001 (0.001) Batch 3.071 (3.369) Remain 05:13:10 loss: 0.1389 Lr: 0.01554
[2023-08-08 05:01:08,419 INFO misc.py line 115 22900] Train: [65/100][39/156] Data 0.001 (0.001) Batch 4.467 (3.399) Remain 05:15:57 loss: 0.3767 Lr: 0.01553
[2023-08-08 05:01:11,066 INFO misc.py line 115 22900] Train: [65/100][40/156] Data 0.001 (0.001) Batch 2.647 (3.379) Remain 05:14:00 loss: 0.2489 Lr: 0.01553
[2023-08-08 05:01:13,735 INFO misc.py line 115 22900] Train: [65/100][41/156] Data 0.001 (0.001) Batch 2.669 (3.360) Remain 05:12:13 loss: 0.1011 Lr: 0.01552
[2023-08-08 05:01:17,765 INFO misc.py line 115 22900] Train: [65/100][42/156] Data 0.001 (0.001) Batch 4.030 (3.377) Remain 05:13:45 loss: 0.1929 Lr: 0.01552
[2023-08-08 05:01:21,174 INFO misc.py line 115 22900] Train: [65/100][43/156] Data 0.001 (0.001) Batch 3.409 (3.378) Remain 05:13:46 loss: 0.2006 Lr: 0.01551
[2023-08-08 05:01:25,482 INFO misc.py line 115 22900] Train: [65/100][44/156] Data 0.001 (0.001) Batch 4.308 (3.401) Remain 05:15:49 loss: 0.4374 Lr: 0.01551
[2023-08-08 05:01:28,723 INFO misc.py line 115 22900] Train: [65/100][45/156] Data 0.001 (0.001) Batch 3.242 (3.397) Remain 05:15:24 loss: 0.3121 Lr: 0.01550
[2023-08-08 05:01:31,942 INFO misc.py line 115 22900] Train: [65/100][46/156] Data 0.001 (0.001) Batch 3.218 (3.393) Remain 05:14:58 loss: 0.1857 Lr: 0.01550
[2023-08-08 05:01:35,632 INFO misc.py line 115 22900] Train: [65/100][47/156] Data 0.001 (0.001) Batch 3.691 (3.400) Remain 05:15:32 loss: 0.1726 Lr: 0.01549
[2023-08-08 05:01:40,164 INFO misc.py line 115 22900] Train: [65/100][48/156] Data 0.001 (0.001) Batch 4.532 (3.425) Remain 05:17:49 loss: 0.2515 Lr: 0.01549
[2023-08-08 05:01:44,131 INFO misc.py line 115 22900] Train: [65/100][49/156] Data 0.001 (0.001) Batch 3.967 (3.437) Remain 05:18:51 loss: 0.1607 Lr: 0.01548
[2023-08-08 05:01:46,879 INFO misc.py line 115 22900] Train: [65/100][50/156] Data 0.001 (0.001) Batch 2.748 (3.422) Remain 05:17:26 loss: 0.1571 Lr: 0.01548
[2023-08-08 05:01:50,118 INFO misc.py line 115 22900] Train: [65/100][51/156] Data 0.001 (0.001) Batch 3.239 (3.418) Remain 05:17:01 loss: 0.1663 Lr: 0.01547
[2023-08-08 05:01:53,838 INFO misc.py line 115 22900] Train: [65/100][52/156] Data 0.001 (0.001) Batch 3.720 (3.424) Remain 05:17:32 loss: 0.2620 Lr: 0.01547
[2023-08-08 05:01:56,828 INFO misc.py line 115 22900] Train: [65/100][53/156] Data 0.001 (0.001) Batch 2.990 (3.416) Remain 05:16:40 loss: 0.2318 Lr: 0.01547
[2023-08-08 05:02:00,190 INFO misc.py line 115 22900] Train: [65/100][54/156] Data 0.001 (0.001) Batch 3.362 (3.415) Remain 05:16:31 loss: 0.0894 Lr: 0.01546
[2023-08-08 05:02:04,406 INFO misc.py line 115 22900] Train: [65/100][55/156] Data 0.001 (0.001) Batch 4.215 (3.430) Remain 05:17:53 loss: 0.1996 Lr: 0.01546
[2023-08-08 05:02:08,488 INFO misc.py line 115 22900] Train: [65/100][56/156] Data 0.001 (0.001) Batch 4.083 (3.442) Remain 05:18:59 loss: 0.3149 Lr: 0.01545
[2023-08-08 05:02:10,527 INFO misc.py line 115 22900] Train: [65/100][57/156] Data 0.001 (0.001) Batch 2.038 (3.416) Remain 05:16:31 loss: 0.1170 Lr: 0.01545
[2023-08-08 05:02:14,292 INFO misc.py line 115 22900] Train: [65/100][58/156] Data 0.001 (0.001) Batch 3.765 (3.423) Remain 05:17:02 loss: 0.2531 Lr: 0.01544
[2023-08-08 05:02:17,660 INFO misc.py line 115 22900] Train: [65/100][59/156] Data 0.001 (0.001) Batch 3.368 (3.422) Remain 05:16:54 loss: 0.2835 Lr: 0.01544
[2023-08-08 05:02:20,956 INFO misc.py line 115 22900] Train: [65/100][60/156] Data 0.001 (0.001) Batch 3.296 (3.419) Remain 05:16:38 loss: 0.3034 Lr: 0.01543
[2023-08-08 05:02:24,830 INFO misc.py line 115 22900] Train: [65/100][61/156] Data 0.001 (0.001) Batch 3.873 (3.427) Remain 05:17:18 loss: 0.1244 Lr: 0.01543
[2023-08-08 05:02:27,592 INFO misc.py line 115 22900] Train: [65/100][62/156] Data 0.001 (0.001) Batch 2.762 (3.416) Remain 05:16:12 loss: 0.1089 Lr: 0.01542
[2023-08-08 05:02:31,044 INFO misc.py line 115 22900] Train: [65/100][63/156] Data 0.001 (0.001) Batch 3.452 (3.417) Remain 05:16:12 loss: 0.1546 Lr: 0.01542
[2023-08-08 05:02:34,570 INFO misc.py line 115 22900] Train: [65/100][64/156] Data 0.001 (0.001) Batch 3.526 (3.418) Remain 05:16:18 loss: 0.0984 Lr: 0.01541
[2023-08-08 05:02:38,565 INFO misc.py line 115 22900] Train: [65/100][65/156] Data 0.001 (0.001) Batch 3.995 (3.428) Remain 05:17:07 loss: 0.2831 Lr: 0.01541
[2023-08-08 05:02:42,489 INFO misc.py line 115 22900] Train: [65/100][66/156] Data 0.001 (0.001) Batch 3.925 (3.436) Remain 05:17:47 loss: 0.2365 Lr: 0.01540
[2023-08-08 05:02:45,165 INFO misc.py line 115 22900] Train: [65/100][67/156] Data 0.001 (0.001) Batch 2.675 (3.424) Remain 05:16:38 loss: 0.1018 Lr: 0.01540
[2023-08-08 05:02:48,334 INFO misc.py line 115 22900] Train: [65/100][68/156] Data 0.001 (0.001) Batch 3.169 (3.420) Remain 05:16:12 loss: 0.1774 Lr: 0.01539
[2023-08-08 05:02:51,484 INFO misc.py line 115 22900] Train: [65/100][69/156] Data 0.001 (0.001) Batch 3.150 (3.416) Remain 05:15:46 loss: 0.1902 Lr: 0.01539
[2023-08-08 05:02:55,160 INFO misc.py line 115 22900] Train: [65/100][70/156] Data 0.001 (0.001) Batch 3.676 (3.420) Remain 05:16:04 loss: 0.2389 Lr: 0.01538
[2023-08-08 05:02:58,676 INFO misc.py line 115 22900] Train: [65/100][71/156] Data 0.001 (0.001) Batch 3.516 (3.421) Remain 05:16:09 loss: 0.2823 Lr: 0.01538
[2023-08-08 05:03:01,395 INFO misc.py line 115 22900] Train: [65/100][72/156] Data 0.001 (0.001) Batch 2.719 (3.411) Remain 05:15:09 loss: 0.1206 Lr: 0.01537
[2023-08-08 05:03:05,151 INFO misc.py line 115 22900] Train: [65/100][73/156] Data 0.001 (0.001) Batch 3.756 (3.416) Remain 05:15:33 loss: 0.2266 Lr: 0.01537
[2023-08-08 05:03:09,020 INFO misc.py line 115 22900] Train: [65/100][74/156] Data 0.001 (0.001) Batch 3.869 (3.422) Remain 05:16:05 loss: 0.1531 Lr: 0.01536
[2023-08-08 05:03:12,246 INFO misc.py line 115 22900] Train: [65/100][75/156] Data 0.001 (0.001) Batch 3.226 (3.419) Remain 05:15:46 loss: 0.2140 Lr: 0.01536
[2023-08-08 05:03:14,675 INFO misc.py line 115 22900] Train: [65/100][76/156] Data 0.001 (0.001) Batch 2.429 (3.406) Remain 05:14:28 loss: 0.1225 Lr: 0.01535
[2023-08-08 05:03:18,016 INFO misc.py line 115 22900] Train: [65/100][77/156] Data 0.001 (0.001) Batch 3.340 (3.405) Remain 05:14:20 loss: 0.3496 Lr: 0.01535
[2023-08-08 05:03:22,054 INFO misc.py line 115 22900] Train: [65/100][78/156] Data 0.001 (0.001) Batch 4.038 (3.413) Remain 05:15:03 loss: 0.2063 Lr: 0.01534
[2023-08-08 05:03:25,131 INFO misc.py line 115 22900] Train: [65/100][79/156] Data 0.001 (0.001) Batch 3.077 (3.409) Remain 05:14:35 loss: 0.0655 Lr: 0.01534
[2023-08-08 05:03:27,734 INFO misc.py line 115 22900] Train: [65/100][80/156] Data 0.001 (0.001) Batch 2.603 (3.399) Remain 05:13:34 loss: 0.1795 Lr: 0.01533
[2023-08-08 05:03:31,461 INFO misc.py line 115 22900] Train: [65/100][81/156] Data 0.001 (0.001) Batch 3.728 (3.403) Remain 05:13:54 loss: 0.1545 Lr: 0.01533
[2023-08-08 05:03:34,982 INFO misc.py line 115 22900] Train: [65/100][82/156] Data 0.001 (0.001) Batch 3.521 (3.404) Remain 05:13:58 loss: 0.1239 Lr: 0.01532
[2023-08-08 05:03:38,696 INFO misc.py line 115 22900] Train: [65/100][83/156] Data 0.001 (0.001) Batch 3.714 (3.408) Remain 05:14:16 loss: 0.3053 Lr: 0.01532
[2023-08-08 05:03:42,449 INFO misc.py line 115 22900] Train: [65/100][84/156] Data 0.001 (0.001) Batch 3.754 (3.412) Remain 05:14:37 loss: 0.1948 Lr: 0.01531
[2023-08-08 05:03:46,069 INFO misc.py line 115 22900] Train: [65/100][85/156] Data 0.001 (0.001) Batch 3.620 (3.415) Remain 05:14:47 loss: 0.3115 Lr: 0.01531
[2023-08-08 05:03:49,579 INFO misc.py line 115 22900] Train: [65/100][86/156] Data 0.001 (0.001) Batch 3.510 (3.416) Remain 05:14:50 loss: 0.1578 Lr: 0.01530
[2023-08-08 05:03:53,069 INFO misc.py line 115 22900] Train: [65/100][87/156] Data 0.001 (0.001) Batch 3.491 (3.417) Remain 05:14:52 loss: 0.1013 Lr: 0.01530
[2023-08-08 05:03:56,487 INFO misc.py line 115 22900] Train: [65/100][88/156] Data 0.001 (0.001) Batch 3.417 (3.417) Remain 05:14:48 loss: 0.1290 Lr: 0.01529
[2023-08-08 05:04:00,933 INFO misc.py line 115 22900] Train: [65/100][89/156] Data 0.001 (0.001) Batch 4.447 (3.429) Remain 05:15:51 loss: 0.3437 Lr: 0.01529
[2023-08-08 05:04:03,728 INFO misc.py line 115 22900] Train: [65/100][90/156] Data 0.001 (0.001) Batch 2.794 (3.422) Remain 05:15:07 loss: 0.2114 Lr: 0.01528
[2023-08-08 05:04:06,554 INFO misc.py line 115 22900] Train: [65/100][91/156] Data 0.001 (0.001) Batch 2.826 (3.415) Remain 05:14:26 loss: 0.1334 Lr: 0.01528
[2023-08-08 05:04:09,980 INFO misc.py line 115 22900] Train: [65/100][92/156] Data 0.001 (0.001) Batch 3.426 (3.415) Remain 05:14:24 loss: 0.1592 Lr: 0.01527
[2023-08-08 05:04:13,270 INFO misc.py line 115 22900] Train: [65/100][93/156] Data 0.001 (0.001) Batch 3.290 (3.414) Remain 05:14:13 loss: 0.0973 Lr: 0.01527
[2023-08-08 05:04:16,495 INFO misc.py line 115 22900] Train: [65/100][94/156] Data 0.001 (0.001) Batch 3.225 (3.411) Remain 05:13:58 loss: 0.1407 Lr: 0.01526
[2023-08-08 05:04:20,271 INFO misc.py line 115 22900] Train: [65/100][95/156] Data 0.001 (0.001) Batch 3.777 (3.415) Remain 05:14:16 loss: 0.1590 Lr: 0.01526
[2023-08-08 05:04:24,205 INFO misc.py line 115 22900] Train: [65/100][96/156] Data 0.001 (0.001) Batch 3.934 (3.421) Remain 05:14:44 loss: 0.1908 Lr: 0.01525
[2023-08-08 05:04:27,898 INFO misc.py line 115 22900] Train: [65/100][97/156] Data 0.001 (0.001) Batch 3.693 (3.424) Remain 05:14:56 loss: 0.2248 Lr: 0.01525
[2023-08-08 05:04:31,528 INFO misc.py line 115 22900] Train: [65/100][98/156] Data 0.001 (0.001) Batch 3.630 (3.426) Remain 05:15:05 loss: 0.3010 Lr: 0.01525
[2023-08-08 05:04:35,617 INFO misc.py line 115 22900] Train: [65/100][99/156] Data 0.001 (0.001) Batch 4.089 (3.433) Remain 05:15:39 loss: 0.1902 Lr: 0.01524
[2023-08-08 05:04:39,130 INFO misc.py line 115 22900] Train: [65/100][100/156] Data 0.001 (0.001) Batch 3.512 (3.434) Remain 05:15:40 loss: 0.1431 Lr: 0.01524
[2023-08-08 05:04:42,376 INFO misc.py line 115 22900] Train: [65/100][101/156] Data 0.001 (0.001) Batch 3.246 (3.432) Remain 05:15:27 loss: 0.1685 Lr: 0.01523
[2023-08-08 05:04:46,491 INFO misc.py line 115 22900] Train: [65/100][102/156] Data 0.001 (0.001) Batch 4.115 (3.439) Remain 05:16:01 loss: 0.1573 Lr: 0.01523
[2023-08-08 05:04:50,199 INFO misc.py line 115 22900] Train: [65/100][103/156] Data 0.001 (0.001) Batch 3.708 (3.442) Remain 05:16:13 loss: 0.0977 Lr: 0.01522
[2023-08-08 05:04:53,435 INFO misc.py line 115 22900] Train: [65/100][104/156] Data 0.001 (0.001) Batch 3.235 (3.439) Remain 05:15:58 loss: 0.1710 Lr: 0.01522
[2023-08-08 05:04:57,103 INFO misc.py line 115 22900] Train: [65/100][105/156] Data 0.001 (0.001) Batch 3.668 (3.442) Remain 05:16:07 loss: 0.1340 Lr: 0.01521
[2023-08-08 05:05:01,127 INFO misc.py line 115 22900] Train: [65/100][106/156] Data 0.001 (0.001) Batch 4.024 (3.447) Remain 05:16:34 loss: 0.2731 Lr: 0.01521
[2023-08-08 05:05:04,500 INFO misc.py line 115 22900] Train: [65/100][107/156] Data 0.001 (0.001) Batch 3.373 (3.447) Remain 05:16:27 loss: 0.1959 Lr: 0.01520
[2023-08-08 05:05:06,967 INFO misc.py line 115 22900] Train: [65/100][108/156] Data 0.001 (0.001) Batch 2.467 (3.437) Remain 05:15:32 loss: 0.0327 Lr: 0.01520
[2023-08-08 05:05:09,850 INFO misc.py line 115 22900] Train: [65/100][109/156] Data 0.001 (0.001) Batch 2.883 (3.432) Remain 05:15:00 loss: 0.2291 Lr: 0.01519
[2023-08-08 05:05:13,050 INFO misc.py line 115 22900] Train: [65/100][110/156] Data 0.001 (0.001) Batch 3.201 (3.430) Remain 05:14:45 loss: 0.0607 Lr: 0.01519
[2023-08-08 05:05:17,150 INFO misc.py line 115 22900] Train: [65/100][111/156] Data 0.001 (0.001) Batch 4.100 (3.436) Remain 05:15:15 loss: 0.2567 Lr: 0.01518
[2023-08-08 05:05:20,680 INFO misc.py line 115 22900] Train: [65/100][112/156] Data 0.001 (0.001) Batch 3.529 (3.437) Remain 05:15:17 loss: 0.3377 Lr: 0.01518
[2023-08-08 05:05:24,177 INFO misc.py line 115 22900] Train: [65/100][113/156] Data 0.001 (0.001) Batch 3.497 (3.438) Remain 05:15:16 loss: 0.2324 Lr: 0.01517
[2023-08-08 05:05:27,580 INFO misc.py line 115 22900] Train: [65/100][114/156] Data 0.001 (0.001) Batch 3.403 (3.437) Remain 05:15:11 loss: 0.1642 Lr: 0.01517
[2023-08-08 05:05:29,623 INFO misc.py line 115 22900] Train: [65/100][115/156] Data 0.001 (0.001) Batch 2.043 (3.425) Remain 05:13:59 loss: 0.1222 Lr: 0.01516
[2023-08-08 05:05:32,907 INFO misc.py line 115 22900] Train: [65/100][116/156] Data 0.001 (0.001) Batch 3.284 (3.424) Remain 05:13:49 loss: 0.1182 Lr: 0.01516
[2023-08-08 05:05:36,986 INFO misc.py line 115 22900] Train: [65/100][117/156] Data 0.001 (0.001) Batch 4.079 (3.429) Remain 05:14:17 loss: 0.1768 Lr: 0.01515
[2023-08-08 05:05:39,904 INFO misc.py line 115 22900] Train: [65/100][118/156] Data 0.001 (0.001) Batch 2.918 (3.425) Remain 05:13:49 loss: 0.0918 Lr: 0.01515
[2023-08-08 05:05:42,583 INFO misc.py line 115 22900] Train: [65/100][119/156] Data 0.001 (0.001) Batch 2.680 (3.418) Remain 05:13:10 loss: 0.1479 Lr: 0.01514
[2023-08-08 05:05:45,554 INFO misc.py line 115 22900] Train: [65/100][120/156] Data 0.001 (0.001) Batch 2.971 (3.415) Remain 05:12:46 loss: 0.1676 Lr: 0.01514
[2023-08-08 05:05:49,688 INFO misc.py line 115 22900] Train: [65/100][121/156] Data 0.001 (0.001) Batch 4.134 (3.421) Remain 05:13:16 loss: 0.1647 Lr: 0.01513
[2023-08-08 05:05:53,813 INFO misc.py line 115 22900] Train: [65/100][122/156] Data 0.001 (0.001) Batch 4.125 (3.427) Remain 05:13:45 loss: 0.1331 Lr: 0.01513
[2023-08-08 05:05:56,303 INFO misc.py line 115 22900] Train: [65/100][123/156] Data 0.001 (0.001) Batch 2.490 (3.419) Remain 05:12:59 loss: 0.2257 Lr: 0.01512
[2023-08-08 05:05:59,701 INFO misc.py line 115 22900] Train: [65/100][124/156] Data 0.001 (0.001) Batch 3.399 (3.419) Remain 05:12:55 loss: 0.3528 Lr: 0.01512
[2023-08-08 05:06:02,831 INFO misc.py line 115 22900] Train: [65/100][125/156] Data 0.001 (0.001) Batch 3.130 (3.416) Remain 05:12:38 loss: 0.1818 Lr: 0.01511
[2023-08-08 05:06:05,708 INFO misc.py line 115 22900] Train: [65/100][126/156] Data 0.001 (0.001) Batch 2.877 (3.412) Remain 05:12:11 loss: 0.1139 Lr: 0.01511
[2023-08-08 05:06:08,659 INFO misc.py line 115 22900] Train: [65/100][127/156] Data 0.001 (0.001) Batch 2.951 (3.408) Remain 05:11:47 loss: 0.1160 Lr: 0.01510
[2023-08-08 05:06:12,101 INFO misc.py line 115 22900] Train: [65/100][128/156] Data 0.001 (0.001) Batch 3.442 (3.408) Remain 05:11:45 loss: 0.2272 Lr: 0.01510
[2023-08-08 05:06:15,259 INFO misc.py line 115 22900] Train: [65/100][129/156] Data 0.001 (0.001) Batch 3.157 (3.406) Remain 05:11:31 loss: 0.1613 Lr: 0.01509
[2023-08-08 05:06:18,305 INFO misc.py line 115 22900] Train: [65/100][130/156] Data 0.001 (0.001) Batch 3.046 (3.404) Remain 05:11:12 loss: 0.1676 Lr: 0.01509
[2023-08-08 05:06:21,794 INFO misc.py line 115 22900] Train: [65/100][131/156] Data 0.001 (0.001) Batch 3.490 (3.404) Remain 05:11:12 loss: 0.1340 Lr: 0.01508
[2023-08-08 05:06:25,739 INFO misc.py line 115 22900] Train: [65/100][132/156] Data 0.001 (0.001) Batch 3.945 (3.408) Remain 05:11:31 loss: 0.1101 Lr: 0.01508
[2023-08-08 05:06:29,379 INFO misc.py line 115 22900] Train: [65/100][133/156] Data 0.001 (0.001) Batch 3.640 (3.410) Remain 05:11:38 loss: 0.0799 Lr: 0.01507
[2023-08-08 05:06:33,318 INFO misc.py line 115 22900] Train: [65/100][134/156] Data 0.001 (0.001) Batch 3.938 (3.414) Remain 05:11:57 loss: 0.1054 Lr: 0.01507
[2023-08-08 05:06:36,361 INFO misc.py line 115 22900] Train: [65/100][135/156] Data 0.001 (0.001) Batch 3.043 (3.411) Remain 05:11:38 loss: 0.2377 Lr: 0.01506
[2023-08-08 05:06:39,777 INFO misc.py line 115 22900] Train: [65/100][136/156] Data 0.001 (0.001) Batch 3.417 (3.411) Remain 05:11:34 loss: 0.1187 Lr: 0.01506
[2023-08-08 05:06:43,469 INFO misc.py line 115 22900] Train: [65/100][137/156] Data 0.001 (0.001) Batch 3.692 (3.414) Remain 05:11:43 loss: 0.2572 Lr: 0.01506
[2023-08-08 05:06:47,676 INFO misc.py line 115 22900] Train: [65/100][138/156] Data 0.001 (0.001) Batch 4.207 (3.419) Remain 05:12:11 loss: 0.3320 Lr: 0.01505
[2023-08-08 05:06:51,693 INFO misc.py line 115 22900] Train: [65/100][139/156] Data 0.001 (0.001) Batch 4.018 (3.424) Remain 05:12:32 loss: 0.2007 Lr: 0.01505
[2023-08-08 05:06:55,524 INFO misc.py line 115 22900] Train: [65/100][140/156] Data 0.001 (0.001) Batch 3.831 (3.427) Remain 05:12:45 loss: 0.1486 Lr: 0.01504
[2023-08-08 05:06:59,577 INFO misc.py line 115 22900] Train: [65/100][141/156] Data 0.001 (0.001) Batch 4.053 (3.431) Remain 05:13:06 loss: 0.1425 Lr: 0.01504
[2023-08-08 05:07:02,704 INFO misc.py line 115 22900] Train: [65/100][142/156] Data 0.001 (0.001) Batch 3.126 (3.429) Remain 05:12:51 loss: 0.1342 Lr: 0.01503
[2023-08-08 05:07:07,429 INFO misc.py line 115 22900] Train: [65/100][143/156] Data 0.001 (0.001) Batch 4.725 (3.438) Remain 05:13:38 loss: 0.2533 Lr: 0.01503
[2023-08-08 05:07:10,837 INFO misc.py line 115 22900] Train: [65/100][144/156] Data 0.001 (0.001) Batch 3.408 (3.438) Remain 05:13:33 loss: 0.2361 Lr: 0.01502
[2023-08-08 05:07:15,209 INFO misc.py line 115 22900] Train: [65/100][145/156] Data 0.001 (0.001) Batch 4.372 (3.445) Remain 05:14:06 loss: 0.2772 Lr: 0.01502
[2023-08-08 05:07:18,544 INFO misc.py line 115 22900] Train: [65/100][146/156] Data 0.001 (0.001) Batch 3.335 (3.444) Remain 05:13:58 loss: 0.2533 Lr: 0.01501
[2023-08-08 05:07:22,718 INFO misc.py line 115 22900] Train: [65/100][147/156] Data 0.001 (0.001) Batch 4.174 (3.449) Remain 05:14:23 loss: 0.2859 Lr: 0.01501
[2023-08-08 05:07:25,961 INFO misc.py line 115 22900] Train: [65/100][148/156] Data 0.001 (0.001) Batch 3.243 (3.448) Remain 05:14:11 loss: 0.1595 Lr: 0.01500
[2023-08-08 05:07:29,249 INFO misc.py line 115 22900] Train: [65/100][149/156] Data 0.001 (0.001) Batch 3.288 (3.447) Remain 05:14:02 loss: 0.1865 Lr: 0.01500
[2023-08-08 05:07:33,056 INFO misc.py line 115 22900] Train: [65/100][150/156] Data 0.001 (0.001) Batch 3.807 (3.449) Remain 05:14:12 loss: 0.2351 Lr: 0.01499
[2023-08-08 05:07:37,356 INFO misc.py line 115 22900] Train: [65/100][151/156] Data 0.001 (0.001) Batch 4.300 (3.455) Remain 05:14:40 loss: 0.2566 Lr: 0.01499
[2023-08-08 05:07:40,901 INFO misc.py line 115 22900] Train: [65/100][152/156] Data 0.001 (0.001) Batch 3.545 (3.455) Remain 05:14:40 loss: 0.3716 Lr: 0.01498
[2023-08-08 05:07:44,462 INFO misc.py line 115 22900] Train: [65/100][153/156] Data 0.001 (0.001) Batch 3.560 (3.456) Remain 05:14:40 loss: 0.1832 Lr: 0.01498
[2023-08-08 05:07:48,440 INFO misc.py line 115 22900] Train: [65/100][154/156] Data 0.001 (0.001) Batch 3.978 (3.460) Remain 05:14:56 loss: 0.2049 Lr: 0.01497
[2023-08-08 05:07:52,143 INFO misc.py line 115 22900] Train: [65/100][155/156] Data 0.001 (0.001) Batch 3.703 (3.461) Remain 05:15:01 loss: 0.1974 Lr: 0.01497
[2023-08-08 05:07:56,185 INFO misc.py line 115 22900] Train: [65/100][156/156] Data 0.001 (0.001) Batch 4.042 (3.465) Remain 05:15:18 loss: 0.1378 Lr: 0.01496
[2023-08-08 05:07:56,186 INFO misc.py line 129 22900] Train result: loss: 0.1968 
[2023-08-08 05:07:56,186 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 05:07:58,282 INFO evaluator.py line 122 22900] Test: [1/24] Loss 1.9155 
[2023-08-08 05:07:59,151 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.6031 
[2023-08-08 05:08:00,815 INFO evaluator.py line 122 22900] Test: [3/24] Loss 1.2984 
[2023-08-08 05:08:02,336 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.3744 
[2023-08-08 05:08:04,180 INFO evaluator.py line 122 22900] Test: [5/24] Loss 2.1764 
[2023-08-08 05:08:05,846 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.5369 
[2023-08-08 05:08:07,984 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.6634 
[2023-08-08 05:08:09,791 INFO evaluator.py line 122 22900] Test: [8/24] Loss 1.9948 
[2023-08-08 05:08:11,074 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.4343 
[2023-08-08 05:08:13,205 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.4299 
[2023-08-08 05:08:13,731 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.4058 
[2023-08-08 05:08:15,266 INFO evaluator.py line 122 22900] Test: [12/24] Loss 1.1776 
[2023-08-08 05:08:17,981 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.3046 
[2023-08-08 05:08:19,662 INFO evaluator.py line 122 22900] Test: [14/24] Loss 1.4670 
[2023-08-08 05:08:21,683 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.3404 
[2023-08-08 05:08:24,392 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.2056 
[2023-08-08 05:08:27,098 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.6596 
[2023-08-08 05:08:28,946 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.6153 
[2023-08-08 05:08:29,695 INFO evaluator.py line 122 22900] Test: [19/24] Loss 0.9422 
[2023-08-08 05:08:30,580 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.8046 
[2023-08-08 05:08:32,841 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.4895 
[2023-08-08 05:08:34,805 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.9473 
[2023-08-08 05:08:36,651 INFO evaluator.py line 122 22900] Test: [23/24] Loss 3.7887 
[2023-08-08 05:08:38,585 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.7083 
[2023-08-08 05:08:38,634 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2503/0.3393/0.6907.
[2023-08-08 05:08:38,634 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6478/0.9633
[2023-08-08 05:08:38,634 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9401/0.9858
[2023-08-08 05:08:38,634 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1654/0.3113
[2023-08-08 05:08:38,634 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1551/0.2075
[2023-08-08 05:08:38,634 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6253/0.7106
[2023-08-08 05:08:38,634 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.2947/0.3928
[2023-08-08 05:08:38,634 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.4294/0.8447
[2023-08-08 05:08:38,634 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1298/0.1414
[2023-08-08 05:08:38,634 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1997/0.4148
[2023-08-08 05:08:38,634 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0163/0.0163
[2023-08-08 05:08:38,634 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 05:08:38,634 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0197/0.0201
[2023-08-08 05:08:38,634 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1432/0.1812
[2023-08-08 05:08:38,634 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.1046/0.1167
[2023-08-08 05:08:38,634 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0615/0.0705
[2023-08-08 05:08:38,634 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.3690/0.4504
[2023-08-08 05:08:38,634 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.2843/0.3900
[2023-08-08 05:08:38,634 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.3280/0.4029
[2023-08-08 05:08:38,634 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 05:08:38,634 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0924/0.1656
[2023-08-08 05:08:38,634 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 05:08:38,635 INFO misc.py line 152 22900] Currently Best mIoU: 0.2650
[2023-08-08 05:08:38,635 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 05:08:44,275 INFO misc.py line 115 22900] Train: [66/100][1/156] Data 1.111 (1.111) Batch 4.792 (4.792) Remain 07:16:00 loss: 0.2544 Lr: 0.01496
[2023-08-08 05:08:48,327 INFO misc.py line 115 22900] Train: [66/100][2/156] Data 0.001 (0.001) Batch 4.052 (4.052) Remain 06:08:37 loss: 0.2630 Lr: 0.01495
[2023-08-08 05:08:51,411 INFO misc.py line 115 22900] Train: [66/100][3/156] Data 0.001 (0.001) Batch 3.084 (3.084) Remain 04:40:29 loss: 0.0945 Lr: 0.01495
[2023-08-08 05:08:55,065 INFO misc.py line 115 22900] Train: [66/100][4/156] Data 0.001 (0.001) Batch 3.654 (3.654) Remain 05:32:18 loss: 0.1699 Lr: 0.01494
[2023-08-08 05:08:58,112 INFO misc.py line 115 22900] Train: [66/100][5/156] Data 0.001 (0.001) Batch 3.047 (3.351) Remain 05:04:38 loss: 0.1021 Lr: 0.01494
[2023-08-08 05:09:00,739 INFO misc.py line 115 22900] Train: [66/100][6/156] Data 0.001 (0.001) Batch 2.627 (3.109) Remain 04:42:39 loss: 0.1227 Lr: 0.01493
[2023-08-08 05:09:04,817 INFO misc.py line 115 22900] Train: [66/100][7/156] Data 0.001 (0.001) Batch 4.078 (3.352) Remain 05:04:35 loss: 0.1851 Lr: 0.01493
[2023-08-08 05:09:07,664 INFO misc.py line 115 22900] Train: [66/100][8/156] Data 0.001 (0.001) Batch 2.847 (3.251) Remain 04:55:22 loss: 0.0884 Lr: 0.01492
[2023-08-08 05:09:11,651 INFO misc.py line 115 22900] Train: [66/100][9/156] Data 0.001 (0.001) Batch 3.988 (3.373) Remain 05:06:28 loss: 0.2954 Lr: 0.01492
[2023-08-08 05:09:14,825 INFO misc.py line 115 22900] Train: [66/100][10/156] Data 0.001 (0.001) Batch 3.174 (3.345) Remain 05:03:49 loss: 0.1898 Lr: 0.01491
[2023-08-08 05:09:18,598 INFO misc.py line 115 22900] Train: [66/100][11/156] Data 0.001 (0.001) Batch 3.772 (3.398) Remain 05:08:37 loss: 0.1257 Lr: 0.01491
[2023-08-08 05:09:22,718 INFO misc.py line 115 22900] Train: [66/100][12/156] Data 0.001 (0.001) Batch 4.120 (3.479) Remain 05:15:50 loss: 0.2031 Lr: 0.01490
[2023-08-08 05:09:26,382 INFO misc.py line 115 22900] Train: [66/100][13/156] Data 0.001 (0.001) Batch 3.665 (3.497) Remain 05:17:28 loss: 0.0637 Lr: 0.01490
[2023-08-08 05:09:30,375 INFO misc.py line 115 22900] Train: [66/100][14/156] Data 0.001 (0.001) Batch 3.993 (3.542) Remain 05:21:30 loss: 0.2321 Lr: 0.01489
[2023-08-08 05:09:34,590 INFO misc.py line 115 22900] Train: [66/100][15/156] Data 0.001 (0.001) Batch 4.215 (3.598) Remain 05:26:32 loss: 0.2170 Lr: 0.01489
[2023-08-08 05:09:38,164 INFO misc.py line 115 22900] Train: [66/100][16/156] Data 0.001 (0.001) Batch 3.574 (3.596) Remain 05:26:18 loss: 0.2602 Lr: 0.01489
[2023-08-08 05:09:41,523 INFO misc.py line 115 22900] Train: [66/100][17/156] Data 0.001 (0.001) Batch 3.359 (3.579) Remain 05:24:43 loss: 0.0703 Lr: 0.01488
[2023-08-08 05:09:45,087 INFO misc.py line 115 22900] Train: [66/100][18/156] Data 0.001 (0.001) Batch 3.564 (3.578) Remain 05:24:33 loss: 0.1571 Lr: 0.01488
[2023-08-08 05:09:48,623 INFO misc.py line 115 22900] Train: [66/100][19/156] Data 0.001 (0.001) Batch 3.535 (3.576) Remain 05:24:15 loss: 0.2625 Lr: 0.01487
[2023-08-08 05:09:52,718 INFO misc.py line 115 22900] Train: [66/100][20/156] Data 0.001 (0.001) Batch 4.095 (3.606) Remain 05:26:58 loss: 0.1946 Lr: 0.01487
[2023-08-08 05:09:56,464 INFO misc.py line 115 22900] Train: [66/100][21/156] Data 0.001 (0.001) Batch 3.747 (3.614) Remain 05:27:36 loss: 0.1799 Lr: 0.01486
[2023-08-08 05:09:59,062 INFO misc.py line 115 22900] Train: [66/100][22/156] Data 0.001 (0.001) Batch 2.598 (3.561) Remain 05:22:42 loss: 0.1073 Lr: 0.01486
[2023-08-08 05:10:01,957 INFO misc.py line 115 22900] Train: [66/100][23/156] Data 0.001 (0.001) Batch 2.894 (3.527) Remain 05:19:37 loss: 0.0753 Lr: 0.01485
[2023-08-08 05:10:05,697 INFO misc.py line 115 22900] Train: [66/100][24/156] Data 0.001 (0.001) Batch 3.741 (3.537) Remain 05:20:29 loss: 0.0801 Lr: 0.01485
[2023-08-08 05:10:09,313 INFO misc.py line 115 22900] Train: [66/100][25/156] Data 0.001 (0.001) Batch 3.616 (3.541) Remain 05:20:45 loss: 0.1315 Lr: 0.01484
[2023-08-08 05:10:11,652 INFO misc.py line 115 22900] Train: [66/100][26/156] Data 0.001 (0.001) Batch 2.339 (3.489) Remain 05:15:57 loss: 0.0527 Lr: 0.01484
[2023-08-08 05:10:14,528 INFO misc.py line 115 22900] Train: [66/100][27/156] Data 0.001 (0.001) Batch 2.876 (3.463) Remain 05:13:35 loss: 0.1924 Lr: 0.01483
[2023-08-08 05:10:18,541 INFO misc.py line 115 22900] Train: [66/100][28/156] Data 0.001 (0.001) Batch 4.013 (3.485) Remain 05:15:31 loss: 0.2285 Lr: 0.01483
[2023-08-08 05:10:21,766 INFO misc.py line 115 22900] Train: [66/100][29/156] Data 0.001 (0.001) Batch 3.225 (3.475) Remain 05:14:33 loss: 0.0963 Lr: 0.01482
[2023-08-08 05:10:24,663 INFO misc.py line 115 22900] Train: [66/100][30/156] Data 0.001 (0.001) Batch 2.897 (3.454) Remain 05:12:33 loss: 0.1916 Lr: 0.01482
[2023-08-08 05:10:27,433 INFO misc.py line 115 22900] Train: [66/100][31/156] Data 0.001 (0.001) Batch 2.771 (3.429) Remain 05:10:18 loss: 0.2616 Lr: 0.01481
[2023-08-08 05:10:30,834 INFO misc.py line 115 22900] Train: [66/100][32/156] Data 0.001 (0.001) Batch 3.401 (3.428) Remain 05:10:09 loss: 0.2542 Lr: 0.01481
[2023-08-08 05:10:34,334 INFO misc.py line 115 22900] Train: [66/100][33/156] Data 0.001 (0.001) Batch 3.500 (3.431) Remain 05:10:18 loss: 0.2327 Lr: 0.01480
[2023-08-08 05:10:37,450 INFO misc.py line 115 22900] Train: [66/100][34/156] Data 0.001 (0.001) Batch 3.115 (3.421) Remain 05:09:20 loss: 0.1347 Lr: 0.01480
[2023-08-08 05:10:40,683 INFO misc.py line 115 22900] Train: [66/100][35/156] Data 0.001 (0.001) Batch 3.233 (3.415) Remain 05:08:45 loss: 0.1042 Lr: 0.01479
[2023-08-08 05:10:43,471 INFO misc.py line 115 22900] Train: [66/100][36/156] Data 0.001 (0.001) Batch 2.788 (3.396) Remain 05:06:58 loss: 0.1183 Lr: 0.01479
[2023-08-08 05:10:47,054 INFO misc.py line 115 22900] Train: [66/100][37/156] Data 0.001 (0.001) Batch 3.583 (3.401) Remain 05:07:24 loss: 0.2065 Lr: 0.01478
[2023-08-08 05:10:50,157 INFO misc.py line 115 22900] Train: [66/100][38/156] Data 0.001 (0.001) Batch 3.103 (3.393) Remain 05:06:35 loss: 0.2803 Lr: 0.01478
[2023-08-08 05:10:53,016 INFO misc.py line 115 22900] Train: [66/100][39/156] Data 0.001 (0.001) Batch 2.860 (3.378) Remain 05:05:11 loss: 0.1799 Lr: 0.01477
[2023-08-08 05:10:56,369 INFO misc.py line 115 22900] Train: [66/100][40/156] Data 0.001 (0.001) Batch 3.352 (3.377) Remain 05:05:04 loss: 0.1021 Lr: 0.01477
[2023-08-08 05:11:00,010 INFO misc.py line 115 22900] Train: [66/100][41/156] Data 0.001 (0.001) Batch 3.641 (3.384) Remain 05:05:38 loss: 0.1884 Lr: 0.01476
[2023-08-08 05:11:03,685 INFO misc.py line 115 22900] Train: [66/100][42/156] Data 0.001 (0.001) Batch 3.676 (3.392) Remain 05:06:15 loss: 0.1374 Lr: 0.01476
[2023-08-08 05:11:07,394 INFO misc.py line 115 22900] Train: [66/100][43/156] Data 0.001 (0.001) Batch 3.709 (3.400) Remain 05:06:55 loss: 0.2894 Lr: 0.01475
[2023-08-08 05:11:10,932 INFO misc.py line 115 22900] Train: [66/100][44/156] Data 0.001 (0.001) Batch 3.538 (3.403) Remain 05:07:10 loss: 0.1226 Lr: 0.01475
[2023-08-08 05:11:14,698 INFO misc.py line 115 22900] Train: [66/100][45/156] Data 0.001 (0.001) Batch 3.767 (3.412) Remain 05:07:53 loss: 0.2847 Lr: 0.01474
[2023-08-08 05:11:18,751 INFO misc.py line 115 22900] Train: [66/100][46/156] Data 0.001 (0.001) Batch 4.052 (3.427) Remain 05:09:11 loss: 0.2732 Lr: 0.01474
[2023-08-08 05:11:22,091 INFO misc.py line 115 22900] Train: [66/100][47/156] Data 0.001 (0.001) Batch 3.340 (3.425) Remain 05:08:57 loss: 0.1343 Lr: 0.01474
[2023-08-08 05:11:25,454 INFO misc.py line 115 22900] Train: [66/100][48/156] Data 0.001 (0.001) Batch 3.362 (3.423) Remain 05:08:46 loss: 0.1497 Lr: 0.01473
[2023-08-08 05:11:29,398 INFO misc.py line 115 22900] Train: [66/100][49/156] Data 0.001 (0.001) Batch 3.945 (3.435) Remain 05:09:44 loss: 0.2066 Lr: 0.01473
[2023-08-08 05:11:33,396 INFO misc.py line 115 22900] Train: [66/100][50/156] Data 0.001 (0.001) Batch 3.998 (3.446) Remain 05:10:45 loss: 0.3664 Lr: 0.01472
[2023-08-08 05:11:37,443 INFO misc.py line 115 22900] Train: [66/100][51/156] Data 0.001 (0.001) Batch 4.047 (3.459) Remain 05:11:49 loss: 0.5456 Lr: 0.01472
[2023-08-08 05:11:41,103 INFO misc.py line 115 22900] Train: [66/100][52/156] Data 0.001 (0.001) Batch 3.661 (3.463) Remain 05:12:08 loss: 0.2523 Lr: 0.01471
[2023-08-08 05:11:44,002 INFO misc.py line 115 22900] Train: [66/100][53/156] Data 0.001 (0.001) Batch 2.899 (3.452) Remain 05:11:04 loss: 0.2477 Lr: 0.01471
[2023-08-08 05:11:46,575 INFO misc.py line 115 22900] Train: [66/100][54/156] Data 0.001 (0.001) Batch 2.572 (3.435) Remain 05:09:27 loss: 0.2732 Lr: 0.01470
[2023-08-08 05:11:49,259 INFO misc.py line 115 22900] Train: [66/100][55/156] Data 0.001 (0.001) Batch 2.685 (3.420) Remain 05:08:05 loss: 0.1009 Lr: 0.01470
[2023-08-08 05:11:52,479 INFO misc.py line 115 22900] Train: [66/100][56/156] Data 0.001 (0.001) Batch 3.220 (3.416) Remain 05:07:42 loss: 0.2240 Lr: 0.01469
[2023-08-08 05:11:56,079 INFO misc.py line 115 22900] Train: [66/100][57/156] Data 0.001 (0.001) Batch 3.600 (3.420) Remain 05:07:57 loss: 0.2446 Lr: 0.01469
[2023-08-08 05:12:00,413 INFO misc.py line 115 22900] Train: [66/100][58/156] Data 0.001 (0.001) Batch 4.333 (3.436) Remain 05:09:23 loss: 0.2510 Lr: 0.01468
[2023-08-08 05:12:03,989 INFO misc.py line 115 22900] Train: [66/100][59/156] Data 0.001 (0.001) Batch 3.577 (3.439) Remain 05:09:33 loss: 0.2347 Lr: 0.01468
[2023-08-08 05:12:08,248 INFO misc.py line 115 22900] Train: [66/100][60/156] Data 0.001 (0.001) Batch 4.259 (3.453) Remain 05:10:47 loss: 0.2907 Lr: 0.01467
[2023-08-08 05:12:12,250 INFO misc.py line 115 22900] Train: [66/100][61/156] Data 0.001 (0.001) Batch 4.002 (3.463) Remain 05:11:35 loss: 0.2087 Lr: 0.01467
[2023-08-08 05:12:16,217 INFO misc.py line 115 22900] Train: [66/100][62/156] Data 0.001 (0.001) Batch 3.967 (3.471) Remain 05:12:18 loss: 0.2766 Lr: 0.01466
[2023-08-08 05:12:19,829 INFO misc.py line 115 22900] Train: [66/100][63/156] Data 0.001 (0.001) Batch 3.612 (3.474) Remain 05:12:27 loss: 0.3083 Lr: 0.01466
[2023-08-08 05:12:23,189 INFO misc.py line 115 22900] Train: [66/100][64/156] Data 0.001 (0.001) Batch 3.360 (3.472) Remain 05:12:13 loss: 0.2410 Lr: 0.01465
[2023-08-08 05:12:27,397 INFO misc.py line 115 22900] Train: [66/100][65/156] Data 0.001 (0.001) Batch 4.209 (3.484) Remain 05:13:14 loss: 0.3465 Lr: 0.01465
[2023-08-08 05:12:31,404 INFO misc.py line 115 22900] Train: [66/100][66/156] Data 0.001 (0.001) Batch 4.007 (3.492) Remain 05:13:55 loss: 0.2050 Lr: 0.01464
[2023-08-08 05:12:35,507 INFO misc.py line 115 22900] Train: [66/100][67/156] Data 0.001 (0.001) Batch 4.102 (3.501) Remain 05:14:43 loss: 0.3300 Lr: 0.01464
[2023-08-08 05:12:38,784 INFO misc.py line 115 22900] Train: [66/100][68/156] Data 0.001 (0.001) Batch 3.277 (3.498) Remain 05:14:21 loss: 0.1025 Lr: 0.01463
[2023-08-08 05:12:42,378 INFO misc.py line 115 22900] Train: [66/100][69/156] Data 0.001 (0.001) Batch 3.594 (3.500) Remain 05:14:25 loss: 0.2725 Lr: 0.01463
[2023-08-08 05:12:44,913 INFO misc.py line 115 22900] Train: [66/100][70/156] Data 0.001 (0.001) Batch 2.534 (3.485) Remain 05:13:04 loss: 0.0615 Lr: 0.01462
[2023-08-08 05:12:48,273 INFO misc.py line 115 22900] Train: [66/100][71/156] Data 0.001 (0.001) Batch 3.361 (3.483) Remain 05:12:51 loss: 0.0914 Lr: 0.01462
[2023-08-08 05:12:52,355 INFO misc.py line 115 22900] Train: [66/100][72/156] Data 0.001 (0.001) Batch 4.082 (3.492) Remain 05:13:34 loss: 0.2852 Lr: 0.01461
[2023-08-08 05:12:56,174 INFO misc.py line 115 22900] Train: [66/100][73/156] Data 0.001 (0.001) Batch 3.819 (3.497) Remain 05:13:56 loss: 0.2868 Lr: 0.01461
[2023-08-08 05:13:00,160 INFO misc.py line 115 22900] Train: [66/100][74/156] Data 0.001 (0.001) Batch 3.986 (3.504) Remain 05:14:29 loss: 0.2171 Lr: 0.01460
[2023-08-08 05:13:04,229 INFO misc.py line 115 22900] Train: [66/100][75/156] Data 0.001 (0.001) Batch 4.069 (3.511) Remain 05:15:08 loss: 0.2663 Lr: 0.01460
[2023-08-08 05:13:08,340 INFO misc.py line 115 22900] Train: [66/100][76/156] Data 0.001 (0.001) Batch 4.111 (3.520) Remain 05:15:49 loss: 0.1246 Lr: 0.01460
[2023-08-08 05:13:12,438 INFO misc.py line 115 22900] Train: [66/100][77/156] Data 0.001 (0.001) Batch 4.097 (3.527) Remain 05:16:27 loss: 0.2288 Lr: 0.01459
[2023-08-08 05:13:15,097 INFO misc.py line 115 22900] Train: [66/100][78/156] Data 0.001 (0.001) Batch 2.659 (3.516) Remain 05:15:22 loss: 0.1327 Lr: 0.01459
[2023-08-08 05:13:19,239 INFO misc.py line 115 22900] Train: [66/100][79/156] Data 0.001 (0.001) Batch 4.142 (3.524) Remain 05:16:02 loss: 0.2198 Lr: 0.01458
[2023-08-08 05:13:22,951 INFO misc.py line 115 22900] Train: [66/100][80/156] Data 0.001 (0.001) Batch 3.711 (3.526) Remain 05:16:12 loss: 0.1365 Lr: 0.01458
[2023-08-08 05:13:26,158 INFO misc.py line 115 22900] Train: [66/100][81/156] Data 0.001 (0.001) Batch 3.207 (3.522) Remain 05:15:46 loss: 0.2332 Lr: 0.01457
[2023-08-08 05:13:28,193 INFO misc.py line 115 22900] Train: [66/100][82/156] Data 0.001 (0.001) Batch 2.035 (3.504) Remain 05:14:02 loss: 0.2026 Lr: 0.01457
[2023-08-08 05:13:30,852 INFO misc.py line 115 22900] Train: [66/100][83/156] Data 0.001 (0.001) Batch 2.659 (3.493) Remain 05:13:01 loss: 0.1051 Lr: 0.01456
[2023-08-08 05:13:35,023 INFO misc.py line 115 22900] Train: [66/100][84/156] Data 0.001 (0.001) Batch 4.171 (3.501) Remain 05:13:43 loss: 0.1339 Lr: 0.01456
[2023-08-08 05:13:39,388 INFO misc.py line 115 22900] Train: [66/100][85/156] Data 0.001 (0.001) Batch 4.365 (3.512) Remain 05:14:36 loss: 0.3833 Lr: 0.01455
[2023-08-08 05:13:42,978 INFO misc.py line 115 22900] Train: [66/100][86/156] Data 0.001 (0.001) Batch 3.590 (3.513) Remain 05:14:38 loss: 0.1488 Lr: 0.01455
[2023-08-08 05:13:46,476 INFO misc.py line 115 22900] Train: [66/100][87/156] Data 0.001 (0.001) Batch 3.498 (3.513) Remain 05:14:33 loss: 0.3740 Lr: 0.01454
[2023-08-08 05:13:50,054 INFO misc.py line 115 22900] Train: [66/100][88/156] Data 0.001 (0.001) Batch 3.578 (3.513) Remain 05:14:34 loss: 0.2977 Lr: 0.01454
[2023-08-08 05:13:52,367 INFO misc.py line 115 22900] Train: [66/100][89/156] Data 0.001 (0.001) Batch 2.313 (3.499) Remain 05:13:15 loss: 0.1044 Lr: 0.01453
[2023-08-08 05:13:56,422 INFO misc.py line 115 22900] Train: [66/100][90/156] Data 0.001 (0.001) Batch 4.055 (3.506) Remain 05:13:46 loss: 0.2877 Lr: 0.01453
[2023-08-08 05:13:59,500 INFO misc.py line 115 22900] Train: [66/100][91/156] Data 0.001 (0.001) Batch 3.078 (3.501) Remain 05:13:16 loss: 0.1388 Lr: 0.01452
[2023-08-08 05:14:02,577 INFO misc.py line 115 22900] Train: [66/100][92/156] Data 0.001 (0.001) Batch 3.077 (3.496) Remain 05:12:47 loss: 0.1859 Lr: 0.01452
[2023-08-08 05:14:05,766 INFO misc.py line 115 22900] Train: [66/100][93/156] Data 0.001 (0.001) Batch 3.188 (3.493) Remain 05:12:26 loss: 0.3059 Lr: 0.01451
[2023-08-08 05:14:08,920 INFO misc.py line 115 22900] Train: [66/100][94/156] Data 0.001 (0.001) Batch 3.154 (3.489) Remain 05:12:02 loss: 0.1698 Lr: 0.01451
[2023-08-08 05:14:11,275 INFO misc.py line 115 22900] Train: [66/100][95/156] Data 0.001 (0.001) Batch 2.355 (3.477) Remain 05:10:52 loss: 0.1465 Lr: 0.01450
[2023-08-08 05:14:15,175 INFO misc.py line 115 22900] Train: [66/100][96/156] Data 0.001 (0.001) Batch 3.900 (3.481) Remain 05:11:13 loss: 0.2803 Lr: 0.01450
[2023-08-08 05:14:18,147 INFO misc.py line 115 22900] Train: [66/100][97/156] Data 0.001 (0.001) Batch 2.972 (3.476) Remain 05:10:41 loss: 0.1724 Lr: 0.01449
[2023-08-08 05:14:22,698 INFO misc.py line 115 22900] Train: [66/100][98/156] Data 0.001 (0.001) Batch 4.551 (3.487) Remain 05:11:38 loss: 0.2042 Lr: 0.01449
[2023-08-08 05:14:25,826 INFO misc.py line 115 22900] Train: [66/100][99/156] Data 0.001 (0.001) Batch 3.128 (3.483) Remain 05:11:14 loss: 0.1943 Lr: 0.01448
[2023-08-08 05:14:29,133 INFO misc.py line 115 22900] Train: [66/100][100/156] Data 0.001 (0.001) Batch 3.307 (3.482) Remain 05:11:01 loss: 0.1590 Lr: 0.01448
[2023-08-08 05:14:33,559 INFO misc.py line 115 22900] Train: [66/100][101/156] Data 0.001 (0.001) Batch 4.426 (3.491) Remain 05:11:49 loss: 0.2160 Lr: 0.01447
[2023-08-08 05:14:36,795 INFO misc.py line 115 22900] Train: [66/100][102/156] Data 0.001 (0.001) Batch 3.235 (3.489) Remain 05:11:32 loss: 0.2693 Lr: 0.01447
[2023-08-08 05:14:39,726 INFO misc.py line 115 22900] Train: [66/100][103/156] Data 0.001 (0.001) Batch 2.932 (3.483) Remain 05:10:59 loss: 0.1255 Lr: 0.01447
[2023-08-08 05:14:42,233 INFO misc.py line 115 22900] Train: [66/100][104/156] Data 0.001 (0.001) Batch 2.507 (3.473) Remain 05:10:03 loss: 0.1335 Lr: 0.01446
[2023-08-08 05:14:45,552 INFO misc.py line 115 22900] Train: [66/100][105/156] Data 0.001 (0.001) Batch 3.318 (3.472) Remain 05:09:52 loss: 0.1684 Lr: 0.01446
[2023-08-08 05:14:49,548 INFO misc.py line 115 22900] Train: [66/100][106/156] Data 0.001 (0.001) Batch 3.996 (3.477) Remain 05:10:16 loss: 0.2050 Lr: 0.01445
[2023-08-08 05:14:52,824 INFO misc.py line 115 22900] Train: [66/100][107/156] Data 0.001 (0.001) Batch 3.276 (3.475) Remain 05:10:02 loss: 0.1305 Lr: 0.01445
[2023-08-08 05:14:55,765 INFO misc.py line 115 22900] Train: [66/100][108/156] Data 0.001 (0.001) Batch 2.941 (3.470) Remain 05:09:31 loss: 0.1892 Lr: 0.01444
[2023-08-08 05:14:59,876 INFO misc.py line 115 22900] Train: [66/100][109/156] Data 0.001 (0.001) Batch 4.111 (3.476) Remain 05:10:00 loss: 0.1919 Lr: 0.01444
[2023-08-08 05:15:03,114 INFO misc.py line 115 22900] Train: [66/100][110/156] Data 0.001 (0.001) Batch 3.238 (3.474) Remain 05:09:45 loss: 0.1382 Lr: 0.01443
[2023-08-08 05:15:07,312 INFO misc.py line 115 22900] Train: [66/100][111/156] Data 0.001 (0.001) Batch 4.199 (3.481) Remain 05:10:17 loss: 0.0642 Lr: 0.01443
[2023-08-08 05:15:11,332 INFO misc.py line 115 22900] Train: [66/100][112/156] Data 0.001 (0.001) Batch 4.020 (3.486) Remain 05:10:40 loss: 0.4931 Lr: 0.01442
[2023-08-08 05:15:14,893 INFO misc.py line 115 22900] Train: [66/100][113/156] Data 0.001 (0.001) Batch 3.560 (3.486) Remain 05:10:40 loss: 0.3813 Lr: 0.01442
[2023-08-08 05:15:18,863 INFO misc.py line 115 22900] Train: [66/100][114/156] Data 0.001 (0.001) Batch 3.971 (3.491) Remain 05:11:00 loss: 0.2003 Lr: 0.01441
[2023-08-08 05:15:22,529 INFO misc.py line 115 22900] Train: [66/100][115/156] Data 0.001 (0.001) Batch 3.666 (3.492) Remain 05:11:05 loss: 0.1149 Lr: 0.01441
[2023-08-08 05:15:25,806 INFO misc.py line 115 22900] Train: [66/100][116/156] Data 0.001 (0.001) Batch 3.277 (3.490) Remain 05:10:51 loss: 0.1977 Lr: 0.01440
[2023-08-08 05:15:29,601 INFO misc.py line 115 22900] Train: [66/100][117/156] Data 0.001 (0.001) Batch 3.796 (3.493) Remain 05:11:02 loss: 0.1364 Lr: 0.01440
[2023-08-08 05:15:32,307 INFO misc.py line 115 22900] Train: [66/100][118/156] Data 0.001 (0.001) Batch 2.706 (3.486) Remain 05:10:22 loss: 0.2414 Lr: 0.01439
[2023-08-08 05:15:36,047 INFO misc.py line 115 22900] Train: [66/100][119/156] Data 0.001 (0.001) Batch 3.740 (3.488) Remain 05:10:30 loss: 0.1524 Lr: 0.01439
[2023-08-08 05:15:40,185 INFO misc.py line 115 22900] Train: [66/100][120/156] Data 0.001 (0.001) Batch 4.138 (3.494) Remain 05:10:56 loss: 0.2735 Lr: 0.01438
[2023-08-08 05:15:43,896 INFO misc.py line 115 22900] Train: [66/100][121/156] Data 0.001 (0.001) Batch 3.712 (3.496) Remain 05:11:03 loss: 0.2096 Lr: 0.01438
[2023-08-08 05:15:47,959 INFO misc.py line 115 22900] Train: [66/100][122/156] Data 0.001 (0.001) Batch 4.062 (3.500) Remain 05:11:25 loss: 0.1489 Lr: 0.01437
[2023-08-08 05:15:51,197 INFO misc.py line 115 22900] Train: [66/100][123/156] Data 0.001 (0.001) Batch 3.238 (3.498) Remain 05:11:09 loss: 0.1487 Lr: 0.01437
[2023-08-08 05:15:53,913 INFO misc.py line 115 22900] Train: [66/100][124/156] Data 0.001 (0.001) Batch 2.717 (3.492) Remain 05:10:31 loss: 0.1143 Lr: 0.01436
[2023-08-08 05:15:57,171 INFO misc.py line 115 22900] Train: [66/100][125/156] Data 0.001 (0.001) Batch 3.258 (3.490) Remain 05:10:18 loss: 0.2275 Lr: 0.01436
[2023-08-08 05:16:00,235 INFO misc.py line 115 22900] Train: [66/100][126/156] Data 0.001 (0.001) Batch 3.064 (3.486) Remain 05:09:56 loss: 0.2204 Lr: 0.01435
[2023-08-08 05:16:02,865 INFO misc.py line 115 22900] Train: [66/100][127/156] Data 0.001 (0.001) Batch 2.630 (3.479) Remain 05:09:16 loss: 0.2144 Lr: 0.01435
[2023-08-08 05:16:05,831 INFO misc.py line 115 22900] Train: [66/100][128/156] Data 0.001 (0.001) Batch 2.966 (3.475) Remain 05:08:50 loss: 0.2176 Lr: 0.01435
[2023-08-08 05:16:09,672 INFO misc.py line 115 22900] Train: [66/100][129/156] Data 0.001 (0.001) Batch 3.840 (3.478) Remain 05:09:02 loss: 0.3200 Lr: 0.01434
[2023-08-08 05:16:13,622 INFO misc.py line 115 22900] Train: [66/100][130/156] Data 0.001 (0.001) Batch 3.950 (3.482) Remain 05:09:18 loss: 0.2393 Lr: 0.01434
[2023-08-08 05:16:16,868 INFO misc.py line 115 22900] Train: [66/100][131/156] Data 0.001 (0.001) Batch 3.246 (3.480) Remain 05:09:05 loss: 0.2465 Lr: 0.01433
[2023-08-08 05:16:20,447 INFO misc.py line 115 22900] Train: [66/100][132/156] Data 0.001 (0.001) Batch 3.579 (3.481) Remain 05:09:06 loss: 0.2147 Lr: 0.01433
[2023-08-08 05:16:24,205 INFO misc.py line 115 22900] Train: [66/100][133/156] Data 0.001 (0.001) Batch 3.758 (3.483) Remain 05:09:14 loss: 0.1104 Lr: 0.01432
[2023-08-08 05:16:27,503 INFO misc.py line 115 22900] Train: [66/100][134/156] Data 0.010 (0.001) Batch 3.298 (3.482) Remain 05:09:03 loss: 0.1636 Lr: 0.01432
[2023-08-08 05:16:30,431 INFO misc.py line 115 22900] Train: [66/100][135/156] Data 0.001 (0.001) Batch 2.928 (3.477) Remain 05:08:37 loss: 0.1250 Lr: 0.01431
[2023-08-08 05:16:33,866 INFO misc.py line 115 22900] Train: [66/100][136/156] Data 0.001 (0.001) Batch 3.435 (3.477) Remain 05:08:32 loss: 0.2155 Lr: 0.01431
[2023-08-08 05:16:37,386 INFO misc.py line 115 22900] Train: [66/100][137/156] Data 0.001 (0.001) Batch 3.520 (3.477) Remain 05:08:30 loss: 0.1534 Lr: 0.01430
[2023-08-08 05:16:41,013 INFO misc.py line 115 22900] Train: [66/100][138/156] Data 0.001 (0.001) Batch 3.627 (3.479) Remain 05:08:32 loss: 0.3065 Lr: 0.01430
[2023-08-08 05:16:44,130 INFO misc.py line 115 22900] Train: [66/100][139/156] Data 0.001 (0.001) Batch 3.117 (3.476) Remain 05:08:15 loss: 0.1772 Lr: 0.01429
[2023-08-08 05:16:48,085 INFO misc.py line 115 22900] Train: [66/100][140/156] Data 0.001 (0.001) Batch 3.955 (3.479) Remain 05:08:30 loss: 0.2588 Lr: 0.01429
[2023-08-08 05:16:52,541 INFO misc.py line 115 22900] Train: [66/100][141/156] Data 0.001 (0.001) Batch 4.456 (3.486) Remain 05:09:04 loss: 0.2684 Lr: 0.01428
[2023-08-08 05:16:55,815 INFO misc.py line 115 22900] Train: [66/100][142/156] Data 0.001 (0.001) Batch 3.274 (3.485) Remain 05:08:52 loss: 0.1474 Lr: 0.01428
[2023-08-08 05:17:00,131 INFO misc.py line 115 22900] Train: [66/100][143/156] Data 0.001 (0.001) Batch 4.316 (3.491) Remain 05:09:20 loss: 0.1869 Lr: 0.01427
[2023-08-08 05:17:04,063 INFO misc.py line 115 22900] Train: [66/100][144/156] Data 0.001 (0.001) Batch 3.931 (3.494) Remain 05:09:34 loss: 0.3336 Lr: 0.01427
[2023-08-08 05:17:07,606 INFO misc.py line 115 22900] Train: [66/100][145/156] Data 0.001 (0.001) Batch 3.543 (3.494) Remain 05:09:32 loss: 0.2378 Lr: 0.01426
[2023-08-08 05:17:11,515 INFO misc.py line 115 22900] Train: [66/100][146/156] Data 0.001 (0.001) Batch 3.909 (3.497) Remain 05:09:44 loss: 0.1695 Lr: 0.01426
[2023-08-08 05:17:13,594 INFO misc.py line 115 22900] Train: [66/100][147/156] Data 0.001 (0.001) Batch 2.079 (3.487) Remain 05:08:48 loss: 0.0978 Lr: 0.01425
[2023-08-08 05:17:16,794 INFO misc.py line 115 22900] Train: [66/100][148/156] Data 0.001 (0.001) Batch 3.199 (3.485) Remain 05:08:34 loss: 0.2328 Lr: 0.01425
[2023-08-08 05:17:20,387 INFO misc.py line 115 22900] Train: [66/100][149/156] Data 0.001 (0.001) Batch 3.593 (3.486) Remain 05:08:34 loss: 0.1248 Lr: 0.01424
[2023-08-08 05:17:22,509 INFO misc.py line 115 22900] Train: [66/100][150/156] Data 0.001 (0.001) Batch 2.123 (3.477) Remain 05:07:42 loss: 0.0564 Lr: 0.01424
[2023-08-08 05:17:25,826 INFO misc.py line 115 22900] Train: [66/100][151/156] Data 0.001 (0.001) Batch 3.317 (3.476) Remain 05:07:32 loss: 0.0580 Lr: 0.01424
[2023-08-08 05:17:29,455 INFO misc.py line 115 22900] Train: [66/100][152/156] Data 0.001 (0.001) Batch 3.629 (3.477) Remain 05:07:34 loss: 0.2016 Lr: 0.01423
[2023-08-08 05:17:31,191 INFO misc.py line 115 22900] Train: [66/100][153/156] Data 0.001 (0.001) Batch 1.736 (3.465) Remain 05:06:29 loss: 0.0345 Lr: 0.01423
[2023-08-08 05:17:34,224 INFO misc.py line 115 22900] Train: [66/100][154/156] Data 0.001 (0.001) Batch 3.033 (3.462) Remain 05:06:11 loss: 0.2310 Lr: 0.01422
[2023-08-08 05:17:36,479 INFO misc.py line 115 22900] Train: [66/100][155/156] Data 0.001 (0.001) Batch 2.255 (3.454) Remain 05:05:25 loss: 0.0484 Lr: 0.01422
[2023-08-08 05:17:40,539 INFO misc.py line 115 22900] Train: [66/100][156/156] Data 0.001 (0.001) Batch 4.060 (3.458) Remain 05:05:43 loss: 0.2627 Lr: 0.01421
[2023-08-08 05:17:40,539 INFO misc.py line 129 22900] Train result: loss: 0.1980 
[2023-08-08 05:17:40,539 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 05:17:42,631 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.3190 
[2023-08-08 05:17:43,501 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.5468 
[2023-08-08 05:17:45,165 INFO evaluator.py line 122 22900] Test: [3/24] Loss 1.0580 
[2023-08-08 05:17:46,688 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.2570 
[2023-08-08 05:17:48,534 INFO evaluator.py line 122 22900] Test: [5/24] Loss 2.0607 
[2023-08-08 05:17:50,196 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.4558 
[2023-08-08 05:17:52,338 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.5269 
[2023-08-08 05:17:54,144 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.3416 
[2023-08-08 05:17:55,428 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.4308 
[2023-08-08 05:17:57,557 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.5656 
[2023-08-08 05:17:58,083 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.0618 
[2023-08-08 05:17:59,618 INFO evaluator.py line 122 22900] Test: [12/24] Loss 1.0242 
[2023-08-08 05:18:02,328 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.0751 
[2023-08-08 05:18:04,009 INFO evaluator.py line 122 22900] Test: [14/24] Loss 1.0992 
[2023-08-08 05:18:06,031 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.2441 
[2023-08-08 05:18:08,739 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.0442 
[2023-08-08 05:18:11,445 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.3720 
[2023-08-08 05:18:13,293 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.6763 
[2023-08-08 05:18:14,040 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.2047 
[2023-08-08 05:18:14,925 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.8912 
[2023-08-08 05:18:17,185 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.4258 
[2023-08-08 05:18:19,145 INFO evaluator.py line 122 22900] Test: [22/24] Loss 2.1795 
[2023-08-08 05:18:20,994 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.9216 
[2023-08-08 05:18:22,930 INFO evaluator.py line 122 22900] Test: [24/24] Loss 2.1195 
[2023-08-08 05:18:22,976 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2340/0.3167/0.7008.
[2023-08-08 05:18:22,976 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6344/0.9709
[2023-08-08 05:18:22,976 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9488/0.9880
[2023-08-08 05:18:22,976 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1797/0.3361
[2023-08-08 05:18:22,976 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.2451/0.2826
[2023-08-08 05:18:22,976 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6340/0.8230
[2023-08-08 05:18:22,976 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.2729/0.3487
[2023-08-08 05:18:22,976 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5351/0.7596
[2023-08-08 05:18:22,976 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1428/0.1555
[2023-08-08 05:18:22,976 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1355/0.2816
[2023-08-08 05:18:22,976 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0477/0.0479
[2023-08-08 05:18:22,976 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 05:18:22,976 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0187/0.0192
[2023-08-08 05:18:22,976 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.2842/0.4245
[2023-08-08 05:18:22,976 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0258/0.0279
[2023-08-08 05:18:22,976 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0208/0.0262
[2023-08-08 05:18:22,976 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0000/0.0000
[2023-08-08 05:18:22,977 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0464/0.0631
[2023-08-08 05:18:22,977 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.3949/0.5833
[2023-08-08 05:18:22,977 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 05:18:22,977 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.1127/0.1959
[2023-08-08 05:18:22,977 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 05:18:22,977 INFO misc.py line 152 22900] Currently Best mIoU: 0.2650
[2023-08-08 05:18:22,977 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 05:18:29,655 INFO misc.py line 115 22900] Train: [67/100][1/156] Data 1.167 (1.167) Batch 5.874 (5.874) Remain 08:39:07 loss: 0.1594 Lr: 0.01421
[2023-08-08 05:18:31,671 INFO misc.py line 115 22900] Train: [67/100][2/156] Data 0.001 (0.001) Batch 2.015 (2.015) Remain 02:58:05 loss: 0.1297 Lr: 0.01420
[2023-08-08 05:18:34,237 INFO misc.py line 115 22900] Train: [67/100][3/156] Data 0.001 (0.001) Batch 2.566 (2.566) Remain 03:46:43 loss: 0.2032 Lr: 0.01420
[2023-08-08 05:18:36,890 INFO misc.py line 115 22900] Train: [67/100][4/156] Data 0.001 (0.001) Batch 2.653 (2.653) Remain 03:54:19 loss: 0.2056 Lr: 0.01419
[2023-08-08 05:18:40,685 INFO misc.py line 115 22900] Train: [67/100][5/156] Data 0.001 (0.001) Batch 3.795 (3.224) Remain 04:44:43 loss: 0.1888 Lr: 0.01419
[2023-08-08 05:18:44,459 INFO misc.py line 115 22900] Train: [67/100][6/156] Data 0.001 (0.001) Batch 3.775 (3.408) Remain 05:00:53 loss: 0.0820 Lr: 0.01418
[2023-08-08 05:18:48,890 INFO misc.py line 115 22900] Train: [67/100][7/156] Data 0.001 (0.001) Batch 4.430 (3.663) Remain 05:23:24 loss: 0.3049 Lr: 0.01418
[2023-08-08 05:18:53,374 INFO misc.py line 115 22900] Train: [67/100][8/156] Data 0.001 (0.001) Batch 4.484 (3.827) Remain 05:37:49 loss: 0.2492 Lr: 0.01417
[2023-08-08 05:18:57,286 INFO misc.py line 115 22900] Train: [67/100][9/156] Data 0.001 (0.001) Batch 3.913 (3.842) Remain 05:39:01 loss: 0.1171 Lr: 0.01417
[2023-08-08 05:19:00,511 INFO misc.py line 115 22900] Train: [67/100][10/156] Data 0.001 (0.001) Batch 3.225 (3.753) Remain 05:31:10 loss: 0.0866 Lr: 0.01416
[2023-08-08 05:19:03,023 INFO misc.py line 115 22900] Train: [67/100][11/156] Data 0.001 (0.001) Batch 2.512 (3.598) Remain 05:17:25 loss: 0.2791 Lr: 0.01416
[2023-08-08 05:19:06,553 INFO misc.py line 115 22900] Train: [67/100][12/156] Data 0.001 (0.001) Batch 3.529 (3.591) Remain 05:16:41 loss: 0.2466 Lr: 0.01415
[2023-08-08 05:19:09,926 INFO misc.py line 115 22900] Train: [67/100][13/156] Data 0.001 (0.001) Batch 3.374 (3.569) Remain 05:14:43 loss: 0.1775 Lr: 0.01415
[2023-08-08 05:19:13,921 INFO misc.py line 115 22900] Train: [67/100][14/156] Data 0.001 (0.001) Batch 3.995 (3.608) Remain 05:18:04 loss: 0.2067 Lr: 0.01414
[2023-08-08 05:19:15,944 INFO misc.py line 115 22900] Train: [67/100][15/156] Data 0.001 (0.001) Batch 2.023 (3.476) Remain 05:06:22 loss: 0.0769 Lr: 0.01414
[2023-08-08 05:19:19,953 INFO misc.py line 115 22900] Train: [67/100][16/156] Data 0.001 (0.001) Batch 4.009 (3.517) Remain 05:09:56 loss: 0.0784 Lr: 0.01413
[2023-08-08 05:19:24,405 INFO misc.py line 115 22900] Train: [67/100][17/156] Data 0.001 (0.001) Batch 4.452 (3.583) Remain 05:15:45 loss: 0.3015 Lr: 0.01413
[2023-08-08 05:19:27,141 INFO misc.py line 115 22900] Train: [67/100][18/156] Data 0.001 (0.001) Batch 2.736 (3.527) Remain 05:10:43 loss: 0.0724 Lr: 0.01413
[2023-08-08 05:19:29,621 INFO misc.py line 115 22900] Train: [67/100][19/156] Data 0.001 (0.001) Batch 2.479 (3.462) Remain 05:04:54 loss: 0.0525 Lr: 0.01412
[2023-08-08 05:19:33,603 INFO misc.py line 115 22900] Train: [67/100][20/156] Data 0.001 (0.001) Batch 3.982 (3.492) Remain 05:07:32 loss: 0.5083 Lr: 0.01412
[2023-08-08 05:19:37,453 INFO misc.py line 115 22900] Train: [67/100][21/156] Data 0.001 (0.001) Batch 3.850 (3.512) Remain 05:09:13 loss: 0.3307 Lr: 0.01411
[2023-08-08 05:19:41,041 INFO misc.py line 115 22900] Train: [67/100][22/156] Data 0.001 (0.001) Batch 3.588 (3.516) Remain 05:09:31 loss: 0.1718 Lr: 0.01411
[2023-08-08 05:19:44,552 INFO misc.py line 115 22900] Train: [67/100][23/156] Data 0.001 (0.001) Batch 3.511 (3.516) Remain 05:09:26 loss: 0.1311 Lr: 0.01410
[2023-08-08 05:19:48,543 INFO misc.py line 115 22900] Train: [67/100][24/156] Data 0.001 (0.001) Batch 3.991 (3.538) Remain 05:11:22 loss: 0.1281 Lr: 0.01410
[2023-08-08 05:19:51,805 INFO misc.py line 115 22900] Train: [67/100][25/156] Data 0.001 (0.001) Batch 3.263 (3.526) Remain 05:10:12 loss: 0.1567 Lr: 0.01409
[2023-08-08 05:19:55,500 INFO misc.py line 115 22900] Train: [67/100][26/156] Data 0.001 (0.001) Batch 3.694 (3.533) Remain 05:10:48 loss: 0.1694 Lr: 0.01409
[2023-08-08 05:19:59,173 INFO misc.py line 115 22900] Train: [67/100][27/156] Data 0.001 (0.001) Batch 3.673 (3.539) Remain 05:11:15 loss: 0.1476 Lr: 0.01408
[2023-08-08 05:20:03,561 INFO misc.py line 115 22900] Train: [67/100][28/156] Data 0.001 (0.001) Batch 4.388 (3.573) Remain 05:14:11 loss: 0.3415 Lr: 0.01408
[2023-08-08 05:20:07,543 INFO misc.py line 115 22900] Train: [67/100][29/156] Data 0.001 (0.001) Batch 3.982 (3.589) Remain 05:15:30 loss: 0.2085 Lr: 0.01407
[2023-08-08 05:20:12,124 INFO misc.py line 115 22900] Train: [67/100][30/156] Data 0.001 (0.001) Batch 4.581 (3.625) Remain 05:18:40 loss: 0.3393 Lr: 0.01407
[2023-08-08 05:20:14,841 INFO misc.py line 115 22900] Train: [67/100][31/156] Data 0.001 (0.001) Batch 2.717 (3.593) Remain 05:15:45 loss: 0.1223 Lr: 0.01406
[2023-08-08 05:20:18,243 INFO misc.py line 115 22900] Train: [67/100][32/156] Data 0.001 (0.001) Batch 3.402 (3.586) Remain 05:15:07 loss: 0.2805 Lr: 0.01406
[2023-08-08 05:20:21,001 INFO misc.py line 115 22900] Train: [67/100][33/156] Data 0.001 (0.001) Batch 2.759 (3.559) Remain 05:12:38 loss: 0.0524 Lr: 0.01405
[2023-08-08 05:20:24,667 INFO misc.py line 115 22900] Train: [67/100][34/156] Data 0.001 (0.001) Batch 3.666 (3.562) Remain 05:12:53 loss: 0.2969 Lr: 0.01405
[2023-08-08 05:20:27,257 INFO misc.py line 115 22900] Train: [67/100][35/156] Data 0.001 (0.001) Batch 2.590 (3.532) Remain 05:10:09 loss: 0.1665 Lr: 0.01404
[2023-08-08 05:20:30,126 INFO misc.py line 115 22900] Train: [67/100][36/156] Data 0.001 (0.001) Batch 2.868 (3.512) Remain 05:08:20 loss: 0.1214 Lr: 0.01404
[2023-08-08 05:20:33,593 INFO misc.py line 115 22900] Train: [67/100][37/156] Data 0.001 (0.001) Batch 3.467 (3.510) Remain 05:08:09 loss: 0.0974 Lr: 0.01403
[2023-08-08 05:20:36,397 INFO misc.py line 115 22900] Train: [67/100][38/156] Data 0.001 (0.001) Batch 2.804 (3.490) Remain 05:06:19 loss: 0.1019 Lr: 0.01403
[2023-08-08 05:20:39,750 INFO misc.py line 115 22900] Train: [67/100][39/156] Data 0.001 (0.001) Batch 3.354 (3.486) Remain 05:05:56 loss: 0.2702 Lr: 0.01403
[2023-08-08 05:20:43,215 INFO misc.py line 115 22900] Train: [67/100][40/156] Data 0.001 (0.001) Batch 3.464 (3.486) Remain 05:05:49 loss: 0.1618 Lr: 0.01402
[2023-08-08 05:20:46,944 INFO misc.py line 115 22900] Train: [67/100][41/156] Data 0.001 (0.001) Batch 3.729 (3.492) Remain 05:06:19 loss: 0.2137 Lr: 0.01402
[2023-08-08 05:20:50,101 INFO misc.py line 115 22900] Train: [67/100][42/156] Data 0.001 (0.001) Batch 3.157 (3.484) Remain 05:05:31 loss: 0.2307 Lr: 0.01401
[2023-08-08 05:20:53,244 INFO misc.py line 115 22900] Train: [67/100][43/156] Data 0.001 (0.001) Batch 3.142 (3.475) Remain 05:04:42 loss: 0.1112 Lr: 0.01401
[2023-08-08 05:20:56,589 INFO misc.py line 115 22900] Train: [67/100][44/156] Data 0.001 (0.001) Batch 3.345 (3.472) Remain 05:04:22 loss: 0.2051 Lr: 0.01400
[2023-08-08 05:20:59,862 INFO misc.py line 115 22900] Train: [67/100][45/156] Data 0.001 (0.001) Batch 3.273 (3.467) Remain 05:03:54 loss: 0.1332 Lr: 0.01400
[2023-08-08 05:21:03,916 INFO misc.py line 115 22900] Train: [67/100][46/156] Data 0.001 (0.001) Batch 4.054 (3.481) Remain 05:05:02 loss: 0.3238 Lr: 0.01399
[2023-08-08 05:21:07,506 INFO misc.py line 115 22900] Train: [67/100][47/156] Data 0.001 (0.001) Batch 3.591 (3.483) Remain 05:05:12 loss: 0.2595 Lr: 0.01399
[2023-08-08 05:21:11,518 INFO misc.py line 115 22900] Train: [67/100][48/156] Data 0.001 (0.001) Batch 4.012 (3.495) Remain 05:06:10 loss: 0.1678 Lr: 0.01398
[2023-08-08 05:21:15,288 INFO misc.py line 115 22900] Train: [67/100][49/156] Data 0.001 (0.001) Batch 3.770 (3.501) Remain 05:06:38 loss: 0.3708 Lr: 0.01398
[2023-08-08 05:21:18,146 INFO misc.py line 115 22900] Train: [67/100][50/156] Data 0.001 (0.001) Batch 2.858 (3.487) Remain 05:05:22 loss: 0.0936 Lr: 0.01397
[2023-08-08 05:21:20,213 INFO misc.py line 115 22900] Train: [67/100][51/156] Data 0.001 (0.001) Batch 2.067 (3.458) Remain 05:02:44 loss: 0.0688 Lr: 0.01397
[2023-08-08 05:21:23,566 INFO misc.py line 115 22900] Train: [67/100][52/156] Data 0.001 (0.001) Batch 3.353 (3.456) Remain 05:02:29 loss: 0.3133 Lr: 0.01396
[2023-08-08 05:21:27,824 INFO misc.py line 115 22900] Train: [67/100][53/156] Data 0.001 (0.001) Batch 4.258 (3.472) Remain 05:03:50 loss: 0.2243 Lr: 0.01396
[2023-08-08 05:21:31,868 INFO misc.py line 115 22900] Train: [67/100][54/156] Data 0.001 (0.001) Batch 4.044 (3.483) Remain 05:04:45 loss: 0.1220 Lr: 0.01395
[2023-08-08 05:21:35,220 INFO misc.py line 115 22900] Train: [67/100][55/156] Data 0.001 (0.001) Batch 3.353 (3.480) Remain 05:04:28 loss: 0.1294 Lr: 0.01395
[2023-08-08 05:21:37,558 INFO misc.py line 115 22900] Train: [67/100][56/156] Data 0.001 (0.001) Batch 2.337 (3.459) Remain 05:02:32 loss: 0.1187 Lr: 0.01394
[2023-08-08 05:21:41,002 INFO misc.py line 115 22900] Train: [67/100][57/156] Data 0.001 (0.001) Batch 3.444 (3.459) Remain 05:02:27 loss: 0.2764 Lr: 0.01394
[2023-08-08 05:21:43,826 INFO misc.py line 115 22900] Train: [67/100][58/156] Data 0.001 (0.001) Batch 2.824 (3.447) Remain 05:01:23 loss: 0.0851 Lr: 0.01393
[2023-08-08 05:21:46,941 INFO misc.py line 115 22900] Train: [67/100][59/156] Data 0.001 (0.001) Batch 3.115 (3.441) Remain 05:00:48 loss: 0.1533 Lr: 0.01393
[2023-08-08 05:21:50,474 INFO misc.py line 115 22900] Train: [67/100][60/156] Data 0.001 (0.001) Batch 3.533 (3.443) Remain 05:00:53 loss: 0.1814 Lr: 0.01393
[2023-08-08 05:21:54,414 INFO misc.py line 115 22900] Train: [67/100][61/156] Data 0.001 (0.001) Batch 3.941 (3.451) Remain 05:01:35 loss: 0.2332 Lr: 0.01392
[2023-08-08 05:21:57,794 INFO misc.py line 115 22900] Train: [67/100][62/156] Data 0.001 (0.001) Batch 3.380 (3.450) Remain 05:01:25 loss: 0.4009 Lr: 0.01392
[2023-08-08 05:22:01,076 INFO misc.py line 115 22900] Train: [67/100][63/156] Data 0.001 (0.001) Batch 3.282 (3.447) Remain 05:01:07 loss: 0.2083 Lr: 0.01391
[2023-08-08 05:22:05,106 INFO misc.py line 115 22900] Train: [67/100][64/156] Data 0.001 (0.001) Batch 4.030 (3.457) Remain 05:01:53 loss: 0.1484 Lr: 0.01391
[2023-08-08 05:22:07,937 INFO misc.py line 115 22900] Train: [67/100][65/156] Data 0.001 (0.001) Batch 2.831 (3.447) Remain 05:00:57 loss: 0.1494 Lr: 0.01390
[2023-08-08 05:22:11,592 INFO misc.py line 115 22900] Train: [67/100][66/156] Data 0.001 (0.001) Batch 3.655 (3.450) Remain 05:01:11 loss: 0.2407 Lr: 0.01390
[2023-08-08 05:22:15,670 INFO misc.py line 115 22900] Train: [67/100][67/156] Data 0.001 (0.001) Batch 4.077 (3.460) Remain 05:01:59 loss: 0.2463 Lr: 0.01389
[2023-08-08 05:22:19,598 INFO misc.py line 115 22900] Train: [67/100][68/156] Data 0.001 (0.001) Batch 3.928 (3.467) Remain 05:02:33 loss: 0.2051 Lr: 0.01389
[2023-08-08 05:22:23,157 INFO misc.py line 115 22900] Train: [67/100][69/156] Data 0.001 (0.001) Batch 3.560 (3.468) Remain 05:02:37 loss: 0.3702 Lr: 0.01388
[2023-08-08 05:22:27,655 INFO misc.py line 115 22900] Train: [67/100][70/156] Data 0.001 (0.001) Batch 4.498 (3.484) Remain 05:03:54 loss: 0.2452 Lr: 0.01388
[2023-08-08 05:22:30,876 INFO misc.py line 115 22900] Train: [67/100][71/156] Data 0.001 (0.001) Batch 3.221 (3.480) Remain 05:03:30 loss: 0.1137 Lr: 0.01387
[2023-08-08 05:22:34,417 INFO misc.py line 115 22900] Train: [67/100][72/156] Data 0.001 (0.001) Batch 3.540 (3.481) Remain 05:03:31 loss: 0.1764 Lr: 0.01387
[2023-08-08 05:22:37,895 INFO misc.py line 115 22900] Train: [67/100][73/156] Data 0.001 (0.001) Batch 3.478 (3.481) Remain 05:03:28 loss: 0.1320 Lr: 0.01386
[2023-08-08 05:22:40,407 INFO misc.py line 115 22900] Train: [67/100][74/156] Data 0.001 (0.001) Batch 2.512 (3.467) Remain 05:02:13 loss: 0.1041 Lr: 0.01386
[2023-08-08 05:22:43,040 INFO misc.py line 115 22900] Train: [67/100][75/156] Data 0.001 (0.001) Batch 2.633 (3.456) Remain 05:01:09 loss: 0.1494 Lr: 0.01385
[2023-08-08 05:22:46,166 INFO misc.py line 115 22900] Train: [67/100][76/156] Data 0.001 (0.001) Batch 3.126 (3.451) Remain 05:00:42 loss: 0.1151 Lr: 0.01385
[2023-08-08 05:22:50,168 INFO misc.py line 115 22900] Train: [67/100][77/156] Data 0.001 (0.001) Batch 4.003 (3.459) Remain 05:01:17 loss: 0.3385 Lr: 0.01384
[2023-08-08 05:22:53,700 INFO misc.py line 115 22900] Train: [67/100][78/156] Data 0.001 (0.001) Batch 3.532 (3.460) Remain 05:01:19 loss: 0.2385 Lr: 0.01384
[2023-08-08 05:22:56,397 INFO misc.py line 115 22900] Train: [67/100][79/156] Data 0.001 (0.001) Batch 2.698 (3.449) Remain 05:00:23 loss: 0.1058 Lr: 0.01384
[2023-08-08 05:22:59,767 INFO misc.py line 115 22900] Train: [67/100][80/156] Data 0.001 (0.001) Batch 3.369 (3.448) Remain 05:00:14 loss: 0.1112 Lr: 0.01383
[2023-08-08 05:23:03,309 INFO misc.py line 115 22900] Train: [67/100][81/156] Data 0.001 (0.001) Batch 3.542 (3.450) Remain 05:00:17 loss: 0.1918 Lr: 0.01383
[2023-08-08 05:23:06,900 INFO misc.py line 115 22900] Train: [67/100][82/156] Data 0.001 (0.001) Batch 3.591 (3.451) Remain 05:00:23 loss: 0.2353 Lr: 0.01382
[2023-08-08 05:23:10,949 INFO misc.py line 115 22900] Train: [67/100][83/156] Data 0.001 (0.001) Batch 4.050 (3.459) Remain 05:00:58 loss: 0.4682 Lr: 0.01382
[2023-08-08 05:23:13,344 INFO misc.py line 115 22900] Train: [67/100][84/156] Data 0.001 (0.001) Batch 2.395 (3.446) Remain 04:59:46 loss: 0.0761 Lr: 0.01381
[2023-08-08 05:23:16,591 INFO misc.py line 115 22900] Train: [67/100][85/156] Data 0.001 (0.001) Batch 3.247 (3.443) Remain 04:59:30 loss: 0.1351 Lr: 0.01381
[2023-08-08 05:23:19,946 INFO misc.py line 115 22900] Train: [67/100][86/156] Data 0.001 (0.001) Batch 3.354 (3.442) Remain 04:59:21 loss: 0.0877 Lr: 0.01380
[2023-08-08 05:23:23,802 INFO misc.py line 115 22900] Train: [67/100][87/156] Data 0.001 (0.001) Batch 3.857 (3.447) Remain 04:59:44 loss: 0.1926 Lr: 0.01380
[2023-08-08 05:23:27,126 INFO misc.py line 115 22900] Train: [67/100][88/156] Data 0.001 (0.001) Batch 3.324 (3.446) Remain 04:59:33 loss: 0.0957 Lr: 0.01379
[2023-08-08 05:23:31,184 INFO misc.py line 115 22900] Train: [67/100][89/156] Data 0.001 (0.001) Batch 4.058 (3.453) Remain 05:00:06 loss: 0.2121 Lr: 0.01379
[2023-08-08 05:23:34,653 INFO misc.py line 115 22900] Train: [67/100][90/156] Data 0.001 (0.001) Batch 3.469 (3.453) Remain 05:00:04 loss: 0.1864 Lr: 0.01378
[2023-08-08 05:23:38,288 INFO misc.py line 115 22900] Train: [67/100][91/156] Data 0.001 (0.001) Batch 3.635 (3.455) Remain 05:00:11 loss: 0.1435 Lr: 0.01378
[2023-08-08 05:23:42,375 INFO misc.py line 115 22900] Train: [67/100][92/156] Data 0.001 (0.001) Batch 4.088 (3.462) Remain 05:00:45 loss: 0.2055 Lr: 0.01377
[2023-08-08 05:23:46,051 INFO misc.py line 115 22900] Train: [67/100][93/156] Data 0.001 (0.001) Batch 3.675 (3.465) Remain 05:00:54 loss: 0.1403 Lr: 0.01377
[2023-08-08 05:23:50,030 INFO misc.py line 115 22900] Train: [67/100][94/156] Data 0.001 (0.001) Batch 3.979 (3.470) Remain 05:01:20 loss: 0.1469 Lr: 0.01376
[2023-08-08 05:23:54,154 INFO misc.py line 115 22900] Train: [67/100][95/156] Data 0.001 (0.001) Batch 4.124 (3.477) Remain 05:01:53 loss: 0.2375 Lr: 0.01376
[2023-08-08 05:23:57,664 INFO misc.py line 115 22900] Train: [67/100][96/156] Data 0.001 (0.001) Batch 3.510 (3.478) Remain 05:01:51 loss: 0.1240 Lr: 0.01375
[2023-08-08 05:24:01,565 INFO misc.py line 115 22900] Train: [67/100][97/156] Data 0.001 (0.001) Batch 3.901 (3.482) Remain 05:02:11 loss: 0.2480 Lr: 0.01375
[2023-08-08 05:24:05,543 INFO misc.py line 115 22900] Train: [67/100][98/156] Data 0.001 (0.001) Batch 3.977 (3.487) Remain 05:02:35 loss: 0.2466 Lr: 0.01375
[2023-08-08 05:24:09,149 INFO misc.py line 115 22900] Train: [67/100][99/156] Data 0.001 (0.001) Batch 3.606 (3.489) Remain 05:02:38 loss: 0.1851 Lr: 0.01374
[2023-08-08 05:24:13,214 INFO misc.py line 115 22900] Train: [67/100][100/156] Data 0.001 (0.001) Batch 4.065 (3.495) Remain 05:03:05 loss: 0.2406 Lr: 0.01374
[2023-08-08 05:24:16,304 INFO misc.py line 115 22900] Train: [67/100][101/156] Data 0.001 (0.001) Batch 3.090 (3.490) Remain 05:02:40 loss: 0.1532 Lr: 0.01373
[2023-08-08 05:24:20,198 INFO misc.py line 115 22900] Train: [67/100][102/156] Data 0.001 (0.001) Batch 3.894 (3.495) Remain 05:02:58 loss: 0.2443 Lr: 0.01373
[2023-08-08 05:24:23,261 INFO misc.py line 115 22900] Train: [67/100][103/156] Data 0.001 (0.001) Batch 3.064 (3.490) Remain 05:02:32 loss: 0.1422 Lr: 0.01372
[2023-08-08 05:24:25,793 INFO misc.py line 115 22900] Train: [67/100][104/156] Data 0.001 (0.001) Batch 2.531 (3.481) Remain 05:01:39 loss: 0.0378 Lr: 0.01372
[2023-08-08 05:24:27,619 INFO misc.py line 115 22900] Train: [67/100][105/156] Data 0.001 (0.001) Batch 1.826 (3.465) Remain 05:00:12 loss: 0.0671 Lr: 0.01371
[2023-08-08 05:24:31,403 INFO misc.py line 115 22900] Train: [67/100][106/156] Data 0.002 (0.001) Batch 3.785 (3.468) Remain 05:00:24 loss: 0.4186 Lr: 0.01371
[2023-08-08 05:24:35,407 INFO misc.py line 115 22900] Train: [67/100][107/156] Data 0.001 (0.001) Batch 4.003 (3.473) Remain 05:00:48 loss: 0.2679 Lr: 0.01370
[2023-08-08 05:24:38,409 INFO misc.py line 115 22900] Train: [67/100][108/156] Data 0.001 (0.001) Batch 3.003 (3.468) Remain 05:00:21 loss: 0.1019 Lr: 0.01370
[2023-08-08 05:24:42,074 INFO misc.py line 115 22900] Train: [67/100][109/156] Data 0.001 (0.001) Batch 3.665 (3.470) Remain 05:00:27 loss: 0.2257 Lr: 0.01369
[2023-08-08 05:24:45,711 INFO misc.py line 115 22900] Train: [67/100][110/156] Data 0.001 (0.001) Batch 3.636 (3.472) Remain 05:00:32 loss: 0.3362 Lr: 0.01369
[2023-08-08 05:24:48,538 INFO misc.py line 115 22900] Train: [67/100][111/156] Data 0.001 (0.001) Batch 2.827 (3.466) Remain 04:59:57 loss: 0.0891 Lr: 0.01368
[2023-08-08 05:24:52,543 INFO misc.py line 115 22900] Train: [67/100][112/156] Data 0.001 (0.001) Batch 4.005 (3.471) Remain 05:00:19 loss: 0.1560 Lr: 0.01368
[2023-08-08 05:24:56,383 INFO misc.py line 115 22900] Train: [67/100][113/156] Data 0.001 (0.001) Batch 3.840 (3.474) Remain 05:00:33 loss: 0.2054 Lr: 0.01367
[2023-08-08 05:24:59,957 INFO misc.py line 115 22900] Train: [67/100][114/156] Data 0.001 (0.001) Batch 3.574 (3.475) Remain 05:00:35 loss: 0.1289 Lr: 0.01367
[2023-08-08 05:25:02,725 INFO misc.py line 115 22900] Train: [67/100][115/156] Data 0.001 (0.001) Batch 2.768 (3.469) Remain 04:59:58 loss: 0.1248 Lr: 0.01366
[2023-08-08 05:25:05,911 INFO misc.py line 115 22900] Train: [67/100][116/156] Data 0.001 (0.001) Batch 3.186 (3.466) Remain 04:59:42 loss: 0.3241 Lr: 0.01366
[2023-08-08 05:25:09,976 INFO misc.py line 115 22900] Train: [67/100][117/156] Data 0.001 (0.001) Batch 4.065 (3.471) Remain 05:00:06 loss: 0.2638 Lr: 0.01366
[2023-08-08 05:25:13,273 INFO misc.py line 115 22900] Train: [67/100][118/156] Data 0.001 (0.001) Batch 3.297 (3.470) Remain 04:59:54 loss: 0.1927 Lr: 0.01365
[2023-08-08 05:25:17,228 INFO misc.py line 115 22900] Train: [67/100][119/156] Data 0.001 (0.001) Batch 3.955 (3.474) Remain 05:00:12 loss: 0.3207 Lr: 0.01365
[2023-08-08 05:25:21,247 INFO misc.py line 115 22900] Train: [67/100][120/156] Data 0.001 (0.001) Batch 4.019 (3.479) Remain 05:00:33 loss: 0.1169 Lr: 0.01364
[2023-08-08 05:25:24,547 INFO misc.py line 115 22900] Train: [67/100][121/156] Data 0.001 (0.001) Batch 3.300 (3.477) Remain 05:00:22 loss: 0.1965 Lr: 0.01364
[2023-08-08 05:25:28,082 INFO misc.py line 115 22900] Train: [67/100][122/156] Data 0.001 (0.001) Batch 3.535 (3.478) Remain 05:00:21 loss: 0.1891 Lr: 0.01363
[2023-08-08 05:25:31,788 INFO misc.py line 115 22900] Train: [67/100][123/156] Data 0.001 (0.001) Batch 3.706 (3.480) Remain 05:00:27 loss: 0.2434 Lr: 0.01363
[2023-08-08 05:25:35,942 INFO misc.py line 115 22900] Train: [67/100][124/156] Data 0.001 (0.001) Batch 4.154 (3.485) Remain 05:00:53 loss: 0.1392 Lr: 0.01362
[2023-08-08 05:25:39,241 INFO misc.py line 115 22900] Train: [67/100][125/156] Data 0.001 (0.001) Batch 3.299 (3.484) Remain 05:00:41 loss: 0.1107 Lr: 0.01362
[2023-08-08 05:25:42,042 INFO misc.py line 115 22900] Train: [67/100][126/156] Data 0.001 (0.001) Batch 2.801 (3.478) Remain 05:00:09 loss: 0.0764 Lr: 0.01361
[2023-08-08 05:25:45,621 INFO misc.py line 115 22900] Train: [67/100][127/156] Data 0.001 (0.001) Batch 3.580 (3.479) Remain 05:00:10 loss: 0.1492 Lr: 0.01361
[2023-08-08 05:25:47,932 INFO misc.py line 115 22900] Train: [67/100][128/156] Data 0.001 (0.001) Batch 2.310 (3.470) Remain 04:59:18 loss: 0.1733 Lr: 0.01360
[2023-08-08 05:25:51,146 INFO misc.py line 115 22900] Train: [67/100][129/156] Data 0.001 (0.001) Batch 3.215 (3.468) Remain 04:59:04 loss: 0.2070 Lr: 0.01360
[2023-08-08 05:25:54,178 INFO misc.py line 115 22900] Train: [67/100][130/156] Data 0.001 (0.001) Batch 3.032 (3.464) Remain 04:58:43 loss: 0.1366 Lr: 0.01359
[2023-08-08 05:25:58,156 INFO misc.py line 115 22900] Train: [67/100][131/156] Data 0.001 (0.001) Batch 3.977 (3.468) Remain 04:59:00 loss: 0.1849 Lr: 0.01359
[2023-08-08 05:26:00,753 INFO misc.py line 115 22900] Train: [67/100][132/156] Data 0.001 (0.001) Batch 2.597 (3.461) Remain 04:58:22 loss: 0.1209 Lr: 0.01358
[2023-08-08 05:26:05,006 INFO misc.py line 115 22900] Train: [67/100][133/156] Data 0.001 (0.001) Batch 4.253 (3.467) Remain 04:58:50 loss: 0.1668 Lr: 0.01358
[2023-08-08 05:26:08,234 INFO misc.py line 115 22900] Train: [67/100][134/156] Data 0.001 (0.001) Batch 3.229 (3.466) Remain 04:58:37 loss: 0.0973 Lr: 0.01358
[2023-08-08 05:26:10,757 INFO misc.py line 115 22900] Train: [67/100][135/156] Data 0.001 (0.001) Batch 2.523 (3.458) Remain 04:57:56 loss: 0.0844 Lr: 0.01357
[2023-08-08 05:26:14,325 INFO misc.py line 115 22900] Train: [67/100][136/156] Data 0.001 (0.001) Batch 3.568 (3.459) Remain 04:57:57 loss: 0.3022 Lr: 0.01357
[2023-08-08 05:26:16,943 INFO misc.py line 115 22900] Train: [67/100][137/156] Data 0.001 (0.001) Batch 2.618 (3.453) Remain 04:57:21 loss: 0.1715 Lr: 0.01356
[2023-08-08 05:26:21,096 INFO misc.py line 115 22900] Train: [67/100][138/156] Data 0.001 (0.001) Batch 4.153 (3.458) Remain 04:57:45 loss: 0.1751 Lr: 0.01356
[2023-08-08 05:26:24,373 INFO misc.py line 115 22900] Train: [67/100][139/156] Data 0.001 (0.001) Batch 3.277 (3.457) Remain 04:57:34 loss: 0.1030 Lr: 0.01355
[2023-08-08 05:26:28,220 INFO misc.py line 115 22900] Train: [67/100][140/156] Data 0.001 (0.001) Batch 3.847 (3.460) Remain 04:57:46 loss: 0.2627 Lr: 0.01355
[2023-08-08 05:26:32,296 INFO misc.py line 115 22900] Train: [67/100][141/156] Data 0.001 (0.001) Batch 4.075 (3.464) Remain 04:58:05 loss: 0.1692 Lr: 0.01354
[2023-08-08 05:26:35,742 INFO misc.py line 115 22900] Train: [67/100][142/156] Data 0.001 (0.001) Batch 3.447 (3.464) Remain 04:58:01 loss: 0.2498 Lr: 0.01354
[2023-08-08 05:26:39,843 INFO misc.py line 115 22900] Train: [67/100][143/156] Data 0.001 (0.001) Batch 4.101 (3.469) Remain 04:58:21 loss: 0.2383 Lr: 0.01353
[2023-08-08 05:26:44,021 INFO misc.py line 115 22900] Train: [67/100][144/156] Data 0.001 (0.001) Batch 4.178 (3.474) Remain 04:58:44 loss: 0.2199 Lr: 0.01353
[2023-08-08 05:26:47,230 INFO misc.py line 115 22900] Train: [67/100][145/156] Data 0.001 (0.001) Batch 3.209 (3.472) Remain 04:58:30 loss: 0.1100 Lr: 0.01352
[2023-08-08 05:26:51,113 INFO misc.py line 115 22900] Train: [67/100][146/156] Data 0.001 (0.001) Batch 3.883 (3.475) Remain 04:58:42 loss: 0.1614 Lr: 0.01352
[2023-08-08 05:26:55,415 INFO misc.py line 115 22900] Train: [67/100][147/156] Data 0.001 (0.001) Batch 4.302 (3.480) Remain 04:59:08 loss: 0.2978 Lr: 0.01351
[2023-08-08 05:26:59,085 INFO misc.py line 115 22900] Train: [67/100][148/156] Data 0.001 (0.001) Batch 3.670 (3.482) Remain 04:59:11 loss: 0.2034 Lr: 0.01351
[2023-08-08 05:27:02,555 INFO misc.py line 115 22900] Train: [67/100][149/156] Data 0.001 (0.001) Batch 3.470 (3.482) Remain 04:59:07 loss: 0.1454 Lr: 0.01350
[2023-08-08 05:27:05,390 INFO misc.py line 115 22900] Train: [67/100][150/156] Data 0.001 (0.001) Batch 2.835 (3.477) Remain 04:58:41 loss: 0.0878 Lr: 0.01350
[2023-08-08 05:27:08,868 INFO misc.py line 115 22900] Train: [67/100][151/156] Data 0.001 (0.001) Batch 3.479 (3.477) Remain 04:58:38 loss: 0.2221 Lr: 0.01350
[2023-08-08 05:27:12,349 INFO misc.py line 115 22900] Train: [67/100][152/156] Data 0.001 (0.001) Batch 3.481 (3.477) Remain 04:58:34 loss: 0.1558 Lr: 0.01349
[2023-08-08 05:27:15,584 INFO misc.py line 115 22900] Train: [67/100][153/156] Data 0.001 (0.001) Batch 3.235 (3.476) Remain 04:58:23 loss: 0.1392 Lr: 0.01349
[2023-08-08 05:27:18,208 INFO misc.py line 115 22900] Train: [67/100][154/156] Data 0.001 (0.001) Batch 2.624 (3.470) Remain 04:57:50 loss: 0.1495 Lr: 0.01348
[2023-08-08 05:27:22,181 INFO misc.py line 115 22900] Train: [67/100][155/156] Data 0.001 (0.001) Batch 3.973 (3.473) Remain 04:58:04 loss: 0.0886 Lr: 0.01348
[2023-08-08 05:27:26,306 INFO misc.py line 115 22900] Train: [67/100][156/156] Data 0.001 (0.001) Batch 4.125 (3.478) Remain 04:58:22 loss: 0.1311 Lr: 0.01347
[2023-08-08 05:27:26,306 INFO misc.py line 129 22900] Train result: loss: 0.1866 
[2023-08-08 05:27:26,307 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 05:27:28,409 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.0279 
[2023-08-08 05:27:29,277 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.6418 
[2023-08-08 05:27:30,939 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.9115 
[2023-08-08 05:27:32,461 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.3661 
[2023-08-08 05:27:34,306 INFO evaluator.py line 122 22900] Test: [5/24] Loss 2.1975 
[2023-08-08 05:27:35,972 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.5545 
[2023-08-08 05:27:38,110 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.5902 
[2023-08-08 05:27:39,916 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.1711 
[2023-08-08 05:27:41,201 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.4201 
[2023-08-08 05:27:43,332 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3166 
[2023-08-08 05:27:43,857 INFO evaluator.py line 122 22900] Test: [11/24] Loss 2.3290 
[2023-08-08 05:27:45,393 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.6772 
[2023-08-08 05:27:48,102 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.1227 
[2023-08-08 05:27:49,782 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.9794 
[2023-08-08 05:27:51,804 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.3298 
[2023-08-08 05:27:54,513 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.1693 
[2023-08-08 05:27:57,219 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.5513 
[2023-08-08 05:27:59,066 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.6302 
[2023-08-08 05:27:59,814 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.0968 
[2023-08-08 05:28:00,700 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.6097 
[2023-08-08 05:28:02,959 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.2945 
[2023-08-08 05:28:04,924 INFO evaluator.py line 122 22900] Test: [22/24] Loss 2.0585 
[2023-08-08 05:28:06,770 INFO evaluator.py line 122 22900] Test: [23/24] Loss 3.0856 
[2023-08-08 05:28:08,705 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.5829 
[2023-08-08 05:28:08,754 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2529/0.3455/0.7042.
[2023-08-08 05:28:08,754 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6659/0.9596
[2023-08-08 05:28:08,754 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9464/0.9904
[2023-08-08 05:28:08,754 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1881/0.4599
[2023-08-08 05:28:08,754 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.2122/0.2976
[2023-08-08 05:28:08,755 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6502/0.7758
[2023-08-08 05:28:08,755 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.3567/0.4777
[2023-08-08 05:28:08,755 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5329/0.7659
[2023-08-08 05:28:08,755 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1817/0.2007
[2023-08-08 05:28:08,755 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1782/0.4211
[2023-08-08 05:28:08,755 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0086/0.0086
[2023-08-08 05:28:08,755 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 05:28:08,755 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0135/0.0140
[2023-08-08 05:28:08,755 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1075/0.1308
[2023-08-08 05:28:08,755 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0836/0.0940
[2023-08-08 05:28:08,755 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0260/0.0292
[2023-08-08 05:28:08,755 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.2815/0.3319
[2023-08-08 05:28:08,755 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0571/0.0706
[2023-08-08 05:28:08,755 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.4504/0.6391
[2023-08-08 05:28:08,755 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 05:28:08,755 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.1175/0.2434
[2023-08-08 05:28:08,755 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 05:28:08,755 INFO misc.py line 152 22900] Currently Best mIoU: 0.2650
[2023-08-08 05:28:08,755 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 05:28:14,365 INFO misc.py line 115 22900] Train: [68/100][1/156] Data 1.197 (1.197) Batch 4.829 (4.829) Remain 06:54:15 loss: 0.0982 Lr: 0.01347
[2023-08-08 05:28:18,112 INFO misc.py line 115 22900] Train: [68/100][2/156] Data 0.001 (0.001) Batch 3.748 (3.748) Remain 05:21:24 loss: 0.3194 Lr: 0.01346
[2023-08-08 05:28:22,265 INFO misc.py line 115 22900] Train: [68/100][3/156] Data 0.001 (0.001) Batch 4.152 (4.152) Remain 05:56:01 loss: 0.1440 Lr: 0.01346
[2023-08-08 05:28:25,886 INFO misc.py line 115 22900] Train: [68/100][4/156] Data 0.001 (0.001) Batch 3.621 (3.621) Remain 05:10:28 loss: 0.2248 Lr: 0.01345
[2023-08-08 05:28:28,573 INFO misc.py line 115 22900] Train: [68/100][5/156] Data 0.001 (0.001) Batch 2.687 (3.154) Remain 04:30:23 loss: 0.1678 Lr: 0.01345
[2023-08-08 05:28:32,684 INFO misc.py line 115 22900] Train: [68/100][6/156] Data 0.001 (0.001) Batch 4.111 (3.473) Remain 04:57:38 loss: 0.2043 Lr: 0.01344
[2023-08-08 05:28:36,129 INFO misc.py line 115 22900] Train: [68/100][7/156] Data 0.001 (0.001) Batch 3.445 (3.466) Remain 04:56:59 loss: 0.1744 Lr: 0.01344
[2023-08-08 05:28:39,691 INFO misc.py line 115 22900] Train: [68/100][8/156] Data 0.001 (0.001) Batch 3.562 (3.485) Remain 04:58:34 loss: 0.1704 Lr: 0.01343
[2023-08-08 05:28:43,406 INFO misc.py line 115 22900] Train: [68/100][9/156] Data 0.001 (0.001) Batch 3.715 (3.524) Remain 05:01:47 loss: 0.0688 Lr: 0.01343
[2023-08-08 05:28:47,484 INFO misc.py line 115 22900] Train: [68/100][10/156] Data 0.001 (0.001) Batch 4.078 (3.603) Remain 05:08:31 loss: 0.1373 Lr: 0.01342
[2023-08-08 05:28:50,384 INFO misc.py line 115 22900] Train: [68/100][11/156] Data 0.001 (0.001) Batch 2.900 (3.515) Remain 05:00:56 loss: 0.1668 Lr: 0.01342
[2023-08-08 05:28:54,767 INFO misc.py line 115 22900] Train: [68/100][12/156] Data 0.001 (0.001) Batch 4.383 (3.611) Remain 05:09:08 loss: 0.2658 Lr: 0.01342
[2023-08-08 05:28:58,399 INFO misc.py line 115 22900] Train: [68/100][13/156] Data 0.001 (0.001) Batch 3.632 (3.613) Remain 05:09:14 loss: 0.1457 Lr: 0.01341
[2023-08-08 05:29:02,383 INFO misc.py line 115 22900] Train: [68/100][14/156] Data 0.001 (0.001) Batch 3.984 (3.647) Remain 05:12:04 loss: 0.1837 Lr: 0.01341
[2023-08-08 05:29:04,772 INFO misc.py line 115 22900] Train: [68/100][15/156] Data 0.001 (0.001) Batch 2.389 (3.542) Remain 05:03:02 loss: 0.2235 Lr: 0.01340
[2023-08-08 05:29:08,533 INFO misc.py line 115 22900] Train: [68/100][16/156] Data 0.001 (0.001) Batch 3.761 (3.559) Remain 05:04:25 loss: 0.0933 Lr: 0.01340
[2023-08-08 05:29:12,618 INFO misc.py line 115 22900] Train: [68/100][17/156] Data 0.001 (0.001) Batch 4.085 (3.597) Remain 05:07:34 loss: 0.2213 Lr: 0.01339
[2023-08-08 05:29:16,779 INFO misc.py line 115 22900] Train: [68/100][18/156] Data 0.001 (0.001) Batch 4.161 (3.634) Remain 05:10:43 loss: 0.2248 Lr: 0.01339
[2023-08-08 05:29:20,746 INFO misc.py line 115 22900] Train: [68/100][19/156] Data 0.001 (0.001) Batch 3.967 (3.655) Remain 05:12:26 loss: 0.3035 Lr: 0.01338
[2023-08-08 05:29:24,495 INFO misc.py line 115 22900] Train: [68/100][20/156] Data 0.001 (0.001) Batch 3.750 (3.661) Remain 05:12:51 loss: 0.2952 Lr: 0.01338
[2023-08-08 05:29:26,697 INFO misc.py line 115 22900] Train: [68/100][21/156] Data 0.002 (0.001) Batch 2.202 (3.580) Remain 05:05:52 loss: 0.0615 Lr: 0.01337
[2023-08-08 05:29:31,431 INFO misc.py line 115 22900] Train: [68/100][22/156] Data 0.001 (0.001) Batch 4.734 (3.640) Remain 05:11:00 loss: 0.2341 Lr: 0.01337
[2023-08-08 05:29:34,084 INFO misc.py line 115 22900] Train: [68/100][23/156] Data 0.001 (0.001) Batch 2.653 (3.591) Remain 05:06:43 loss: 0.1295 Lr: 0.01336
[2023-08-08 05:29:37,309 INFO misc.py line 115 22900] Train: [68/100][24/156] Data 0.001 (0.001) Batch 3.225 (3.574) Remain 05:05:10 loss: 0.1610 Lr: 0.01336
[2023-08-08 05:29:41,332 INFO misc.py line 115 22900] Train: [68/100][25/156] Data 0.001 (0.001) Batch 4.023 (3.594) Remain 05:06:51 loss: 0.2355 Lr: 0.01335
[2023-08-08 05:29:43,902 INFO misc.py line 115 22900] Train: [68/100][26/156] Data 0.001 (0.001) Batch 2.570 (3.549) Remain 05:03:00 loss: 0.1154 Lr: 0.01335
[2023-08-08 05:29:47,762 INFO misc.py line 115 22900] Train: [68/100][27/156] Data 0.001 (0.001) Batch 3.860 (3.562) Remain 05:04:03 loss: 0.3881 Lr: 0.01334
[2023-08-08 05:29:51,642 INFO misc.py line 115 22900] Train: [68/100][28/156] Data 0.001 (0.001) Batch 3.880 (3.575) Remain 05:05:04 loss: 0.1681 Lr: 0.01334
[2023-08-08 05:29:55,090 INFO misc.py line 115 22900] Train: [68/100][29/156] Data 0.001 (0.001) Batch 3.448 (3.570) Remain 05:04:35 loss: 0.2265 Lr: 0.01334
[2023-08-08 05:29:59,112 INFO misc.py line 115 22900] Train: [68/100][30/156] Data 0.001 (0.001) Batch 4.022 (3.587) Remain 05:05:57 loss: 0.1279 Lr: 0.01333
[2023-08-08 05:30:01,877 INFO misc.py line 115 22900] Train: [68/100][31/156] Data 0.001 (0.001) Batch 2.765 (3.558) Remain 05:03:24 loss: 0.1511 Lr: 0.01333
[2023-08-08 05:30:05,614 INFO misc.py line 115 22900] Train: [68/100][32/156] Data 0.001 (0.001) Batch 3.737 (3.564) Remain 05:03:52 loss: 0.2336 Lr: 0.01332
[2023-08-08 05:30:09,171 INFO misc.py line 115 22900] Train: [68/100][33/156] Data 0.001 (0.001) Batch 3.557 (3.564) Remain 05:03:47 loss: 0.2556 Lr: 0.01332
[2023-08-08 05:30:12,806 INFO misc.py line 115 22900] Train: [68/100][34/156] Data 0.001 (0.001) Batch 3.635 (3.566) Remain 05:03:55 loss: 0.0708 Lr: 0.01331
[2023-08-08 05:30:15,789 INFO misc.py line 115 22900] Train: [68/100][35/156] Data 0.001 (0.001) Batch 2.983 (3.548) Remain 05:02:19 loss: 0.0859 Lr: 0.01331
[2023-08-08 05:30:19,341 INFO misc.py line 115 22900] Train: [68/100][36/156] Data 0.001 (0.001) Batch 3.552 (3.548) Remain 05:02:16 loss: 0.2250 Lr: 0.01330
[2023-08-08 05:30:23,197 INFO misc.py line 115 22900] Train: [68/100][37/156] Data 0.001 (0.001) Batch 3.856 (3.557) Remain 05:02:58 loss: 0.3247 Lr: 0.01330
[2023-08-08 05:30:25,843 INFO misc.py line 115 22900] Train: [68/100][38/156] Data 0.001 (0.001) Batch 2.647 (3.531) Remain 05:00:42 loss: 0.1476 Lr: 0.01329
[2023-08-08 05:30:29,357 INFO misc.py line 115 22900] Train: [68/100][39/156] Data 0.001 (0.001) Batch 3.513 (3.530) Remain 05:00:36 loss: 0.3218 Lr: 0.01329
[2023-08-08 05:30:34,059 INFO misc.py line 115 22900] Train: [68/100][40/156] Data 0.001 (0.001) Batch 4.703 (3.562) Remain 05:03:14 loss: 0.6263 Lr: 0.01328
[2023-08-08 05:30:37,552 INFO misc.py line 115 22900] Train: [68/100][41/156] Data 0.001 (0.001) Batch 3.492 (3.560) Remain 05:03:01 loss: 0.1800 Lr: 0.01328
[2023-08-08 05:30:40,858 INFO misc.py line 115 22900] Train: [68/100][42/156] Data 0.001 (0.001) Batch 3.306 (3.554) Remain 05:02:25 loss: 0.3514 Lr: 0.01327
[2023-08-08 05:30:44,159 INFO misc.py line 115 22900] Train: [68/100][43/156] Data 0.001 (0.001) Batch 3.301 (3.547) Remain 05:01:49 loss: 0.1458 Lr: 0.01327
[2023-08-08 05:30:46,746 INFO misc.py line 115 22900] Train: [68/100][44/156] Data 0.001 (0.001) Batch 2.587 (3.524) Remain 04:59:46 loss: 0.1199 Lr: 0.01327
[2023-08-08 05:30:49,963 INFO misc.py line 115 22900] Train: [68/100][45/156] Data 0.001 (0.001) Batch 3.217 (3.517) Remain 04:59:05 loss: 0.1991 Lr: 0.01326
[2023-08-08 05:30:53,212 INFO misc.py line 115 22900] Train: [68/100][46/156] Data 0.001 (0.001) Batch 3.249 (3.510) Remain 04:58:30 loss: 0.1706 Lr: 0.01326
[2023-08-08 05:30:57,491 INFO misc.py line 115 22900] Train: [68/100][47/156] Data 0.001 (0.001) Batch 4.280 (3.528) Remain 04:59:55 loss: 0.2894 Lr: 0.01325
[2023-08-08 05:31:00,608 INFO misc.py line 115 22900] Train: [68/100][48/156] Data 0.001 (0.001) Batch 3.117 (3.519) Remain 04:59:05 loss: 0.2297 Lr: 0.01325
[2023-08-08 05:31:04,405 INFO misc.py line 115 22900] Train: [68/100][49/156] Data 0.001 (0.001) Batch 3.797 (3.525) Remain 04:59:32 loss: 0.1798 Lr: 0.01324
[2023-08-08 05:31:08,359 INFO misc.py line 115 22900] Train: [68/100][50/156] Data 0.001 (0.001) Batch 3.954 (3.534) Remain 05:00:15 loss: 0.1824 Lr: 0.01324
[2023-08-08 05:31:10,469 INFO misc.py line 115 22900] Train: [68/100][51/156] Data 0.001 (0.001) Batch 2.110 (3.504) Remain 04:57:41 loss: 0.1086 Lr: 0.01323
[2023-08-08 05:31:14,325 INFO misc.py line 115 22900] Train: [68/100][52/156] Data 0.001 (0.001) Batch 3.856 (3.511) Remain 04:58:14 loss: 0.1283 Lr: 0.01323
[2023-08-08 05:31:18,236 INFO misc.py line 115 22900] Train: [68/100][53/156] Data 0.001 (0.001) Batch 3.911 (3.519) Remain 04:58:51 loss: 0.2978 Lr: 0.01322
[2023-08-08 05:31:21,930 INFO misc.py line 115 22900] Train: [68/100][54/156] Data 0.001 (0.001) Batch 3.694 (3.523) Remain 04:59:05 loss: 0.1977 Lr: 0.01322
[2023-08-08 05:31:24,798 INFO misc.py line 115 22900] Train: [68/100][55/156] Data 0.001 (0.001) Batch 2.868 (3.510) Remain 04:57:57 loss: 0.0875 Lr: 0.01321
[2023-08-08 05:31:28,797 INFO misc.py line 115 22900] Train: [68/100][56/156] Data 0.001 (0.001) Batch 3.999 (3.519) Remain 04:58:41 loss: 0.1310 Lr: 0.01321
[2023-08-08 05:31:31,807 INFO misc.py line 115 22900] Train: [68/100][57/156] Data 0.001 (0.001) Batch 3.010 (3.510) Remain 04:57:49 loss: 0.1263 Lr: 0.01320
[2023-08-08 05:31:34,684 INFO misc.py line 115 22900] Train: [68/100][58/156] Data 0.001 (0.001) Batch 2.877 (3.499) Remain 04:56:47 loss: 0.0807 Lr: 0.01320
[2023-08-08 05:31:38,028 INFO misc.py line 115 22900] Train: [68/100][59/156] Data 0.001 (0.001) Batch 3.343 (3.496) Remain 04:56:29 loss: 0.1151 Lr: 0.01320
[2023-08-08 05:31:41,002 INFO misc.py line 115 22900] Train: [68/100][60/156] Data 0.001 (0.001) Batch 2.975 (3.487) Remain 04:55:39 loss: 0.1356 Lr: 0.01319
[2023-08-08 05:31:44,032 INFO misc.py line 115 22900] Train: [68/100][61/156] Data 0.001 (0.001) Batch 3.029 (3.479) Remain 04:54:56 loss: 0.1501 Lr: 0.01319
[2023-08-08 05:31:48,569 INFO misc.py line 115 22900] Train: [68/100][62/156] Data 0.001 (0.001) Batch 4.537 (3.497) Remain 04:56:24 loss: 0.2854 Lr: 0.01318
[2023-08-08 05:31:52,227 INFO misc.py line 115 22900] Train: [68/100][63/156] Data 0.001 (0.001) Batch 3.659 (3.499) Remain 04:56:34 loss: 0.0943 Lr: 0.01318
[2023-08-08 05:31:56,219 INFO misc.py line 115 22900] Train: [68/100][64/156] Data 0.001 (0.001) Batch 3.992 (3.507) Remain 04:57:11 loss: 0.1835 Lr: 0.01317
[2023-08-08 05:31:59,769 INFO misc.py line 115 22900] Train: [68/100][65/156] Data 0.001 (0.001) Batch 3.550 (3.508) Remain 04:57:11 loss: 0.1369 Lr: 0.01317
[2023-08-08 05:32:03,282 INFO misc.py line 115 22900] Train: [68/100][66/156] Data 0.001 (0.001) Batch 3.512 (3.508) Remain 04:57:08 loss: 0.2383 Lr: 0.01316
[2023-08-08 05:32:06,566 INFO misc.py line 115 22900] Train: [68/100][67/156] Data 0.001 (0.001) Batch 3.284 (3.505) Remain 04:56:47 loss: 0.2517 Lr: 0.01316
[2023-08-08 05:32:10,446 INFO misc.py line 115 22900] Train: [68/100][68/156] Data 0.001 (0.001) Batch 3.880 (3.510) Remain 04:57:13 loss: 0.1911 Lr: 0.01315
[2023-08-08 05:32:14,468 INFO misc.py line 115 22900] Train: [68/100][69/156] Data 0.001 (0.001) Batch 4.022 (3.518) Remain 04:57:49 loss: 0.1410 Lr: 0.01315
[2023-08-08 05:32:17,198 INFO misc.py line 115 22900] Train: [68/100][70/156] Data 0.001 (0.001) Batch 2.730 (3.506) Remain 04:56:45 loss: 0.0668 Lr: 0.01314
[2023-08-08 05:32:20,303 INFO misc.py line 115 22900] Train: [68/100][71/156] Data 0.001 (0.001) Batch 3.105 (3.501) Remain 04:56:12 loss: 0.1299 Lr: 0.01314
[2023-08-08 05:32:23,401 INFO misc.py line 115 22900] Train: [68/100][72/156] Data 0.001 (0.001) Batch 3.098 (3.495) Remain 04:55:39 loss: 0.1999 Lr: 0.01313
[2023-08-08 05:32:26,443 INFO misc.py line 115 22900] Train: [68/100][73/156] Data 0.001 (0.001) Batch 3.042 (3.488) Remain 04:55:02 loss: 0.0901 Lr: 0.01313
[2023-08-08 05:32:28,307 INFO misc.py line 115 22900] Train: [68/100][74/156] Data 0.001 (0.001) Batch 1.864 (3.465) Remain 04:53:03 loss: 0.0699 Lr: 0.01312
[2023-08-08 05:32:32,360 INFO misc.py line 115 22900] Train: [68/100][75/156] Data 0.001 (0.001) Batch 4.053 (3.474) Remain 04:53:41 loss: 0.3472 Lr: 0.01312
[2023-08-08 05:32:36,013 INFO misc.py line 115 22900] Train: [68/100][76/156] Data 0.001 (0.001) Batch 3.653 (3.476) Remain 04:53:50 loss: 0.1402 Lr: 0.01312
[2023-08-08 05:32:39,875 INFO misc.py line 115 22900] Train: [68/100][77/156] Data 0.001 (0.001) Batch 3.862 (3.481) Remain 04:54:13 loss: 0.1334 Lr: 0.01311
[2023-08-08 05:32:43,011 INFO misc.py line 115 22900] Train: [68/100][78/156] Data 0.001 (0.001) Batch 3.136 (3.477) Remain 04:53:46 loss: 0.0888 Lr: 0.01311
[2023-08-08 05:32:47,313 INFO misc.py line 115 22900] Train: [68/100][79/156] Data 0.001 (0.001) Batch 4.301 (3.487) Remain 04:54:38 loss: 0.2142 Lr: 0.01310
[2023-08-08 05:32:51,169 INFO misc.py line 115 22900] Train: [68/100][80/156] Data 0.002 (0.001) Batch 3.857 (3.492) Remain 04:54:58 loss: 0.2348 Lr: 0.01310
[2023-08-08 05:32:53,455 INFO misc.py line 115 22900] Train: [68/100][81/156] Data 0.001 (0.001) Batch 2.285 (3.477) Remain 04:53:36 loss: 0.0884 Lr: 0.01309
[2023-08-08 05:32:57,540 INFO misc.py line 115 22900] Train: [68/100][82/156] Data 0.001 (0.001) Batch 4.085 (3.484) Remain 04:54:12 loss: 0.1150 Lr: 0.01309
[2023-08-08 05:33:01,143 INFO misc.py line 115 22900] Train: [68/100][83/156] Data 0.001 (0.001) Batch 3.604 (3.486) Remain 04:54:16 loss: 0.1299 Lr: 0.01308
[2023-08-08 05:33:04,039 INFO misc.py line 115 22900] Train: [68/100][84/156] Data 0.001 (0.001) Batch 2.895 (3.479) Remain 04:53:36 loss: 0.1008 Lr: 0.01308
[2023-08-08 05:33:07,857 INFO misc.py line 115 22900] Train: [68/100][85/156] Data 0.001 (0.001) Batch 3.818 (3.483) Remain 04:53:53 loss: 0.1529 Lr: 0.01307
[2023-08-08 05:33:11,475 INFO misc.py line 115 22900] Train: [68/100][86/156] Data 0.001 (0.001) Batch 3.618 (3.484) Remain 04:53:58 loss: 0.3392 Lr: 0.01307
[2023-08-08 05:33:14,302 INFO misc.py line 115 22900] Train: [68/100][87/156] Data 0.001 (0.001) Batch 2.827 (3.477) Remain 04:53:15 loss: 0.1031 Lr: 0.01306
[2023-08-08 05:33:16,717 INFO misc.py line 115 22900] Train: [68/100][88/156] Data 0.001 (0.001) Batch 2.415 (3.464) Remain 04:52:08 loss: 0.1300 Lr: 0.01306
[2023-08-08 05:33:19,663 INFO misc.py line 115 22900] Train: [68/100][89/156] Data 0.001 (0.001) Batch 2.946 (3.458) Remain 04:51:34 loss: 0.1310 Lr: 0.01306
[2023-08-08 05:33:23,319 INFO misc.py line 115 22900] Train: [68/100][90/156] Data 0.001 (0.001) Batch 3.656 (3.460) Remain 04:51:42 loss: 0.1566 Lr: 0.01305
[2023-08-08 05:33:25,709 INFO misc.py line 115 22900] Train: [68/100][91/156] Data 0.001 (0.001) Batch 2.390 (3.448) Remain 04:50:37 loss: 0.0833 Lr: 0.01305
[2023-08-08 05:33:29,194 INFO misc.py line 115 22900] Train: [68/100][92/156] Data 0.001 (0.001) Batch 3.485 (3.449) Remain 04:50:36 loss: 0.2249 Lr: 0.01304
[2023-08-08 05:33:32,816 INFO misc.py line 115 22900] Train: [68/100][93/156] Data 0.001 (0.001) Batch 3.622 (3.451) Remain 04:50:42 loss: 0.2969 Lr: 0.01304
[2023-08-08 05:33:36,826 INFO misc.py line 115 22900] Train: [68/100][94/156] Data 0.001 (0.001) Batch 4.010 (3.457) Remain 04:51:10 loss: 0.1166 Lr: 0.01303
[2023-08-08 05:33:38,867 INFO misc.py line 115 22900] Train: [68/100][95/156] Data 0.001 (0.001) Batch 2.041 (3.441) Remain 04:49:49 loss: 0.1684 Lr: 0.01303
[2023-08-08 05:33:42,608 INFO misc.py line 115 22900] Train: [68/100][96/156] Data 0.001 (0.001) Batch 3.740 (3.445) Remain 04:50:01 loss: 0.1197 Lr: 0.01302
[2023-08-08 05:33:45,724 INFO misc.py line 115 22900] Train: [68/100][97/156] Data 0.001 (0.001) Batch 3.117 (3.441) Remain 04:49:40 loss: 0.1219 Lr: 0.01302
[2023-08-08 05:33:49,223 INFO misc.py line 115 22900] Train: [68/100][98/156] Data 0.001 (0.001) Batch 3.499 (3.442) Remain 04:49:40 loss: 0.2612 Lr: 0.01301
[2023-08-08 05:33:52,824 INFO misc.py line 115 22900] Train: [68/100][99/156] Data 0.001 (0.001) Batch 3.601 (3.443) Remain 04:49:45 loss: 0.1616 Lr: 0.01301
[2023-08-08 05:33:56,167 INFO misc.py line 115 22900] Train: [68/100][100/156] Data 0.001 (0.001) Batch 3.343 (3.442) Remain 04:49:36 loss: 0.2197 Lr: 0.01300
[2023-08-08 05:33:59,761 INFO misc.py line 115 22900] Train: [68/100][101/156] Data 0.001 (0.001) Batch 3.594 (3.444) Remain 04:49:41 loss: 0.0930 Lr: 0.01300
[2023-08-08 05:34:03,091 INFO misc.py line 115 22900] Train: [68/100][102/156] Data 0.001 (0.001) Batch 3.330 (3.443) Remain 04:49:31 loss: 0.1324 Lr: 0.01299
[2023-08-08 05:34:05,986 INFO misc.py line 115 22900] Train: [68/100][103/156] Data 0.001 (0.001) Batch 2.895 (3.437) Remain 04:49:00 loss: 0.0546 Lr: 0.01299
[2023-08-08 05:34:08,698 INFO misc.py line 115 22900] Train: [68/100][104/156] Data 0.001 (0.001) Batch 2.711 (3.430) Remain 04:48:21 loss: 0.1101 Lr: 0.01299
[2023-08-08 05:34:12,693 INFO misc.py line 115 22900] Train: [68/100][105/156] Data 0.001 (0.001) Batch 3.996 (3.436) Remain 04:48:45 loss: 0.2978 Lr: 0.01298
[2023-08-08 05:34:16,543 INFO misc.py line 115 22900] Train: [68/100][106/156] Data 0.001 (0.001) Batch 3.850 (3.440) Remain 04:49:02 loss: 0.2119 Lr: 0.01298
[2023-08-08 05:34:19,865 INFO misc.py line 115 22900] Train: [68/100][107/156] Data 0.001 (0.001) Batch 3.322 (3.438) Remain 04:48:53 loss: 0.1190 Lr: 0.01297
[2023-08-08 05:34:23,245 INFO misc.py line 115 22900] Train: [68/100][108/156] Data 0.001 (0.001) Batch 3.380 (3.438) Remain 04:48:47 loss: 0.2189 Lr: 0.01297
[2023-08-08 05:34:26,442 INFO misc.py line 115 22900] Train: [68/100][109/156] Data 0.001 (0.001) Batch 3.198 (3.436) Remain 04:48:32 loss: 0.1312 Lr: 0.01296
[2023-08-08 05:34:30,178 INFO misc.py line 115 22900] Train: [68/100][110/156] Data 0.001 (0.001) Batch 3.735 (3.438) Remain 04:48:42 loss: 0.2910 Lr: 0.01296
[2023-08-08 05:34:33,943 INFO misc.py line 115 22900] Train: [68/100][111/156] Data 0.001 (0.001) Batch 3.765 (3.441) Remain 04:48:54 loss: 0.2421 Lr: 0.01295
[2023-08-08 05:34:37,571 INFO misc.py line 115 22900] Train: [68/100][112/156] Data 0.001 (0.001) Batch 3.628 (3.443) Remain 04:48:59 loss: 0.2983 Lr: 0.01295
[2023-08-08 05:34:40,501 INFO misc.py line 115 22900] Train: [68/100][113/156] Data 0.001 (0.001) Batch 2.930 (3.439) Remain 04:48:32 loss: 0.0735 Lr: 0.01294
[2023-08-08 05:34:44,509 INFO misc.py line 115 22900] Train: [68/100][114/156] Data 0.001 (0.001) Batch 4.008 (3.444) Remain 04:48:55 loss: 0.2087 Lr: 0.01294
[2023-08-08 05:34:48,418 INFO misc.py line 115 22900] Train: [68/100][115/156] Data 0.001 (0.001) Batch 3.909 (3.448) Remain 04:49:12 loss: 0.1712 Lr: 0.01293
[2023-08-08 05:34:52,094 INFO misc.py line 115 22900] Train: [68/100][116/156] Data 0.001 (0.001) Batch 3.676 (3.450) Remain 04:49:19 loss: 0.1848 Lr: 0.01293
[2023-08-08 05:34:55,457 INFO misc.py line 115 22900] Train: [68/100][117/156] Data 0.001 (0.001) Batch 3.363 (3.449) Remain 04:49:12 loss: 0.0898 Lr: 0.01292
[2023-08-08 05:34:59,045 INFO misc.py line 115 22900] Train: [68/100][118/156] Data 0.001 (0.001) Batch 3.588 (3.450) Remain 04:49:14 loss: 0.2434 Lr: 0.01292
[2023-08-08 05:35:02,520 INFO misc.py line 115 22900] Train: [68/100][119/156] Data 0.001 (0.001) Batch 3.475 (3.450) Remain 04:49:12 loss: 0.2152 Lr: 0.01292
[2023-08-08 05:35:06,052 INFO misc.py line 115 22900] Train: [68/100][120/156] Data 0.001 (0.001) Batch 3.533 (3.451) Remain 04:49:12 loss: 0.1658 Lr: 0.01291
[2023-08-08 05:35:10,334 INFO misc.py line 115 22900] Train: [68/100][121/156] Data 0.001 (0.001) Batch 4.282 (3.458) Remain 04:49:44 loss: 0.2287 Lr: 0.01291
[2023-08-08 05:35:13,378 INFO misc.py line 115 22900] Train: [68/100][122/156] Data 0.001 (0.001) Batch 3.044 (3.455) Remain 04:49:23 loss: 0.2149 Lr: 0.01290
[2023-08-08 05:35:17,862 INFO misc.py line 115 22900] Train: [68/100][123/156] Data 0.001 (0.001) Batch 4.484 (3.463) Remain 04:50:03 loss: 0.2096 Lr: 0.01290
[2023-08-08 05:35:20,493 INFO misc.py line 115 22900] Train: [68/100][124/156] Data 0.001 (0.001) Batch 2.631 (3.456) Remain 04:49:25 loss: 0.0833 Lr: 0.01289
[2023-08-08 05:35:23,759 INFO misc.py line 115 22900] Train: [68/100][125/156] Data 0.001 (0.001) Batch 3.267 (3.455) Remain 04:49:13 loss: 0.1703 Lr: 0.01289
[2023-08-08 05:35:26,971 INFO misc.py line 115 22900] Train: [68/100][126/156] Data 0.001 (0.001) Batch 3.212 (3.453) Remain 04:49:00 loss: 0.1428 Lr: 0.01288
[2023-08-08 05:35:30,290 INFO misc.py line 115 22900] Train: [68/100][127/156] Data 0.001 (0.001) Batch 3.319 (3.452) Remain 04:48:51 loss: 0.2248 Lr: 0.01288
[2023-08-08 05:35:33,169 INFO misc.py line 115 22900] Train: [68/100][128/156] Data 0.001 (0.001) Batch 2.879 (3.447) Remain 04:48:25 loss: 0.1510 Lr: 0.01287
[2023-08-08 05:35:36,109 INFO misc.py line 115 22900] Train: [68/100][129/156] Data 0.001 (0.001) Batch 2.940 (3.443) Remain 04:48:01 loss: 0.2162 Lr: 0.01287
[2023-08-08 05:35:40,101 INFO misc.py line 115 22900] Train: [68/100][130/156] Data 0.001 (0.001) Batch 3.992 (3.448) Remain 04:48:19 loss: 0.2447 Lr: 0.01286
[2023-08-08 05:35:44,043 INFO misc.py line 115 22900] Train: [68/100][131/156] Data 0.001 (0.001) Batch 3.942 (3.451) Remain 04:48:35 loss: 0.2350 Lr: 0.01286
[2023-08-08 05:35:47,513 INFO misc.py line 115 22900] Train: [68/100][132/156] Data 0.001 (0.001) Batch 3.471 (3.452) Remain 04:48:32 loss: 0.1848 Lr: 0.01286
[2023-08-08 05:35:51,726 INFO misc.py line 115 22900] Train: [68/100][133/156] Data 0.001 (0.001) Batch 4.212 (3.457) Remain 04:48:58 loss: 0.1740 Lr: 0.01285
[2023-08-08 05:35:54,928 INFO misc.py line 115 22900] Train: [68/100][134/156] Data 0.001 (0.001) Batch 3.202 (3.455) Remain 04:48:45 loss: 0.1529 Lr: 0.01285
[2023-08-08 05:35:59,025 INFO misc.py line 115 22900] Train: [68/100][135/156] Data 0.001 (0.001) Batch 4.096 (3.460) Remain 04:49:06 loss: 0.1363 Lr: 0.01284
[2023-08-08 05:36:01,965 INFO misc.py line 115 22900] Train: [68/100][136/156] Data 0.001 (0.001) Batch 2.941 (3.456) Remain 04:48:43 loss: 0.1144 Lr: 0.01284
[2023-08-08 05:36:05,655 INFO misc.py line 115 22900] Train: [68/100][137/156] Data 0.001 (0.001) Batch 3.690 (3.458) Remain 04:48:48 loss: 0.1131 Lr: 0.01283
[2023-08-08 05:36:09,728 INFO misc.py line 115 22900] Train: [68/100][138/156] Data 0.001 (0.001) Batch 4.073 (3.463) Remain 04:49:08 loss: 0.3997 Lr: 0.01283
[2023-08-08 05:36:13,165 INFO misc.py line 115 22900] Train: [68/100][139/156] Data 0.001 (0.001) Batch 3.437 (3.463) Remain 04:49:03 loss: 0.3295 Lr: 0.01282
[2023-08-08 05:36:15,937 INFO misc.py line 115 22900] Train: [68/100][140/156] Data 0.001 (0.001) Batch 2.772 (3.457) Remain 04:48:34 loss: 0.1075 Lr: 0.01282
[2023-08-08 05:36:19,547 INFO misc.py line 115 22900] Train: [68/100][141/156] Data 0.001 (0.001) Batch 3.609 (3.459) Remain 04:48:37 loss: 0.1640 Lr: 0.01281
[2023-08-08 05:36:23,456 INFO misc.py line 115 22900] Train: [68/100][142/156] Data 0.001 (0.001) Batch 3.909 (3.462) Remain 04:48:49 loss: 0.2647 Lr: 0.01281
[2023-08-08 05:36:27,268 INFO misc.py line 115 22900] Train: [68/100][143/156] Data 0.001 (0.001) Batch 3.813 (3.464) Remain 04:48:58 loss: 0.1885 Lr: 0.01280
[2023-08-08 05:36:30,266 INFO misc.py line 115 22900] Train: [68/100][144/156] Data 0.001 (0.001) Batch 2.998 (3.461) Remain 04:48:38 loss: 0.0813 Lr: 0.01280
[2023-08-08 05:36:33,174 INFO misc.py line 115 22900] Train: [68/100][145/156] Data 0.001 (0.001) Batch 2.908 (3.457) Remain 04:48:15 loss: 0.0958 Lr: 0.01280
[2023-08-08 05:36:36,452 INFO misc.py line 115 22900] Train: [68/100][146/156] Data 0.001 (0.001) Batch 3.277 (3.456) Remain 04:48:06 loss: 0.0637 Lr: 0.01279
[2023-08-08 05:36:39,994 INFO misc.py line 115 22900] Train: [68/100][147/156] Data 0.001 (0.001) Batch 3.542 (3.456) Remain 04:48:05 loss: 0.1884 Lr: 0.01279
[2023-08-08 05:36:43,581 INFO misc.py line 115 22900] Train: [68/100][148/156] Data 0.001 (0.001) Batch 3.587 (3.457) Remain 04:48:06 loss: 0.2878 Lr: 0.01278
[2023-08-08 05:36:47,033 INFO misc.py line 115 22900] Train: [68/100][149/156] Data 0.001 (0.001) Batch 3.451 (3.457) Remain 04:48:03 loss: 0.1974 Lr: 0.01278
[2023-08-08 05:36:49,476 INFO misc.py line 115 22900] Train: [68/100][150/156] Data 0.001 (0.001) Batch 2.444 (3.450) Remain 04:47:25 loss: 0.1578 Lr: 0.01277
[2023-08-08 05:36:53,640 INFO misc.py line 115 22900] Train: [68/100][151/156] Data 0.001 (0.001) Batch 4.164 (3.455) Remain 04:47:45 loss: 0.4043 Lr: 0.01277
[2023-08-08 05:36:56,708 INFO misc.py line 115 22900] Train: [68/100][152/156] Data 0.001 (0.001) Batch 3.067 (3.453) Remain 04:47:29 loss: 0.1876 Lr: 0.01276
[2023-08-08 05:36:59,386 INFO misc.py line 115 22900] Train: [68/100][153/156] Data 0.001 (0.001) Batch 2.678 (3.447) Remain 04:47:00 loss: 0.0945 Lr: 0.01276
[2023-08-08 05:37:02,961 INFO misc.py line 115 22900] Train: [68/100][154/156] Data 0.001 (0.001) Batch 3.575 (3.448) Remain 04:47:00 loss: 0.2546 Lr: 0.01275
[2023-08-08 05:37:07,061 INFO misc.py line 115 22900] Train: [68/100][155/156] Data 0.001 (0.001) Batch 4.100 (3.453) Remain 04:47:18 loss: 0.1462 Lr: 0.01275
[2023-08-08 05:37:11,080 INFO misc.py line 115 22900] Train: [68/100][156/156] Data 0.001 (0.001) Batch 4.019 (3.456) Remain 04:47:33 loss: 0.2069 Lr: 0.01274
[2023-08-08 05:37:11,080 INFO misc.py line 129 22900] Train result: loss: 0.1831 
[2023-08-08 05:37:11,080 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 05:37:13,204 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.1379 
[2023-08-08 05:37:14,073 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.5515 
[2023-08-08 05:37:15,735 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.7506 
[2023-08-08 05:37:17,256 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.5405 
[2023-08-08 05:37:19,100 INFO evaluator.py line 122 22900] Test: [5/24] Loss 2.1025 
[2023-08-08 05:37:20,766 INFO evaluator.py line 122 22900] Test: [6/24] Loss 1.0847 
[2023-08-08 05:37:22,902 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.8207 
[2023-08-08 05:37:24,710 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.3199 
[2023-08-08 05:37:25,994 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.6349 
[2023-08-08 05:37:28,123 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.4444 
[2023-08-08 05:37:28,648 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.7270 
[2023-08-08 05:37:30,182 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.7842 
[2023-08-08 05:37:32,892 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.4175 
[2023-08-08 05:37:34,573 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.7922 
[2023-08-08 05:37:36,596 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.3357 
[2023-08-08 05:37:39,307 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.3715 
[2023-08-08 05:37:42,013 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.8435 
[2023-08-08 05:37:43,859 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.7524 
[2023-08-08 05:37:44,609 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.0772 
[2023-08-08 05:37:45,494 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.6227 
[2023-08-08 05:37:47,754 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.5566 
[2023-08-08 05:37:49,718 INFO evaluator.py line 122 22900] Test: [22/24] Loss 2.1066 
[2023-08-08 05:37:51,564 INFO evaluator.py line 122 22900] Test: [23/24] Loss 3.4909 
[2023-08-08 05:37:53,500 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.6061 
[2023-08-08 05:37:53,551 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2353/0.3235/0.6898.
[2023-08-08 05:37:53,551 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6251/0.9585
[2023-08-08 05:37:53,551 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9358/0.9923
[2023-08-08 05:37:53,551 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1745/0.3923
[2023-08-08 05:37:53,551 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1687/0.3063
[2023-08-08 05:37:53,551 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6180/0.6852
[2023-08-08 05:37:53,551 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.3990/0.5551
[2023-08-08 05:37:53,551 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.4885/0.5982
[2023-08-08 05:37:53,551 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1940/0.2263
[2023-08-08 05:37:53,551 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1771/0.3941
[2023-08-08 05:37:53,551 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0210/0.0210
[2023-08-08 05:37:53,551 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 05:37:53,551 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0256/0.0265
[2023-08-08 05:37:53,552 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.2297/0.4959
[2023-08-08 05:37:53,552 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0929/0.1100
[2023-08-08 05:37:53,552 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0310/0.0401
[2023-08-08 05:37:53,552 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.0129/0.0144
[2023-08-08 05:37:53,552 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.1074/0.1196
[2023-08-08 05:37:53,552 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.3162/0.3893
[2023-08-08 05:37:53,552 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 05:37:53,552 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0893/0.1456
[2023-08-08 05:37:53,552 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 05:37:53,552 INFO misc.py line 152 22900] Currently Best mIoU: 0.2650
[2023-08-08 05:37:53,552 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 05:37:58,077 INFO misc.py line 115 22900] Train: [69/100][1/156] Data 0.594 (0.594) Batch 3.680 (3.680) Remain 05:06:04 loss: 0.0776 Lr: 0.01274
[2023-08-08 05:38:00,416 INFO misc.py line 115 22900] Train: [69/100][2/156] Data 0.001 (0.001) Batch 2.339 (2.339) Remain 03:14:29 loss: 0.1709 Lr: 0.01274
[2023-08-08 05:38:03,640 INFO misc.py line 115 22900] Train: [69/100][3/156] Data 0.001 (0.001) Batch 3.225 (3.225) Remain 04:28:07 loss: 0.2491 Lr: 0.01273
[2023-08-08 05:38:07,193 INFO misc.py line 115 22900] Train: [69/100][4/156] Data 0.001 (0.001) Batch 3.553 (3.553) Remain 04:55:20 loss: 0.2161 Lr: 0.01273
[2023-08-08 05:38:09,314 INFO misc.py line 115 22900] Train: [69/100][5/156] Data 0.001 (0.001) Batch 2.121 (2.837) Remain 03:55:47 loss: 0.1356 Lr: 0.01272
[2023-08-08 05:38:12,831 INFO misc.py line 115 22900] Train: [69/100][6/156] Data 0.001 (0.001) Batch 3.517 (3.064) Remain 04:14:34 loss: 0.0672 Lr: 0.01272
[2023-08-08 05:38:15,177 INFO misc.py line 115 22900] Train: [69/100][7/156] Data 0.001 (0.001) Batch 2.346 (2.884) Remain 03:59:37 loss: 0.0576 Lr: 0.01271
[2023-08-08 05:38:19,429 INFO misc.py line 115 22900] Train: [69/100][8/156] Data 0.001 (0.001) Batch 4.252 (3.158) Remain 04:22:17 loss: 0.2834 Lr: 0.01271
[2023-08-08 05:38:23,123 INFO misc.py line 115 22900] Train: [69/100][9/156] Data 0.001 (0.001) Batch 3.695 (3.247) Remain 04:29:40 loss: 0.2118 Lr: 0.01270
[2023-08-08 05:38:26,210 INFO misc.py line 115 22900] Train: [69/100][10/156] Data 0.001 (0.001) Batch 3.087 (3.224) Remain 04:27:43 loss: 0.1377 Lr: 0.01270
[2023-08-08 05:38:30,096 INFO misc.py line 115 22900] Train: [69/100][11/156] Data 0.001 (0.001) Batch 3.886 (3.307) Remain 04:34:32 loss: 0.2871 Lr: 0.01269
[2023-08-08 05:38:33,801 INFO misc.py line 115 22900] Train: [69/100][12/156] Data 0.001 (0.001) Batch 3.705 (3.351) Remain 04:38:08 loss: 0.1335 Lr: 0.01269
[2023-08-08 05:38:38,362 INFO misc.py line 115 22900] Train: [69/100][13/156] Data 0.001 (0.001) Batch 4.561 (3.472) Remain 04:48:08 loss: 0.2369 Lr: 0.01268
[2023-08-08 05:38:40,967 INFO misc.py line 115 22900] Train: [69/100][14/156] Data 0.001 (0.001) Batch 2.605 (3.393) Remain 04:41:32 loss: 0.0928 Lr: 0.01268
[2023-08-08 05:38:44,746 INFO misc.py line 115 22900] Train: [69/100][15/156] Data 0.001 (0.001) Batch 3.779 (3.425) Remain 04:44:08 loss: 0.2275 Lr: 0.01268
[2023-08-08 05:38:47,504 INFO misc.py line 115 22900] Train: [69/100][16/156] Data 0.001 (0.001) Batch 2.758 (3.374) Remain 04:39:49 loss: 0.1259 Lr: 0.01267
[2023-08-08 05:38:51,190 INFO misc.py line 115 22900] Train: [69/100][17/156] Data 0.001 (0.001) Batch 3.687 (3.396) Remain 04:41:37 loss: 0.1925 Lr: 0.01267
[2023-08-08 05:38:55,248 INFO misc.py line 115 22900] Train: [69/100][18/156] Data 0.001 (0.001) Batch 4.058 (3.441) Remain 04:45:13 loss: 0.2737 Lr: 0.01266
[2023-08-08 05:38:57,905 INFO misc.py line 115 22900] Train: [69/100][19/156] Data 0.001 (0.001) Batch 2.656 (3.392) Remain 04:41:06 loss: 0.0950 Lr: 0.01266
[2023-08-08 05:39:01,984 INFO misc.py line 115 22900] Train: [69/100][20/156] Data 0.001 (0.001) Batch 4.080 (3.432) Remain 04:44:23 loss: 0.1283 Lr: 0.01265
[2023-08-08 05:39:05,756 INFO misc.py line 115 22900] Train: [69/100][21/156] Data 0.001 (0.001) Batch 3.771 (3.451) Remain 04:45:54 loss: 0.2337 Lr: 0.01265
[2023-08-08 05:39:09,472 INFO misc.py line 115 22900] Train: [69/100][22/156] Data 0.001 (0.001) Batch 3.716 (3.465) Remain 04:47:00 loss: 0.3930 Lr: 0.01264
[2023-08-08 05:39:13,977 INFO misc.py line 115 22900] Train: [69/100][23/156] Data 0.001 (0.001) Batch 4.505 (3.517) Remain 04:51:15 loss: 0.2573 Lr: 0.01264
[2023-08-08 05:39:18,015 INFO misc.py line 115 22900] Train: [69/100][24/156] Data 0.001 (0.001) Batch 4.037 (3.542) Remain 04:53:14 loss: 0.3621 Lr: 0.01263
[2023-08-08 05:39:21,276 INFO misc.py line 115 22900] Train: [69/100][25/156] Data 0.001 (0.001) Batch 3.262 (3.529) Remain 04:52:08 loss: 0.1078 Lr: 0.01263
[2023-08-08 05:39:24,874 INFO misc.py line 115 22900] Train: [69/100][26/156] Data 0.001 (0.001) Batch 3.598 (3.532) Remain 04:52:19 loss: 0.1548 Lr: 0.01262
[2023-08-08 05:39:28,155 INFO misc.py line 115 22900] Train: [69/100][27/156] Data 0.001 (0.001) Batch 3.281 (3.521) Remain 04:51:23 loss: 0.1314 Lr: 0.01262
[2023-08-08 05:39:31,529 INFO misc.py line 115 22900] Train: [69/100][28/156] Data 0.001 (0.001) Batch 3.374 (3.516) Remain 04:50:51 loss: 0.1638 Lr: 0.01262
[2023-08-08 05:39:35,457 INFO misc.py line 115 22900] Train: [69/100][29/156] Data 0.001 (0.001) Batch 3.928 (3.531) Remain 04:52:06 loss: 0.2654 Lr: 0.01261
[2023-08-08 05:39:39,219 INFO misc.py line 115 22900] Train: [69/100][30/156] Data 0.001 (0.001) Batch 3.762 (3.540) Remain 04:52:45 loss: 0.1807 Lr: 0.01261
[2023-08-08 05:39:42,522 INFO misc.py line 115 22900] Train: [69/100][31/156] Data 0.001 (0.001) Batch 3.303 (3.532) Remain 04:51:59 loss: 0.1389 Lr: 0.01260
[2023-08-08 05:39:45,696 INFO misc.py line 115 22900] Train: [69/100][32/156] Data 0.001 (0.001) Batch 3.173 (3.519) Remain 04:50:54 loss: 0.3096 Lr: 0.01260
[2023-08-08 05:39:49,783 INFO misc.py line 115 22900] Train: [69/100][33/156] Data 0.001 (0.001) Batch 4.088 (3.538) Remain 04:52:25 loss: 0.2433 Lr: 0.01259
[2023-08-08 05:39:53,071 INFO misc.py line 115 22900] Train: [69/100][34/156] Data 0.001 (0.001) Batch 3.288 (3.530) Remain 04:51:41 loss: 0.1744 Lr: 0.01259
[2023-08-08 05:39:56,962 INFO misc.py line 115 22900] Train: [69/100][35/156] Data 0.001 (0.001) Batch 3.891 (3.541) Remain 04:52:34 loss: 0.2313 Lr: 0.01258
[2023-08-08 05:40:00,997 INFO misc.py line 115 22900] Train: [69/100][36/156] Data 0.001 (0.001) Batch 4.036 (3.556) Remain 04:53:44 loss: 0.2183 Lr: 0.01258
[2023-08-08 05:40:04,451 INFO misc.py line 115 22900] Train: [69/100][37/156] Data 0.001 (0.001) Batch 3.453 (3.553) Remain 04:53:26 loss: 0.2896 Lr: 0.01257
[2023-08-08 05:40:07,403 INFO misc.py line 115 22900] Train: [69/100][38/156] Data 0.001 (0.001) Batch 2.953 (3.536) Remain 04:51:57 loss: 0.1352 Lr: 0.01257
[2023-08-08 05:40:11,687 INFO misc.py line 115 22900] Train: [69/100][39/156] Data 0.001 (0.001) Batch 4.283 (3.557) Remain 04:53:37 loss: 0.3664 Lr: 0.01256
[2023-08-08 05:40:15,024 INFO misc.py line 115 22900] Train: [69/100][40/156] Data 0.001 (0.001) Batch 3.338 (3.551) Remain 04:53:04 loss: 0.1186 Lr: 0.01256
[2023-08-08 05:40:19,015 INFO misc.py line 115 22900] Train: [69/100][41/156] Data 0.001 (0.001) Batch 3.991 (3.563) Remain 04:53:57 loss: 0.2887 Lr: 0.01256
[2023-08-08 05:40:21,945 INFO misc.py line 115 22900] Train: [69/100][42/156] Data 0.001 (0.001) Batch 2.930 (3.546) Remain 04:52:34 loss: 0.1958 Lr: 0.01255
[2023-08-08 05:40:24,861 INFO misc.py line 115 22900] Train: [69/100][43/156] Data 0.001 (0.001) Batch 2.915 (3.531) Remain 04:51:12 loss: 0.0581 Lr: 0.01255
[2023-08-08 05:40:28,044 INFO misc.py line 115 22900] Train: [69/100][44/156] Data 0.001 (0.001) Batch 3.183 (3.522) Remain 04:50:27 loss: 0.0883 Lr: 0.01254
[2023-08-08 05:40:31,495 INFO misc.py line 115 22900] Train: [69/100][45/156] Data 0.001 (0.001) Batch 3.452 (3.520) Remain 04:50:15 loss: 0.2770 Lr: 0.01254
[2023-08-08 05:40:35,316 INFO misc.py line 115 22900] Train: [69/100][46/156] Data 0.001 (0.001) Batch 3.821 (3.527) Remain 04:50:46 loss: 0.2574 Lr: 0.01253
[2023-08-08 05:40:38,723 INFO misc.py line 115 22900] Train: [69/100][47/156] Data 0.001 (0.001) Batch 3.407 (3.525) Remain 04:50:29 loss: 0.0975 Lr: 0.01253
[2023-08-08 05:40:42,724 INFO misc.py line 115 22900] Train: [69/100][48/156] Data 0.001 (0.001) Batch 4.001 (3.535) Remain 04:51:17 loss: 0.2412 Lr: 0.01252
[2023-08-08 05:40:45,801 INFO misc.py line 115 22900] Train: [69/100][49/156] Data 0.001 (0.001) Batch 3.077 (3.525) Remain 04:50:25 loss: 0.0762 Lr: 0.01252
[2023-08-08 05:40:49,847 INFO misc.py line 115 22900] Train: [69/100][50/156] Data 0.001 (0.001) Batch 4.046 (3.536) Remain 04:51:16 loss: 0.1338 Lr: 0.01251
[2023-08-08 05:40:53,916 INFO misc.py line 115 22900] Train: [69/100][51/156] Data 0.001 (0.001) Batch 4.069 (3.547) Remain 04:52:07 loss: 0.3347 Lr: 0.01251
[2023-08-08 05:40:56,622 INFO misc.py line 115 22900] Train: [69/100][52/156] Data 0.001 (0.001) Batch 2.706 (3.530) Remain 04:50:39 loss: 0.0944 Lr: 0.01250
[2023-08-08 05:40:59,867 INFO misc.py line 115 22900] Train: [69/100][53/156] Data 0.001 (0.001) Batch 3.245 (3.525) Remain 04:50:07 loss: 0.1727 Lr: 0.01250
[2023-08-08 05:41:03,433 INFO misc.py line 115 22900] Train: [69/100][54/156] Data 0.001 (0.001) Batch 3.566 (3.525) Remain 04:50:08 loss: 0.1248 Lr: 0.01250
[2023-08-08 05:41:08,060 INFO misc.py line 115 22900] Train: [69/100][55/156] Data 0.001 (0.001) Batch 4.627 (3.547) Remain 04:51:49 loss: 0.2937 Lr: 0.01249
[2023-08-08 05:41:12,533 INFO misc.py line 115 22900] Train: [69/100][56/156] Data 0.001 (0.001) Batch 4.473 (3.564) Remain 04:53:11 loss: 0.2237 Lr: 0.01249
[2023-08-08 05:41:16,133 INFO misc.py line 115 22900] Train: [69/100][57/156] Data 0.001 (0.001) Batch 3.600 (3.565) Remain 04:53:11 loss: 0.1214 Lr: 0.01248
[2023-08-08 05:41:19,548 INFO misc.py line 115 22900] Train: [69/100][58/156] Data 0.001 (0.001) Batch 3.415 (3.562) Remain 04:52:54 loss: 0.1802 Lr: 0.01248
[2023-08-08 05:41:23,002 INFO misc.py line 115 22900] Train: [69/100][59/156] Data 0.001 (0.001) Batch 3.455 (3.560) Remain 04:52:41 loss: 0.2434 Lr: 0.01247
[2023-08-08 05:41:26,170 INFO misc.py line 115 22900] Train: [69/100][60/156] Data 0.001 (0.001) Batch 3.167 (3.553) Remain 04:52:04 loss: 0.0787 Lr: 0.01247
[2023-08-08 05:41:29,247 INFO misc.py line 115 22900] Train: [69/100][61/156] Data 0.001 (0.001) Batch 3.078 (3.545) Remain 04:51:20 loss: 0.1666 Lr: 0.01246
[2023-08-08 05:41:32,755 INFO misc.py line 115 22900] Train: [69/100][62/156] Data 0.001 (0.001) Batch 3.508 (3.544) Remain 04:51:13 loss: 0.1986 Lr: 0.01246
[2023-08-08 05:41:35,859 INFO misc.py line 115 22900] Train: [69/100][63/156] Data 0.001 (0.001) Batch 3.104 (3.537) Remain 04:50:33 loss: 0.0716 Lr: 0.01245
[2023-08-08 05:41:39,027 INFO misc.py line 115 22900] Train: [69/100][64/156] Data 0.001 (0.001) Batch 3.168 (3.531) Remain 04:50:00 loss: 0.1126 Lr: 0.01245
[2023-08-08 05:41:41,704 INFO misc.py line 115 22900] Train: [69/100][65/156] Data 0.001 (0.001) Batch 2.677 (3.517) Remain 04:48:49 loss: 0.2007 Lr: 0.01245
[2023-08-08 05:41:45,675 INFO misc.py line 115 22900] Train: [69/100][66/156] Data 0.001 (0.001) Batch 3.972 (3.524) Remain 04:49:21 loss: 0.2028 Lr: 0.01244
[2023-08-08 05:41:48,984 INFO misc.py line 115 22900] Train: [69/100][67/156] Data 0.001 (0.001) Batch 3.309 (3.521) Remain 04:49:00 loss: 0.1244 Lr: 0.01244
[2023-08-08 05:41:52,338 INFO misc.py line 115 22900] Train: [69/100][68/156] Data 0.001 (0.001) Batch 3.353 (3.518) Remain 04:48:44 loss: 0.1981 Lr: 0.01243
[2023-08-08 05:41:56,191 INFO misc.py line 115 22900] Train: [69/100][69/156] Data 0.001 (0.001) Batch 3.854 (3.524) Remain 04:49:06 loss: 0.3155 Lr: 0.01243
[2023-08-08 05:41:59,718 INFO misc.py line 115 22900] Train: [69/100][70/156] Data 0.001 (0.001) Batch 3.526 (3.524) Remain 04:49:02 loss: 0.1120 Lr: 0.01242
[2023-08-08 05:42:03,695 INFO misc.py line 115 22900] Train: [69/100][71/156] Data 0.001 (0.001) Batch 3.977 (3.530) Remain 04:49:32 loss: 0.2092 Lr: 0.01242
[2023-08-08 05:42:07,895 INFO misc.py line 115 22900] Train: [69/100][72/156] Data 0.001 (0.001) Batch 4.200 (3.540) Remain 04:50:16 loss: 0.2772 Lr: 0.01241
[2023-08-08 05:42:11,243 INFO misc.py line 115 22900] Train: [69/100][73/156] Data 0.001 (0.001) Batch 3.347 (3.537) Remain 04:49:59 loss: 0.2540 Lr: 0.01241
[2023-08-08 05:42:14,916 INFO misc.py line 115 22900] Train: [69/100][74/156] Data 0.001 (0.001) Batch 3.673 (3.539) Remain 04:50:05 loss: 0.1888 Lr: 0.01240
[2023-08-08 05:42:18,432 INFO misc.py line 115 22900] Train: [69/100][75/156] Data 0.001 (0.001) Batch 3.516 (3.539) Remain 04:50:00 loss: 0.1232 Lr: 0.01240
[2023-08-08 05:42:21,790 INFO misc.py line 115 22900] Train: [69/100][76/156] Data 0.001 (0.001) Batch 3.358 (3.536) Remain 04:49:44 loss: 0.1252 Lr: 0.01239
[2023-08-08 05:42:25,068 INFO misc.py line 115 22900] Train: [69/100][77/156] Data 0.001 (0.001) Batch 3.278 (3.533) Remain 04:49:23 loss: 0.2047 Lr: 0.01239
[2023-08-08 05:42:28,396 INFO misc.py line 115 22900] Train: [69/100][78/156] Data 0.001 (0.001) Batch 3.328 (3.530) Remain 04:49:06 loss: 0.1106 Lr: 0.01239
[2023-08-08 05:42:32,422 INFO misc.py line 115 22900] Train: [69/100][79/156] Data 0.001 (0.001) Batch 4.026 (3.537) Remain 04:49:35 loss: 0.2969 Lr: 0.01238
[2023-08-08 05:42:34,860 INFO misc.py line 115 22900] Train: [69/100][80/156] Data 0.001 (0.001) Batch 2.438 (3.522) Remain 04:48:21 loss: 0.0870 Lr: 0.01238
[2023-08-08 05:42:37,772 INFO misc.py line 115 22900] Train: [69/100][81/156] Data 0.001 (0.001) Batch 2.912 (3.515) Remain 04:47:39 loss: 0.1011 Lr: 0.01237
[2023-08-08 05:42:41,538 INFO misc.py line 115 22900] Train: [69/100][82/156] Data 0.001 (0.001) Batch 3.766 (3.518) Remain 04:47:51 loss: 0.2123 Lr: 0.01237
[2023-08-08 05:42:45,507 INFO misc.py line 115 22900] Train: [69/100][83/156] Data 0.001 (0.001) Batch 3.968 (3.523) Remain 04:48:16 loss: 0.2104 Lr: 0.01236
[2023-08-08 05:42:48,625 INFO misc.py line 115 22900] Train: [69/100][84/156] Data 0.001 (0.001) Batch 3.118 (3.518) Remain 04:47:47 loss: 0.2185 Lr: 0.01236
[2023-08-08 05:42:52,674 INFO misc.py line 115 22900] Train: [69/100][85/156] Data 0.001 (0.001) Batch 4.050 (3.525) Remain 04:48:16 loss: 0.1553 Lr: 0.01235
[2023-08-08 05:42:56,094 INFO misc.py line 115 22900] Train: [69/100][86/156] Data 0.001 (0.001) Batch 3.419 (3.524) Remain 04:48:06 loss: 0.4953 Lr: 0.01235
[2023-08-08 05:43:00,690 INFO misc.py line 115 22900] Train: [69/100][87/156] Data 0.001 (0.001) Batch 4.596 (3.536) Remain 04:49:05 loss: 0.2616 Lr: 0.01234
[2023-08-08 05:43:04,803 INFO misc.py line 115 22900] Train: [69/100][88/156] Data 0.001 (0.001) Batch 4.113 (3.543) Remain 04:49:35 loss: 0.2555 Lr: 0.01234
[2023-08-08 05:43:08,398 INFO misc.py line 115 22900] Train: [69/100][89/156] Data 0.001 (0.001) Batch 3.595 (3.544) Remain 04:49:34 loss: 0.2521 Lr: 0.01234
[2023-08-08 05:43:12,490 INFO misc.py line 115 22900] Train: [69/100][90/156] Data 0.001 (0.001) Batch 4.092 (3.550) Remain 04:50:02 loss: 0.3101 Lr: 0.01233
[2023-08-08 05:43:15,749 INFO misc.py line 115 22900] Train: [69/100][91/156] Data 0.001 (0.001) Batch 3.260 (3.547) Remain 04:49:42 loss: 0.1255 Lr: 0.01233
[2023-08-08 05:43:19,146 INFO misc.py line 115 22900] Train: [69/100][92/156] Data 0.001 (0.001) Batch 3.396 (3.545) Remain 04:49:30 loss: 0.3248 Lr: 0.01232
[2023-08-08 05:43:20,814 INFO misc.py line 115 22900] Train: [69/100][93/156] Data 0.001 (0.001) Batch 1.668 (3.524) Remain 04:47:44 loss: 0.3117 Lr: 0.01232
[2023-08-08 05:43:24,316 INFO misc.py line 115 22900] Train: [69/100][94/156] Data 0.001 (0.001) Batch 3.502 (3.524) Remain 04:47:40 loss: 0.1065 Lr: 0.01232
[2023-08-08 05:43:28,751 INFO misc.py line 115 22900] Train: [69/100][95/156] Data 0.001 (0.001) Batch 4.435 (3.534) Remain 04:48:25 loss: 0.3440 Lr: 0.01231
[2023-08-08 05:43:32,127 INFO misc.py line 115 22900] Train: [69/100][96/156] Data 0.001 (0.001) Batch 3.376 (3.532) Remain 04:48:13 loss: 0.2701 Lr: 0.01231
[2023-08-08 05:43:35,343 INFO misc.py line 115 22900] Train: [69/100][97/156] Data 0.001 (0.001) Batch 3.216 (3.529) Remain 04:47:53 loss: 0.1582 Lr: 0.01230
[2023-08-08 05:43:39,177 INFO misc.py line 115 22900] Train: [69/100][98/156] Data 0.001 (0.001) Batch 3.835 (3.532) Remain 04:48:05 loss: 0.2706 Lr: 0.01230
[2023-08-08 05:43:43,269 INFO misc.py line 115 22900] Train: [69/100][99/156] Data 0.001 (0.001) Batch 4.092 (3.538) Remain 04:48:30 loss: 0.2907 Lr: 0.01229
[2023-08-08 05:43:46,500 INFO misc.py line 115 22900] Train: [69/100][100/156] Data 0.001 (0.001) Batch 3.231 (3.535) Remain 04:48:11 loss: 0.1557 Lr: 0.01229
[2023-08-08 05:43:50,699 INFO misc.py line 115 22900] Train: [69/100][101/156] Data 0.001 (0.001) Batch 4.199 (3.541) Remain 04:48:41 loss: 0.2299 Lr: 0.01229
[2023-08-08 05:43:53,803 INFO misc.py line 115 22900] Train: [69/100][102/156] Data 0.001 (0.001) Batch 3.105 (3.537) Remain 04:48:15 loss: 0.1491 Lr: 0.01228
[2023-08-08 05:43:56,900 INFO misc.py line 115 22900] Train: [69/100][103/156] Data 0.001 (0.001) Batch 3.097 (3.533) Remain 04:47:50 loss: 0.2971 Lr: 0.01228
[2023-08-08 05:44:00,587 INFO misc.py line 115 22900] Train: [69/100][104/156] Data 0.001 (0.001) Batch 3.687 (3.534) Remain 04:47:54 loss: 0.2106 Lr: 0.01227
[2023-08-08 05:44:04,654 INFO misc.py line 115 22900] Train: [69/100][105/156] Data 0.001 (0.001) Batch 4.067 (3.539) Remain 04:48:16 loss: 0.1273 Lr: 0.01227
[2023-08-08 05:44:08,029 INFO misc.py line 115 22900] Train: [69/100][106/156] Data 0.001 (0.001) Batch 3.374 (3.538) Remain 04:48:05 loss: 0.1482 Lr: 0.01226
[2023-08-08 05:44:12,188 INFO misc.py line 115 22900] Train: [69/100][107/156] Data 0.001 (0.001) Batch 4.159 (3.544) Remain 04:48:31 loss: 0.1647 Lr: 0.01226
[2023-08-08 05:44:16,250 INFO misc.py line 115 22900] Train: [69/100][108/156] Data 0.001 (0.001) Batch 4.062 (3.549) Remain 04:48:51 loss: 0.1311 Lr: 0.01225
[2023-08-08 05:44:19,003 INFO misc.py line 115 22900] Train: [69/100][109/156] Data 0.001 (0.001) Batch 2.753 (3.541) Remain 04:48:11 loss: 0.2808 Lr: 0.01225
[2023-08-08 05:44:23,185 INFO misc.py line 115 22900] Train: [69/100][110/156] Data 0.001 (0.001) Batch 4.182 (3.547) Remain 04:48:37 loss: 0.3680 Lr: 0.01224
[2023-08-08 05:44:27,326 INFO misc.py line 115 22900] Train: [69/100][111/156] Data 0.001 (0.001) Batch 4.141 (3.553) Remain 04:49:00 loss: 0.1693 Lr: 0.01224
[2023-08-08 05:44:30,792 INFO misc.py line 115 22900] Train: [69/100][112/156] Data 0.001 (0.001) Batch 3.466 (3.552) Remain 04:48:53 loss: 0.2371 Lr: 0.01224
[2023-08-08 05:44:34,423 INFO misc.py line 115 22900] Train: [69/100][113/156] Data 0.001 (0.001) Batch 3.631 (3.553) Remain 04:48:52 loss: 0.2507 Lr: 0.01223
[2023-08-08 05:44:38,656 INFO misc.py line 115 22900] Train: [69/100][114/156] Data 0.001 (0.001) Batch 4.233 (3.559) Remain 04:49:19 loss: 0.3626 Lr: 0.01223
[2023-08-08 05:44:40,989 INFO misc.py line 115 22900] Train: [69/100][115/156] Data 0.001 (0.001) Batch 2.334 (3.548) Remain 04:48:22 loss: 0.1422 Lr: 0.01222
[2023-08-08 05:44:44,986 INFO misc.py line 115 22900] Train: [69/100][116/156] Data 0.001 (0.001) Batch 3.996 (3.552) Remain 04:48:38 loss: 0.3599 Lr: 0.01222
[2023-08-08 05:44:47,737 INFO misc.py line 115 22900] Train: [69/100][117/156] Data 0.001 (0.001) Batch 2.751 (3.545) Remain 04:48:00 loss: 0.1172 Lr: 0.01221
[2023-08-08 05:44:51,083 INFO misc.py line 115 22900] Train: [69/100][118/156] Data 0.001 (0.001) Batch 3.346 (3.543) Remain 04:47:48 loss: 0.2556 Lr: 0.01221
[2023-08-08 05:44:55,091 INFO misc.py line 115 22900] Train: [69/100][119/156] Data 0.001 (0.001) Batch 4.008 (3.547) Remain 04:48:04 loss: 0.1856 Lr: 0.01220
[2023-08-08 05:44:57,858 INFO misc.py line 115 22900] Train: [69/100][120/156] Data 0.001 (0.001) Batch 2.766 (3.540) Remain 04:47:28 loss: 0.1184 Lr: 0.01220
[2023-08-08 05:45:01,579 INFO misc.py line 115 22900] Train: [69/100][121/156] Data 0.001 (0.001) Batch 3.721 (3.542) Remain 04:47:32 loss: 0.2986 Lr: 0.01219
[2023-08-08 05:45:04,253 INFO misc.py line 115 22900] Train: [69/100][122/156] Data 0.001 (0.001) Batch 2.674 (3.535) Remain 04:46:53 loss: 0.3979 Lr: 0.01219
[2023-08-08 05:45:07,796 INFO misc.py line 115 22900] Train: [69/100][123/156] Data 0.001 (0.001) Batch 3.543 (3.535) Remain 04:46:50 loss: 0.1824 Lr: 0.01219
[2023-08-08 05:45:11,385 INFO misc.py line 115 22900] Train: [69/100][124/156] Data 0.001 (0.001) Batch 3.590 (3.535) Remain 04:46:48 loss: 0.1553 Lr: 0.01218
[2023-08-08 05:45:14,156 INFO misc.py line 115 22900] Train: [69/100][125/156] Data 0.001 (0.001) Batch 2.771 (3.529) Remain 04:46:14 loss: 0.0839 Lr: 0.01218
[2023-08-08 05:45:17,564 INFO misc.py line 115 22900] Train: [69/100][126/156] Data 0.001 (0.001) Batch 3.408 (3.528) Remain 04:46:06 loss: 0.1217 Lr: 0.01217
[2023-08-08 05:45:21,448 INFO misc.py line 115 22900] Train: [69/100][127/156] Data 0.001 (0.001) Batch 3.883 (3.531) Remain 04:46:16 loss: 0.2538 Lr: 0.01217
[2023-08-08 05:45:24,923 INFO misc.py line 115 22900] Train: [69/100][128/156] Data 0.001 (0.001) Batch 3.475 (3.530) Remain 04:46:11 loss: 0.1389 Lr: 0.01216
[2023-08-08 05:45:28,617 INFO misc.py line 115 22900] Train: [69/100][129/156] Data 0.001 (0.001) Batch 3.694 (3.532) Remain 04:46:13 loss: 0.2124 Lr: 0.01216
[2023-08-08 05:45:31,651 INFO misc.py line 115 22900] Train: [69/100][130/156] Data 0.001 (0.001) Batch 3.034 (3.528) Remain 04:45:51 loss: 0.0821 Lr: 0.01215
[2023-08-08 05:45:34,886 INFO misc.py line 115 22900] Train: [69/100][131/156] Data 0.001 (0.001) Batch 3.235 (3.525) Remain 04:45:36 loss: 0.2566 Lr: 0.01215
[2023-08-08 05:45:36,712 INFO misc.py line 115 22900] Train: [69/100][132/156] Data 0.001 (0.001) Batch 1.827 (3.512) Remain 04:44:29 loss: 0.1182 Lr: 0.01214
[2023-08-08 05:45:39,613 INFO misc.py line 115 22900] Train: [69/100][133/156] Data 0.001 (0.001) Batch 2.900 (3.507) Remain 04:44:02 loss: 0.0910 Lr: 0.01214
[2023-08-08 05:45:43,629 INFO misc.py line 115 22900] Train: [69/100][134/156] Data 0.001 (0.001) Batch 4.016 (3.511) Remain 04:44:18 loss: 0.3061 Lr: 0.01214
[2023-08-08 05:45:46,231 INFO misc.py line 115 22900] Train: [69/100][135/156] Data 0.001 (0.001) Batch 2.603 (3.504) Remain 04:43:41 loss: 0.1629 Lr: 0.01213
[2023-08-08 05:45:50,297 INFO misc.py line 115 22900] Train: [69/100][136/156] Data 0.002 (0.001) Batch 4.065 (3.509) Remain 04:43:58 loss: 0.2065 Lr: 0.01213
[2023-08-08 05:45:53,170 INFO misc.py line 115 22900] Train: [69/100][137/156] Data 0.001 (0.001) Batch 2.873 (3.504) Remain 04:43:31 loss: 0.2110 Lr: 0.01212
[2023-08-08 05:45:56,959 INFO misc.py line 115 22900] Train: [69/100][138/156] Data 0.001 (0.001) Batch 3.789 (3.506) Remain 04:43:38 loss: 0.2944 Lr: 0.01212
[2023-08-08 05:46:00,667 INFO misc.py line 115 22900] Train: [69/100][139/156] Data 0.001 (0.001) Batch 3.708 (3.508) Remain 04:43:42 loss: 0.1773 Lr: 0.01211
[2023-08-08 05:46:04,965 INFO misc.py line 115 22900] Train: [69/100][140/156] Data 0.001 (0.001) Batch 4.298 (3.513) Remain 04:44:06 loss: 0.2525 Lr: 0.01211
[2023-08-08 05:46:08,702 INFO misc.py line 115 22900] Train: [69/100][141/156] Data 0.001 (0.001) Batch 3.737 (3.515) Remain 04:44:10 loss: 0.2153 Lr: 0.01210
[2023-08-08 05:46:12,351 INFO misc.py line 115 22900] Train: [69/100][142/156] Data 0.001 (0.001) Batch 3.649 (3.516) Remain 04:44:12 loss: 0.1515 Lr: 0.01210
[2023-08-08 05:46:15,374 INFO misc.py line 115 22900] Train: [69/100][143/156] Data 0.001 (0.001) Batch 3.023 (3.512) Remain 04:43:51 loss: 0.1328 Lr: 0.01209
[2023-08-08 05:46:18,233 INFO misc.py line 115 22900] Train: [69/100][144/156] Data 0.001 (0.001) Batch 2.859 (3.508) Remain 04:43:25 loss: 0.0667 Lr: 0.01209
[2023-08-08 05:46:21,774 INFO misc.py line 115 22900] Train: [69/100][145/156] Data 0.001 (0.001) Batch 3.541 (3.508) Remain 04:43:23 loss: 0.2638 Lr: 0.01209
[2023-08-08 05:46:25,302 INFO misc.py line 115 22900] Train: [69/100][146/156] Data 0.001 (0.001) Batch 3.528 (3.508) Remain 04:43:20 loss: 0.2685 Lr: 0.01208
[2023-08-08 05:46:28,794 INFO misc.py line 115 22900] Train: [69/100][147/156] Data 0.001 (0.001) Batch 3.491 (3.508) Remain 04:43:16 loss: 0.1468 Lr: 0.01208
[2023-08-08 05:46:31,697 INFO misc.py line 115 22900] Train: [69/100][148/156] Data 0.001 (0.001) Batch 2.903 (3.504) Remain 04:42:52 loss: 0.1233 Lr: 0.01207
[2023-08-08 05:46:35,058 INFO misc.py line 115 22900] Train: [69/100][149/156] Data 0.001 (0.001) Batch 3.361 (3.503) Remain 04:42:44 loss: 0.2899 Lr: 0.01207
[2023-08-08 05:46:38,009 INFO misc.py line 115 22900] Train: [69/100][150/156] Data 0.001 (0.001) Batch 2.950 (3.499) Remain 04:42:22 loss: 0.1951 Lr: 0.01206
[2023-08-08 05:46:41,444 INFO misc.py line 115 22900] Train: [69/100][151/156] Data 0.001 (0.001) Batch 3.436 (3.499) Remain 04:42:17 loss: 0.1644 Lr: 0.01206
[2023-08-08 05:46:44,498 INFO misc.py line 115 22900] Train: [69/100][152/156] Data 0.001 (0.001) Batch 3.054 (3.496) Remain 04:41:59 loss: 0.1393 Lr: 0.01205
[2023-08-08 05:46:47,794 INFO misc.py line 115 22900] Train: [69/100][153/156] Data 0.001 (0.001) Batch 3.296 (3.494) Remain 04:41:49 loss: 0.2313 Lr: 0.01205
[2023-08-08 05:46:50,899 INFO misc.py line 115 22900] Train: [69/100][154/156] Data 0.001 (0.001) Batch 3.105 (3.492) Remain 04:41:33 loss: 0.1646 Lr: 0.01204
[2023-08-08 05:46:53,515 INFO misc.py line 115 22900] Train: [69/100][155/156] Data 0.001 (0.001) Batch 2.615 (3.486) Remain 04:41:01 loss: 0.0969 Lr: 0.01204
[2023-08-08 05:46:56,906 INFO misc.py line 115 22900] Train: [69/100][156/156] Data 0.001 (0.001) Batch 3.392 (3.485) Remain 04:40:55 loss: 0.2744 Lr: 0.01204
[2023-08-08 05:46:56,907 INFO misc.py line 129 22900] Train result: loss: 0.1994 
[2023-08-08 05:46:56,907 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 05:46:59,015 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.3686 
[2023-08-08 05:46:59,885 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.5476 
[2023-08-08 05:47:01,550 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.6422 
[2023-08-08 05:47:03,071 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.5607 
[2023-08-08 05:47:04,917 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.7609 
[2023-08-08 05:47:06,579 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.4538 
[2023-08-08 05:47:08,720 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.2941 
[2023-08-08 05:47:10,525 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.5468 
[2023-08-08 05:47:11,808 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.6087 
[2023-08-08 05:47:13,939 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.3325 
[2023-08-08 05:47:14,464 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.1357 
[2023-08-08 05:47:15,998 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.7418 
[2023-08-08 05:47:18,709 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.0963 
[2023-08-08 05:47:20,388 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.6939 
[2023-08-08 05:47:22,411 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.2859 
[2023-08-08 05:47:25,119 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.0985 
[2023-08-08 05:47:27,825 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.4593 
[2023-08-08 05:47:29,673 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.5783 
[2023-08-08 05:47:30,421 INFO evaluator.py line 122 22900] Test: [19/24] Loss 0.9121 
[2023-08-08 05:47:31,306 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.6492 
[2023-08-08 05:47:33,568 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.3002 
[2023-08-08 05:47:35,533 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.7639 
[2023-08-08 05:47:37,380 INFO evaluator.py line 122 22900] Test: [23/24] Loss 2.8919 
[2023-08-08 05:47:39,318 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.9715 
[2023-08-08 05:47:39,368 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2580/0.3568/0.7054.
[2023-08-08 05:47:39,368 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6589/0.9579
[2023-08-08 05:47:39,368 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9509/0.9888
[2023-08-08 05:47:39,368 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.2070/0.5360
[2023-08-08 05:47:39,368 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.2101/0.2544
[2023-08-08 05:47:39,368 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6530/0.7639
[2023-08-08 05:47:39,368 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.3568/0.5159
[2023-08-08 05:47:39,368 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5834/0.7552
[2023-08-08 05:47:39,368 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1736/0.1808
[2023-08-08 05:47:39,368 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1735/0.4699
[2023-08-08 05:47:39,368 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0062/0.0062
[2023-08-08 05:47:39,368 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 05:47:39,368 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0438/0.0519
[2023-08-08 05:47:39,368 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.2729/0.4371
[2023-08-08 05:47:39,368 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0381/0.0427
[2023-08-08 05:47:39,368 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0486/0.0560
[2023-08-08 05:47:39,368 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.2074/0.2281
[2023-08-08 05:47:39,368 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.1350/0.1804
[2023-08-08 05:47:39,368 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.3374/0.5155
[2023-08-08 05:47:39,368 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 05:47:39,368 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.1038/0.1952
[2023-08-08 05:47:39,369 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 05:47:39,369 INFO misc.py line 152 22900] Currently Best mIoU: 0.2650
[2023-08-08 05:47:39,369 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 05:47:46,147 INFO misc.py line 115 22900] Train: [70/100][1/156] Data 1.737 (1.737) Batch 5.999 (5.999) Remain 08:03:25 loss: 0.1650 Lr: 0.01203
[2023-08-08 05:47:50,079 INFO misc.py line 115 22900] Train: [70/100][2/156] Data 0.001 (0.001) Batch 3.932 (3.932) Remain 05:16:47 loss: 0.2123 Lr: 0.01203
[2023-08-08 05:47:53,079 INFO misc.py line 115 22900] Train: [70/100][3/156] Data 0.001 (0.001) Batch 3.000 (3.000) Remain 04:01:38 loss: 0.1965 Lr: 0.01202
[2023-08-08 05:47:56,613 INFO misc.py line 115 22900] Train: [70/100][4/156] Data 0.001 (0.001) Batch 3.535 (3.535) Remain 04:44:40 loss: 0.2325 Lr: 0.01202
[2023-08-08 05:48:00,150 INFO misc.py line 115 22900] Train: [70/100][5/156] Data 0.001 (0.001) Batch 3.537 (3.536) Remain 04:44:40 loss: 0.1537 Lr: 0.01201
[2023-08-08 05:48:03,417 INFO misc.py line 115 22900] Train: [70/100][6/156] Data 0.001 (0.001) Batch 3.267 (3.446) Remain 04:37:24 loss: 0.2313 Lr: 0.01201
[2023-08-08 05:48:07,465 INFO misc.py line 115 22900] Train: [70/100][7/156] Data 0.001 (0.001) Batch 4.048 (3.597) Remain 04:49:27 loss: 0.3657 Lr: 0.01200
[2023-08-08 05:48:11,614 INFO misc.py line 115 22900] Train: [70/100][8/156] Data 0.001 (0.001) Batch 4.149 (3.707) Remain 04:58:17 loss: 0.1716 Lr: 0.01200
[2023-08-08 05:48:14,553 INFO misc.py line 115 22900] Train: [70/100][9/156] Data 0.001 (0.001) Batch 2.938 (3.579) Remain 04:47:55 loss: 0.2158 Lr: 0.01199
[2023-08-08 05:48:17,886 INFO misc.py line 115 22900] Train: [70/100][10/156] Data 0.001 (0.001) Batch 3.333 (3.544) Remain 04:45:02 loss: 0.1828 Lr: 0.01199
[2023-08-08 05:48:21,291 INFO misc.py line 115 22900] Train: [70/100][11/156] Data 0.001 (0.001) Batch 3.404 (3.526) Remain 04:43:35 loss: 0.1091 Lr: 0.01199
[2023-08-08 05:48:25,429 INFO misc.py line 115 22900] Train: [70/100][12/156] Data 0.001 (0.001) Batch 4.138 (3.594) Remain 04:48:59 loss: 0.3070 Lr: 0.01198
[2023-08-08 05:48:28,728 INFO misc.py line 115 22900] Train: [70/100][13/156] Data 0.001 (0.001) Batch 3.299 (3.565) Remain 04:46:33 loss: 0.0784 Lr: 0.01198
[2023-08-08 05:48:31,241 INFO misc.py line 115 22900] Train: [70/100][14/156] Data 0.001 (0.001) Batch 2.513 (3.469) Remain 04:38:48 loss: 0.0961 Lr: 0.01197
[2023-08-08 05:48:35,520 INFO misc.py line 115 22900] Train: [70/100][15/156] Data 0.001 (0.001) Batch 4.279 (3.537) Remain 04:44:10 loss: 0.3741 Lr: 0.01197
[2023-08-08 05:48:38,704 INFO misc.py line 115 22900] Train: [70/100][16/156] Data 0.001 (0.001) Batch 3.184 (3.510) Remain 04:41:56 loss: 0.1406 Lr: 0.01196
[2023-08-08 05:48:41,292 INFO misc.py line 115 22900] Train: [70/100][17/156] Data 0.001 (0.001) Batch 2.588 (3.444) Remain 04:36:35 loss: 0.1622 Lr: 0.01196
[2023-08-08 05:48:45,145 INFO misc.py line 115 22900] Train: [70/100][18/156] Data 0.001 (0.001) Batch 3.854 (3.471) Remain 04:38:43 loss: 0.2065 Lr: 0.01195
[2023-08-08 05:48:49,532 INFO misc.py line 115 22900] Train: [70/100][19/156] Data 0.001 (0.001) Batch 4.387 (3.528) Remain 04:43:15 loss: 0.4323 Lr: 0.01195
[2023-08-08 05:48:52,908 INFO misc.py line 115 22900] Train: [70/100][20/156] Data 0.001 (0.001) Batch 3.376 (3.519) Remain 04:42:29 loss: 0.2106 Lr: 0.01194
[2023-08-08 05:48:55,776 INFO misc.py line 115 22900] Train: [70/100][21/156] Data 0.001 (0.001) Batch 2.868 (3.483) Remain 04:39:31 loss: 0.0958 Lr: 0.01194
[2023-08-08 05:48:58,958 INFO misc.py line 115 22900] Train: [70/100][22/156] Data 0.001 (0.001) Batch 3.182 (3.467) Remain 04:38:11 loss: 0.1956 Lr: 0.01194
[2023-08-08 05:49:02,006 INFO misc.py line 115 22900] Train: [70/100][23/156] Data 0.001 (0.001) Batch 3.048 (3.446) Remain 04:36:27 loss: 0.0792 Lr: 0.01193
[2023-08-08 05:49:05,262 INFO misc.py line 115 22900] Train: [70/100][24/156] Data 0.001 (0.001) Batch 3.256 (3.437) Remain 04:35:40 loss: 0.0697 Lr: 0.01193
[2023-08-08 05:49:09,110 INFO misc.py line 115 22900] Train: [70/100][25/156] Data 0.001 (0.001) Batch 3.849 (3.456) Remain 04:37:06 loss: 0.2103 Lr: 0.01192
[2023-08-08 05:49:12,717 INFO misc.py line 115 22900] Train: [70/100][26/156] Data 0.001 (0.001) Batch 3.607 (3.463) Remain 04:37:34 loss: 0.2145 Lr: 0.01192
[2023-08-08 05:49:16,130 INFO misc.py line 115 22900] Train: [70/100][27/156] Data 0.001 (0.001) Batch 3.413 (3.460) Remain 04:37:21 loss: 0.1555 Lr: 0.01191
[2023-08-08 05:49:19,297 INFO misc.py line 115 22900] Train: [70/100][28/156] Data 0.001 (0.001) Batch 3.168 (3.449) Remain 04:36:21 loss: 0.2171 Lr: 0.01191
[2023-08-08 05:49:23,439 INFO misc.py line 115 22900] Train: [70/100][29/156] Data 0.001 (0.001) Batch 4.142 (3.475) Remain 04:38:26 loss: 0.2096 Lr: 0.01190
[2023-08-08 05:49:26,737 INFO misc.py line 115 22900] Train: [70/100][30/156] Data 0.002 (0.001) Batch 3.298 (3.469) Remain 04:37:51 loss: 0.3003 Lr: 0.01190
[2023-08-08 05:49:28,911 INFO misc.py line 115 22900] Train: [70/100][31/156] Data 0.001 (0.001) Batch 2.174 (3.423) Remain 04:34:05 loss: 0.1762 Lr: 0.01190
[2023-08-08 05:49:32,580 INFO misc.py line 115 22900] Train: [70/100][32/156] Data 0.001 (0.001) Batch 3.669 (3.431) Remain 04:34:42 loss: 0.1164 Lr: 0.01189
[2023-08-08 05:49:35,872 INFO misc.py line 115 22900] Train: [70/100][33/156] Data 0.001 (0.001) Batch 3.292 (3.426) Remain 04:34:17 loss: 0.1142 Lr: 0.01189
[2023-08-08 05:49:39,196 INFO misc.py line 115 22900] Train: [70/100][34/156] Data 0.001 (0.001) Batch 3.324 (3.423) Remain 04:33:57 loss: 0.1320 Lr: 0.01188
[2023-08-08 05:49:41,876 INFO misc.py line 115 22900] Train: [70/100][35/156] Data 0.001 (0.001) Batch 2.680 (3.400) Remain 04:32:02 loss: 0.1335 Lr: 0.01188
[2023-08-08 05:49:45,924 INFO misc.py line 115 22900] Train: [70/100][36/156] Data 0.001 (0.001) Batch 4.049 (3.420) Remain 04:33:33 loss: 0.2974 Lr: 0.01187
[2023-08-08 05:49:49,570 INFO misc.py line 115 22900] Train: [70/100][37/156] Data 0.001 (0.001) Batch 3.646 (3.426) Remain 04:34:02 loss: 0.1097 Lr: 0.01187
[2023-08-08 05:49:53,211 INFO misc.py line 115 22900] Train: [70/100][38/156] Data 0.001 (0.001) Batch 3.641 (3.432) Remain 04:34:28 loss: 0.2258 Lr: 0.01186
[2023-08-08 05:49:57,209 INFO misc.py line 115 22900] Train: [70/100][39/156] Data 0.001 (0.001) Batch 3.999 (3.448) Remain 04:35:40 loss: 0.2682 Lr: 0.01186
[2023-08-08 05:50:00,291 INFO misc.py line 115 22900] Train: [70/100][40/156] Data 0.001 (0.001) Batch 3.082 (3.438) Remain 04:34:49 loss: 0.1124 Lr: 0.01185
[2023-08-08 05:50:03,587 INFO misc.py line 115 22900] Train: [70/100][41/156] Data 0.001 (0.001) Batch 3.295 (3.434) Remain 04:34:28 loss: 0.0765 Lr: 0.01185
[2023-08-08 05:50:06,356 INFO misc.py line 115 22900] Train: [70/100][42/156] Data 0.001 (0.001) Batch 2.769 (3.417) Remain 04:33:02 loss: 0.2132 Lr: 0.01185
[2023-08-08 05:50:09,858 INFO misc.py line 115 22900] Train: [70/100][43/156] Data 0.001 (0.001) Batch 3.502 (3.419) Remain 04:33:09 loss: 0.1228 Lr: 0.01184
[2023-08-08 05:50:13,603 INFO misc.py line 115 22900] Train: [70/100][44/156] Data 0.001 (0.001) Batch 3.746 (3.427) Remain 04:33:44 loss: 0.1749 Lr: 0.01184
[2023-08-08 05:50:16,816 INFO misc.py line 115 22900] Train: [70/100][45/156] Data 0.001 (0.001) Batch 3.213 (3.422) Remain 04:33:16 loss: 0.2342 Lr: 0.01183
[2023-08-08 05:50:20,827 INFO misc.py line 115 22900] Train: [70/100][46/156] Data 0.001 (0.001) Batch 4.011 (3.436) Remain 04:34:18 loss: 0.5265 Lr: 0.01183
[2023-08-08 05:50:24,695 INFO misc.py line 115 22900] Train: [70/100][47/156] Data 0.001 (0.001) Batch 3.867 (3.446) Remain 04:35:02 loss: 0.3093 Lr: 0.01182
[2023-08-08 05:50:28,218 INFO misc.py line 115 22900] Train: [70/100][48/156] Data 0.001 (0.001) Batch 3.524 (3.448) Remain 04:35:06 loss: 0.2188 Lr: 0.01182
[2023-08-08 05:50:31,560 INFO misc.py line 115 22900] Train: [70/100][49/156] Data 0.001 (0.001) Batch 3.342 (3.445) Remain 04:34:52 loss: 0.1795 Lr: 0.01181
[2023-08-08 05:50:35,891 INFO misc.py line 115 22900] Train: [70/100][50/156] Data 0.001 (0.001) Batch 4.331 (3.464) Remain 04:36:19 loss: 0.3443 Lr: 0.01181
[2023-08-08 05:50:39,883 INFO misc.py line 115 22900] Train: [70/100][51/156] Data 0.001 (0.001) Batch 3.992 (3.475) Remain 04:37:08 loss: 0.3123 Lr: 0.01180
[2023-08-08 05:50:43,663 INFO misc.py line 115 22900] Train: [70/100][52/156] Data 0.001 (0.001) Batch 3.780 (3.481) Remain 04:37:34 loss: 0.1284 Lr: 0.01180
[2023-08-08 05:50:47,246 INFO misc.py line 115 22900] Train: [70/100][53/156] Data 0.001 (0.001) Batch 3.583 (3.483) Remain 04:37:40 loss: 0.0801 Lr: 0.01180
[2023-08-08 05:50:49,908 INFO misc.py line 115 22900] Train: [70/100][54/156] Data 0.001 (0.001) Batch 2.661 (3.467) Remain 04:36:20 loss: 0.0649 Lr: 0.01179
[2023-08-08 05:50:54,026 INFO misc.py line 115 22900] Train: [70/100][55/156] Data 0.001 (0.001) Batch 4.119 (3.480) Remain 04:37:16 loss: 0.1232 Lr: 0.01179
[2023-08-08 05:50:57,282 INFO misc.py line 115 22900] Train: [70/100][56/156] Data 0.001 (0.001) Batch 3.256 (3.476) Remain 04:36:53 loss: 0.2078 Lr: 0.01178
[2023-08-08 05:50:59,733 INFO misc.py line 115 22900] Train: [70/100][57/156] Data 0.001 (0.001) Batch 2.451 (3.457) Remain 04:35:18 loss: 0.1616 Lr: 0.01178
[2023-08-08 05:51:03,527 INFO misc.py line 115 22900] Train: [70/100][58/156] Data 0.001 (0.001) Batch 3.793 (3.463) Remain 04:35:44 loss: 0.1898 Lr: 0.01177
[2023-08-08 05:51:06,442 INFO misc.py line 115 22900] Train: [70/100][59/156] Data 0.001 (0.001) Batch 2.915 (3.453) Remain 04:34:54 loss: 0.0916 Lr: 0.01177
[2023-08-08 05:51:09,496 INFO misc.py line 115 22900] Train: [70/100][60/156] Data 0.001 (0.001) Batch 3.054 (3.446) Remain 04:34:17 loss: 0.2229 Lr: 0.01176
[2023-08-08 05:51:12,824 INFO misc.py line 115 22900] Train: [70/100][61/156] Data 0.001 (0.001) Batch 3.328 (3.444) Remain 04:34:04 loss: 0.1516 Lr: 0.01176
[2023-08-08 05:51:16,277 INFO misc.py line 115 22900] Train: [70/100][62/156] Data 0.001 (0.001) Batch 3.453 (3.444) Remain 04:34:01 loss: 0.1380 Lr: 0.01176
[2023-08-08 05:51:20,363 INFO misc.py line 115 22900] Train: [70/100][63/156] Data 0.001 (0.001) Batch 4.086 (3.455) Remain 04:34:49 loss: 0.3143 Lr: 0.01175
[2023-08-08 05:51:23,904 INFO misc.py line 115 22900] Train: [70/100][64/156] Data 0.001 (0.001) Batch 3.540 (3.456) Remain 04:34:52 loss: 0.1325 Lr: 0.01175
[2023-08-08 05:51:27,075 INFO misc.py line 115 22900] Train: [70/100][65/156] Data 0.001 (0.001) Batch 3.172 (3.452) Remain 04:34:27 loss: 0.1172 Lr: 0.01174
[2023-08-08 05:51:30,685 INFO misc.py line 115 22900] Train: [70/100][66/156] Data 0.001 (0.001) Batch 3.610 (3.454) Remain 04:34:35 loss: 0.0843 Lr: 0.01174
[2023-08-08 05:51:33,985 INFO misc.py line 115 22900] Train: [70/100][67/156] Data 0.001 (0.001) Batch 3.300 (3.452) Remain 04:34:20 loss: 0.3101 Lr: 0.01173
[2023-08-08 05:51:36,621 INFO misc.py line 115 22900] Train: [70/100][68/156] Data 0.001 (0.001) Batch 2.636 (3.439) Remain 04:33:17 loss: 0.1151 Lr: 0.01173
[2023-08-08 05:51:38,178 INFO misc.py line 115 22900] Train: [70/100][69/156] Data 0.001 (0.001) Batch 1.556 (3.411) Remain 04:30:58 loss: 0.0519 Lr: 0.01172
[2023-08-08 05:51:42,085 INFO misc.py line 115 22900] Train: [70/100][70/156] Data 0.001 (0.001) Batch 3.907 (3.418) Remain 04:31:30 loss: 0.2671 Lr: 0.01172
[2023-08-08 05:51:46,064 INFO misc.py line 115 22900] Train: [70/100][71/156] Data 0.001 (0.001) Batch 3.979 (3.426) Remain 04:32:06 loss: 0.1817 Lr: 0.01171
[2023-08-08 05:51:48,676 INFO misc.py line 115 22900] Train: [70/100][72/156] Data 0.001 (0.001) Batch 2.611 (3.414) Remain 04:31:06 loss: 0.0910 Lr: 0.01171
[2023-08-08 05:51:52,225 INFO misc.py line 115 22900] Train: [70/100][73/156] Data 0.001 (0.001) Batch 3.549 (3.416) Remain 04:31:12 loss: 0.0777 Lr: 0.01171
[2023-08-08 05:51:54,530 INFO misc.py line 115 22900] Train: [70/100][74/156] Data 0.001 (0.001) Batch 2.306 (3.401) Remain 04:29:54 loss: 0.0954 Lr: 0.01170
[2023-08-08 05:51:58,399 INFO misc.py line 115 22900] Train: [70/100][75/156] Data 0.001 (0.001) Batch 3.868 (3.407) Remain 04:30:21 loss: 0.2684 Lr: 0.01170
[2023-08-08 05:52:00,773 INFO misc.py line 115 22900] Train: [70/100][76/156] Data 0.001 (0.001) Batch 2.374 (3.393) Remain 04:29:11 loss: 0.1193 Lr: 0.01169
[2023-08-08 05:52:03,735 INFO misc.py line 115 22900] Train: [70/100][77/156] Data 0.001 (0.001) Batch 2.962 (3.387) Remain 04:28:39 loss: 0.0709 Lr: 0.01169
[2023-08-08 05:52:06,946 INFO misc.py line 115 22900] Train: [70/100][78/156] Data 0.001 (0.001) Batch 3.210 (3.385) Remain 04:28:25 loss: 0.2152 Lr: 0.01168
[2023-08-08 05:52:10,942 INFO misc.py line 115 22900] Train: [70/100][79/156] Data 0.001 (0.001) Batch 3.996 (3.393) Remain 04:29:00 loss: 0.3018 Lr: 0.01168
[2023-08-08 05:52:14,975 INFO misc.py line 115 22900] Train: [70/100][80/156] Data 0.001 (0.001) Batch 4.033 (3.401) Remain 04:29:36 loss: 0.1556 Lr: 0.01167
[2023-08-08 05:52:17,201 INFO misc.py line 115 22900] Train: [70/100][81/156] Data 0.001 (0.001) Batch 2.226 (3.386) Remain 04:28:21 loss: 0.1181 Lr: 0.01167
[2023-08-08 05:52:21,043 INFO misc.py line 115 22900] Train: [70/100][82/156] Data 0.001 (0.001) Batch 3.842 (3.392) Remain 04:28:45 loss: 0.1472 Lr: 0.01167
[2023-08-08 05:52:23,808 INFO misc.py line 115 22900] Train: [70/100][83/156] Data 0.001 (0.001) Batch 2.764 (3.384) Remain 04:28:04 loss: 0.1429 Lr: 0.01166
[2023-08-08 05:52:26,724 INFO misc.py line 115 22900] Train: [70/100][84/156] Data 0.001 (0.001) Batch 2.916 (3.378) Remain 04:27:33 loss: 0.1386 Lr: 0.01166
[2023-08-08 05:52:29,966 INFO misc.py line 115 22900] Train: [70/100][85/156] Data 0.001 (0.001) Batch 3.242 (3.377) Remain 04:27:22 loss: 0.2070 Lr: 0.01165
[2023-08-08 05:52:33,664 INFO misc.py line 115 22900] Train: [70/100][86/156] Data 0.001 (0.001) Batch 3.698 (3.381) Remain 04:27:37 loss: 0.2526 Lr: 0.01165
[2023-08-08 05:52:36,890 INFO misc.py line 115 22900] Train: [70/100][87/156] Data 0.001 (0.001) Batch 3.225 (3.379) Remain 04:27:25 loss: 0.0674 Lr: 0.01164
[2023-08-08 05:52:39,881 INFO misc.py line 115 22900] Train: [70/100][88/156] Data 0.001 (0.001) Batch 2.992 (3.374) Remain 04:27:00 loss: 0.0753 Lr: 0.01164
[2023-08-08 05:52:43,609 INFO misc.py line 115 22900] Train: [70/100][89/156] Data 0.001 (0.001) Batch 3.727 (3.378) Remain 04:27:16 loss: 0.3850 Lr: 0.01163
[2023-08-08 05:52:46,960 INFO misc.py line 115 22900] Train: [70/100][90/156] Data 0.001 (0.001) Batch 3.351 (3.378) Remain 04:27:11 loss: 0.1828 Lr: 0.01163
[2023-08-08 05:52:49,036 INFO misc.py line 115 22900] Train: [70/100][91/156] Data 0.001 (0.001) Batch 2.077 (3.363) Remain 04:25:58 loss: 0.0818 Lr: 0.01163
[2023-08-08 05:52:52,827 INFO misc.py line 115 22900] Train: [70/100][92/156] Data 0.001 (0.001) Batch 3.790 (3.368) Remain 04:26:17 loss: 0.2515 Lr: 0.01162
[2023-08-08 05:52:56,279 INFO misc.py line 115 22900] Train: [70/100][93/156] Data 0.001 (0.001) Batch 3.452 (3.369) Remain 04:26:18 loss: 0.0967 Lr: 0.01162
[2023-08-08 05:53:00,248 INFO misc.py line 115 22900] Train: [70/100][94/156] Data 0.001 (0.001) Batch 3.969 (3.375) Remain 04:26:46 loss: 0.1079 Lr: 0.01161
[2023-08-08 05:53:03,820 INFO misc.py line 115 22900] Train: [70/100][95/156] Data 0.001 (0.001) Batch 3.572 (3.378) Remain 04:26:53 loss: 0.1026 Lr: 0.01161
[2023-08-08 05:53:07,238 INFO misc.py line 115 22900] Train: [70/100][96/156] Data 0.001 (0.001) Batch 3.418 (3.378) Remain 04:26:51 loss: 0.1347 Lr: 0.01160
[2023-08-08 05:53:11,339 INFO misc.py line 115 22900] Train: [70/100][97/156] Data 0.001 (0.001) Batch 4.100 (3.386) Remain 04:27:25 loss: 0.1938 Lr: 0.01160
[2023-08-08 05:53:15,507 INFO misc.py line 115 22900] Train: [70/100][98/156] Data 0.001 (0.001) Batch 4.168 (3.394) Remain 04:28:00 loss: 0.2792 Lr: 0.01159
[2023-08-08 05:53:19,732 INFO misc.py line 115 22900] Train: [70/100][99/156] Data 0.001 (0.001) Batch 4.226 (3.403) Remain 04:28:38 loss: 0.2985 Lr: 0.01159
[2023-08-08 05:53:23,735 INFO misc.py line 115 22900] Train: [70/100][100/156] Data 0.001 (0.001) Batch 4.003 (3.409) Remain 04:29:04 loss: 0.2551 Lr: 0.01159
[2023-08-08 05:53:27,908 INFO misc.py line 115 22900] Train: [70/100][101/156] Data 0.001 (0.001) Batch 4.173 (3.417) Remain 04:29:37 loss: 0.2958 Lr: 0.01158
[2023-08-08 05:53:31,690 INFO misc.py line 115 22900] Train: [70/100][102/156] Data 0.001 (0.001) Batch 3.781 (3.420) Remain 04:29:51 loss: 0.0928 Lr: 0.01158
[2023-08-08 05:53:34,180 INFO misc.py line 115 22900] Train: [70/100][103/156] Data 0.001 (0.001) Batch 2.490 (3.411) Remain 04:29:04 loss: 0.1945 Lr: 0.01157
[2023-08-08 05:53:38,021 INFO misc.py line 115 22900] Train: [70/100][104/156] Data 0.001 (0.001) Batch 3.841 (3.415) Remain 04:29:21 loss: 0.2882 Lr: 0.01157
[2023-08-08 05:53:41,380 INFO misc.py line 115 22900] Train: [70/100][105/156] Data 0.001 (0.001) Batch 3.359 (3.415) Remain 04:29:15 loss: 0.1939 Lr: 0.01156
[2023-08-08 05:53:44,189 INFO misc.py line 115 22900] Train: [70/100][106/156] Data 0.001 (0.001) Batch 2.810 (3.409) Remain 04:28:43 loss: 0.1239 Lr: 0.01156
[2023-08-08 05:53:47,727 INFO misc.py line 115 22900] Train: [70/100][107/156] Data 0.001 (0.001) Batch 3.537 (3.410) Remain 04:28:46 loss: 0.1730 Lr: 0.01155
[2023-08-08 05:53:51,833 INFO misc.py line 115 22900] Train: [70/100][108/156] Data 0.001 (0.001) Batch 4.106 (3.417) Remain 04:29:14 loss: 0.1795 Lr: 0.01155
[2023-08-08 05:53:56,152 INFO misc.py line 115 22900] Train: [70/100][109/156] Data 0.001 (0.001) Batch 4.319 (3.425) Remain 04:29:51 loss: 0.1562 Lr: 0.01154
[2023-08-08 05:53:59,214 INFO misc.py line 115 22900] Train: [70/100][110/156] Data 0.001 (0.001) Batch 3.062 (3.422) Remain 04:29:31 loss: 0.2951 Lr: 0.01154
[2023-08-08 05:54:03,237 INFO misc.py line 115 22900] Train: [70/100][111/156] Data 0.001 (0.001) Batch 4.024 (3.427) Remain 04:29:54 loss: 0.2096 Lr: 0.01154
[2023-08-08 05:54:06,771 INFO misc.py line 115 22900] Train: [70/100][112/156] Data 0.001 (0.001) Batch 3.534 (3.428) Remain 04:29:55 loss: 0.3378 Lr: 0.01153
[2023-08-08 05:54:10,790 INFO misc.py line 115 22900] Train: [70/100][113/156] Data 0.001 (0.001) Batch 4.019 (3.434) Remain 04:30:17 loss: 0.3338 Lr: 0.01153
[2023-08-08 05:54:14,486 INFO misc.py line 115 22900] Train: [70/100][114/156] Data 0.001 (0.001) Batch 3.696 (3.436) Remain 04:30:25 loss: 0.1219 Lr: 0.01152
[2023-08-08 05:54:18,264 INFO misc.py line 115 22900] Train: [70/100][115/156] Data 0.001 (0.001) Batch 3.779 (3.439) Remain 04:30:36 loss: 0.2367 Lr: 0.01152
[2023-08-08 05:54:20,920 INFO misc.py line 115 22900] Train: [70/100][116/156] Data 0.001 (0.001) Batch 2.655 (3.432) Remain 04:30:00 loss: 0.1158 Lr: 0.01151
[2023-08-08 05:54:24,901 INFO misc.py line 115 22900] Train: [70/100][117/156] Data 0.001 (0.001) Batch 3.982 (3.437) Remain 04:30:19 loss: 0.2487 Lr: 0.01151
[2023-08-08 05:54:28,017 INFO misc.py line 115 22900] Train: [70/100][118/156] Data 0.001 (0.001) Batch 3.116 (3.434) Remain 04:30:02 loss: 0.1229 Lr: 0.01150
[2023-08-08 05:54:30,476 INFO misc.py line 115 22900] Train: [70/100][119/156] Data 0.001 (0.001) Batch 2.458 (3.426) Remain 04:29:19 loss: 0.1009 Lr: 0.01150
[2023-08-08 05:54:34,162 INFO misc.py line 115 22900] Train: [70/100][120/156] Data 0.001 (0.001) Batch 3.686 (3.428) Remain 04:29:26 loss: 0.1141 Lr: 0.01150
[2023-08-08 05:54:38,085 INFO misc.py line 115 22900] Train: [70/100][121/156] Data 0.001 (0.001) Batch 3.923 (3.432) Remain 04:29:43 loss: 0.0860 Lr: 0.01149
[2023-08-08 05:54:42,165 INFO misc.py line 115 22900] Train: [70/100][122/156] Data 0.001 (0.001) Batch 4.080 (3.438) Remain 04:30:05 loss: 0.2356 Lr: 0.01149
[2023-08-08 05:54:45,429 INFO misc.py line 115 22900] Train: [70/100][123/156] Data 0.001 (0.001) Batch 3.264 (3.436) Remain 04:29:55 loss: 0.0987 Lr: 0.01148
[2023-08-08 05:54:47,841 INFO misc.py line 115 22900] Train: [70/100][124/156] Data 0.001 (0.001) Batch 2.412 (3.428) Remain 04:29:11 loss: 0.0997 Lr: 0.01148
[2023-08-08 05:54:50,280 INFO misc.py line 115 22900] Train: [70/100][125/156] Data 0.001 (0.001) Batch 2.439 (3.420) Remain 04:28:30 loss: 0.1171 Lr: 0.01147
[2023-08-08 05:54:54,319 INFO misc.py line 115 22900] Train: [70/100][126/156] Data 0.001 (0.001) Batch 4.039 (3.425) Remain 04:28:50 loss: 0.3490 Lr: 0.01147
[2023-08-08 05:54:57,234 INFO misc.py line 115 22900] Train: [70/100][127/156] Data 0.001 (0.001) Batch 2.915 (3.421) Remain 04:28:27 loss: 0.0972 Lr: 0.01146
[2023-08-08 05:55:00,351 INFO misc.py line 115 22900] Train: [70/100][128/156] Data 0.001 (0.001) Batch 3.117 (3.418) Remain 04:28:12 loss: 0.1116 Lr: 0.01146
[2023-08-08 05:55:04,090 INFO misc.py line 115 22900] Train: [70/100][129/156] Data 0.001 (0.001) Batch 3.739 (3.421) Remain 04:28:21 loss: 0.1263 Lr: 0.01146
[2023-08-08 05:55:07,582 INFO misc.py line 115 22900] Train: [70/100][130/156] Data 0.001 (0.001) Batch 3.492 (3.421) Remain 04:28:20 loss: 0.2509 Lr: 0.01145
[2023-08-08 05:55:11,320 INFO misc.py line 115 22900] Train: [70/100][131/156] Data 0.001 (0.001) Batch 3.739 (3.424) Remain 04:28:28 loss: 0.0900 Lr: 0.01145
[2023-08-08 05:55:15,314 INFO misc.py line 115 22900] Train: [70/100][132/156] Data 0.001 (0.001) Batch 3.993 (3.428) Remain 04:28:46 loss: 0.3458 Lr: 0.01144
[2023-08-08 05:55:18,739 INFO misc.py line 115 22900] Train: [70/100][133/156] Data 0.001 (0.001) Batch 3.425 (3.428) Remain 04:28:42 loss: 0.1426 Lr: 0.01144
[2023-08-08 05:55:22,473 INFO misc.py line 115 22900] Train: [70/100][134/156] Data 0.001 (0.001) Batch 3.734 (3.430) Remain 04:28:50 loss: 0.1617 Lr: 0.01143
[2023-08-08 05:55:26,456 INFO misc.py line 115 22900] Train: [70/100][135/156] Data 0.001 (0.001) Batch 3.984 (3.435) Remain 04:29:06 loss: 0.1920 Lr: 0.01143
[2023-08-08 05:55:29,522 INFO misc.py line 115 22900] Train: [70/100][136/156] Data 0.001 (0.001) Batch 3.065 (3.432) Remain 04:28:49 loss: 0.1550 Lr: 0.01142
[2023-08-08 05:55:32,660 INFO misc.py line 115 22900] Train: [70/100][137/156] Data 0.001 (0.001) Batch 3.139 (3.430) Remain 04:28:36 loss: 0.1919 Lr: 0.01142
[2023-08-08 05:55:35,893 INFO misc.py line 115 22900] Train: [70/100][138/156] Data 0.001 (0.001) Batch 3.233 (3.428) Remain 04:28:25 loss: 0.1524 Lr: 0.01142
[2023-08-08 05:55:39,112 INFO misc.py line 115 22900] Train: [70/100][139/156] Data 0.001 (0.001) Batch 3.218 (3.427) Remain 04:28:15 loss: 0.1578 Lr: 0.01141
[2023-08-08 05:55:43,305 INFO misc.py line 115 22900] Train: [70/100][140/156] Data 0.001 (0.001) Batch 4.194 (3.432) Remain 04:28:38 loss: 0.3356 Lr: 0.01141
[2023-08-08 05:55:46,215 INFO misc.py line 115 22900] Train: [70/100][141/156] Data 0.001 (0.001) Batch 2.910 (3.429) Remain 04:28:16 loss: 0.2012 Lr: 0.01140
[2023-08-08 05:55:49,377 INFO misc.py line 115 22900] Train: [70/100][142/156] Data 0.001 (0.001) Batch 3.162 (3.427) Remain 04:28:04 loss: 0.2030 Lr: 0.01140
[2023-08-08 05:55:52,329 INFO misc.py line 115 22900] Train: [70/100][143/156] Data 0.001 (0.001) Batch 2.951 (3.423) Remain 04:27:45 loss: 0.0547 Lr: 0.01139
[2023-08-08 05:55:55,789 INFO misc.py line 115 22900] Train: [70/100][144/156] Data 0.001 (0.001) Batch 3.460 (3.423) Remain 04:27:42 loss: 0.2129 Lr: 0.01139
[2023-08-08 05:55:58,662 INFO misc.py line 115 22900] Train: [70/100][145/156] Data 0.001 (0.001) Batch 2.874 (3.420) Remain 04:27:21 loss: 0.1090 Lr: 0.01138
[2023-08-08 05:56:02,778 INFO misc.py line 115 22900] Train: [70/100][146/156] Data 0.001 (0.001) Batch 4.116 (3.424) Remain 04:27:40 loss: 0.1846 Lr: 0.01138
[2023-08-08 05:56:05,761 INFO misc.py line 115 22900] Train: [70/100][147/156] Data 0.001 (0.001) Batch 2.982 (3.421) Remain 04:27:22 loss: 0.1378 Lr: 0.01138
[2023-08-08 05:56:08,883 INFO misc.py line 115 22900] Train: [70/100][148/156] Data 0.001 (0.001) Batch 3.123 (3.419) Remain 04:27:09 loss: 0.1722 Lr: 0.01137
[2023-08-08 05:56:12,995 INFO misc.py line 115 22900] Train: [70/100][149/156] Data 0.001 (0.001) Batch 4.111 (3.424) Remain 04:27:28 loss: 0.1608 Lr: 0.01137
[2023-08-08 05:56:16,832 INFO misc.py line 115 22900] Train: [70/100][150/156] Data 0.001 (0.001) Batch 3.837 (3.427) Remain 04:27:38 loss: 0.2080 Lr: 0.01136
[2023-08-08 05:56:20,577 INFO misc.py line 115 22900] Train: [70/100][151/156] Data 0.001 (0.001) Batch 3.746 (3.429) Remain 04:27:45 loss: 0.1963 Lr: 0.01136
[2023-08-08 05:56:24,312 INFO misc.py line 115 22900] Train: [70/100][152/156] Data 0.001 (0.001) Batch 3.734 (3.431) Remain 04:27:51 loss: 0.2018 Lr: 0.01135
[2023-08-08 05:56:28,421 INFO misc.py line 115 22900] Train: [70/100][153/156] Data 0.001 (0.001) Batch 4.109 (3.436) Remain 04:28:08 loss: 0.2043 Lr: 0.01135
[2023-08-08 05:56:32,011 INFO misc.py line 115 22900] Train: [70/100][154/156] Data 0.001 (0.001) Batch 3.590 (3.437) Remain 04:28:10 loss: 0.1036 Lr: 0.01134
[2023-08-08 05:56:36,670 INFO misc.py line 115 22900] Train: [70/100][155/156] Data 0.001 (0.001) Batch 4.659 (3.445) Remain 04:28:44 loss: 0.1471 Lr: 0.01134
[2023-08-08 05:56:39,691 INFO misc.py line 115 22900] Train: [70/100][156/156] Data 0.001 (0.001) Batch 3.021 (3.442) Remain 04:28:28 loss: 0.1901 Lr: 0.01134
[2023-08-08 05:56:39,691 INFO misc.py line 129 22900] Train result: loss: 0.1831 
[2023-08-08 05:56:39,691 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 05:56:41,811 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.0987 
[2023-08-08 05:56:42,680 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.6578 
[2023-08-08 05:56:44,346 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.8558 
[2023-08-08 05:56:45,870 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.5024 
[2023-08-08 05:56:47,714 INFO evaluator.py line 122 22900] Test: [5/24] Loss 2.2959 
[2023-08-08 05:56:49,378 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.6815 
[2023-08-08 05:56:51,517 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.7708 
[2023-08-08 05:56:53,321 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.3107 
[2023-08-08 05:56:54,605 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.6573 
[2023-08-08 05:56:56,737 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.4054 
[2023-08-08 05:56:57,262 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.3594 
[2023-08-08 05:56:58,795 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.8816 
[2023-08-08 05:57:01,506 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.3349 
[2023-08-08 05:57:03,187 INFO evaluator.py line 122 22900] Test: [14/24] Loss 1.1278 
[2023-08-08 05:57:05,210 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.2911 
[2023-08-08 05:57:07,921 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.2709 
[2023-08-08 05:57:10,626 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.6635 
[2023-08-08 05:57:12,473 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.8196 
[2023-08-08 05:57:13,222 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.2102 
[2023-08-08 05:57:14,107 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.6524 
[2023-08-08 05:57:16,370 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.5559 
[2023-08-08 05:57:18,335 INFO evaluator.py line 122 22900] Test: [22/24] Loss 2.0884 
[2023-08-08 05:57:20,184 INFO evaluator.py line 122 22900] Test: [23/24] Loss 3.2487 
[2023-08-08 05:57:22,118 INFO evaluator.py line 122 22900] Test: [24/24] Loss 1.9227 
[2023-08-08 05:57:22,166 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2518/0.3514/0.6937.
[2023-08-08 05:57:22,166 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6311/0.9525
[2023-08-08 05:57:22,166 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9340/0.9935
[2023-08-08 05:57:22,166 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1841/0.4001
[2023-08-08 05:57:22,166 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.1927/0.2155
[2023-08-08 05:57:22,166 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6564/0.7663
[2023-08-08 05:57:22,166 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.3336/0.4196
[2023-08-08 05:57:22,166 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5381/0.6680
[2023-08-08 05:57:22,166 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.1498/0.1648
[2023-08-08 05:57:22,166 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.2184/0.5719
[2023-08-08 05:57:22,166 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0157/0.0158
[2023-08-08 05:57:22,166 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 05:57:22,166 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0114/0.0120
[2023-08-08 05:57:22,166 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1686/0.4042
[2023-08-08 05:57:22,166 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0534/0.0594
[2023-08-08 05:57:22,166 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0203/0.0228
[2023-08-08 05:57:22,166 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.2791/0.3354
[2023-08-08 05:57:22,166 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.1267/0.1824
[2023-08-08 05:57:22,166 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.4338/0.6829
[2023-08-08 05:57:22,166 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 05:57:22,166 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0895/0.1600
[2023-08-08 05:57:22,167 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 05:57:22,167 INFO misc.py line 152 22900] Currently Best mIoU: 0.2650
[2023-08-08 05:57:22,167 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 05:57:28,056 INFO misc.py line 115 22900] Train: [71/100][1/156] Data 1.377 (1.377) Batch 5.113 (5.113) Remain 06:38:43 loss: 0.2085 Lr: 0.01133
[2023-08-08 05:57:32,131 INFO misc.py line 115 22900] Train: [71/100][2/156] Data 0.001 (0.001) Batch 4.075 (4.075) Remain 05:17:43 loss: 0.1488 Lr: 0.01133
[2023-08-08 05:57:34,044 INFO misc.py line 115 22900] Train: [71/100][3/156] Data 0.001 (0.001) Batch 1.913 (1.913) Remain 02:29:05 loss: 0.1351 Lr: 0.01132
[2023-08-08 05:57:37,608 INFO misc.py line 115 22900] Train: [71/100][4/156] Data 0.001 (0.001) Batch 3.564 (3.564) Remain 04:37:46 loss: 0.1406 Lr: 0.01132
[2023-08-08 05:57:41,538 INFO misc.py line 115 22900] Train: [71/100][5/156] Data 0.001 (0.001) Batch 3.930 (3.747) Remain 04:51:59 loss: 0.1811 Lr: 0.01131
[2023-08-08 05:57:45,260 INFO misc.py line 115 22900] Train: [71/100][6/156] Data 0.001 (0.001) Batch 3.722 (3.739) Remain 04:51:15 loss: 0.1651 Lr: 0.01131
[2023-08-08 05:57:48,130 INFO misc.py line 115 22900] Train: [71/100][7/156] Data 0.001 (0.001) Batch 2.870 (3.521) Remain 04:34:15 loss: 0.1187 Lr: 0.01130
[2023-08-08 05:57:52,125 INFO misc.py line 115 22900] Train: [71/100][8/156] Data 0.001 (0.001) Batch 3.996 (3.616) Remain 04:41:35 loss: 0.2396 Lr: 0.01130
[2023-08-08 05:57:55,707 INFO misc.py line 115 22900] Train: [71/100][9/156] Data 0.001 (0.001) Batch 3.582 (3.611) Remain 04:41:04 loss: 0.2085 Lr: 0.01130
[2023-08-08 05:57:59,765 INFO misc.py line 115 22900] Train: [71/100][10/156] Data 0.001 (0.001) Batch 4.058 (3.675) Remain 04:46:00 loss: 0.1983 Lr: 0.01129
[2023-08-08 05:58:03,088 INFO misc.py line 115 22900] Train: [71/100][11/156] Data 0.001 (0.001) Batch 3.323 (3.631) Remain 04:42:31 loss: 0.1024 Lr: 0.01129
[2023-08-08 05:58:06,611 INFO misc.py line 115 22900] Train: [71/100][12/156] Data 0.001 (0.001) Batch 3.523 (3.619) Remain 04:41:31 loss: 0.3799 Lr: 0.01128
[2023-08-08 05:58:09,857 INFO misc.py line 115 22900] Train: [71/100][13/156] Data 0.001 (0.001) Batch 3.246 (3.581) Remain 04:38:34 loss: 0.1767 Lr: 0.01128
[2023-08-08 05:58:14,874 INFO misc.py line 115 22900] Train: [71/100][14/156] Data 0.001 (0.001) Batch 5.017 (3.712) Remain 04:48:39 loss: 0.4934 Lr: 0.01127
[2023-08-08 05:58:18,851 INFO misc.py line 115 22900] Train: [71/100][15/156] Data 0.001 (0.001) Batch 3.977 (3.734) Remain 04:50:18 loss: 0.1757 Lr: 0.01127
[2023-08-08 05:58:22,172 INFO misc.py line 115 22900] Train: [71/100][16/156] Data 0.001 (0.001) Batch 3.320 (3.702) Remain 04:47:46 loss: 0.0909 Lr: 0.01126
[2023-08-08 05:58:25,638 INFO misc.py line 115 22900] Train: [71/100][17/156] Data 0.001 (0.001) Batch 3.466 (3.685) Remain 04:46:24 loss: 0.1313 Lr: 0.01126
[2023-08-08 05:58:28,944 INFO misc.py line 115 22900] Train: [71/100][18/156] Data 0.001 (0.001) Batch 3.306 (3.660) Remain 04:44:23 loss: 0.1330 Lr: 0.01126
[2023-08-08 05:58:33,170 INFO misc.py line 115 22900] Train: [71/100][19/156] Data 0.001 (0.001) Batch 4.226 (3.695) Remain 04:47:04 loss: 0.2472 Lr: 0.01125
[2023-08-08 05:58:36,571 INFO misc.py line 115 22900] Train: [71/100][20/156] Data 0.001 (0.001) Batch 3.401 (3.678) Remain 04:45:39 loss: 0.1554 Lr: 0.01125
[2023-08-08 05:58:39,393 INFO misc.py line 115 22900] Train: [71/100][21/156] Data 0.001 (0.001) Batch 2.823 (3.631) Remain 04:41:54 loss: 0.0749 Lr: 0.01124
[2023-08-08 05:58:43,569 INFO misc.py line 115 22900] Train: [71/100][22/156] Data 0.001 (0.001) Batch 4.176 (3.659) Remain 04:44:04 loss: 0.1529 Lr: 0.01124
[2023-08-08 05:58:47,566 INFO misc.py line 115 22900] Train: [71/100][23/156] Data 0.001 (0.001) Batch 3.997 (3.676) Remain 04:45:19 loss: 0.1660 Lr: 0.01123
[2023-08-08 05:58:51,624 INFO misc.py line 115 22900] Train: [71/100][24/156] Data 0.001 (0.001) Batch 4.058 (3.694) Remain 04:46:40 loss: 0.1451 Lr: 0.01123
[2023-08-08 05:58:54,255 INFO misc.py line 115 22900] Train: [71/100][25/156] Data 0.001 (0.001) Batch 2.631 (3.646) Remain 04:42:52 loss: 0.0950 Lr: 0.01122
[2023-08-08 05:58:58,110 INFO misc.py line 115 22900] Train: [71/100][26/156] Data 0.001 (0.001) Batch 3.854 (3.655) Remain 04:43:30 loss: 0.1518 Lr: 0.01122
[2023-08-08 05:59:01,776 INFO misc.py line 115 22900] Train: [71/100][27/156] Data 0.001 (0.001) Batch 3.667 (3.656) Remain 04:43:29 loss: 0.2862 Lr: 0.01122
[2023-08-08 05:59:05,078 INFO misc.py line 115 22900] Train: [71/100][28/156] Data 0.001 (0.001) Batch 3.301 (3.641) Remain 04:42:19 loss: 0.3221 Lr: 0.01121
[2023-08-08 05:59:09,083 INFO misc.py line 115 22900] Train: [71/100][29/156] Data 0.001 (0.001) Batch 4.005 (3.655) Remain 04:43:21 loss: 0.2518 Lr: 0.01121
[2023-08-08 05:59:13,032 INFO misc.py line 115 22900] Train: [71/100][30/156] Data 0.001 (0.001) Batch 3.950 (3.666) Remain 04:44:08 loss: 0.2223 Lr: 0.01120
[2023-08-08 05:59:16,559 INFO misc.py line 115 22900] Train: [71/100][31/156] Data 0.001 (0.001) Batch 3.526 (3.661) Remain 04:43:41 loss: 0.1891 Lr: 0.01120
[2023-08-08 05:59:20,178 INFO misc.py line 115 22900] Train: [71/100][32/156] Data 0.001 (0.001) Batch 3.619 (3.660) Remain 04:43:30 loss: 0.1413 Lr: 0.01119
[2023-08-08 05:59:23,099 INFO misc.py line 115 22900] Train: [71/100][33/156] Data 0.001 (0.001) Batch 2.920 (3.635) Remain 04:41:32 loss: 0.1163 Lr: 0.01119
[2023-08-08 05:59:27,159 INFO misc.py line 115 22900] Train: [71/100][34/156] Data 0.001 (0.001) Batch 4.060 (3.649) Remain 04:42:32 loss: 0.4659 Lr: 0.01119
[2023-08-08 05:59:30,767 INFO misc.py line 115 22900] Train: [71/100][35/156] Data 0.001 (0.001) Batch 3.608 (3.648) Remain 04:42:23 loss: 0.1130 Lr: 0.01118
[2023-08-08 05:59:34,702 INFO misc.py line 115 22900] Train: [71/100][36/156] Data 0.001 (0.001) Batch 3.935 (3.656) Remain 04:42:59 loss: 0.3210 Lr: 0.01118
[2023-08-08 05:59:38,264 INFO misc.py line 115 22900] Train: [71/100][37/156] Data 0.001 (0.001) Batch 3.562 (3.654) Remain 04:42:43 loss: 0.2187 Lr: 0.01117
[2023-08-08 05:59:41,781 INFO misc.py line 115 22900] Train: [71/100][38/156] Data 0.001 (0.001) Batch 3.517 (3.650) Remain 04:42:21 loss: 0.0946 Lr: 0.01117
[2023-08-08 05:59:44,946 INFO misc.py line 115 22900] Train: [71/100][39/156] Data 0.001 (0.001) Batch 3.166 (3.636) Remain 04:41:15 loss: 0.0829 Lr: 0.01116
[2023-08-08 05:59:47,896 INFO misc.py line 115 22900] Train: [71/100][40/156] Data 0.001 (0.001) Batch 2.950 (3.618) Remain 04:39:45 loss: 0.1378 Lr: 0.01116
[2023-08-08 05:59:51,315 INFO misc.py line 115 22900] Train: [71/100][41/156] Data 0.001 (0.001) Batch 3.419 (3.612) Remain 04:39:17 loss: 0.1942 Lr: 0.01115
[2023-08-08 05:59:54,204 INFO misc.py line 115 22900] Train: [71/100][42/156] Data 0.001 (0.001) Batch 2.889 (3.594) Remain 04:37:48 loss: 0.1828 Lr: 0.01115
[2023-08-08 05:59:57,452 INFO misc.py line 115 22900] Train: [71/100][43/156] Data 0.001 (0.001) Batch 3.248 (3.585) Remain 04:37:04 loss: 0.1682 Lr: 0.01115
[2023-08-08 06:00:01,884 INFO misc.py line 115 22900] Train: [71/100][44/156] Data 0.001 (0.001) Batch 4.432 (3.606) Remain 04:38:36 loss: 0.3278 Lr: 0.01114
[2023-08-08 06:00:05,199 INFO misc.py line 115 22900] Train: [71/100][45/156] Data 0.001 (0.001) Batch 3.315 (3.599) Remain 04:38:01 loss: 0.2444 Lr: 0.01114
[2023-08-08 06:00:08,382 INFO misc.py line 115 22900] Train: [71/100][46/156] Data 0.001 (0.001) Batch 3.183 (3.589) Remain 04:37:12 loss: 0.1786 Lr: 0.01113
[2023-08-08 06:00:11,877 INFO misc.py line 115 22900] Train: [71/100][47/156] Data 0.001 (0.001) Batch 3.496 (3.587) Remain 04:36:59 loss: 0.0790 Lr: 0.01113
[2023-08-08 06:00:16,393 INFO misc.py line 115 22900] Train: [71/100][48/156] Data 0.001 (0.001) Batch 4.515 (3.608) Remain 04:38:31 loss: 0.2281 Lr: 0.01112
[2023-08-08 06:00:18,941 INFO misc.py line 115 22900] Train: [71/100][49/156] Data 0.001 (0.001) Batch 2.549 (3.585) Remain 04:36:40 loss: 0.0916 Lr: 0.01112
[2023-08-08 06:00:21,490 INFO misc.py line 115 22900] Train: [71/100][50/156] Data 0.001 (0.001) Batch 2.549 (3.563) Remain 04:34:55 loss: 0.1158 Lr: 0.01111
[2023-08-08 06:00:23,406 INFO misc.py line 115 22900] Train: [71/100][51/156] Data 0.001 (0.001) Batch 1.916 (3.528) Remain 04:32:12 loss: 0.0547 Lr: 0.01111
[2023-08-08 06:00:26,819 INFO misc.py line 115 22900] Train: [71/100][52/156] Data 0.001 (0.001) Batch 3.414 (3.526) Remain 04:31:58 loss: 0.2606 Lr: 0.01111
[2023-08-08 06:00:29,432 INFO misc.py line 115 22900] Train: [71/100][53/156] Data 0.001 (0.001) Batch 2.612 (3.508) Remain 04:30:30 loss: 0.1008 Lr: 0.01110
[2023-08-08 06:00:33,409 INFO misc.py line 115 22900] Train: [71/100][54/156] Data 0.001 (0.001) Batch 3.978 (3.517) Remain 04:31:09 loss: 0.1400 Lr: 0.01110
[2023-08-08 06:00:36,655 INFO misc.py line 115 22900] Train: [71/100][55/156] Data 0.001 (0.001) Batch 3.245 (3.512) Remain 04:30:41 loss: 0.1180 Lr: 0.01109
[2023-08-08 06:00:40,740 INFO misc.py line 115 22900] Train: [71/100][56/156] Data 0.001 (0.001) Batch 4.086 (3.523) Remain 04:31:28 loss: 0.2152 Lr: 0.01109
[2023-08-08 06:00:43,298 INFO misc.py line 115 22900] Train: [71/100][57/156] Data 0.001 (0.001) Batch 2.557 (3.505) Remain 04:30:02 loss: 0.0441 Lr: 0.01108
[2023-08-08 06:00:46,698 INFO misc.py line 115 22900] Train: [71/100][58/156] Data 0.001 (0.001) Batch 3.400 (3.503) Remain 04:29:49 loss: 0.0919 Lr: 0.01108
[2023-08-08 06:00:50,241 INFO misc.py line 115 22900] Train: [71/100][59/156] Data 0.001 (0.001) Batch 3.544 (3.504) Remain 04:29:49 loss: 0.1152 Lr: 0.01107
[2023-08-08 06:00:53,772 INFO misc.py line 115 22900] Train: [71/100][60/156] Data 0.001 (0.001) Batch 3.531 (3.504) Remain 04:29:48 loss: 0.1299 Lr: 0.01107
[2023-08-08 06:00:56,645 INFO misc.py line 115 22900] Train: [71/100][61/156] Data 0.001 (0.001) Batch 2.873 (3.493) Remain 04:28:54 loss: 0.2032 Lr: 0.01107
[2023-08-08 06:00:59,383 INFO misc.py line 115 22900] Train: [71/100][62/156] Data 0.001 (0.001) Batch 2.738 (3.480) Remain 04:27:52 loss: 0.1684 Lr: 0.01106
[2023-08-08 06:01:03,280 INFO misc.py line 115 22900] Train: [71/100][63/156] Data 0.001 (0.001) Batch 3.897 (3.487) Remain 04:28:20 loss: 0.1317 Lr: 0.01106
[2023-08-08 06:01:06,403 INFO misc.py line 115 22900] Train: [71/100][64/156] Data 0.001 (0.001) Batch 3.123 (3.481) Remain 04:27:49 loss: 0.1629 Lr: 0.01105
[2023-08-08 06:01:10,470 INFO misc.py line 115 22900] Train: [71/100][65/156] Data 0.001 (0.001) Batch 4.067 (3.491) Remain 04:28:29 loss: 0.1651 Lr: 0.01105
[2023-08-08 06:01:13,045 INFO misc.py line 115 22900] Train: [71/100][66/156] Data 0.001 (0.001) Batch 2.575 (3.476) Remain 04:27:19 loss: 0.1653 Lr: 0.01104
[2023-08-08 06:01:16,183 INFO misc.py line 115 22900] Train: [71/100][67/156] Data 0.001 (0.001) Batch 3.138 (3.471) Remain 04:26:51 loss: 0.1470 Lr: 0.01104
[2023-08-08 06:01:19,959 INFO misc.py line 115 22900] Train: [71/100][68/156] Data 0.001 (0.001) Batch 3.776 (3.476) Remain 04:27:09 loss: 0.2774 Lr: 0.01104
[2023-08-08 06:01:22,688 INFO misc.py line 115 22900] Train: [71/100][69/156] Data 0.001 (0.001) Batch 2.729 (3.464) Remain 04:26:13 loss: 0.2031 Lr: 0.01103
[2023-08-08 06:01:27,144 INFO misc.py line 115 22900] Train: [71/100][70/156] Data 0.001 (0.001) Batch 4.456 (3.479) Remain 04:27:18 loss: 0.2388 Lr: 0.01103
[2023-08-08 06:01:30,557 INFO misc.py line 115 22900] Train: [71/100][71/156] Data 0.001 (0.001) Batch 3.412 (3.478) Remain 04:27:10 loss: 0.1187 Lr: 0.01102
[2023-08-08 06:01:32,820 INFO misc.py line 115 22900] Train: [71/100][72/156] Data 0.001 (0.001) Batch 2.264 (3.461) Remain 04:25:46 loss: 0.0662 Lr: 0.01102
[2023-08-08 06:01:36,458 INFO misc.py line 115 22900] Train: [71/100][73/156] Data 0.001 (0.001) Batch 3.638 (3.463) Remain 04:25:54 loss: 0.2827 Lr: 0.01101
[2023-08-08 06:01:40,571 INFO misc.py line 115 22900] Train: [71/100][74/156] Data 0.001 (0.001) Batch 4.113 (3.472) Remain 04:26:33 loss: 0.1199 Lr: 0.01101
[2023-08-08 06:01:45,059 INFO misc.py line 115 22900] Train: [71/100][75/156] Data 0.001 (0.001) Batch 4.488 (3.486) Remain 04:27:34 loss: 0.1651 Lr: 0.01100
[2023-08-08 06:01:48,293 INFO misc.py line 115 22900] Train: [71/100][76/156] Data 0.001 (0.001) Batch 3.233 (3.483) Remain 04:27:15 loss: 0.1201 Lr: 0.01100
[2023-08-08 06:01:50,705 INFO misc.py line 115 22900] Train: [71/100][77/156] Data 0.001 (0.001) Batch 2.412 (3.468) Remain 04:26:05 loss: 0.1021 Lr: 0.01100
[2023-08-08 06:01:54,518 INFO misc.py line 115 22900] Train: [71/100][78/156] Data 0.001 (0.001) Batch 3.813 (3.473) Remain 04:26:22 loss: 0.1293 Lr: 0.01099
[2023-08-08 06:01:58,482 INFO misc.py line 115 22900] Train: [71/100][79/156] Data 0.001 (0.001) Batch 3.964 (3.479) Remain 04:26:48 loss: 0.2425 Lr: 0.01099
[2023-08-08 06:02:01,389 INFO misc.py line 115 22900] Train: [71/100][80/156] Data 0.001 (0.001) Batch 2.907 (3.472) Remain 04:26:11 loss: 0.1727 Lr: 0.01098
[2023-08-08 06:02:05,104 INFO misc.py line 115 22900] Train: [71/100][81/156] Data 0.001 (0.001) Batch 3.715 (3.475) Remain 04:26:22 loss: 0.1937 Lr: 0.01098
[2023-08-08 06:02:08,959 INFO misc.py line 115 22900] Train: [71/100][82/156] Data 0.001 (0.001) Batch 3.855 (3.480) Remain 04:26:40 loss: 0.0975 Lr: 0.01097
[2023-08-08 06:02:12,978 INFO misc.py line 115 22900] Train: [71/100][83/156] Data 0.001 (0.001) Batch 4.019 (3.487) Remain 04:27:08 loss: 0.3492 Lr: 0.01097
[2023-08-08 06:02:16,130 INFO misc.py line 115 22900] Train: [71/100][84/156] Data 0.001 (0.001) Batch 3.152 (3.483) Remain 04:26:45 loss: 0.0977 Lr: 0.01096
[2023-08-08 06:02:19,339 INFO misc.py line 115 22900] Train: [71/100][85/156] Data 0.001 (0.001) Batch 3.209 (3.479) Remain 04:26:26 loss: 0.1742 Lr: 0.01096
[2023-08-08 06:02:22,460 INFO misc.py line 115 22900] Train: [71/100][86/156] Data 0.001 (0.001) Batch 3.121 (3.475) Remain 04:26:03 loss: 0.1082 Lr: 0.01096
[2023-08-08 06:02:26,244 INFO misc.py line 115 22900] Train: [71/100][87/156] Data 0.001 (0.001) Batch 3.785 (3.479) Remain 04:26:17 loss: 0.1826 Lr: 0.01095
[2023-08-08 06:02:29,491 INFO misc.py line 115 22900] Train: [71/100][88/156] Data 0.001 (0.001) Batch 3.247 (3.476) Remain 04:26:01 loss: 0.1198 Lr: 0.01095
[2023-08-08 06:02:33,214 INFO misc.py line 115 22900] Train: [71/100][89/156] Data 0.001 (0.001) Batch 3.723 (3.479) Remain 04:26:10 loss: 0.1744 Lr: 0.01094
[2023-08-08 06:02:36,257 INFO misc.py line 115 22900] Train: [71/100][90/156] Data 0.001 (0.001) Batch 3.043 (3.474) Remain 04:25:44 loss: 0.0926 Lr: 0.01094
[2023-08-08 06:02:38,798 INFO misc.py line 115 22900] Train: [71/100][91/156] Data 0.001 (0.001) Batch 2.540 (3.463) Remain 04:24:52 loss: 0.1407 Lr: 0.01093
[2023-08-08 06:02:43,024 INFO misc.py line 115 22900] Train: [71/100][92/156] Data 0.001 (0.001) Batch 4.226 (3.472) Remain 04:25:28 loss: 0.2335 Lr: 0.01093
[2023-08-08 06:02:46,316 INFO misc.py line 115 22900] Train: [71/100][93/156] Data 0.001 (0.001) Batch 3.292 (3.470) Remain 04:25:15 loss: 0.2861 Lr: 0.01093
[2023-08-08 06:02:50,327 INFO misc.py line 115 22900] Train: [71/100][94/156] Data 0.001 (0.001) Batch 4.012 (3.476) Remain 04:25:39 loss: 0.1262 Lr: 0.01092
[2023-08-08 06:02:53,428 INFO misc.py line 115 22900] Train: [71/100][95/156] Data 0.001 (0.001) Batch 3.101 (3.472) Remain 04:25:17 loss: 0.1914 Lr: 0.01092
[2023-08-08 06:02:56,937 INFO misc.py line 115 22900] Train: [71/100][96/156] Data 0.001 (0.001) Batch 3.508 (3.472) Remain 04:25:15 loss: 0.1443 Lr: 0.01091
[2023-08-08 06:03:00,692 INFO misc.py line 115 22900] Train: [71/100][97/156] Data 0.001 (0.001) Batch 3.755 (3.475) Remain 04:25:25 loss: 0.1357 Lr: 0.01091
[2023-08-08 06:03:03,686 INFO misc.py line 115 22900] Train: [71/100][98/156] Data 0.001 (0.001) Batch 2.994 (3.470) Remain 04:24:59 loss: 0.0756 Lr: 0.01090
[2023-08-08 06:03:06,982 INFO misc.py line 115 22900] Train: [71/100][99/156] Data 0.001 (0.001) Batch 3.296 (3.468) Remain 04:24:47 loss: 0.1225 Lr: 0.01090
[2023-08-08 06:03:11,924 INFO misc.py line 115 22900] Train: [71/100][100/156] Data 0.001 (0.001) Batch 4.942 (3.483) Remain 04:25:53 loss: 0.4876 Lr: 0.01089
[2023-08-08 06:03:15,222 INFO misc.py line 115 22900] Train: [71/100][101/156] Data 0.001 (0.001) Batch 3.298 (3.481) Remain 04:25:41 loss: 0.1243 Lr: 0.01089
[2023-08-08 06:03:19,133 INFO misc.py line 115 22900] Train: [71/100][102/156] Data 0.001 (0.001) Batch 3.912 (3.486) Remain 04:25:57 loss: 0.2708 Lr: 0.01089
[2023-08-08 06:03:22,398 INFO misc.py line 115 22900] Train: [71/100][103/156] Data 0.001 (0.001) Batch 3.265 (3.484) Remain 04:25:44 loss: 0.1430 Lr: 0.01088
[2023-08-08 06:03:25,665 INFO misc.py line 115 22900] Train: [71/100][104/156] Data 0.001 (0.001) Batch 3.267 (3.481) Remain 04:25:30 loss: 0.1207 Lr: 0.01088
[2023-08-08 06:03:29,681 INFO misc.py line 115 22900] Train: [71/100][105/156] Data 0.001 (0.001) Batch 4.015 (3.487) Remain 04:25:51 loss: 0.4158 Lr: 0.01087
[2023-08-08 06:03:32,338 INFO misc.py line 115 22900] Train: [71/100][106/156] Data 0.001 (0.001) Batch 2.657 (3.479) Remain 04:25:11 loss: 0.1897 Lr: 0.01087
[2023-08-08 06:03:36,379 INFO misc.py line 115 22900] Train: [71/100][107/156] Data 0.001 (0.001) Batch 4.041 (3.484) Remain 04:25:32 loss: 0.2083 Lr: 0.01086
[2023-08-08 06:03:40,705 INFO misc.py line 115 22900] Train: [71/100][108/156] Data 0.001 (0.001) Batch 4.326 (3.492) Remain 04:26:05 loss: 0.1652 Lr: 0.01086
[2023-08-08 06:03:44,785 INFO misc.py line 115 22900] Train: [71/100][109/156] Data 0.001 (0.001) Batch 4.080 (3.498) Remain 04:26:27 loss: 0.1572 Lr: 0.01086
[2023-08-08 06:03:48,346 INFO misc.py line 115 22900] Train: [71/100][110/156] Data 0.001 (0.001) Batch 3.561 (3.498) Remain 04:26:26 loss: 0.2042 Lr: 0.01085
[2023-08-08 06:03:52,247 INFO misc.py line 115 22900] Train: [71/100][111/156] Data 0.001 (0.001) Batch 3.901 (3.502) Remain 04:26:40 loss: 0.1617 Lr: 0.01085
[2023-08-08 06:03:56,038 INFO misc.py line 115 22900] Train: [71/100][112/156] Data 0.001 (0.001) Batch 3.791 (3.505) Remain 04:26:48 loss: 0.2388 Lr: 0.01084
[2023-08-08 06:04:00,039 INFO misc.py line 115 22900] Train: [71/100][113/156] Data 0.001 (0.001) Batch 4.001 (3.509) Remain 04:27:05 loss: 0.2702 Lr: 0.01084
[2023-08-08 06:04:02,895 INFO misc.py line 115 22900] Train: [71/100][114/156] Data 0.001 (0.001) Batch 2.856 (3.503) Remain 04:26:35 loss: 0.1799 Lr: 0.01083
[2023-08-08 06:04:07,021 INFO misc.py line 115 22900] Train: [71/100][115/156] Data 0.001 (0.001) Batch 4.126 (3.509) Remain 04:26:57 loss: 0.3861 Lr: 0.01083
[2023-08-08 06:04:11,075 INFO misc.py line 115 22900] Train: [71/100][116/156] Data 0.001 (0.001) Batch 4.054 (3.514) Remain 04:27:15 loss: 0.1116 Lr: 0.01082
[2023-08-08 06:04:14,278 INFO misc.py line 115 22900] Train: [71/100][117/156] Data 0.001 (0.001) Batch 3.204 (3.511) Remain 04:26:59 loss: 0.0580 Lr: 0.01082
[2023-08-08 06:04:17,039 INFO misc.py line 115 22900] Train: [71/100][118/156] Data 0.001 (0.001) Batch 2.760 (3.504) Remain 04:26:26 loss: 0.0641 Lr: 0.01082
[2023-08-08 06:04:20,594 INFO misc.py line 115 22900] Train: [71/100][119/156] Data 0.001 (0.001) Batch 3.555 (3.505) Remain 04:26:25 loss: 0.1286 Lr: 0.01081
[2023-08-08 06:04:23,545 INFO misc.py line 115 22900] Train: [71/100][120/156] Data 0.001 (0.001) Batch 2.952 (3.500) Remain 04:26:00 loss: 0.1344 Lr: 0.01081
[2023-08-08 06:04:26,794 INFO misc.py line 115 22900] Train: [71/100][121/156] Data 0.001 (0.001) Batch 3.249 (3.498) Remain 04:25:46 loss: 0.0787 Lr: 0.01080
[2023-08-08 06:04:29,433 INFO misc.py line 115 22900] Train: [71/100][122/156] Data 0.001 (0.001) Batch 2.639 (3.491) Remain 04:25:10 loss: 0.2401 Lr: 0.01080
[2023-08-08 06:04:32,842 INFO misc.py line 115 22900] Train: [71/100][123/156] Data 0.001 (0.001) Batch 3.409 (3.490) Remain 04:25:03 loss: 0.1301 Lr: 0.01079
[2023-08-08 06:04:35,838 INFO misc.py line 115 22900] Train: [71/100][124/156] Data 0.001 (0.001) Batch 2.996 (3.486) Remain 04:24:41 loss: 0.2165 Lr: 0.01079
[2023-08-08 06:04:39,042 INFO misc.py line 115 22900] Train: [71/100][125/156] Data 0.001 (0.001) Batch 3.204 (3.484) Remain 04:24:27 loss: 0.1375 Lr: 0.01079
[2023-08-08 06:04:41,668 INFO misc.py line 115 22900] Train: [71/100][126/156] Data 0.001 (0.001) Batch 2.626 (3.477) Remain 04:23:52 loss: 0.1424 Lr: 0.01078
[2023-08-08 06:04:44,107 INFO misc.py line 115 22900] Train: [71/100][127/156] Data 0.001 (0.001) Batch 2.439 (3.468) Remain 04:23:10 loss: 0.1409 Lr: 0.01078
[2023-08-08 06:04:46,163 INFO misc.py line 115 22900] Train: [71/100][128/156] Data 0.001 (0.001) Batch 2.056 (3.457) Remain 04:22:16 loss: 0.1139 Lr: 0.01077
[2023-08-08 06:04:48,966 INFO misc.py line 115 22900] Train: [71/100][129/156] Data 0.001 (0.001) Batch 2.803 (3.452) Remain 04:21:48 loss: 0.0985 Lr: 0.01077
[2023-08-08 06:04:52,157 INFO misc.py line 115 22900] Train: [71/100][130/156] Data 0.001 (0.001) Batch 3.192 (3.450) Remain 04:21:36 loss: 0.2024 Lr: 0.01076
[2023-08-08 06:04:55,283 INFO misc.py line 115 22900] Train: [71/100][131/156] Data 0.001 (0.001) Batch 3.126 (3.447) Remain 04:21:21 loss: 0.1905 Lr: 0.01076
[2023-08-08 06:04:59,097 INFO misc.py line 115 22900] Train: [71/100][132/156] Data 0.001 (0.001) Batch 3.813 (3.450) Remain 04:21:30 loss: 0.3350 Lr: 0.01076
[2023-08-08 06:05:02,662 INFO misc.py line 115 22900] Train: [71/100][133/156] Data 0.001 (0.001) Batch 3.565 (3.451) Remain 04:21:31 loss: 0.2479 Lr: 0.01075
[2023-08-08 06:05:06,064 INFO misc.py line 115 22900] Train: [71/100][134/156] Data 0.001 (0.001) Batch 3.402 (3.451) Remain 04:21:26 loss: 0.2177 Lr: 0.01075
[2023-08-08 06:05:09,665 INFO misc.py line 115 22900] Train: [71/100][135/156] Data 0.001 (0.001) Batch 3.601 (3.452) Remain 04:21:27 loss: 0.2950 Lr: 0.01074
[2023-08-08 06:05:13,808 INFO misc.py line 115 22900] Train: [71/100][136/156] Data 0.001 (0.001) Batch 4.143 (3.457) Remain 04:21:48 loss: 0.1481 Lr: 0.01074
[2023-08-08 06:05:18,122 INFO misc.py line 115 22900] Train: [71/100][137/156] Data 0.001 (0.001) Batch 4.313 (3.463) Remain 04:22:13 loss: 0.3226 Lr: 0.01073
[2023-08-08 06:05:21,384 INFO misc.py line 115 22900] Train: [71/100][138/156] Data 0.001 (0.001) Batch 3.263 (3.462) Remain 04:22:03 loss: 0.0890 Lr: 0.01073
[2023-08-08 06:05:23,995 INFO misc.py line 115 22900] Train: [71/100][139/156] Data 0.001 (0.001) Batch 2.611 (3.456) Remain 04:21:31 loss: 0.1159 Lr: 0.01072
[2023-08-08 06:05:28,125 INFO misc.py line 115 22900] Train: [71/100][140/156] Data 0.001 (0.001) Batch 4.130 (3.460) Remain 04:21:50 loss: 0.1713 Lr: 0.01072
[2023-08-08 06:05:30,727 INFO misc.py line 115 22900] Train: [71/100][141/156] Data 0.001 (0.001) Batch 2.602 (3.454) Remain 04:21:18 loss: 0.1233 Lr: 0.01072
[2023-08-08 06:05:34,715 INFO misc.py line 115 22900] Train: [71/100][142/156] Data 0.001 (0.001) Batch 3.988 (3.458) Remain 04:21:32 loss: 0.1763 Lr: 0.01071
[2023-08-08 06:05:38,831 INFO misc.py line 115 22900] Train: [71/100][143/156] Data 0.001 (0.001) Batch 4.116 (3.463) Remain 04:21:50 loss: 0.3119 Lr: 0.01071
[2023-08-08 06:05:42,609 INFO misc.py line 115 22900] Train: [71/100][144/156] Data 0.001 (0.001) Batch 3.779 (3.465) Remain 04:21:57 loss: 0.3259 Lr: 0.01070
[2023-08-08 06:05:46,611 INFO misc.py line 115 22900] Train: [71/100][145/156] Data 0.001 (0.001) Batch 4.002 (3.469) Remain 04:22:10 loss: 0.2418 Lr: 0.01070
[2023-08-08 06:05:49,673 INFO misc.py line 115 22900] Train: [71/100][146/156] Data 0.001 (0.001) Batch 3.062 (3.466) Remain 04:21:54 loss: 0.1522 Lr: 0.01069
[2023-08-08 06:05:52,862 INFO misc.py line 115 22900] Train: [71/100][147/156] Data 0.001 (0.001) Batch 3.189 (3.464) Remain 04:21:42 loss: 0.1741 Lr: 0.01069
[2023-08-08 06:05:55,828 INFO misc.py line 115 22900] Train: [71/100][148/156] Data 0.001 (0.001) Batch 2.966 (3.461) Remain 04:21:23 loss: 0.1157 Lr: 0.01069
[2023-08-08 06:05:59,022 INFO misc.py line 115 22900] Train: [71/100][149/156] Data 0.001 (0.001) Batch 3.194 (3.459) Remain 04:21:11 loss: 0.1612 Lr: 0.01068
[2023-08-08 06:06:03,069 INFO misc.py line 115 22900] Train: [71/100][150/156] Data 0.001 (0.001) Batch 4.047 (3.463) Remain 04:21:26 loss: 0.3016 Lr: 0.01068
[2023-08-08 06:06:06,513 INFO misc.py line 115 22900] Train: [71/100][151/156] Data 0.001 (0.001) Batch 3.444 (3.463) Remain 04:21:22 loss: 0.1285 Lr: 0.01067
[2023-08-08 06:06:10,579 INFO misc.py line 115 22900] Train: [71/100][152/156] Data 0.001 (0.001) Batch 4.066 (3.467) Remain 04:21:37 loss: 0.1949 Lr: 0.01067
[2023-08-08 06:06:14,071 INFO misc.py line 115 22900] Train: [71/100][153/156] Data 0.001 (0.001) Batch 3.493 (3.467) Remain 04:21:34 loss: 0.1389 Lr: 0.01066
[2023-08-08 06:06:17,460 INFO misc.py line 115 22900] Train: [71/100][154/156] Data 0.001 (0.001) Batch 3.389 (3.466) Remain 04:21:28 loss: 0.2266 Lr: 0.01066
[2023-08-08 06:06:21,543 INFO misc.py line 115 22900] Train: [71/100][155/156] Data 0.001 (0.001) Batch 4.083 (3.470) Remain 04:21:43 loss: 0.2060 Lr: 0.01066
[2023-08-08 06:06:25,496 INFO misc.py line 115 22900] Train: [71/100][156/156] Data 0.001 (0.001) Batch 3.953 (3.474) Remain 04:21:54 loss: 0.1482 Lr: 0.01065
[2023-08-08 06:06:25,496 INFO misc.py line 129 22900] Train result: loss: 0.1799 
[2023-08-08 06:06:25,497 INFO evaluator.py line 91 22900] >>>>>>>>>>>>>>>> Start Evaluation >>>>>>>>>>>>>>>>
[2023-08-08 06:06:27,584 INFO evaluator.py line 122 22900] Test: [1/24] Loss 2.2663 
[2023-08-08 06:06:28,453 INFO evaluator.py line 122 22900] Test: [2/24] Loss 0.6707 
[2023-08-08 06:06:30,117 INFO evaluator.py line 122 22900] Test: [3/24] Loss 0.7519 
[2023-08-08 06:06:31,638 INFO evaluator.py line 122 22900] Test: [4/24] Loss 1.4753 
[2023-08-08 06:06:33,480 INFO evaluator.py line 122 22900] Test: [5/24] Loss 1.7558 
[2023-08-08 06:06:35,144 INFO evaluator.py line 122 22900] Test: [6/24] Loss 0.7718 
[2023-08-08 06:06:37,282 INFO evaluator.py line 122 22900] Test: [7/24] Loss 3.4380 
[2023-08-08 06:06:39,086 INFO evaluator.py line 122 22900] Test: [8/24] Loss 2.3750 
[2023-08-08 06:06:40,369 INFO evaluator.py line 122 22900] Test: [9/24] Loss 1.7196 
[2023-08-08 06:06:42,500 INFO evaluator.py line 122 22900] Test: [10/24] Loss 1.5140 
[2023-08-08 06:06:43,025 INFO evaluator.py line 122 22900] Test: [11/24] Loss 1.1876 
[2023-08-08 06:06:44,556 INFO evaluator.py line 122 22900] Test: [12/24] Loss 0.7138 
[2023-08-08 06:06:47,269 INFO evaluator.py line 122 22900] Test: [13/24] Loss 1.2592 
[2023-08-08 06:06:48,951 INFO evaluator.py line 122 22900] Test: [14/24] Loss 0.8730 
[2023-08-08 06:06:50,973 INFO evaluator.py line 122 22900] Test: [15/24] Loss 0.3249 
[2023-08-08 06:06:53,685 INFO evaluator.py line 122 22900] Test: [16/24] Loss 1.2984 
[2023-08-08 06:06:56,389 INFO evaluator.py line 122 22900] Test: [17/24] Loss 1.7349 
[2023-08-08 06:06:58,236 INFO evaluator.py line 122 22900] Test: [18/24] Loss 1.3105 
[2023-08-08 06:06:58,984 INFO evaluator.py line 122 22900] Test: [19/24] Loss 1.2711 
[2023-08-08 06:06:59,868 INFO evaluator.py line 122 22900] Test: [20/24] Loss 0.7549 
[2023-08-08 06:07:02,129 INFO evaluator.py line 122 22900] Test: [21/24] Loss 1.4553 
[2023-08-08 06:07:04,093 INFO evaluator.py line 122 22900] Test: [22/24] Loss 1.6005 
[2023-08-08 06:07:05,938 INFO evaluator.py line 122 22900] Test: [23/24] Loss 3.0647 
[2023-08-08 06:07:07,874 INFO evaluator.py line 122 22900] Test: [24/24] Loss 2.0529 
[2023-08-08 06:07:07,922 INFO evaluator.py line 134 22900] Val result: mIoU/mAcc/allAcc 0.2488/0.3397/0.6955.
[2023-08-08 06:07:07,922 INFO evaluator.py line 137 22900] Class_0-wall Result: iou/accuracy 0.6299/0.9709
[2023-08-08 06:07:07,922 INFO evaluator.py line 137 22900] Class_1-floor Result: iou/accuracy 0.9433/0.9891
[2023-08-08 06:07:07,922 INFO evaluator.py line 137 22900] Class_2-cabinet Result: iou/accuracy 0.1675/0.3818
[2023-08-08 06:07:07,922 INFO evaluator.py line 137 22900] Class_3-bed Result: iou/accuracy 0.2020/0.3237
[2023-08-08 06:07:07,922 INFO evaluator.py line 137 22900] Class_4-chair Result: iou/accuracy 0.6188/0.6660
[2023-08-08 06:07:07,922 INFO evaluator.py line 137 22900] Class_5-sofa Result: iou/accuracy 0.4110/0.6387
[2023-08-08 06:07:07,922 INFO evaluator.py line 137 22900] Class_6-table Result: iou/accuracy 0.5263/0.6602
[2023-08-08 06:07:07,922 INFO evaluator.py line 137 22900] Class_7-door Result: iou/accuracy 0.2309/0.2507
[2023-08-08 06:07:07,922 INFO evaluator.py line 137 22900] Class_8-window Result: iou/accuracy 0.1812/0.3035
[2023-08-08 06:07:07,922 INFO evaluator.py line 137 22900] Class_9-bookshelf Result: iou/accuracy 0.0174/0.0174
[2023-08-08 06:07:07,922 INFO evaluator.py line 137 22900] Class_10-picture Result: iou/accuracy 0.0000/0.0000
[2023-08-08 06:07:07,922 INFO evaluator.py line 137 22900] Class_11-counter Result: iou/accuracy 0.0151/0.0160
[2023-08-08 06:07:07,922 INFO evaluator.py line 137 22900] Class_12-desk Result: iou/accuracy 0.1565/0.4198
[2023-08-08 06:07:07,922 INFO evaluator.py line 137 22900] Class_13-curtain Result: iou/accuracy 0.0177/0.0184
[2023-08-08 06:07:07,922 INFO evaluator.py line 137 22900] Class_14-refridgerator Result: iou/accuracy 0.0382/0.0449
[2023-08-08 06:07:07,922 INFO evaluator.py line 137 22900] Class_15-shower curtain Result: iou/accuracy 0.3109/0.3738
[2023-08-08 06:07:07,922 INFO evaluator.py line 137 22900] Class_16-toilet Result: iou/accuracy 0.0985/0.1195
[2023-08-08 06:07:07,922 INFO evaluator.py line 137 22900] Class_17-sink Result: iou/accuracy 0.3180/0.4267
[2023-08-08 06:07:07,922 INFO evaluator.py line 137 22900] Class_18-bathtub Result: iou/accuracy 0.0000/0.0000
[2023-08-08 06:07:07,922 INFO evaluator.py line 137 22900] Class_19-otherfurniture Result: iou/accuracy 0.0933/0.1738
[2023-08-08 06:07:07,922 INFO evaluator.py line 145 22900] <<<<<<<<<<<<<<<<< End Evaluation <<<<<<<<<<<<<<<<<
[2023-08-08 06:07:07,923 INFO misc.py line 152 22900] Currently Best mIoU: 0.2650
[2023-08-08 06:07:07,923 INFO misc.py line 156 22900] Saving checkpoint to: exp/scannet/semseg-spunet-v1m1-0-base/model/model_last.pth
[2023-08-08 06:07:14,086 INFO misc.py line 115 22900] Train: [72/100][1/156] Data 0.926 (0.926) Batch 5.380 (5.380) Remain 06:45:31 loss: 0.2522 Lr: 0.01065
[2023-08-08 06:07:17,444 INFO misc.py line 115 22900] Train: [72/100][2/156] Data 0.001 (0.001) Batch 3.359 (3.359) Remain 04:13:07 loss: 0.1280 Lr: 0.01064
[2023-08-08 06:07:20,929 INFO misc.py line 115 22900] Train: [72/100][3/156] Data 0.001 (0.001) Batch 3.485 (3.485) Remain 04:22:35 loss: 0.2396 Lr: 0.01064
[2023-08-08 06:07:24,151 INFO misc.py line 115 22900] Train: [72/100][4/156] Data 0.001 (0.001) Batch 3.222 (3.222) Remain 04:02:41 loss: 0.1360 Lr: 0.01063
[2023-08-08 06:07:27,840 INFO misc.py line 115 22900] Train: [72/100][5/156] Data 0.001 (0.001) Batch 3.689 (3.455) Remain 04:20:14 loss: 0.3919 Lr: 0.01063
[2023-08-08 06:07:30,969 INFO misc.py line 115 22900] Train: [72/100][6/156] Data 0.001 (0.001) Batch 3.129 (3.347) Remain 04:12:00 loss: 0.2857 Lr: 0.01062
[2023-08-08 06:07:34,132 INFO misc.py line 115 22900] Train: [72/100][7/156] Data 0.001 (0.001) Batch 3.163 (3.301) Remain 04:08:28 loss: 0.3585 Lr: 0.01062
[2023-08-08 06:07:37,047 INFO misc.py line 115 22900] Train: [72/100][8/156] Data 0.001 (0.001) Batch 2.916 (3.224) Remain 04:02:38 loss: 0.0890 Lr: 0.01062
[2023-08-08 06:07:41,051 INFO misc.py line 115 22900] Train: [72/100][9/156] Data 0.001 (0.001) Batch 4.003 (3.354) Remain 04:12:21 loss: 0.2065 Lr: 0.01061
[2023-08-08 06:07:43,848 INFO misc.py line 115 22900] Train: [72/100][10/156] Data 0.001 (0.001) Batch 2.797 (3.274) Remain 04:06:19 loss: 0.1180 Lr: 0.01061
[2023-08-08 06:07:46,851 INFO misc.py line 115 22900] Train: [72/100][11/156] Data 0.001 (0.001) Batch 3.003 (3.240) Remain 04:03:43 loss: 0.2062 Lr: 0.01060
[2023-08-08 06:07:49,772 INFO misc.py line 115 22900] Train: [72/100][12/156] Data 0.001 (0.001) Batch 2.921 (3.205) Remain 04:01:00 loss: 0.0895 Lr: 0.01060
[2023-08-08 06:07:52,153 INFO misc.py line 115 22900] Train: [72/100][13/156] Data 0.001 (0.001) Batch 2.380 (3.122) Remain 03:54:44 loss: 0.1154 Lr: 0.01059
[2023-08-08 06:07:56,232 INFO misc.py line 115 22900] Train: [72/100][14/156] Data 0.001 (0.001) Batch 4.079 (3.209) Remain 04:01:14 loss: 0.2326 Lr: 0.01059
[2023-08-08 06:08:00,257 INFO misc.py line 115 22900] Train: [72/100][15/156] Data 0.001 (0.001) Batch 4.025 (3.277) Remain 04:06:17 loss: 0.1990 Lr: 0.01059
[2023-08-08 06:08:03,119 INFO misc.py line 115 22900] Train: [72/100][16/156] Data 0.001 (0.001) Batch 2.862 (3.245) Remain 04:03:50 loss: 0.1740 Lr: 0.01058
[2023-08-08 06:08:06,321 INFO misc.py line 115 22900] Train: [72/100][17/156] Data 0.001 (0.001) Batch 3.202 (3.242) Remain 04:03:32 loss: 0.1226 Lr: 0.01058
[2023-08-08 06:08:09,699 INFO misc.py line 115 22900] Train: [72/100][18/156] Data 0.001 (0.001) Batch 3.378 (3.251) Remain 04:04:10 loss: 0.1330 Lr: 0.01057
[2023-08-08 06:08:13,702 INFO misc.py line 115 22900] Train: [72/100][19/156] Data 0.001 (0.001) Batch 4.003 (3.298) Remain 04:07:38 loss: 0.1280 Lr: 0.01057
[2023-08-08 06:08:17,602 INFO misc.py line 115 22900] Train: [72/100][20/156] Data 0.001 (0.001) Batch 3.900 (3.334) Remain 04:10:14 loss: 0.1175 Lr: 0.01056
[2023-08-08 06:08:20,829 INFO misc.py line 115 22900] Train: [72/100][21/156] Data 0.001 (0.001) Batch 3.227 (3.328) Remain 04:09:45 loss: 0.1803 Lr: 0.01056
[2023-08-08 06:08:24,116 INFO misc.py line 115 22900] Train: [72/100][22/156] Data 0.001 (0.001) Batch 3.287 (3.326) Remain 04:09:31 loss: 0.1950 Lr: 0.01056
[2023-08-08 06:08:27,650 INFO misc.py line 115 22900] Train: [72/100][23/156] Data 0.001 (0.001) Batch 3.533 (3.336) Remain 04:10:15 loss: 0.1426 Lr: 0.01055
[2023-08-08 06:08:31,213 INFO misc.py line 115 22900] Train: [72/100][24/156] Data 0.001 (0.001) Batch 3.563 (3.347) Remain 04:11:00 loss: 0.2394 Lr: 0.01055
[2023-08-08 06:08:34,331 INFO misc.py line 115 22900] Train: [72/100][25/156] Data 0.001 (0.001) Batch 3.118 (3.336) Remain 04:10:10 loss: 0.0962 Lr: 0.01054
[2023-08-08 06:08:36,944 INFO misc.py line 115 22900] Train: [72/100][26/156] Data 0.001 (0.001) Batch 2.614 (3.305) Remain 04:07:45 loss: 0.1568 Lr: 0.01054
[2023-08-08 06:08:40,660 INFO misc.py line 115 22900] Train: [72/100][27/156] Data 0.001 (0.001) Batch 3.715 (3.322) Remain 04:08:59 loss: 0.1083 Lr: 0.01053
[2023-08-08 06:08:44,573 INFO misc.py line 115 22900] Train: [72/100][28/156] Data 0.001 (0.001) Batch 3.914 (3.346) Remain 04:10:42 loss: 0.2237 Lr: 0.01053
[2023-08-08 06:08:47,602 INFO misc.py line 115 22900] Train: [72/100][29/156] Data 0.001 (0.001) Batch 3.029 (3.334) Remain 04:09:44 loss: 0.0937 Lr: 0.01053
[2023-08-08 06:08:51,062 INFO misc.py line 115 22900] Train: [72/100][30/156] Data 0.001 (0.001) Batch 3.460 (3.338) Remain 04:10:02 loss: 0.3072 Lr: 0.01052
[2023-08-08 06:08:54,570 INFO misc.py line 115 22900] Train: [72/100][31/156] Data 0.001 (0.001) Batch 3.507 (3.344) Remain 04:10:25 loss: 0.1943 Lr: 0.01052
[2023-08-08 06:08:57,883 INFO misc.py line 115 22900] Train: [72/100][32/156] Data 0.001 (0.001) Batch 3.313 (3.343) Remain 04:10:17 loss: 0.0693 Lr: 0.01051
[2023-08-08 06:09:02,024 INFO misc.py line 115 22900] Train: [72/100][33/156] Data 0.001 (0.001) Batch 4.141 (3.370) Remain 04:12:13 loss: 0.1414 Lr: 0.01051
[2023-08-08 06:09:05,054 INFO misc.py line 115 22900] Train: [72/100][34/156] Data 0.001 (0.001) Batch 3.031 (3.359) Remain 04:11:21 loss: 0.1676 Lr: 0.01050
[2023-08-08 06:09:09,495 INFO misc.py line 115 22900] Train: [72/100][35/156] Data 0.001 (0.001) Batch 4.441 (3.393) Remain 04:13:49 loss: 0.1968 Lr: 0.01050
[2023-08-08 06:09:12,451 INFO misc.py line 115 22900] Train: [72/100][36/156] Data 0.001 (0.001) Batch 2.956 (3.379) Remain 04:12:47 loss: 0.0822 Lr: 0.01050
[2023-08-08 06:09:16,586 INFO misc.py line 115 22900] Train: [72/100][37/156] Data 0.001 (0.001) Batch 4.134 (3.402) Remain 04:14:23 loss: 0.1211 Lr: 0.01049
[2023-08-08 06:09:20,173 INFO misc.py line 115 22900] Train: [72/100][38/156] Data 0.001 (0.001) Batch 3.587 (3.407) Remain 04:14:43 loss: 0.1708 Lr: 0.01049
[2023-08-08 06:09:23,466 INFO misc.py line 115 22900] Train: [72/100][39/156] Data 0.001 (0.001) Batch 3.293 (3.404) Remain 04:14:26 loss: 0.2323 Lr: 0.01048
[2023-08-08 06:09:26,451 INFO misc.py line 115 22900] Train: [72/100][40/156] Data 0.001 (0.001) Batch 2.985 (3.392) Remain 04:13:31 loss: 0.2993 Lr: 0.01048
[2023-08-08 06:09:28,921 INFO misc.py line 115 22900] Train: [72/100][41/156] Data 0.001 (0.001) Batch 2.470 (3.368) Remain 04:11:39 loss: 0.0729 Lr: 0.01047
[2023-08-08 06:09:32,641 INFO misc.py line 115 22900] Train: [72/100][42/156] Data 0.001 (0.001) Batch 3.719 (3.377) Remain 04:12:16 loss: 0.1528 Lr: 0.01047
[2023-08-08 06:09:36,170 INFO misc.py line 115 22900] Train: [72/100][43/156] Data 0.001 (0.001) Batch 3.530 (3.381) Remain 04:12:30 loss: 0.1492 Lr: 0.01046
[2023-08-08 06:09:38,826 INFO misc.py line 115 22900] Train: [72/100][44/156] Data 0.001 (0.001) Batch 2.656 (3.363) Remain 04:11:07 loss: 0.0953 Lr: 0.01046
[2023-08-08 06:09:41,526 INFO misc.py line 115 22900] Train: [72/100][45/156] Data 0.001 (0.001) Batch 2.700 (3.348) Remain 04:09:53 loss: 0.1010 Lr: 0.01046
[2023-08-08 06:09:44,663 INFO misc.py line 115 22900] Train: [72/100][46/156] Data 0.001 (0.001) Batch 3.136 (3.343) Remain 04:09:28 loss: 0.1782 Lr: 0.01045
[2023-08-08 06:09:48,535 INFO misc.py line 115 22900] Train: [72/100][47/156] Data 0.001 (0.001) Batch 3.872 (3.355) Remain 04:10:18 loss: 0.3260 Lr: 0.01045
[2023-08-08 06:09:52,544 INFO misc.py line 115 22900] Train: [72/100][48/156] Data 0.001 (0.001) Batch 4.009 (3.369) Remain 04:11:20 loss: 0.2920 Lr: 0.01044
[2023-08-08 06:09:55,894 INFO misc.py line 115 22900] Train: [72/100][49/156] Data 0.001 (0.001) Batch 3.351 (3.369) Remain 04:11:15 loss: 0.1112 Lr: 0.01044
[2023-08-08 06:09:59,792 INFO misc.py line 115 22900] Train: [72/100][50/156] Data 0.001 (0.001) Batch 3.897 (3.380) Remain 04:12:02 loss: 0.1974 Lr: 0.01043
[2023-08-08 06:10:03,787 INFO misc.py line 115 22900] Train: [72/100][51/156] Data 0.001 (0.001) Batch 3.995 (3.393) Remain 04:12:56 loss: 0.2335 Lr: 0.01043
[2023-08-08 06:10:07,485 INFO misc.py line 115 22900] Train: [72/100][52/156] Data 0.001 (0.001) Batch 3.698 (3.399) Remain 04:13:20 loss: 0.2164 Lr: 0.01043
[2023-08-08 06:10:10,295 INFO misc.py line 115 22900] Train: [72/100][53/156] Data 0.001 (0.001) Batch 2.810 (3.387) Remain 04:12:24 loss: 0.0483 Lr: 0.01042
[2023-08-08 06:10:14,235 INFO misc.py line 115 22900] Train: [72/100][54/156] Data 0.001 (0.001) Batch 3.939 (3.398) Remain 04:13:09 loss: 0.2058 Lr: 0.01042
[2023-08-08 06:10:17,487 INFO misc.py line 115 22900] Train: [72/100][55/156] Data 0.001 (0.001) Batch 3.252 (3.395) Remain 04:12:53 loss: 0.0834 Lr: 0.01041
[2023-08-08 06:10:20,687 INFO misc.py line 115 22900] Train: [72/100][56/156] Data 0.001 (0.001) Batch 3.200 (3.392) Remain 04:12:33 loss: 0.2710 Lr: 0.01041
[2023-08-08 06:10:23,771 INFO misc.py line 115 22900] Train: [72/100][57/156] Data 0.001 (0.001) Batch 3.084 (3.386) Remain 04:12:05 loss: 0.0661 Lr: 0.01040
[2023-08-08 06:10:27,814 INFO misc.py line 115 22900] Train: [72/100][58/156] Data 0.001 (0.001) Batch 4.043 (3.398) Remain 04:12:55 loss: 0.2151 Lr: 0.01040
[2023-08-08 06:10:31,510 INFO misc.py line 115 22900] Train: [72/100][59/156] Data 0.001 (0.001) Batch 3.696 (3.403) Remain 04:13:15 loss: 0.1046 Lr: 0.01040
[2023-08-08 06:10:35,580 INFO misc.py line 115 22900] Train: [72/100][60/156] Data 0.001 (0.001) Batch 4.071 (3.415) Remain 04:14:04 loss: 0.1917 Lr: 0.01039
[2023-08-08 06:10:38,224 INFO misc.py line 115 22900] Train: [72/100][61/156] Data 0.001 (0.001) Batch 2.644 (3.402) Remain 04:13:01 loss: 0.1267 Lr: 0.01039
[2023-08-08 06:10:41,759 INFO misc.py line 115 22900] Train: [72/100][62/156] Data 0.001 (0.001) Batch 3.534 (3.404) Remain 04:13:08 loss: 0.1883 Lr: 0.01038
[2023-08-08 06:10:44,869 INFO misc.py line 115 22900] Train: [72/100][63/156] Data 0.001 (0.001) Batch 3.111 (3.399) Remain 04:12:42 loss: 0.1027 Lr: 0.01038
[2023-08-08 06:10:48,054 INFO misc.py line 115 22900] Train: [72/100][64/156] Data 0.001 (0.001) Batch 3.185 (3.395) Remain 04:12:23 loss: 0.1018 Lr: 0.01037
[2023-08-08 06:10:51,727 INFO misc.py line 115 22900] Train: [72/100][65/156] Data 0.001 (0.001) Batch 3.672 (3.400) Remain 04:12:40 loss: 0.4450 Lr: 0.01037
[2023-08-08 06:10:55,669 INFO misc.py line 115 22900] Train: [72/100][66/156] Data 0.001 (0.001) Batch 3.942 (3.409) Remain 04:13:15 loss: 0.1660 Lr: 0.01037
[2023-08-08 06:10:59,999 INFO misc.py line 115 22900] Train: [72/100][67/156] Data 0.001 (0.001) Batch 4.330 (3.423) Remain 04:14:16 loss: 0.3297 Lr: 0.01036
[2023-08-08 06:11:03,809 INFO misc.py line 115 22900] Train: [72/100][68/156] Data 0.001 (0.001) Batch 3.810 (3.429) Remain 04:14:39 loss: 0.2162 Lr: 0.01036
[2023-08-08 06:11:07,913 INFO misc.py line 115 22900] Train: [72/100][69/156] Data 0.001 (0.001) Batch 4.104 (3.439) Remain 04:15:21 loss: 0.2219 Lr: 0.01035
[2023-08-08 06:11:11,697 INFO misc.py line 115 22900] Train: [72/100][70/156] Data 0.001 (0.001) Batch 3.785 (3.444) Remain 04:15:40 loss: 0.2145 Lr: 0.01035
[2023-08-08 06:11:15,509 INFO misc.py line 115 22900] Train: [72/100][71/156] Data 0.001 (0.001) Batch 3.811 (3.450) Remain 04:16:01 loss: 0.1410 Lr: 0.01034
[2023-08-08 06:11:19,518 INFO misc.py line 115 22900] Train: [72/100][72/156] Data 0.001 (0.001) Batch 4.009 (3.458) Remain 04:16:34 loss: 0.1618 Lr: 0.01034
[2023-08-08 06:11:23,667 INFO misc.py line 115 22900] Train: [72/100][73/156] Data 0.001 (0.001) Batch 4.148 (3.468) Remain 04:17:14 loss: 0.2098 Lr: 0.01034
[2023-08-08 06:11:26,753 INFO misc.py line 115 22900] Train: [72/100][74/156] Data 0.001 (0.001) Batch 3.086 (3.462) Remain 04:16:47 loss: 0.1460 Lr: 0.01033
[2023-08-08 06:11:29,499 INFO misc.py line 115 22900] Train: [72/100][75/156] Data 0.001 (0.001) Batch 2.746 (3.452) Remain 04:15:59 loss: 0.0542 Lr: 0.01033
[2023-08-08 06:11:31,273 INFO misc.py line 115 22900] Train: [72/100][76/156] Data 0.001 (0.001) Batch 1.774 (3.429) Remain 04:14:13 loss: 0.0382 Lr: 0.01032
[2023-08-08 06:11:35,264 INFO misc.py line 115 22900] Train: [72/100][77/156] Data 0.001 (0.001) Batch 3.991 (3.437) Remain 04:14:44 loss: 0.1410 Lr: 0.01032
[2023-08-08 06:11:38,351 INFO misc.py line 115 22900] Train: [72/100][78/156] Data 0.001 (0.001) Batch 3.087 (3.432) Remain 04:14:19 loss: 0.2289 Lr: 0.01031
[2023-08-08 06:11:41,554 INFO misc.py line 115 22900] Train: [72/100][79/156] Data 0.001 (0.001) Batch 3.204 (3.429) Remain 04:14:03 loss: 0.1151 Lr: 0.01031
[2023-08-08 06:11:45,563 INFO misc.py line 115 22900] Train: [72/100][80/156] Data 0.001 (0.001) Batch 4.009 (3.437) Remain 04:14:33 loss: 0.1302 Lr: 0.01031
[2023-08-08 06:11:48,513 INFO misc.py line 115 22900] Train: [72/100][81/156] Data 0.001 (0.001) Batch 2.950 (3.431) Remain 04:14:02 loss: 0.0906 Lr: 0.01030
[2023-08-08 06:11:51,863 INFO misc.py line 115 22900] Train: [72/100][82/156] Data 0.001 (0.001) Batch 3.350 (3.430) Remain 04:13:54 loss: 0.1095 Lr: 0.01030
[2023-08-08 06:11:54,976 INFO misc.py line 115 22900] Train: [72/100][83/156] Data 0.001 (0.001) Batch 3.113 (3.426) Remain 04:13:33 loss: 0.1352 Lr: 0.01029
[2023-08-08 06:11:58,131 INFO misc.py line 115 22900] Train: [72/100][84/156] Data 0.001 (0.001) Batch 3.155 (3.422) Remain 04:13:14 loss: 0.1363 Lr: 0.01029
[2023-08-08 06:12:01,286 INFO misc.py line 115 22900] Train: [72/100][85/156] Data 0.001 (0.001) Batch 3.155 (3.419) Remain 04:12:56 loss: 0.1192 Lr: 0.01028
[2023-08-08 06:12:05,378 INFO misc.py line 115 22900] Train: [72/100][86/156] Data 0.001 (0.001) Batch 4.093 (3.427) Remain 04:13:29 loss: 0.1394 Lr: 0.01028
[2023-08-08 06:12:08,654 INFO misc.py line 115 22900] Train: [72/100][87/156] Data 0.001 (0.001) Batch 3.276 (3.425) Remain 04:13:18 loss: 0.2270 Lr: 0.01028
[2023-08-08 06:12:12,059 INFO misc.py line 115 22900] Train: [72/100][88/156] Data 0.001 (0.001) Batch 3.405 (3.425) Remain 04:13:13 loss: 0.1744 Lr: 0.01027
[2023-08-08 06:12:16,607 INFO misc.py line 115 22900] Train: [72/100][89/156] Data 0.001 (0.001) Batch 4.548 (3.438) Remain 04:14:08 loss: 0.5559 Lr: 0.01027
[2023-08-08 06:12:21,207 INFO misc.py line 115 22900] Train: [72/100][90/156] Data 0.001 (0.001) Batch 4.600 (3.451) Remain 04:15:03 loss: 0.2490 Lr: 0.01026
[2023-08-08 06:12:24,819 INFO misc.py line 115 22900] Train: [72/100][91/156] Data 0.001 (0.001) Batch 3.612 (3.453) Remain 04:15:08 loss: 0.2110 Lr: 0.01026
[2023-08-08 06:12:28,485 INFO misc.py line 115 22900] Train: [72/100][92/156] Data 0.001 (0.001) Batch 3.666 (3.456) Remain 04:15:15 loss: 0.1332 Lr: 0.01025
[2023-08-08 06:12:31,948 INFO misc.py line 115 22900] Train: [72/100][93/156] Data 0.001 (0.001) Batch 3.463 (3.456) Remain 04:15:12 loss: 0.4253 Lr: 0.01025
[2023-08-08 06:12:35,874 INFO misc.py line 115 22900] Train: [72/100][94/156] Data 0.001 (0.001) Batch 3.926 (3.461) Remain 04:15:31 loss: 0.1173 Lr: 0.01025
[2023-08-08 06:12:38,459 INFO misc.py line 115 22900] Train: [72/100][95/156] Data 0.001 (0.001) Batch 2.585 (3.451) Remain 04:14:46 loss: 0.2225 Lr: 0.01024
[2023-08-08 06:12:41,527 INFO misc.py line 115 22900] Train: [72/100][96/156] Data 0.001 (0.001) Batch 3.068 (3.447) Remain 04:14:24 loss: 0.1023 Lr: 0.01024
[2023-08-08 06:12:45,107 INFO misc.py line 115 22900] Train: [72/100][97/156] Data 0.001 (0.001) Batch 3.579 (3.449) Remain 04:14:27 loss: 0.2210 Lr: 0.01023
[2023-08-08 06:12:47,379 INFO misc.py line 115 22900] Train: [72/100][98/156] Data 0.001 (0.001) Batch 2.272 (3.436) Remain 04:13:29 loss: 0.1540 Lr: 0.01023
[2023-08-08 06:12:50,763 INFO misc.py line 115 22900] Train: [72/100][99/156] Data 0.001 (0.001) Batch 3.384 (3.436) Remain 04:13:23 loss: 0.1891 Lr: 0.01022
[2023-08-08 06:12:53,597 INFO misc.py line 115 22900] Train: [72/100][100/156] Data 0.001 (0.001) Batch 2.834 (3.430) Remain 04:12:52 loss: 0.0920 Lr: 0.01022
[2023-08-08 06:12:57,286 INFO misc.py line 115 22900] Train: [72/100][101/156] Data 0.001 (0.001) Batch 3.690 (3.432) Remain 04:13:00 loss: 0.1493 Lr: 0.01022
[2023-08-08 06:12:59,816 INFO misc.py line 115 22900] Train: [72/100][102/156] Data 0.001 (0.001) Batch 2.530 (3.423) Remain 04:12:16 loss: 0.1790 Lr: 0.01021
[2023-08-08 06:13:03,734 INFO misc.py line 115 22900] Train: [72/100][103/156] Data 0.001 (0.001) Batch 3.918 (3.428) Remain 04:12:35 loss: 0.2126 Lr: 0.01021
[2023-08-08 06:13:07,886 INFO misc.py line 115 22900] Train: [72/100][104/156] Data 0.001 (0.001) Batch 4.152 (3.435) Remain 04:13:03 loss: 0.1306 Lr: 0.01020
[2023-08-08 06:13:12,266 INFO misc.py line 115 22900] Train: [72/100][105/156] Data 0.001 (0.001) Batch 4.379 (3.444) Remain 04:13:41 loss: 0.2980 Lr: 0.01020
[2023-08-08 06:13:15,210 INFO misc.py line 115 22900] Train: [72/100][106/156] Data 0.001 (0.001) Batch 2.944 (3.440) Remain 04:13:16 loss: 0.0624 Lr: 0.01019
[2023-08-08 06:13:18,303 INFO misc.py line 115 22900] Train: [72/100][107/156] Data 0.001 (0.001) Batch 3.093 (3.436) Remain 04:12:58 loss: 0.1456 Lr: 0.01019
[2023-08-08 06:13:21,670 INFO misc.py line 115 22900] Train: [72/100][108/156] Data 0.001 (0.001) Batch 3.367 (3.436) Remain 04:12:51 loss: 0.2118 Lr: 0.01019
[2023-08-08 06:13:24,757 INFO misc.py line 115 22900] Train: [72/100][109/156] Data 0.001 (0.001) Batch 3.087 (3.432) Remain 04:12:33 loss: 0.1050 Lr: 0.01018
[2023-08-08 06:13:27,670 INFO misc.py line 115 22900] Train: [72/100][110/156] Data 0.001 (0.001) Batch 2.913 (3.427) Remain 04:12:08 loss: 0.1700 Lr: 0.01018
[2023-08-08 06:13:32,072 INFO misc.py line 115 22900] Train: [72/100][111/156] Data 0.001 (0.001) Batch 4.402 (3.437) Remain 04:12:45 loss: 0.2637 Lr: 0.01017
[2023-08-08 06:13:34,737 INFO misc.py line 115 22900] Train: [72/100][112/156] Data 0.001 (0.001) Batch 2.665 (3.429) Remain 04:12:10 loss: 0.1072 Lr: 0.01017
[2023-08-08 06:13:39,046 INFO misc.py line 115 22900] Train: [72/100][113/156] Data 0.001 (0.001) Batch 4.309 (3.437) Remain 04:12:42 loss: 0.2072 Lr: 0.01016
[2023-08-08 06:13:41,781 INFO misc.py line 115 22900] Train: [72/100][114/156] Data 0.001 (0.001) Batch 2.735 (3.431) Remain 04:12:11 loss: 0.1020 Lr: 0.01016
[2023-08-08 06:13:45,838 INFO misc.py line 115 22900] Train: [72/100][115/156] Data 0.001 (0.001) Batch 4.058 (3.437) Remain 04:12:32 loss: 0.2814 Lr: 0.01016
[2023-08-08 06:13:48,322 INFO misc.py line 115 22900] Train: [72/100][116/156] Data 0.001 (0.001) Batch 2.483 (3.428) Remain 04:11:51 loss: 0.0627 Lr: 0.01015
[2023-08-08 06:13:51,081 INFO misc.py line 115 22900] Train: [72/100][117/156] Data 0.001 (0.001) Batch 2.759 (3.422) Remain 04:11:22 loss: 0.1147 Lr: 0.01015
[2023-08-08 06:13:55,211 INFO misc.py line 115 22900] Train: [72/100][118/156] Data 0.001 (0.001) Batch 4.131 (3.429) Remain 04:11:46 loss: 0.3652 Lr: 0.01014
[2023-08-08 06:13:59,086 INFO misc.py line 115 22900] Train: [72/100][119/156] Data 0.001 (0.001) Batch 3.874 (3.432) Remain 04:11:59 loss: 0.2263 Lr: 0.01014
[2023-08-08 06:14:01,169 INFO misc.py line 115 22900] Train: [72/100][120/156] Data 0.001 (0.001) Batch 2.084 (3.421) Remain 04:11:05 loss: 0.1161 Lr: 0.01013
[2023-08-08 06:14:05,262 INFO misc.py line 115 22900] Train: [72/100][121/156] Data 0.001 (0.001) Batch 4.093 (3.427) Remain 04:11:27 loss: 0.2750 Lr: 0.01013
[2023-08-08 06:14:08,056 INFO misc.py line 115 22900] Train: [72/100][122/156] Data 0.001 (0.001) Batch 2.793 (3.421) Remain 04:11:00 loss: 0.1129 Lr: 0.01013
[2023-08-08 06:14:11,865 INFO misc.py line 115 22900] Train: [72/100][123/156] Data 0.001 (0.001) Batch 3.809 (3.424) Remain 04:11:11 loss: 0.2171 Lr: 0.01012
[2023-08-08 06:14:15,411 INFO misc.py line 115 22900] Train: [72/100][124/156] Data 0.001 (0.001) Batch 3.546 (3.425) Remain 04:11:12 loss: 0.1496 Lr: 0.01012
[2023-08-08 06:14:18,362 INFO misc.py line 115 22900] Train: [72/100][125/156] Data 0.001 (0.001) Batch 2.951 (3.422) Remain 04:10:51 loss: 0.1960 Lr: 0.01011
[2023-08-08 06:14:22,227 INFO misc.py line 115 22900] Train: [72/100][126/156] Data 0.001 (0.001) Batch 3.865 (3.425) Remain 04:11:03 loss: 0.3431 Lr: 0.01011
[2023-08-08 06:14:25,548 INFO misc.py line 115 22900] Train: [72/100][127/156] Data 0.001 (0.001) Batch 3.321 (3.424) Remain 04:10:56 loss: 0.0899 Lr: 0.01010
[2023-08-08 06:14:29,825 INFO misc.py line 115 22900] Train: [72/100][128/156] Data 0.001 (0.001) Batch 4.276 (3.431) Remain 04:11:23 loss: 0.3599 Lr: 0.01010
[2023-08-08 06:14:33,759 INFO misc.py line 115 22900] Train: [72/100][129/156] Data 0.001 (0.001) Batch 3.934 (3.435) Remain 04:11:37 loss: 0.1303 Lr: 0.01010
[2023-08-08 06:14:37,077 INFO misc.py line 115 22900] Train: [72/100][130/156] Data 0.001 (0.001) Batch 3.318 (3.434) Remain 04:11:30 loss: 0.1172 Lr: 0.01009
